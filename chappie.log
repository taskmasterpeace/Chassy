ERROR:root:An error occurred:
Traceback (most recent call last):
  File "D:\github\chappymedium\chappie.py", line 262, in <module>
    window = ChappieGUI()
  File "D:\github\chappymedium\chappie.py", line 120, in __init__
    self.setup_ui()
  File "D:\github\chappymedium\chappie.py", line 155, in setup_ui
    self.play_pause_button.clicked.connect(self.toggle_play_pause)
AttributeError: 'ChappieGUI' object has no attribute 'toggle_play_pause'
DEBUG:numba.core.byteflow:bytecode dump:
>          0	NOP(arg=None, lineno=1141)
           2	LOAD_FAST(arg=0, lineno=1144)
           4	LOAD_CONST(arg=1, lineno=1144)
           6	BINARY_SUBSCR(arg=None, lineno=1144)
           8	STORE_FAST(arg=3, lineno=1144)
          10	LOAD_FAST(arg=1, lineno=1145)
          12	UNARY_NEGATIVE(arg=None, lineno=1145)
          14	LOAD_FAST(arg=3, lineno=1145)
          16	DUP_TOP(arg=None, lineno=1145)
          18	ROT_THREE(arg=None, lineno=1145)
          20	COMPARE_OP(arg=1, lineno=1145)
          22	POP_JUMP_IF_FALSE(arg=32, lineno=1145)
          24	LOAD_FAST(arg=1, lineno=1145)
          26	COMPARE_OP(arg=1, lineno=1145)
          28	POP_JUMP_IF_FALSE(arg=40, lineno=1145)
          30	JUMP_FORWARD(arg=4, lineno=1145)
>         32	POP_TOP(arg=None, lineno=1145)
          34	JUMP_FORWARD(arg=4, lineno=1145)
>         36	LOAD_CONST(arg=1, lineno=1146)
          38	STORE_FAST(arg=3, lineno=1146)
>         40	LOAD_FAST(arg=0, lineno=1148)
          42	LOAD_CONST(arg=2, lineno=1148)
          44	BINARY_SUBSCR(arg=None, lineno=1148)
          46	STORE_FAST(arg=4, lineno=1148)
          48	LOAD_FAST(arg=1, lineno=1149)
          50	UNARY_NEGATIVE(arg=None, lineno=1149)
          52	LOAD_FAST(arg=4, lineno=1149)
          54	DUP_TOP(arg=None, lineno=1149)
          56	ROT_THREE(arg=None, lineno=1149)
          58	COMPARE_OP(arg=1, lineno=1149)
          60	POP_JUMP_IF_FALSE(arg=70, lineno=1149)
          62	LOAD_FAST(arg=1, lineno=1149)
          64	COMPARE_OP(arg=1, lineno=1149)
          66	POP_JUMP_IF_FALSE(arg=78, lineno=1149)
          68	JUMP_FORWARD(arg=4, lineno=1149)
>         70	POP_TOP(arg=None, lineno=1149)
          72	JUMP_FORWARD(arg=4, lineno=1149)
>         74	LOAD_CONST(arg=1, lineno=1150)
          76	STORE_FAST(arg=4, lineno=1150)
>         78	LOAD_FAST(arg=2, lineno=1152)
          80	POP_JUMP_IF_FALSE(arg=102, lineno=1152)
          82	LOAD_GLOBAL(arg=0, lineno=1153)
          84	LOAD_METHOD(arg=1, lineno=1153)
          86	LOAD_FAST(arg=3, lineno=1153)
          88	CALL_METHOD(arg=1, lineno=1153)
          90	LOAD_GLOBAL(arg=0, lineno=1153)
          92	LOAD_METHOD(arg=1, lineno=1153)
          94	LOAD_FAST(arg=4, lineno=1153)
          96	CALL_METHOD(arg=1, lineno=1153)
          98	COMPARE_OP(arg=3, lineno=1153)
         100	RETURN_VALUE(arg=None, lineno=1153)
>        102	LOAD_GLOBAL(arg=0, lineno=1155)
         104	LOAD_METHOD(arg=2, lineno=1155)
         106	LOAD_FAST(arg=3, lineno=1155)
         108	CALL_METHOD(arg=1, lineno=1155)
         110	LOAD_GLOBAL(arg=0, lineno=1155)
         112	LOAD_METHOD(arg=2, lineno=1155)
         114	LOAD_FAST(arg=4, lineno=1155)
         116	CALL_METHOD(arg=1, lineno=1155)
         118	COMPARE_OP(arg=3, lineno=1155)
         120	RETURN_VALUE(arg=None, lineno=1155)
         122	LOAD_CONST(arg=3, lineno=1155)
         124	RETURN_VALUE(arg=None, lineno=1155)
DEBUG:numba.core.byteflow:pending: deque([State(pc_initial=0 nstack_initial=0)])
DEBUG:numba.core.byteflow:stack: []
DEBUG:numba.core.byteflow:state.pc_initial: State(pc_initial=0 nstack_initial=0)
DEBUG:numba.core.byteflow:dispatch pc=0, inst=NOP(arg=None, lineno=1141)
DEBUG:numba.core.byteflow:stack []
DEBUG:numba.core.byteflow:dispatch pc=2, inst=LOAD_FAST(arg=0, lineno=1144)
DEBUG:numba.core.byteflow:stack []
DEBUG:numba.core.byteflow:dispatch pc=4, inst=LOAD_CONST(arg=1, lineno=1144)
DEBUG:numba.core.byteflow:stack ['$x2.0']
DEBUG:numba.core.byteflow:dispatch pc=6, inst=BINARY_SUBSCR(arg=None, lineno=1144)
DEBUG:numba.core.byteflow:stack ['$x2.0', '$const4.1']
DEBUG:numba.core.byteflow:dispatch pc=8, inst=STORE_FAST(arg=3, lineno=1144)
DEBUG:numba.core.byteflow:stack ['$6binary_subscr.2']
DEBUG:numba.core.byteflow:dispatch pc=10, inst=LOAD_FAST(arg=1, lineno=1145)
DEBUG:numba.core.byteflow:stack []
DEBUG:numba.core.byteflow:dispatch pc=12, inst=UNARY_NEGATIVE(arg=None, lineno=1145)
DEBUG:numba.core.byteflow:stack ['$threshold10.3']
DEBUG:numba.core.byteflow:dispatch pc=14, inst=LOAD_FAST(arg=3, lineno=1145)
DEBUG:numba.core.byteflow:stack ['$12unary_negative.4']
DEBUG:numba.core.byteflow:dispatch pc=16, inst=DUP_TOP(arg=None, lineno=1145)
DEBUG:numba.core.byteflow:stack ['$12unary_negative.4', '$x014.5']
DEBUG:numba.core.byteflow:dispatch pc=18, inst=ROT_THREE(arg=None, lineno=1145)
DEBUG:numba.core.byteflow:stack ['$12unary_negative.4', '$x014.5', '$16dup_top.6']
DEBUG:numba.core.byteflow:dispatch pc=20, inst=COMPARE_OP(arg=1, lineno=1145)
DEBUG:numba.core.byteflow:stack ['$16dup_top.6', '$12unary_negative.4', '$x014.5']
DEBUG:numba.core.byteflow:dispatch pc=22, inst=POP_JUMP_IF_FALSE(arg=32, lineno=1145)
DEBUG:numba.core.byteflow:stack ['$16dup_top.6', '$20compare_op.7']
DEBUG:numba.core.byteflow:end state. edges=[Edge(pc=24, stack=('$16dup_top.6',), blockstack=(), npush=0), Edge(pc=32, stack=('$16dup_top.6',), blockstack=(), npush=0)]
DEBUG:numba.core.byteflow:pending: deque([State(pc_initial=24 nstack_initial=1), State(pc_initial=32 nstack_initial=1)])
DEBUG:numba.core.byteflow:stack: ['$phi24.0']
DEBUG:numba.core.byteflow:state.pc_initial: State(pc_initial=24 nstack_initial=1)
DEBUG:numba.core.byteflow:dispatch pc=24, inst=LOAD_FAST(arg=1, lineno=1145)
DEBUG:numba.core.byteflow:stack ['$phi24.0']
DEBUG:numba.core.byteflow:dispatch pc=26, inst=COMPARE_OP(arg=1, lineno=1145)
DEBUG:numba.core.byteflow:stack ['$phi24.0', '$threshold24.1']
DEBUG:numba.core.byteflow:dispatch pc=28, inst=POP_JUMP_IF_FALSE(arg=40, lineno=1145)
DEBUG:numba.core.byteflow:stack ['$26compare_op.2']
DEBUG:numba.core.byteflow:end state. edges=[Edge(pc=30, stack=(), blockstack=(), npush=0), Edge(pc=40, stack=(), blockstack=(), npush=0)]
DEBUG:numba.core.byteflow:pending: deque([State(pc_initial=32 nstack_initial=1), State(pc_initial=30 nstack_initial=0), State(pc_initial=40 nstack_initial=0)])
DEBUG:numba.core.byteflow:stack: ['$phi32.0']
DEBUG:numba.core.byteflow:state.pc_initial: State(pc_initial=32 nstack_initial=1)
DEBUG:numba.core.byteflow:dispatch pc=32, inst=POP_TOP(arg=None, lineno=1145)
DEBUG:numba.core.byteflow:stack ['$phi32.0']
DEBUG:numba.core.byteflow:dispatch pc=34, inst=JUMP_FORWARD(arg=4, lineno=1145)
DEBUG:numba.core.byteflow:stack []
DEBUG:numba.core.byteflow:end state. edges=[Edge(pc=40, stack=(), blockstack=(), npush=0)]
DEBUG:numba.core.byteflow:pending: deque([State(pc_initial=30 nstack_initial=0), State(pc_initial=40 nstack_initial=0), State(pc_initial=40 nstack_initial=0)])
DEBUG:numba.core.byteflow:stack: []
DEBUG:numba.core.byteflow:state.pc_initial: State(pc_initial=30 nstack_initial=0)
DEBUG:numba.core.byteflow:dispatch pc=30, inst=JUMP_FORWARD(arg=4, lineno=1145)
DEBUG:numba.core.byteflow:stack []
DEBUG:numba.core.byteflow:end state. edges=[Edge(pc=36, stack=(), blockstack=(), npush=0)]
DEBUG:numba.core.byteflow:pending: deque([State(pc_initial=40 nstack_initial=0), State(pc_initial=40 nstack_initial=0), State(pc_initial=36 nstack_initial=0)])
DEBUG:numba.core.byteflow:stack: []
DEBUG:numba.core.byteflow:state.pc_initial: State(pc_initial=40 nstack_initial=0)
DEBUG:numba.core.byteflow:dispatch pc=40, inst=LOAD_FAST(arg=0, lineno=1148)
DEBUG:numba.core.byteflow:stack []
DEBUG:numba.core.byteflow:dispatch pc=42, inst=LOAD_CONST(arg=2, lineno=1148)
DEBUG:numba.core.byteflow:stack ['$x40.0']
DEBUG:numba.core.byteflow:dispatch pc=44, inst=BINARY_SUBSCR(arg=None, lineno=1148)
DEBUG:numba.core.byteflow:stack ['$x40.0', '$const42.1']
DEBUG:numba.core.byteflow:dispatch pc=46, inst=STORE_FAST(arg=4, lineno=1148)
DEBUG:numba.core.byteflow:stack ['$44binary_subscr.2']
DEBUG:numba.core.byteflow:dispatch pc=48, inst=LOAD_FAST(arg=1, lineno=1149)
DEBUG:numba.core.byteflow:stack []
DEBUG:numba.core.byteflow:dispatch pc=50, inst=UNARY_NEGATIVE(arg=None, lineno=1149)
DEBUG:numba.core.byteflow:stack ['$threshold48.3']
DEBUG:numba.core.byteflow:dispatch pc=52, inst=LOAD_FAST(arg=4, lineno=1149)
DEBUG:numba.core.byteflow:stack ['$50unary_negative.4']
DEBUG:numba.core.byteflow:dispatch pc=54, inst=DUP_TOP(arg=None, lineno=1149)
DEBUG:numba.core.byteflow:stack ['$50unary_negative.4', '$x152.5']
DEBUG:numba.core.byteflow:dispatch pc=56, inst=ROT_THREE(arg=None, lineno=1149)
DEBUG:numba.core.byteflow:stack ['$50unary_negative.4', '$x152.5', '$54dup_top.6']
DEBUG:numba.core.byteflow:dispatch pc=58, inst=COMPARE_OP(arg=1, lineno=1149)
DEBUG:numba.core.byteflow:stack ['$54dup_top.6', '$50unary_negative.4', '$x152.5']
DEBUG:numba.core.byteflow:dispatch pc=60, inst=POP_JUMP_IF_FALSE(arg=70, lineno=1149)
DEBUG:numba.core.byteflow:stack ['$54dup_top.6', '$58compare_op.7']
DEBUG:numba.core.byteflow:end state. edges=[Edge(pc=62, stack=('$54dup_top.6',), blockstack=(), npush=0), Edge(pc=70, stack=('$54dup_top.6',), blockstack=(), npush=0)]
DEBUG:numba.core.byteflow:pending: deque([State(pc_initial=40 nstack_initial=0), State(pc_initial=36 nstack_initial=0), State(pc_initial=62 nstack_initial=1), State(pc_initial=70 nstack_initial=1)])
DEBUG:numba.core.byteflow:pending: deque([State(pc_initial=36 nstack_initial=0), State(pc_initial=62 nstack_initial=1), State(pc_initial=70 nstack_initial=1)])
DEBUG:numba.core.byteflow:stack: []
DEBUG:numba.core.byteflow:state.pc_initial: State(pc_initial=36 nstack_initial=0)
DEBUG:numba.core.byteflow:dispatch pc=36, inst=LOAD_CONST(arg=1, lineno=1146)
DEBUG:numba.core.byteflow:stack []
DEBUG:numba.core.byteflow:dispatch pc=38, inst=STORE_FAST(arg=3, lineno=1146)
DEBUG:numba.core.byteflow:stack ['$const36.0']
DEBUG:numba.core.byteflow:end state. edges=[Edge(pc=40, stack=(), blockstack=(), npush=0)]
DEBUG:numba.core.byteflow:pending: deque([State(pc_initial=62 nstack_initial=1), State(pc_initial=70 nstack_initial=1), State(pc_initial=40 nstack_initial=0)])
DEBUG:numba.core.byteflow:stack: ['$phi62.0']
DEBUG:numba.core.byteflow:state.pc_initial: State(pc_initial=62 nstack_initial=1)
DEBUG:numba.core.byteflow:dispatch pc=62, inst=LOAD_FAST(arg=1, lineno=1149)
DEBUG:numba.core.byteflow:stack ['$phi62.0']
DEBUG:numba.core.byteflow:dispatch pc=64, inst=COMPARE_OP(arg=1, lineno=1149)
DEBUG:numba.core.byteflow:stack ['$phi62.0', '$threshold62.1']
DEBUG:numba.core.byteflow:dispatch pc=66, inst=POP_JUMP_IF_FALSE(arg=78, lineno=1149)
DEBUG:numba.core.byteflow:stack ['$64compare_op.2']
DEBUG:numba.core.byteflow:end state. edges=[Edge(pc=68, stack=(), blockstack=(), npush=0), Edge(pc=78, stack=(), blockstack=(), npush=0)]
DEBUG:numba.core.byteflow:pending: deque([State(pc_initial=70 nstack_initial=1), State(pc_initial=40 nstack_initial=0), State(pc_initial=68 nstack_initial=0), State(pc_initial=78 nstack_initial=0)])
DEBUG:numba.core.byteflow:stack: ['$phi70.0']
DEBUG:numba.core.byteflow:state.pc_initial: State(pc_initial=70 nstack_initial=1)
DEBUG:numba.core.byteflow:dispatch pc=70, inst=POP_TOP(arg=None, lineno=1149)
DEBUG:numba.core.byteflow:stack ['$phi70.0']
DEBUG:numba.core.byteflow:dispatch pc=72, inst=JUMP_FORWARD(arg=4, lineno=1149)
DEBUG:numba.core.byteflow:stack []
DEBUG:numba.core.byteflow:end state. edges=[Edge(pc=78, stack=(), blockstack=(), npush=0)]
DEBUG:numba.core.byteflow:pending: deque([State(pc_initial=40 nstack_initial=0), State(pc_initial=68 nstack_initial=0), State(pc_initial=78 nstack_initial=0), State(pc_initial=78 nstack_initial=0)])
DEBUG:numba.core.byteflow:pending: deque([State(pc_initial=68 nstack_initial=0), State(pc_initial=78 nstack_initial=0), State(pc_initial=78 nstack_initial=0)])
DEBUG:numba.core.byteflow:stack: []
DEBUG:numba.core.byteflow:state.pc_initial: State(pc_initial=68 nstack_initial=0)
DEBUG:numba.core.byteflow:dispatch pc=68, inst=JUMP_FORWARD(arg=4, lineno=1149)
DEBUG:numba.core.byteflow:stack []
DEBUG:numba.core.byteflow:end state. edges=[Edge(pc=74, stack=(), blockstack=(), npush=0)]
DEBUG:numba.core.byteflow:pending: deque([State(pc_initial=78 nstack_initial=0), State(pc_initial=78 nstack_initial=0), State(pc_initial=74 nstack_initial=0)])
DEBUG:numba.core.byteflow:stack: []
DEBUG:numba.core.byteflow:state.pc_initial: State(pc_initial=78 nstack_initial=0)
DEBUG:numba.core.byteflow:dispatch pc=78, inst=LOAD_FAST(arg=2, lineno=1152)
DEBUG:numba.core.byteflow:stack []
DEBUG:numba.core.byteflow:dispatch pc=80, inst=POP_JUMP_IF_FALSE(arg=102, lineno=1152)
DEBUG:numba.core.byteflow:stack ['$zero_pos78.0']
DEBUG:numba.core.byteflow:end state. edges=[Edge(pc=82, stack=(), blockstack=(), npush=0), Edge(pc=102, stack=(), blockstack=(), npush=0)]
DEBUG:numba.core.byteflow:pending: deque([State(pc_initial=78 nstack_initial=0), State(pc_initial=74 nstack_initial=0), State(pc_initial=82 nstack_initial=0), State(pc_initial=102 nstack_initial=0)])
DEBUG:numba.core.byteflow:pending: deque([State(pc_initial=74 nstack_initial=0), State(pc_initial=82 nstack_initial=0), State(pc_initial=102 nstack_initial=0)])
DEBUG:numba.core.byteflow:stack: []
DEBUG:numba.core.byteflow:state.pc_initial: State(pc_initial=74 nstack_initial=0)
DEBUG:numba.core.byteflow:dispatch pc=74, inst=LOAD_CONST(arg=1, lineno=1150)
DEBUG:numba.core.byteflow:stack []
DEBUG:numba.core.byteflow:dispatch pc=76, inst=STORE_FAST(arg=4, lineno=1150)
DEBUG:numba.core.byteflow:stack ['$const74.0']
DEBUG:numba.core.byteflow:end state. edges=[Edge(pc=78, stack=(), blockstack=(), npush=0)]
DEBUG:numba.core.byteflow:pending: deque([State(pc_initial=82 nstack_initial=0), State(pc_initial=102 nstack_initial=0), State(pc_initial=78 nstack_initial=0)])
DEBUG:numba.core.byteflow:stack: []
DEBUG:numba.core.byteflow:state.pc_initial: State(pc_initial=82 nstack_initial=0)
DEBUG:numba.core.byteflow:dispatch pc=82, inst=LOAD_GLOBAL(arg=0, lineno=1153)
DEBUG:numba.core.byteflow:stack []
DEBUG:numba.core.byteflow:dispatch pc=84, inst=LOAD_METHOD(arg=1, lineno=1153)
DEBUG:numba.core.byteflow:stack ['$82load_global.0']
DEBUG:numba.core.byteflow:dispatch pc=86, inst=LOAD_FAST(arg=3, lineno=1153)
DEBUG:numba.core.byteflow:stack ['$84load_method.1']
DEBUG:numba.core.byteflow:dispatch pc=88, inst=CALL_METHOD(arg=1, lineno=1153)
DEBUG:numba.core.byteflow:stack ['$84load_method.1', '$x086.2']
DEBUG:numba.core.byteflow:dispatch pc=90, inst=LOAD_GLOBAL(arg=0, lineno=1153)
DEBUG:numba.core.byteflow:stack ['$88call_method.3']
DEBUG:numba.core.byteflow:dispatch pc=92, inst=LOAD_METHOD(arg=1, lineno=1153)
DEBUG:numba.core.byteflow:stack ['$88call_method.3', '$90load_global.4']
DEBUG:numba.core.byteflow:dispatch pc=94, inst=LOAD_FAST(arg=4, lineno=1153)
DEBUG:numba.core.byteflow:stack ['$88call_method.3', '$92load_method.5']
DEBUG:numba.core.byteflow:dispatch pc=96, inst=CALL_METHOD(arg=1, lineno=1153)
DEBUG:numba.core.byteflow:stack ['$88call_method.3', '$92load_method.5', '$x194.6']
DEBUG:numba.core.byteflow:dispatch pc=98, inst=COMPARE_OP(arg=3, lineno=1153)
DEBUG:numba.core.byteflow:stack ['$88call_method.3', '$96call_method.7']
DEBUG:numba.core.byteflow:dispatch pc=100, inst=RETURN_VALUE(arg=None, lineno=1153)
DEBUG:numba.core.byteflow:stack ['$98compare_op.8']
DEBUG:numba.core.byteflow:end state. edges=[]
DEBUG:numba.core.byteflow:pending: deque([State(pc_initial=102 nstack_initial=0), State(pc_initial=78 nstack_initial=0)])
DEBUG:numba.core.byteflow:stack: []
DEBUG:numba.core.byteflow:state.pc_initial: State(pc_initial=102 nstack_initial=0)
DEBUG:numba.core.byteflow:dispatch pc=102, inst=LOAD_GLOBAL(arg=0, lineno=1155)
DEBUG:numba.core.byteflow:stack []
DEBUG:numba.core.byteflow:dispatch pc=104, inst=LOAD_METHOD(arg=2, lineno=1155)
DEBUG:numba.core.byteflow:stack ['$102load_global.0']
DEBUG:numba.core.byteflow:dispatch pc=106, inst=LOAD_FAST(arg=3, lineno=1155)
DEBUG:numba.core.byteflow:stack ['$104load_method.1']
DEBUG:numba.core.byteflow:dispatch pc=108, inst=CALL_METHOD(arg=1, lineno=1155)
DEBUG:numba.core.byteflow:stack ['$104load_method.1', '$x0106.2']
DEBUG:numba.core.byteflow:dispatch pc=110, inst=LOAD_GLOBAL(arg=0, lineno=1155)
DEBUG:numba.core.byteflow:stack ['$108call_method.3']
DEBUG:numba.core.byteflow:dispatch pc=112, inst=LOAD_METHOD(arg=2, lineno=1155)
DEBUG:numba.core.byteflow:stack ['$108call_method.3', '$110load_global.4']
DEBUG:numba.core.byteflow:dispatch pc=114, inst=LOAD_FAST(arg=4, lineno=1155)
DEBUG:numba.core.byteflow:stack ['$108call_method.3', '$112load_method.5']
DEBUG:numba.core.byteflow:dispatch pc=116, inst=CALL_METHOD(arg=1, lineno=1155)
DEBUG:numba.core.byteflow:stack ['$108call_method.3', '$112load_method.5', '$x1114.6']
DEBUG:numba.core.byteflow:dispatch pc=118, inst=COMPARE_OP(arg=3, lineno=1155)
DEBUG:numba.core.byteflow:stack ['$108call_method.3', '$116call_method.7']
DEBUG:numba.core.byteflow:dispatch pc=120, inst=RETURN_VALUE(arg=None, lineno=1155)
DEBUG:numba.core.byteflow:stack ['$118compare_op.8']
DEBUG:numba.core.byteflow:end state. edges=[]
DEBUG:numba.core.byteflow:pending: deque([State(pc_initial=78 nstack_initial=0)])
DEBUG:numba.core.byteflow:-------------------------Prune PHIs-------------------------
DEBUG:numba.core.byteflow:Used_phis: defaultdict(<class 'set'>,
            {State(pc_initial=0 nstack_initial=0): set(),
             State(pc_initial=24 nstack_initial=1): {'$phi24.0'},
             State(pc_initial=30 nstack_initial=0): set(),
             State(pc_initial=32 nstack_initial=1): set(),
             State(pc_initial=36 nstack_initial=0): set(),
             State(pc_initial=40 nstack_initial=0): set(),
             State(pc_initial=62 nstack_initial=1): {'$phi62.0'},
             State(pc_initial=68 nstack_initial=0): set(),
             State(pc_initial=70 nstack_initial=1): set(),
             State(pc_initial=74 nstack_initial=0): set(),
             State(pc_initial=78 nstack_initial=0): set(),
             State(pc_initial=82 nstack_initial=0): set(),
             State(pc_initial=102 nstack_initial=0): set()})
DEBUG:numba.core.byteflow:defmap: {'$phi24.0': State(pc_initial=0 nstack_initial=0),
 '$phi32.0': State(pc_initial=0 nstack_initial=0),
 '$phi62.0': State(pc_initial=40 nstack_initial=0),
 '$phi70.0': State(pc_initial=40 nstack_initial=0)}
DEBUG:numba.core.byteflow:phismap: defaultdict(<class 'set'>,
            {'$phi24.0': {('$16dup_top.6',
                           State(pc_initial=0 nstack_initial=0))},
             '$phi32.0': {('$16dup_top.6',
                           State(pc_initial=0 nstack_initial=0))},
             '$phi62.0': {('$54dup_top.6',
                           State(pc_initial=40 nstack_initial=0))},
             '$phi70.0': {('$54dup_top.6',
                           State(pc_initial=40 nstack_initial=0))}})
DEBUG:numba.core.byteflow:changing phismap: defaultdict(<class 'set'>,
            {'$phi24.0': {('$16dup_top.6',
                           State(pc_initial=0 nstack_initial=0))},
             '$phi32.0': {('$16dup_top.6',
                           State(pc_initial=0 nstack_initial=0))},
             '$phi62.0': {('$54dup_top.6',
                           State(pc_initial=40 nstack_initial=0))},
             '$phi70.0': {('$54dup_top.6',
                           State(pc_initial=40 nstack_initial=0))}})
DEBUG:numba.core.byteflow:keep phismap: {'$phi24.0': {('$16dup_top.6', State(pc_initial=0 nstack_initial=0))},
 '$phi62.0': {('$54dup_top.6', State(pc_initial=40 nstack_initial=0))}}
DEBUG:numba.core.byteflow:new_out: defaultdict(<class 'dict'>,
            {State(pc_initial=0 nstack_initial=0): {'$phi24.0': '$16dup_top.6'},
             State(pc_initial=40 nstack_initial=0): {'$phi62.0': '$54dup_top.6'}})
DEBUG:numba.core.byteflow:----------------------DONE Prune PHIs-----------------------
DEBUG:numba.core.byteflow:block_infos State(pc_initial=0 nstack_initial=0):
AdaptBlockInfo(insts=((0, {}), (2, {'res': '$x2.0'}), (4, {'res': '$const4.1'}), (6, {'index': '$const4.1', 'target': '$x2.0', 'res': '$6binary_subscr.2'}), (8, {'value': '$6binary_subscr.2'}), (10, {'res': '$threshold10.3'}), (12, {'value': '$threshold10.3', 'res': '$12unary_negative.4'}), (14, {'res': '$x014.5'}), (16, {'orig': ['$x014.5'], 'duped': ['$16dup_top.6']}), (20, {'lhs': '$12unary_negative.4', 'rhs': '$x014.5', 'res': '$20compare_op.7'}), (22, {'pred': '$20compare_op.7'})), outgoing_phis={'$phi24.0': '$16dup_top.6'}, blockstack=(), active_try_block=None, outgoing_edgepushed={24: ('$16dup_top.6',), 32: ('$16dup_top.6',)})
DEBUG:numba.core.byteflow:block_infos State(pc_initial=24 nstack_initial=1):
AdaptBlockInfo(insts=((24, {'res': '$threshold24.1'}), (26, {'lhs': '$phi24.0', 'rhs': '$threshold24.1', 'res': '$26compare_op.2'}), (28, {'pred': '$26compare_op.2'})), outgoing_phis={}, blockstack=(), active_try_block=None, outgoing_edgepushed={30: (), 40: ()})
DEBUG:numba.core.byteflow:block_infos State(pc_initial=30 nstack_initial=0):
AdaptBlockInfo(insts=((30, {}),), outgoing_phis={}, blockstack=(), active_try_block=None, outgoing_edgepushed={36: ()})
DEBUG:numba.core.byteflow:block_infos State(pc_initial=32 nstack_initial=1):
AdaptBlockInfo(insts=((34, {}),), outgoing_phis={}, blockstack=(), active_try_block=None, outgoing_edgepushed={40: ()})
DEBUG:numba.core.byteflow:block_infos State(pc_initial=36 nstack_initial=0):
AdaptBlockInfo(insts=((36, {'res': '$const36.0'}), (38, {'value': '$const36.0'})), outgoing_phis={}, blockstack=(), active_try_block=None, outgoing_edgepushed={40: ()})
DEBUG:numba.core.byteflow:block_infos State(pc_initial=40 nstack_initial=0):
AdaptBlockInfo(insts=((40, {'res': '$x40.0'}), (42, {'res': '$const42.1'}), (44, {'index': '$const42.1', 'target': '$x40.0', 'res': '$44binary_subscr.2'}), (46, {'value': '$44binary_subscr.2'}), (48, {'res': '$threshold48.3'}), (50, {'value': '$threshold48.3', 'res': '$50unary_negative.4'}), (52, {'res': '$x152.5'}), (54, {'orig': ['$x152.5'], 'duped': ['$54dup_top.6']}), (58, {'lhs': '$50unary_negative.4', 'rhs': '$x152.5', 'res': '$58compare_op.7'}), (60, {'pred': '$58compare_op.7'})), outgoing_phis={'$phi62.0': '$54dup_top.6'}, blockstack=(), active_try_block=None, outgoing_edgepushed={62: ('$54dup_top.6',), 70: ('$54dup_top.6',)})
DEBUG:numba.core.byteflow:block_infos State(pc_initial=62 nstack_initial=1):
AdaptBlockInfo(insts=((62, {'res': '$threshold62.1'}), (64, {'lhs': '$phi62.0', 'rhs': '$threshold62.1', 'res': '$64compare_op.2'}), (66, {'pred': '$64compare_op.2'})), outgoing_phis={}, blockstack=(), active_try_block=None, outgoing_edgepushed={68: (), 78: ()})
DEBUG:numba.core.byteflow:block_infos State(pc_initial=68 nstack_initial=0):
AdaptBlockInfo(insts=((68, {}),), outgoing_phis={}, blockstack=(), active_try_block=None, outgoing_edgepushed={74: ()})
DEBUG:numba.core.byteflow:block_infos State(pc_initial=70 nstack_initial=1):
AdaptBlockInfo(insts=((72, {}),), outgoing_phis={}, blockstack=(), active_try_block=None, outgoing_edgepushed={78: ()})
DEBUG:numba.core.byteflow:block_infos State(pc_initial=74 nstack_initial=0):
AdaptBlockInfo(insts=((74, {'res': '$const74.0'}), (76, {'value': '$const74.0'})), outgoing_phis={}, blockstack=(), active_try_block=None, outgoing_edgepushed={78: ()})
DEBUG:numba.core.byteflow:block_infos State(pc_initial=78 nstack_initial=0):
AdaptBlockInfo(insts=((78, {'res': '$zero_pos78.0'}), (80, {'pred': '$zero_pos78.0'})), outgoing_phis={}, blockstack=(), active_try_block=None, outgoing_edgepushed={82: (), 102: ()})
DEBUG:numba.core.byteflow:block_infos State(pc_initial=82 nstack_initial=0):
AdaptBlockInfo(insts=((82, {'res': '$82load_global.0'}), (84, {'item': '$82load_global.0', 'res': '$84load_method.1'}), (86, {'res': '$x086.2'}), (88, {'func': '$84load_method.1', 'args': ['$x086.2'], 'res': '$88call_method.3'}), (90, {'res': '$90load_global.4'}), (92, {'item': '$90load_global.4', 'res': '$92load_method.5'}), (94, {'res': '$x194.6'}), (96, {'func': '$92load_method.5', 'args': ['$x194.6'], 'res': '$96call_method.7'}), (98, {'lhs': '$88call_method.3', 'rhs': '$96call_method.7', 'res': '$98compare_op.8'}), (100, {'retval': '$98compare_op.8', 'castval': '$100return_value.9'})), outgoing_phis={}, blockstack=(), active_try_block=None, outgoing_edgepushed={})
DEBUG:numba.core.byteflow:block_infos State(pc_initial=102 nstack_initial=0):
AdaptBlockInfo(insts=((102, {'res': '$102load_global.0'}), (104, {'item': '$102load_global.0', 'res': '$104load_method.1'}), (106, {'res': '$x0106.2'}), (108, {'func': '$104load_method.1', 'args': ['$x0106.2'], 'res': '$108call_method.3'}), (110, {'res': '$110load_global.4'}), (112, {'item': '$110load_global.4', 'res': '$112load_method.5'}), (114, {'res': '$x1114.6'}), (116, {'func': '$112load_method.5', 'args': ['$x1114.6'], 'res': '$116call_method.7'}), (118, {'lhs': '$108call_method.3', 'rhs': '$116call_method.7', 'res': '$118compare_op.8'}), (120, {'retval': '$118compare_op.8', 'castval': '$120return_value.9'})), outgoing_phis={}, blockstack=(), active_try_block=None, outgoing_edgepushed={})
DEBUG:numba.core.interpreter:label 0:
    x = arg(0, name=x)                       ['x']
    threshold = arg(1, name=threshold)       ['threshold']
    zero_pos = arg(2, name=zero_pos)         ['zero_pos']
    $const4.1 = const(int, 0)                ['$const4.1']
    x0 = getitem(value=x, index=$const4.1, fn=<built-in function getitem>) ['$const4.1', 'x', 'x0']
    $12unary_negative.4 = unary(fn=<built-in function neg>, value=threshold) ['$12unary_negative.4', 'threshold']
    $20compare_op.7 = $12unary_negative.4 <= x0 ['$12unary_negative.4', '$20compare_op.7', 'x0']
    bool22 = global(bool: <class 'bool'>)    ['bool22']
    $22pred = call bool22($20compare_op.7, func=bool22, args=(Var($20compare_op.7, audio.py:1145),), kws=(), vararg=None, varkwarg=None, target=None) ['$20compare_op.7', '$22pred', 'bool22']
    $phi24.0 = x0                            ['$phi24.0', 'x0']
    branch $22pred, 24, 32                   ['$22pred']
label 24:
    $26compare_op.2 = $phi24.0 <= threshold  ['$26compare_op.2', '$phi24.0', 'threshold']
    bool28 = global(bool: <class 'bool'>)    ['bool28']
    $28pred = call bool28($26compare_op.2, func=bool28, args=(Var($26compare_op.2, audio.py:1145),), kws=(), vararg=None, varkwarg=None, target=None) ['$26compare_op.2', '$28pred', 'bool28']
    branch $28pred, 30, 40                   ['$28pred']
label 30:
    jump 36                                  []
label 32:
    jump 40                                  []
label 36:
    x0 = const(int, 0)                       ['x0']
    jump 40                                  []
label 40:
    $const42.1 = const(int, -1)              ['$const42.1']
    x1 = getitem(value=x, index=$const42.1, fn=<built-in function getitem>) ['$const42.1', 'x', 'x1']
    $50unary_negative.4 = unary(fn=<built-in function neg>, value=threshold) ['$50unary_negative.4', 'threshold']
    $58compare_op.7 = $50unary_negative.4 <= x1 ['$50unary_negative.4', '$58compare_op.7', 'x1']
    bool60 = global(bool: <class 'bool'>)    ['bool60']
    $60pred = call bool60($58compare_op.7, func=bool60, args=(Var($58compare_op.7, audio.py:1149),), kws=(), vararg=None, varkwarg=None, target=None) ['$58compare_op.7', '$60pred', 'bool60']
    $phi62.0 = x1                            ['$phi62.0', 'x1']
    branch $60pred, 62, 70                   ['$60pred']
label 62:
    $64compare_op.2 = $phi62.0 <= threshold  ['$64compare_op.2', '$phi62.0', 'threshold']
    bool66 = global(bool: <class 'bool'>)    ['bool66']
    $66pred = call bool66($64compare_op.2, func=bool66, args=(Var($64compare_op.2, audio.py:1149),), kws=(), vararg=None, varkwarg=None, target=None) ['$64compare_op.2', '$66pred', 'bool66']
    branch $66pred, 68, 78                   ['$66pred']
label 68:
    jump 74                                  []
label 70:
    jump 78                                  []
label 74:
    x1 = const(int, 0)                       ['x1']
    jump 78                                  []
label 78:
    bool80 = global(bool: <class 'bool'>)    ['bool80']
    $80pred = call bool80(zero_pos, func=bool80, args=(Var(zero_pos, audio.py:1141),), kws=(), vararg=None, varkwarg=None, target=None) ['$80pred', 'bool80', 'zero_pos']
    branch $80pred, 82, 102                  ['$80pred']
label 82:
    $82load_global.0 = global(np: <module 'numpy' from 'C:\\Users\\taskm\\anaconda3\\envs\\taskai\\lib\\site-packages\\numpy\\__init__.py'>) ['$82load_global.0']
    $84load_method.1 = getattr(value=$82load_global.0, attr=signbit) ['$82load_global.0', '$84load_method.1']
    $88call_method.3 = call $84load_method.1(x0, func=$84load_method.1, args=[Var(x0, audio.py:1144)], kws=(), vararg=None, varkwarg=None, target=None) ['$84load_method.1', '$88call_method.3', 'x0']
    $90load_global.4 = global(np: <module 'numpy' from 'C:\\Users\\taskm\\anaconda3\\envs\\taskai\\lib\\site-packages\\numpy\\__init__.py'>) ['$90load_global.4']
    $92load_method.5 = getattr(value=$90load_global.4, attr=signbit) ['$90load_global.4', '$92load_method.5']
    $96call_method.7 = call $92load_method.5(x1, func=$92load_method.5, args=[Var(x1, audio.py:1148)], kws=(), vararg=None, varkwarg=None, target=None) ['$92load_method.5', '$96call_method.7', 'x1']
    $98compare_op.8 = $88call_method.3 != $96call_method.7 ['$88call_method.3', '$96call_method.7', '$98compare_op.8']
    $100return_value.9 = cast(value=$98compare_op.8) ['$100return_value.9', '$98compare_op.8']
    return $100return_value.9                ['$100return_value.9']
label 102:
    $102load_global.0 = global(np: <module 'numpy' from 'C:\\Users\\taskm\\anaconda3\\envs\\taskai\\lib\\site-packages\\numpy\\__init__.py'>) ['$102load_global.0']
    $104load_method.1 = getattr(value=$102load_global.0, attr=sign) ['$102load_global.0', '$104load_method.1']
    $108call_method.3 = call $104load_method.1(x0, func=$104load_method.1, args=[Var(x0, audio.py:1144)], kws=(), vararg=None, varkwarg=None, target=None) ['$104load_method.1', '$108call_method.3', 'x0']
    $110load_global.4 = global(np: <module 'numpy' from 'C:\\Users\\taskm\\anaconda3\\envs\\taskai\\lib\\site-packages\\numpy\\__init__.py'>) ['$110load_global.4']
    $112load_method.5 = getattr(value=$110load_global.4, attr=sign) ['$110load_global.4', '$112load_method.5']
    $116call_method.7 = call $112load_method.5(x1, func=$112load_method.5, args=[Var(x1, audio.py:1148)], kws=(), vararg=None, varkwarg=None, target=None) ['$112load_method.5', '$116call_method.7', 'x1']
    $118compare_op.8 = $108call_method.3 != $116call_method.7 ['$108call_method.3', '$116call_method.7', '$118compare_op.8']
    $120return_value.9 = cast(value=$118compare_op.8) ['$118compare_op.8', '$120return_value.9']
    return $120return_value.9                ['$120return_value.9']

DEBUG:numba.core.byteflow:bytecode dump:
>          0	NOP(arg=None, lineno=1039)
           2	LOAD_FAST(arg=0, lineno=1042)
           4	LOAD_CONST(arg=1, lineno=1042)
           6	BINARY_SUBSCR(arg=None, lineno=1042)
           8	LOAD_FAST(arg=0, lineno=1042)
          10	LOAD_CONST(arg=2, lineno=1042)
          12	BINARY_SUBSCR(arg=None, lineno=1042)
          14	COMPARE_OP(arg=4, lineno=1042)
          16	LOAD_FAST(arg=0, lineno=1042)
          18	LOAD_CONST(arg=1, lineno=1042)
          20	BINARY_SUBSCR(arg=None, lineno=1042)
          22	LOAD_FAST(arg=0, lineno=1042)
          24	LOAD_CONST(arg=3, lineno=1042)
          26	BINARY_SUBSCR(arg=None, lineno=1042)
          28	COMPARE_OP(arg=5, lineno=1042)
          30	BINARY_AND(arg=None, lineno=1042)
          32	RETURN_VALUE(arg=None, lineno=1042)
DEBUG:numba.core.byteflow:pending: deque([State(pc_initial=0 nstack_initial=0)])
DEBUG:numba.core.byteflow:stack: []
DEBUG:numba.core.byteflow:state.pc_initial: State(pc_initial=0 nstack_initial=0)
DEBUG:numba.core.byteflow:dispatch pc=0, inst=NOP(arg=None, lineno=1039)
DEBUG:numba.core.byteflow:stack []
DEBUG:numba.core.byteflow:dispatch pc=2, inst=LOAD_FAST(arg=0, lineno=1042)
DEBUG:numba.core.byteflow:stack []
DEBUG:numba.core.byteflow:dispatch pc=4, inst=LOAD_CONST(arg=1, lineno=1042)
DEBUG:numba.core.byteflow:stack ['$x2.0']
DEBUG:numba.core.byteflow:dispatch pc=6, inst=BINARY_SUBSCR(arg=None, lineno=1042)
DEBUG:numba.core.byteflow:stack ['$x2.0', '$const4.1']
DEBUG:numba.core.byteflow:dispatch pc=8, inst=LOAD_FAST(arg=0, lineno=1042)
DEBUG:numba.core.byteflow:stack ['$6binary_subscr.2']
DEBUG:numba.core.byteflow:dispatch pc=10, inst=LOAD_CONST(arg=2, lineno=1042)
DEBUG:numba.core.byteflow:stack ['$6binary_subscr.2', '$x8.3']
DEBUG:numba.core.byteflow:dispatch pc=12, inst=BINARY_SUBSCR(arg=None, lineno=1042)
DEBUG:numba.core.byteflow:stack ['$6binary_subscr.2', '$x8.3', '$const10.4']
DEBUG:numba.core.byteflow:dispatch pc=14, inst=COMPARE_OP(arg=4, lineno=1042)
DEBUG:numba.core.byteflow:stack ['$6binary_subscr.2', '$12binary_subscr.5']
DEBUG:numba.core.byteflow:dispatch pc=16, inst=LOAD_FAST(arg=0, lineno=1042)
DEBUG:numba.core.byteflow:stack ['$14compare_op.6']
DEBUG:numba.core.byteflow:dispatch pc=18, inst=LOAD_CONST(arg=1, lineno=1042)
DEBUG:numba.core.byteflow:stack ['$14compare_op.6', '$x16.7']
DEBUG:numba.core.byteflow:dispatch pc=20, inst=BINARY_SUBSCR(arg=None, lineno=1042)
DEBUG:numba.core.byteflow:stack ['$14compare_op.6', '$x16.7', '$const18.8']
DEBUG:numba.core.byteflow:dispatch pc=22, inst=LOAD_FAST(arg=0, lineno=1042)
DEBUG:numba.core.byteflow:stack ['$14compare_op.6', '$20binary_subscr.9']
DEBUG:numba.core.byteflow:dispatch pc=24, inst=LOAD_CONST(arg=3, lineno=1042)
DEBUG:numba.core.byteflow:stack ['$14compare_op.6', '$20binary_subscr.9', '$x22.10']
DEBUG:numba.core.byteflow:dispatch pc=26, inst=BINARY_SUBSCR(arg=None, lineno=1042)
DEBUG:numba.core.byteflow:stack ['$14compare_op.6', '$20binary_subscr.9', '$x22.10', '$const24.11']
DEBUG:numba.core.byteflow:dispatch pc=28, inst=COMPARE_OP(arg=5, lineno=1042)
DEBUG:numba.core.byteflow:stack ['$14compare_op.6', '$20binary_subscr.9', '$26binary_subscr.12']
DEBUG:numba.core.byteflow:dispatch pc=30, inst=BINARY_AND(arg=None, lineno=1042)
DEBUG:numba.core.byteflow:stack ['$14compare_op.6', '$28compare_op.13']
DEBUG:numba.core.byteflow:dispatch pc=32, inst=RETURN_VALUE(arg=None, lineno=1042)
DEBUG:numba.core.byteflow:stack ['$30binary_and.14']
DEBUG:numba.core.byteflow:end state. edges=[]
DEBUG:numba.core.byteflow:-------------------------Prune PHIs-------------------------
DEBUG:numba.core.byteflow:Used_phis: defaultdict(<class 'set'>, {State(pc_initial=0 nstack_initial=0): set()})
DEBUG:numba.core.byteflow:defmap: {}
DEBUG:numba.core.byteflow:phismap: defaultdict(<class 'set'>, {})
DEBUG:numba.core.byteflow:changing phismap: defaultdict(<class 'set'>, {})
DEBUG:numba.core.byteflow:keep phismap: {}
DEBUG:numba.core.byteflow:new_out: defaultdict(<class 'dict'>, {})
DEBUG:numba.core.byteflow:----------------------DONE Prune PHIs-----------------------
DEBUG:numba.core.byteflow:block_infos State(pc_initial=0 nstack_initial=0):
AdaptBlockInfo(insts=((0, {}), (2, {'res': '$x2.0'}), (4, {'res': '$const4.1'}), (6, {'index': '$const4.1', 'target': '$x2.0', 'res': '$6binary_subscr.2'}), (8, {'res': '$x8.3'}), (10, {'res': '$const10.4'}), (12, {'index': '$const10.4', 'target': '$x8.3', 'res': '$12binary_subscr.5'}), (14, {'lhs': '$6binary_subscr.2', 'rhs': '$12binary_subscr.5', 'res': '$14compare_op.6'}), (16, {'res': '$x16.7'}), (18, {'res': '$const18.8'}), (20, {'index': '$const18.8', 'target': '$x16.7', 'res': '$20binary_subscr.9'}), (22, {'res': '$x22.10'}), (24, {'res': '$const24.11'}), (26, {'index': '$const24.11', 'target': '$x22.10', 'res': '$26binary_subscr.12'}), (28, {'lhs': '$20binary_subscr.9', 'rhs': '$26binary_subscr.12', 'res': '$28compare_op.13'}), (30, {'lhs': '$14compare_op.6', 'rhs': '$28compare_op.13', 'res': '$30binary_and.14'}), (32, {'retval': '$30binary_and.14', 'castval': '$32return_value.15'})), outgoing_phis={}, blockstack=(), active_try_block=None, outgoing_edgepushed={})
DEBUG:numba.core.interpreter:label 0:
    x = arg(0, name=x)                       ['x']
    $const4.1 = const(int, 0)                ['$const4.1']
    $6binary_subscr.2 = getitem(value=x, index=$const4.1, fn=<built-in function getitem>) ['$6binary_subscr.2', '$const4.1', 'x']
    $const10.4 = const(int, -1)              ['$const10.4']
    $12binary_subscr.5 = getitem(value=x, index=$const10.4, fn=<built-in function getitem>) ['$12binary_subscr.5', '$const10.4', 'x']
    $14compare_op.6 = $6binary_subscr.2 > $12binary_subscr.5 ['$12binary_subscr.5', '$14compare_op.6', '$6binary_subscr.2']
    $const18.8 = const(int, 0)               ['$const18.8']
    $20binary_subscr.9 = getitem(value=x, index=$const18.8, fn=<built-in function getitem>) ['$20binary_subscr.9', '$const18.8', 'x']
    $const24.11 = const(int, 1)              ['$const24.11']
    $26binary_subscr.12 = getitem(value=x, index=$const24.11, fn=<built-in function getitem>) ['$26binary_subscr.12', '$const24.11', 'x']
    $28compare_op.13 = $20binary_subscr.9 >= $26binary_subscr.12 ['$20binary_subscr.9', '$26binary_subscr.12', '$28compare_op.13']
    $30binary_and.14 = $14compare_op.6 & $28compare_op.13 ['$14compare_op.6', '$28compare_op.13', '$30binary_and.14']
    $32return_value.15 = cast(value=$30binary_and.14) ['$30binary_and.14', '$32return_value.15']
    return $32return_value.15                ['$32return_value.15']

DEBUG:numba.core.byteflow:bytecode dump:
>          0	NOP(arg=None, lineno=1045)
           2	LOAD_FAST(arg=0, lineno=1048)
           4	LOAD_CONST(arg=1, lineno=1048)
           6	BINARY_SUBSCR(arg=None, lineno=1048)
           8	LOAD_FAST(arg=0, lineno=1048)
          10	LOAD_CONST(arg=2, lineno=1048)
          12	BINARY_SUBSCR(arg=None, lineno=1048)
          14	COMPARE_OP(arg=0, lineno=1048)
          16	LOAD_FAST(arg=0, lineno=1048)
          18	LOAD_CONST(arg=1, lineno=1048)
          20	BINARY_SUBSCR(arg=None, lineno=1048)
          22	LOAD_FAST(arg=0, lineno=1048)
          24	LOAD_CONST(arg=3, lineno=1048)
          26	BINARY_SUBSCR(arg=None, lineno=1048)
          28	COMPARE_OP(arg=1, lineno=1048)
          30	BINARY_AND(arg=None, lineno=1048)
          32	RETURN_VALUE(arg=None, lineno=1048)
DEBUG:numba.core.byteflow:pending: deque([State(pc_initial=0 nstack_initial=0)])
DEBUG:numba.core.byteflow:stack: []
DEBUG:numba.core.byteflow:state.pc_initial: State(pc_initial=0 nstack_initial=0)
DEBUG:numba.core.byteflow:dispatch pc=0, inst=NOP(arg=None, lineno=1045)
DEBUG:numba.core.byteflow:stack []
DEBUG:numba.core.byteflow:dispatch pc=2, inst=LOAD_FAST(arg=0, lineno=1048)
DEBUG:numba.core.byteflow:stack []
DEBUG:numba.core.byteflow:dispatch pc=4, inst=LOAD_CONST(arg=1, lineno=1048)
DEBUG:numba.core.byteflow:stack ['$x2.0']
DEBUG:numba.core.byteflow:dispatch pc=6, inst=BINARY_SUBSCR(arg=None, lineno=1048)
DEBUG:numba.core.byteflow:stack ['$x2.0', '$const4.1']
DEBUG:numba.core.byteflow:dispatch pc=8, inst=LOAD_FAST(arg=0, lineno=1048)
DEBUG:numba.core.byteflow:stack ['$6binary_subscr.2']
DEBUG:numba.core.byteflow:dispatch pc=10, inst=LOAD_CONST(arg=2, lineno=1048)
DEBUG:numba.core.byteflow:stack ['$6binary_subscr.2', '$x8.3']
DEBUG:numba.core.byteflow:dispatch pc=12, inst=BINARY_SUBSCR(arg=None, lineno=1048)
DEBUG:numba.core.byteflow:stack ['$6binary_subscr.2', '$x8.3', '$const10.4']
DEBUG:numba.core.byteflow:dispatch pc=14, inst=COMPARE_OP(arg=0, lineno=1048)
DEBUG:numba.core.byteflow:stack ['$6binary_subscr.2', '$12binary_subscr.5']
DEBUG:numba.core.byteflow:dispatch pc=16, inst=LOAD_FAST(arg=0, lineno=1048)
DEBUG:numba.core.byteflow:stack ['$14compare_op.6']
DEBUG:numba.core.byteflow:dispatch pc=18, inst=LOAD_CONST(arg=1, lineno=1048)
DEBUG:numba.core.byteflow:stack ['$14compare_op.6', '$x16.7']
DEBUG:numba.core.byteflow:dispatch pc=20, inst=BINARY_SUBSCR(arg=None, lineno=1048)
DEBUG:numba.core.byteflow:stack ['$14compare_op.6', '$x16.7', '$const18.8']
DEBUG:numba.core.byteflow:dispatch pc=22, inst=LOAD_FAST(arg=0, lineno=1048)
DEBUG:numba.core.byteflow:stack ['$14compare_op.6', '$20binary_subscr.9']
DEBUG:numba.core.byteflow:dispatch pc=24, inst=LOAD_CONST(arg=3, lineno=1048)
DEBUG:numba.core.byteflow:stack ['$14compare_op.6', '$20binary_subscr.9', '$x22.10']
DEBUG:numba.core.byteflow:dispatch pc=26, inst=BINARY_SUBSCR(arg=None, lineno=1048)
DEBUG:numba.core.byteflow:stack ['$14compare_op.6', '$20binary_subscr.9', '$x22.10', '$const24.11']
DEBUG:numba.core.byteflow:dispatch pc=28, inst=COMPARE_OP(arg=1, lineno=1048)
DEBUG:numba.core.byteflow:stack ['$14compare_op.6', '$20binary_subscr.9', '$26binary_subscr.12']
DEBUG:numba.core.byteflow:dispatch pc=30, inst=BINARY_AND(arg=None, lineno=1048)
DEBUG:numba.core.byteflow:stack ['$14compare_op.6', '$28compare_op.13']
DEBUG:numba.core.byteflow:dispatch pc=32, inst=RETURN_VALUE(arg=None, lineno=1048)
DEBUG:numba.core.byteflow:stack ['$30binary_and.14']
DEBUG:numba.core.byteflow:end state. edges=[]
DEBUG:numba.core.byteflow:-------------------------Prune PHIs-------------------------
DEBUG:numba.core.byteflow:Used_phis: defaultdict(<class 'set'>, {State(pc_initial=0 nstack_initial=0): set()})
DEBUG:numba.core.byteflow:defmap: {}
DEBUG:numba.core.byteflow:phismap: defaultdict(<class 'set'>, {})
DEBUG:numba.core.byteflow:changing phismap: defaultdict(<class 'set'>, {})
DEBUG:numba.core.byteflow:keep phismap: {}
DEBUG:numba.core.byteflow:new_out: defaultdict(<class 'dict'>, {})
DEBUG:numba.core.byteflow:----------------------DONE Prune PHIs-----------------------
DEBUG:numba.core.byteflow:block_infos State(pc_initial=0 nstack_initial=0):
AdaptBlockInfo(insts=((0, {}), (2, {'res': '$x2.0'}), (4, {'res': '$const4.1'}), (6, {'index': '$const4.1', 'target': '$x2.0', 'res': '$6binary_subscr.2'}), (8, {'res': '$x8.3'}), (10, {'res': '$const10.4'}), (12, {'index': '$const10.4', 'target': '$x8.3', 'res': '$12binary_subscr.5'}), (14, {'lhs': '$6binary_subscr.2', 'rhs': '$12binary_subscr.5', 'res': '$14compare_op.6'}), (16, {'res': '$x16.7'}), (18, {'res': '$const18.8'}), (20, {'index': '$const18.8', 'target': '$x16.7', 'res': '$20binary_subscr.9'}), (22, {'res': '$x22.10'}), (24, {'res': '$const24.11'}), (26, {'index': '$const24.11', 'target': '$x22.10', 'res': '$26binary_subscr.12'}), (28, {'lhs': '$20binary_subscr.9', 'rhs': '$26binary_subscr.12', 'res': '$28compare_op.13'}), (30, {'lhs': '$14compare_op.6', 'rhs': '$28compare_op.13', 'res': '$30binary_and.14'}), (32, {'retval': '$30binary_and.14', 'castval': '$32return_value.15'})), outgoing_phis={}, blockstack=(), active_try_block=None, outgoing_edgepushed={})
DEBUG:numba.core.interpreter:label 0:
    x = arg(0, name=x)                       ['x']
    $const4.1 = const(int, 0)                ['$const4.1']
    $6binary_subscr.2 = getitem(value=x, index=$const4.1, fn=<built-in function getitem>) ['$6binary_subscr.2', '$const4.1', 'x']
    $const10.4 = const(int, -1)              ['$const10.4']
    $12binary_subscr.5 = getitem(value=x, index=$const10.4, fn=<built-in function getitem>) ['$12binary_subscr.5', '$const10.4', 'x']
    $14compare_op.6 = $6binary_subscr.2 < $12binary_subscr.5 ['$12binary_subscr.5', '$14compare_op.6', '$6binary_subscr.2']
    $const18.8 = const(int, 0)               ['$const18.8']
    $20binary_subscr.9 = getitem(value=x, index=$const18.8, fn=<built-in function getitem>) ['$20binary_subscr.9', '$const18.8', 'x']
    $const24.11 = const(int, 1)              ['$const24.11']
    $26binary_subscr.12 = getitem(value=x, index=$const24.11, fn=<built-in function getitem>) ['$26binary_subscr.12', '$const24.11', 'x']
    $28compare_op.13 = $20binary_subscr.9 <= $26binary_subscr.12 ['$20binary_subscr.9', '$26binary_subscr.12', '$28compare_op.13']
    $30binary_and.14 = $14compare_op.6 & $28compare_op.13 ['$14compare_op.6', '$28compare_op.13', '$30binary_and.14']
    $32return_value.15 = cast(value=$30binary_and.14) ['$30binary_and.14', '$32return_value.15']
    return $32return_value.15                ['$32return_value.15']

DEBUG:numba.core.byteflow:bytecode dump:
>          0	NOP(arg=None, lineno=1141)
           2	LOAD_FAST(arg=0, lineno=1144)
           4	LOAD_CONST(arg=1, lineno=1144)
           6	BINARY_SUBSCR(arg=None, lineno=1144)
           8	STORE_FAST(arg=3, lineno=1144)
          10	LOAD_FAST(arg=1, lineno=1145)
          12	UNARY_NEGATIVE(arg=None, lineno=1145)
          14	LOAD_FAST(arg=3, lineno=1145)
          16	DUP_TOP(arg=None, lineno=1145)
          18	ROT_THREE(arg=None, lineno=1145)
          20	COMPARE_OP(arg=1, lineno=1145)
          22	POP_JUMP_IF_FALSE(arg=32, lineno=1145)
          24	LOAD_FAST(arg=1, lineno=1145)
          26	COMPARE_OP(arg=1, lineno=1145)
          28	POP_JUMP_IF_FALSE(arg=40, lineno=1145)
          30	JUMP_FORWARD(arg=4, lineno=1145)
>         32	POP_TOP(arg=None, lineno=1145)
          34	JUMP_FORWARD(arg=4, lineno=1145)
>         36	LOAD_CONST(arg=1, lineno=1146)
          38	STORE_FAST(arg=3, lineno=1146)
>         40	LOAD_FAST(arg=0, lineno=1148)
          42	LOAD_CONST(arg=2, lineno=1148)
          44	BINARY_SUBSCR(arg=None, lineno=1148)
          46	STORE_FAST(arg=4, lineno=1148)
          48	LOAD_FAST(arg=1, lineno=1149)
          50	UNARY_NEGATIVE(arg=None, lineno=1149)
          52	LOAD_FAST(arg=4, lineno=1149)
          54	DUP_TOP(arg=None, lineno=1149)
          56	ROT_THREE(arg=None, lineno=1149)
          58	COMPARE_OP(arg=1, lineno=1149)
          60	POP_JUMP_IF_FALSE(arg=70, lineno=1149)
          62	LOAD_FAST(arg=1, lineno=1149)
          64	COMPARE_OP(arg=1, lineno=1149)
          66	POP_JUMP_IF_FALSE(arg=78, lineno=1149)
          68	JUMP_FORWARD(arg=4, lineno=1149)
>         70	POP_TOP(arg=None, lineno=1149)
          72	JUMP_FORWARD(arg=4, lineno=1149)
>         74	LOAD_CONST(arg=1, lineno=1150)
          76	STORE_FAST(arg=4, lineno=1150)
>         78	LOAD_FAST(arg=2, lineno=1152)
          80	POP_JUMP_IF_FALSE(arg=102, lineno=1152)
          82	LOAD_GLOBAL(arg=0, lineno=1153)
          84	LOAD_METHOD(arg=1, lineno=1153)
          86	LOAD_FAST(arg=3, lineno=1153)
          88	CALL_METHOD(arg=1, lineno=1153)
          90	LOAD_GLOBAL(arg=0, lineno=1153)
          92	LOAD_METHOD(arg=1, lineno=1153)
          94	LOAD_FAST(arg=4, lineno=1153)
          96	CALL_METHOD(arg=1, lineno=1153)
          98	COMPARE_OP(arg=3, lineno=1153)
         100	RETURN_VALUE(arg=None, lineno=1153)
>        102	LOAD_GLOBAL(arg=0, lineno=1155)
         104	LOAD_METHOD(arg=2, lineno=1155)
         106	LOAD_FAST(arg=3, lineno=1155)
         108	CALL_METHOD(arg=1, lineno=1155)
         110	LOAD_GLOBAL(arg=0, lineno=1155)
         112	LOAD_METHOD(arg=2, lineno=1155)
         114	LOAD_FAST(arg=4, lineno=1155)
         116	CALL_METHOD(arg=1, lineno=1155)
         118	COMPARE_OP(arg=3, lineno=1155)
         120	RETURN_VALUE(arg=None, lineno=1155)
         122	LOAD_CONST(arg=3, lineno=1155)
         124	RETURN_VALUE(arg=None, lineno=1155)
DEBUG:numba.core.byteflow:pending: deque([State(pc_initial=0 nstack_initial=0)])
DEBUG:numba.core.byteflow:stack: []
DEBUG:numba.core.byteflow:state.pc_initial: State(pc_initial=0 nstack_initial=0)
DEBUG:numba.core.byteflow:dispatch pc=0, inst=NOP(arg=None, lineno=1141)
DEBUG:numba.core.byteflow:stack []
DEBUG:numba.core.byteflow:dispatch pc=2, inst=LOAD_FAST(arg=0, lineno=1144)
DEBUG:numba.core.byteflow:stack []
DEBUG:numba.core.byteflow:dispatch pc=4, inst=LOAD_CONST(arg=1, lineno=1144)
DEBUG:numba.core.byteflow:stack ['$x2.0']
DEBUG:numba.core.byteflow:dispatch pc=6, inst=BINARY_SUBSCR(arg=None, lineno=1144)
DEBUG:numba.core.byteflow:stack ['$x2.0', '$const4.1']
DEBUG:numba.core.byteflow:dispatch pc=8, inst=STORE_FAST(arg=3, lineno=1144)
DEBUG:numba.core.byteflow:stack ['$6binary_subscr.2']
DEBUG:numba.core.byteflow:dispatch pc=10, inst=LOAD_FAST(arg=1, lineno=1145)
DEBUG:numba.core.byteflow:stack []
DEBUG:numba.core.byteflow:dispatch pc=12, inst=UNARY_NEGATIVE(arg=None, lineno=1145)
DEBUG:numba.core.byteflow:stack ['$threshold10.3']
DEBUG:numba.core.byteflow:dispatch pc=14, inst=LOAD_FAST(arg=3, lineno=1145)
DEBUG:numba.core.byteflow:stack ['$12unary_negative.4']
DEBUG:numba.core.byteflow:dispatch pc=16, inst=DUP_TOP(arg=None, lineno=1145)
DEBUG:numba.core.byteflow:stack ['$12unary_negative.4', '$x014.5']
DEBUG:numba.core.byteflow:dispatch pc=18, inst=ROT_THREE(arg=None, lineno=1145)
DEBUG:numba.core.byteflow:stack ['$12unary_negative.4', '$x014.5', '$16dup_top.6']
DEBUG:numba.core.byteflow:dispatch pc=20, inst=COMPARE_OP(arg=1, lineno=1145)
DEBUG:numba.core.byteflow:stack ['$16dup_top.6', '$12unary_negative.4', '$x014.5']
DEBUG:numba.core.byteflow:dispatch pc=22, inst=POP_JUMP_IF_FALSE(arg=32, lineno=1145)
DEBUG:numba.core.byteflow:stack ['$16dup_top.6', '$20compare_op.7']
DEBUG:numba.core.byteflow:end state. edges=[Edge(pc=24, stack=('$16dup_top.6',), blockstack=(), npush=0), Edge(pc=32, stack=('$16dup_top.6',), blockstack=(), npush=0)]
DEBUG:numba.core.byteflow:pending: deque([State(pc_initial=24 nstack_initial=1), State(pc_initial=32 nstack_initial=1)])
DEBUG:numba.core.byteflow:stack: ['$phi24.0']
DEBUG:numba.core.byteflow:state.pc_initial: State(pc_initial=24 nstack_initial=1)
DEBUG:numba.core.byteflow:dispatch pc=24, inst=LOAD_FAST(arg=1, lineno=1145)
DEBUG:numba.core.byteflow:stack ['$phi24.0']
DEBUG:numba.core.byteflow:dispatch pc=26, inst=COMPARE_OP(arg=1, lineno=1145)
DEBUG:numba.core.byteflow:stack ['$phi24.0', '$threshold24.1']
DEBUG:numba.core.byteflow:dispatch pc=28, inst=POP_JUMP_IF_FALSE(arg=40, lineno=1145)
DEBUG:numba.core.byteflow:stack ['$26compare_op.2']
DEBUG:numba.core.byteflow:end state. edges=[Edge(pc=30, stack=(), blockstack=(), npush=0), Edge(pc=40, stack=(), blockstack=(), npush=0)]
DEBUG:numba.core.byteflow:pending: deque([State(pc_initial=32 nstack_initial=1), State(pc_initial=30 nstack_initial=0), State(pc_initial=40 nstack_initial=0)])
DEBUG:numba.core.byteflow:stack: ['$phi32.0']
DEBUG:numba.core.byteflow:state.pc_initial: State(pc_initial=32 nstack_initial=1)
DEBUG:numba.core.byteflow:dispatch pc=32, inst=POP_TOP(arg=None, lineno=1145)
DEBUG:numba.core.byteflow:stack ['$phi32.0']
DEBUG:numba.core.byteflow:dispatch pc=34, inst=JUMP_FORWARD(arg=4, lineno=1145)
DEBUG:numba.core.byteflow:stack []
DEBUG:numba.core.byteflow:end state. edges=[Edge(pc=40, stack=(), blockstack=(), npush=0)]
DEBUG:numba.core.byteflow:pending: deque([State(pc_initial=30 nstack_initial=0), State(pc_initial=40 nstack_initial=0), State(pc_initial=40 nstack_initial=0)])
DEBUG:numba.core.byteflow:stack: []
DEBUG:numba.core.byteflow:state.pc_initial: State(pc_initial=30 nstack_initial=0)
DEBUG:numba.core.byteflow:dispatch pc=30, inst=JUMP_FORWARD(arg=4, lineno=1145)
DEBUG:numba.core.byteflow:stack []
DEBUG:numba.core.byteflow:end state. edges=[Edge(pc=36, stack=(), blockstack=(), npush=0)]
DEBUG:numba.core.byteflow:pending: deque([State(pc_initial=40 nstack_initial=0), State(pc_initial=40 nstack_initial=0), State(pc_initial=36 nstack_initial=0)])
DEBUG:numba.core.byteflow:stack: []
DEBUG:numba.core.byteflow:state.pc_initial: State(pc_initial=40 nstack_initial=0)
DEBUG:numba.core.byteflow:dispatch pc=40, inst=LOAD_FAST(arg=0, lineno=1148)
DEBUG:numba.core.byteflow:stack []
DEBUG:numba.core.byteflow:dispatch pc=42, inst=LOAD_CONST(arg=2, lineno=1148)
DEBUG:numba.core.byteflow:stack ['$x40.0']
DEBUG:numba.core.byteflow:dispatch pc=44, inst=BINARY_SUBSCR(arg=None, lineno=1148)
DEBUG:numba.core.byteflow:stack ['$x40.0', '$const42.1']
DEBUG:numba.core.byteflow:dispatch pc=46, inst=STORE_FAST(arg=4, lineno=1148)
DEBUG:numba.core.byteflow:stack ['$44binary_subscr.2']
DEBUG:numba.core.byteflow:dispatch pc=48, inst=LOAD_FAST(arg=1, lineno=1149)
DEBUG:numba.core.byteflow:stack []
DEBUG:numba.core.byteflow:dispatch pc=50, inst=UNARY_NEGATIVE(arg=None, lineno=1149)
DEBUG:numba.core.byteflow:stack ['$threshold48.3']
DEBUG:numba.core.byteflow:dispatch pc=52, inst=LOAD_FAST(arg=4, lineno=1149)
DEBUG:numba.core.byteflow:stack ['$50unary_negative.4']
DEBUG:numba.core.byteflow:dispatch pc=54, inst=DUP_TOP(arg=None, lineno=1149)
DEBUG:numba.core.byteflow:stack ['$50unary_negative.4', '$x152.5']
DEBUG:numba.core.byteflow:dispatch pc=56, inst=ROT_THREE(arg=None, lineno=1149)
DEBUG:numba.core.byteflow:stack ['$50unary_negative.4', '$x152.5', '$54dup_top.6']
DEBUG:numba.core.byteflow:dispatch pc=58, inst=COMPARE_OP(arg=1, lineno=1149)
DEBUG:numba.core.byteflow:stack ['$54dup_top.6', '$50unary_negative.4', '$x152.5']
DEBUG:numba.core.byteflow:dispatch pc=60, inst=POP_JUMP_IF_FALSE(arg=70, lineno=1149)
DEBUG:numba.core.byteflow:stack ['$54dup_top.6', '$58compare_op.7']
DEBUG:numba.core.byteflow:end state. edges=[Edge(pc=62, stack=('$54dup_top.6',), blockstack=(), npush=0), Edge(pc=70, stack=('$54dup_top.6',), blockstack=(), npush=0)]
DEBUG:numba.core.byteflow:pending: deque([State(pc_initial=40 nstack_initial=0), State(pc_initial=36 nstack_initial=0), State(pc_initial=62 nstack_initial=1), State(pc_initial=70 nstack_initial=1)])
DEBUG:numba.core.byteflow:pending: deque([State(pc_initial=36 nstack_initial=0), State(pc_initial=62 nstack_initial=1), State(pc_initial=70 nstack_initial=1)])
DEBUG:numba.core.byteflow:stack: []
DEBUG:numba.core.byteflow:state.pc_initial: State(pc_initial=36 nstack_initial=0)
DEBUG:numba.core.byteflow:dispatch pc=36, inst=LOAD_CONST(arg=1, lineno=1146)
DEBUG:numba.core.byteflow:stack []
DEBUG:numba.core.byteflow:dispatch pc=38, inst=STORE_FAST(arg=3, lineno=1146)
DEBUG:numba.core.byteflow:stack ['$const36.0']
DEBUG:numba.core.byteflow:end state. edges=[Edge(pc=40, stack=(), blockstack=(), npush=0)]
DEBUG:numba.core.byteflow:pending: deque([State(pc_initial=62 nstack_initial=1), State(pc_initial=70 nstack_initial=1), State(pc_initial=40 nstack_initial=0)])
DEBUG:numba.core.byteflow:stack: ['$phi62.0']
DEBUG:numba.core.byteflow:state.pc_initial: State(pc_initial=62 nstack_initial=1)
DEBUG:numba.core.byteflow:dispatch pc=62, inst=LOAD_FAST(arg=1, lineno=1149)
DEBUG:numba.core.byteflow:stack ['$phi62.0']
DEBUG:numba.core.byteflow:dispatch pc=64, inst=COMPARE_OP(arg=1, lineno=1149)
DEBUG:numba.core.byteflow:stack ['$phi62.0', '$threshold62.1']
DEBUG:numba.core.byteflow:dispatch pc=66, inst=POP_JUMP_IF_FALSE(arg=78, lineno=1149)
DEBUG:numba.core.byteflow:stack ['$64compare_op.2']
DEBUG:numba.core.byteflow:end state. edges=[Edge(pc=68, stack=(), blockstack=(), npush=0), Edge(pc=78, stack=(), blockstack=(), npush=0)]
DEBUG:numba.core.byteflow:pending: deque([State(pc_initial=70 nstack_initial=1), State(pc_initial=40 nstack_initial=0), State(pc_initial=68 nstack_initial=0), State(pc_initial=78 nstack_initial=0)])
DEBUG:numba.core.byteflow:stack: ['$phi70.0']
DEBUG:numba.core.byteflow:state.pc_initial: State(pc_initial=70 nstack_initial=1)
DEBUG:numba.core.byteflow:dispatch pc=70, inst=POP_TOP(arg=None, lineno=1149)
DEBUG:numba.core.byteflow:stack ['$phi70.0']
DEBUG:numba.core.byteflow:dispatch pc=72, inst=JUMP_FORWARD(arg=4, lineno=1149)
DEBUG:numba.core.byteflow:stack []
DEBUG:numba.core.byteflow:end state. edges=[Edge(pc=78, stack=(), blockstack=(), npush=0)]
DEBUG:numba.core.byteflow:pending: deque([State(pc_initial=40 nstack_initial=0), State(pc_initial=68 nstack_initial=0), State(pc_initial=78 nstack_initial=0), State(pc_initial=78 nstack_initial=0)])
DEBUG:numba.core.byteflow:pending: deque([State(pc_initial=68 nstack_initial=0), State(pc_initial=78 nstack_initial=0), State(pc_initial=78 nstack_initial=0)])
DEBUG:numba.core.byteflow:stack: []
DEBUG:numba.core.byteflow:state.pc_initial: State(pc_initial=68 nstack_initial=0)
DEBUG:numba.core.byteflow:dispatch pc=68, inst=JUMP_FORWARD(arg=4, lineno=1149)
DEBUG:numba.core.byteflow:stack []
DEBUG:numba.core.byteflow:end state. edges=[Edge(pc=74, stack=(), blockstack=(), npush=0)]
DEBUG:numba.core.byteflow:pending: deque([State(pc_initial=78 nstack_initial=0), State(pc_initial=78 nstack_initial=0), State(pc_initial=74 nstack_initial=0)])
DEBUG:numba.core.byteflow:stack: []
DEBUG:numba.core.byteflow:state.pc_initial: State(pc_initial=78 nstack_initial=0)
DEBUG:numba.core.byteflow:dispatch pc=78, inst=LOAD_FAST(arg=2, lineno=1152)
DEBUG:numba.core.byteflow:stack []
DEBUG:numba.core.byteflow:dispatch pc=80, inst=POP_JUMP_IF_FALSE(arg=102, lineno=1152)
DEBUG:numba.core.byteflow:stack ['$zero_pos78.0']
DEBUG:numba.core.byteflow:end state. edges=[Edge(pc=82, stack=(), blockstack=(), npush=0), Edge(pc=102, stack=(), blockstack=(), npush=0)]
DEBUG:numba.core.byteflow:pending: deque([State(pc_initial=78 nstack_initial=0), State(pc_initial=74 nstack_initial=0), State(pc_initial=82 nstack_initial=0), State(pc_initial=102 nstack_initial=0)])
DEBUG:numba.core.byteflow:pending: deque([State(pc_initial=74 nstack_initial=0), State(pc_initial=82 nstack_initial=0), State(pc_initial=102 nstack_initial=0)])
DEBUG:numba.core.byteflow:stack: []
DEBUG:numba.core.byteflow:state.pc_initial: State(pc_initial=74 nstack_initial=0)
DEBUG:numba.core.byteflow:dispatch pc=74, inst=LOAD_CONST(arg=1, lineno=1150)
DEBUG:numba.core.byteflow:stack []
DEBUG:numba.core.byteflow:dispatch pc=76, inst=STORE_FAST(arg=4, lineno=1150)
DEBUG:numba.core.byteflow:stack ['$const74.0']
DEBUG:numba.core.byteflow:end state. edges=[Edge(pc=78, stack=(), blockstack=(), npush=0)]
DEBUG:numba.core.byteflow:pending: deque([State(pc_initial=82 nstack_initial=0), State(pc_initial=102 nstack_initial=0), State(pc_initial=78 nstack_initial=0)])
DEBUG:numba.core.byteflow:stack: []
DEBUG:numba.core.byteflow:state.pc_initial: State(pc_initial=82 nstack_initial=0)
DEBUG:numba.core.byteflow:dispatch pc=82, inst=LOAD_GLOBAL(arg=0, lineno=1153)
DEBUG:numba.core.byteflow:stack []
DEBUG:numba.core.byteflow:dispatch pc=84, inst=LOAD_METHOD(arg=1, lineno=1153)
DEBUG:numba.core.byteflow:stack ['$82load_global.0']
DEBUG:numba.core.byteflow:dispatch pc=86, inst=LOAD_FAST(arg=3, lineno=1153)
DEBUG:numba.core.byteflow:stack ['$84load_method.1']
DEBUG:numba.core.byteflow:dispatch pc=88, inst=CALL_METHOD(arg=1, lineno=1153)
DEBUG:numba.core.byteflow:stack ['$84load_method.1', '$x086.2']
DEBUG:numba.core.byteflow:dispatch pc=90, inst=LOAD_GLOBAL(arg=0, lineno=1153)
DEBUG:numba.core.byteflow:stack ['$88call_method.3']
DEBUG:numba.core.byteflow:dispatch pc=92, inst=LOAD_METHOD(arg=1, lineno=1153)
DEBUG:numba.core.byteflow:stack ['$88call_method.3', '$90load_global.4']
DEBUG:numba.core.byteflow:dispatch pc=94, inst=LOAD_FAST(arg=4, lineno=1153)
DEBUG:numba.core.byteflow:stack ['$88call_method.3', '$92load_method.5']
DEBUG:numba.core.byteflow:dispatch pc=96, inst=CALL_METHOD(arg=1, lineno=1153)
DEBUG:numba.core.byteflow:stack ['$88call_method.3', '$92load_method.5', '$x194.6']
DEBUG:numba.core.byteflow:dispatch pc=98, inst=COMPARE_OP(arg=3, lineno=1153)
DEBUG:numba.core.byteflow:stack ['$88call_method.3', '$96call_method.7']
DEBUG:numba.core.byteflow:dispatch pc=100, inst=RETURN_VALUE(arg=None, lineno=1153)
DEBUG:numba.core.byteflow:stack ['$98compare_op.8']
DEBUG:numba.core.byteflow:end state. edges=[]
DEBUG:numba.core.byteflow:pending: deque([State(pc_initial=102 nstack_initial=0), State(pc_initial=78 nstack_initial=0)])
DEBUG:numba.core.byteflow:stack: []
DEBUG:numba.core.byteflow:state.pc_initial: State(pc_initial=102 nstack_initial=0)
DEBUG:numba.core.byteflow:dispatch pc=102, inst=LOAD_GLOBAL(arg=0, lineno=1155)
DEBUG:numba.core.byteflow:stack []
DEBUG:numba.core.byteflow:dispatch pc=104, inst=LOAD_METHOD(arg=2, lineno=1155)
DEBUG:numba.core.byteflow:stack ['$102load_global.0']
DEBUG:numba.core.byteflow:dispatch pc=106, inst=LOAD_FAST(arg=3, lineno=1155)
DEBUG:numba.core.byteflow:stack ['$104load_method.1']
DEBUG:numba.core.byteflow:dispatch pc=108, inst=CALL_METHOD(arg=1, lineno=1155)
DEBUG:numba.core.byteflow:stack ['$104load_method.1', '$x0106.2']
DEBUG:numba.core.byteflow:dispatch pc=110, inst=LOAD_GLOBAL(arg=0, lineno=1155)
DEBUG:numba.core.byteflow:stack ['$108call_method.3']
DEBUG:numba.core.byteflow:dispatch pc=112, inst=LOAD_METHOD(arg=2, lineno=1155)
DEBUG:numba.core.byteflow:stack ['$108call_method.3', '$110load_global.4']
DEBUG:numba.core.byteflow:dispatch pc=114, inst=LOAD_FAST(arg=4, lineno=1155)
DEBUG:numba.core.byteflow:stack ['$108call_method.3', '$112load_method.5']
DEBUG:numba.core.byteflow:dispatch pc=116, inst=CALL_METHOD(arg=1, lineno=1155)
DEBUG:numba.core.byteflow:stack ['$108call_method.3', '$112load_method.5', '$x1114.6']
DEBUG:numba.core.byteflow:dispatch pc=118, inst=COMPARE_OP(arg=3, lineno=1155)
DEBUG:numba.core.byteflow:stack ['$108call_method.3', '$116call_method.7']
DEBUG:numba.core.byteflow:dispatch pc=120, inst=RETURN_VALUE(arg=None, lineno=1155)
DEBUG:numba.core.byteflow:stack ['$118compare_op.8']
DEBUG:numba.core.byteflow:end state. edges=[]
DEBUG:numba.core.byteflow:pending: deque([State(pc_initial=78 nstack_initial=0)])
DEBUG:numba.core.byteflow:-------------------------Prune PHIs-------------------------
DEBUG:numba.core.byteflow:Used_phis: defaultdict(<class 'set'>,
            {State(pc_initial=0 nstack_initial=0): set(),
             State(pc_initial=24 nstack_initial=1): {'$phi24.0'},
             State(pc_initial=30 nstack_initial=0): set(),
             State(pc_initial=32 nstack_initial=1): set(),
             State(pc_initial=36 nstack_initial=0): set(),
             State(pc_initial=40 nstack_initial=0): set(),
             State(pc_initial=62 nstack_initial=1): {'$phi62.0'},
             State(pc_initial=68 nstack_initial=0): set(),
             State(pc_initial=70 nstack_initial=1): set(),
             State(pc_initial=74 nstack_initial=0): set(),
             State(pc_initial=78 nstack_initial=0): set(),
             State(pc_initial=82 nstack_initial=0): set(),
             State(pc_initial=102 nstack_initial=0): set()})
DEBUG:numba.core.byteflow:defmap: {'$phi24.0': State(pc_initial=0 nstack_initial=0),
 '$phi32.0': State(pc_initial=0 nstack_initial=0),
 '$phi62.0': State(pc_initial=40 nstack_initial=0),
 '$phi70.0': State(pc_initial=40 nstack_initial=0)}
DEBUG:numba.core.byteflow:phismap: defaultdict(<class 'set'>,
            {'$phi24.0': {('$16dup_top.6',
                           State(pc_initial=0 nstack_initial=0))},
             '$phi32.0': {('$16dup_top.6',
                           State(pc_initial=0 nstack_initial=0))},
             '$phi62.0': {('$54dup_top.6',
                           State(pc_initial=40 nstack_initial=0))},
             '$phi70.0': {('$54dup_top.6',
                           State(pc_initial=40 nstack_initial=0))}})
DEBUG:numba.core.byteflow:changing phismap: defaultdict(<class 'set'>,
            {'$phi24.0': {('$16dup_top.6',
                           State(pc_initial=0 nstack_initial=0))},
             '$phi32.0': {('$16dup_top.6',
                           State(pc_initial=0 nstack_initial=0))},
             '$phi62.0': {('$54dup_top.6',
                           State(pc_initial=40 nstack_initial=0))},
             '$phi70.0': {('$54dup_top.6',
                           State(pc_initial=40 nstack_initial=0))}})
DEBUG:numba.core.byteflow:keep phismap: {'$phi24.0': {('$16dup_top.6', State(pc_initial=0 nstack_initial=0))},
 '$phi62.0': {('$54dup_top.6', State(pc_initial=40 nstack_initial=0))}}
DEBUG:numba.core.byteflow:new_out: defaultdict(<class 'dict'>,
            {State(pc_initial=0 nstack_initial=0): {'$phi24.0': '$16dup_top.6'},
             State(pc_initial=40 nstack_initial=0): {'$phi62.0': '$54dup_top.6'}})
DEBUG:numba.core.byteflow:----------------------DONE Prune PHIs-----------------------
DEBUG:numba.core.byteflow:block_infos State(pc_initial=0 nstack_initial=0):
AdaptBlockInfo(insts=((0, {}), (2, {'res': '$x2.0'}), (4, {'res': '$const4.1'}), (6, {'index': '$const4.1', 'target': '$x2.0', 'res': '$6binary_subscr.2'}), (8, {'value': '$6binary_subscr.2'}), (10, {'res': '$threshold10.3'}), (12, {'value': '$threshold10.3', 'res': '$12unary_negative.4'}), (14, {'res': '$x014.5'}), (16, {'orig': ['$x014.5'], 'duped': ['$16dup_top.6']}), (20, {'lhs': '$12unary_negative.4', 'rhs': '$x014.5', 'res': '$20compare_op.7'}), (22, {'pred': '$20compare_op.7'})), outgoing_phis={'$phi24.0': '$16dup_top.6'}, blockstack=(), active_try_block=None, outgoing_edgepushed={24: ('$16dup_top.6',), 32: ('$16dup_top.6',)})
DEBUG:numba.core.byteflow:block_infos State(pc_initial=24 nstack_initial=1):
AdaptBlockInfo(insts=((24, {'res': '$threshold24.1'}), (26, {'lhs': '$phi24.0', 'rhs': '$threshold24.1', 'res': '$26compare_op.2'}), (28, {'pred': '$26compare_op.2'})), outgoing_phis={}, blockstack=(), active_try_block=None, outgoing_edgepushed={30: (), 40: ()})
DEBUG:numba.core.byteflow:block_infos State(pc_initial=30 nstack_initial=0):
AdaptBlockInfo(insts=((30, {}),), outgoing_phis={}, blockstack=(), active_try_block=None, outgoing_edgepushed={36: ()})
DEBUG:numba.core.byteflow:block_infos State(pc_initial=32 nstack_initial=1):
AdaptBlockInfo(insts=((34, {}),), outgoing_phis={}, blockstack=(), active_try_block=None, outgoing_edgepushed={40: ()})
DEBUG:numba.core.byteflow:block_infos State(pc_initial=36 nstack_initial=0):
AdaptBlockInfo(insts=((36, {'res': '$const36.0'}), (38, {'value': '$const36.0'})), outgoing_phis={}, blockstack=(), active_try_block=None, outgoing_edgepushed={40: ()})
DEBUG:numba.core.byteflow:block_infos State(pc_initial=40 nstack_initial=0):
AdaptBlockInfo(insts=((40, {'res': '$x40.0'}), (42, {'res': '$const42.1'}), (44, {'index': '$const42.1', 'target': '$x40.0', 'res': '$44binary_subscr.2'}), (46, {'value': '$44binary_subscr.2'}), (48, {'res': '$threshold48.3'}), (50, {'value': '$threshold48.3', 'res': '$50unary_negative.4'}), (52, {'res': '$x152.5'}), (54, {'orig': ['$x152.5'], 'duped': ['$54dup_top.6']}), (58, {'lhs': '$50unary_negative.4', 'rhs': '$x152.5', 'res': '$58compare_op.7'}), (60, {'pred': '$58compare_op.7'})), outgoing_phis={'$phi62.0': '$54dup_top.6'}, blockstack=(), active_try_block=None, outgoing_edgepushed={62: ('$54dup_top.6',), 70: ('$54dup_top.6',)})
DEBUG:numba.core.byteflow:block_infos State(pc_initial=62 nstack_initial=1):
AdaptBlockInfo(insts=((62, {'res': '$threshold62.1'}), (64, {'lhs': '$phi62.0', 'rhs': '$threshold62.1', 'res': '$64compare_op.2'}), (66, {'pred': '$64compare_op.2'})), outgoing_phis={}, blockstack=(), active_try_block=None, outgoing_edgepushed={68: (), 78: ()})
DEBUG:numba.core.byteflow:block_infos State(pc_initial=68 nstack_initial=0):
AdaptBlockInfo(insts=((68, {}),), outgoing_phis={}, blockstack=(), active_try_block=None, outgoing_edgepushed={74: ()})
DEBUG:numba.core.byteflow:block_infos State(pc_initial=70 nstack_initial=1):
AdaptBlockInfo(insts=((72, {}),), outgoing_phis={}, blockstack=(), active_try_block=None, outgoing_edgepushed={78: ()})
DEBUG:numba.core.byteflow:block_infos State(pc_initial=74 nstack_initial=0):
AdaptBlockInfo(insts=((74, {'res': '$const74.0'}), (76, {'value': '$const74.0'})), outgoing_phis={}, blockstack=(), active_try_block=None, outgoing_edgepushed={78: ()})
DEBUG:numba.core.byteflow:block_infos State(pc_initial=78 nstack_initial=0):
AdaptBlockInfo(insts=((78, {'res': '$zero_pos78.0'}), (80, {'pred': '$zero_pos78.0'})), outgoing_phis={}, blockstack=(), active_try_block=None, outgoing_edgepushed={82: (), 102: ()})
DEBUG:numba.core.byteflow:block_infos State(pc_initial=82 nstack_initial=0):
AdaptBlockInfo(insts=((82, {'res': '$82load_global.0'}), (84, {'item': '$82load_global.0', 'res': '$84load_method.1'}), (86, {'res': '$x086.2'}), (88, {'func': '$84load_method.1', 'args': ['$x086.2'], 'res': '$88call_method.3'}), (90, {'res': '$90load_global.4'}), (92, {'item': '$90load_global.4', 'res': '$92load_method.5'}), (94, {'res': '$x194.6'}), (96, {'func': '$92load_method.5', 'args': ['$x194.6'], 'res': '$96call_method.7'}), (98, {'lhs': '$88call_method.3', 'rhs': '$96call_method.7', 'res': '$98compare_op.8'}), (100, {'retval': '$98compare_op.8', 'castval': '$100return_value.9'})), outgoing_phis={}, blockstack=(), active_try_block=None, outgoing_edgepushed={})
DEBUG:numba.core.byteflow:block_infos State(pc_initial=102 nstack_initial=0):
AdaptBlockInfo(insts=((102, {'res': '$102load_global.0'}), (104, {'item': '$102load_global.0', 'res': '$104load_method.1'}), (106, {'res': '$x0106.2'}), (108, {'func': '$104load_method.1', 'args': ['$x0106.2'], 'res': '$108call_method.3'}), (110, {'res': '$110load_global.4'}), (112, {'item': '$110load_global.4', 'res': '$112load_method.5'}), (114, {'res': '$x1114.6'}), (116, {'func': '$112load_method.5', 'args': ['$x1114.6'], 'res': '$116call_method.7'}), (118, {'lhs': '$108call_method.3', 'rhs': '$116call_method.7', 'res': '$118compare_op.8'}), (120, {'retval': '$118compare_op.8', 'castval': '$120return_value.9'})), outgoing_phis={}, blockstack=(), active_try_block=None, outgoing_edgepushed={})
DEBUG:numba.core.interpreter:label 0:
    x = arg(0, name=x)                       ['x']
    threshold = arg(1, name=threshold)       ['threshold']
    zero_pos = arg(2, name=zero_pos)         ['zero_pos']
    $const4.1 = const(int, 0)                ['$const4.1']
    x0 = getitem(value=x, index=$const4.1, fn=<built-in function getitem>) ['$const4.1', 'x', 'x0']
    $12unary_negative.4 = unary(fn=<built-in function neg>, value=threshold) ['$12unary_negative.4', 'threshold']
    $20compare_op.7 = $12unary_negative.4 <= x0 ['$12unary_negative.4', '$20compare_op.7', 'x0']
    bool22 = global(bool: <class 'bool'>)    ['bool22']
    $22pred = call bool22($20compare_op.7, func=bool22, args=(Var($20compare_op.7, audio.py:1145),), kws=(), vararg=None, varkwarg=None, target=None) ['$20compare_op.7', '$22pred', 'bool22']
    $phi24.0 = x0                            ['$phi24.0', 'x0']
    branch $22pred, 24, 32                   ['$22pred']
label 24:
    $26compare_op.2 = $phi24.0 <= threshold  ['$26compare_op.2', '$phi24.0', 'threshold']
    bool28 = global(bool: <class 'bool'>)    ['bool28']
    $28pred = call bool28($26compare_op.2, func=bool28, args=(Var($26compare_op.2, audio.py:1145),), kws=(), vararg=None, varkwarg=None, target=None) ['$26compare_op.2', '$28pred', 'bool28']
    branch $28pred, 30, 40                   ['$28pred']
label 30:
    jump 36                                  []
label 32:
    jump 40                                  []
label 36:
    x0 = const(int, 0)                       ['x0']
    jump 40                                  []
label 40:
    $const42.1 = const(int, -1)              ['$const42.1']
    x1 = getitem(value=x, index=$const42.1, fn=<built-in function getitem>) ['$const42.1', 'x', 'x1']
    $50unary_negative.4 = unary(fn=<built-in function neg>, value=threshold) ['$50unary_negative.4', 'threshold']
    $58compare_op.7 = $50unary_negative.4 <= x1 ['$50unary_negative.4', '$58compare_op.7', 'x1']
    bool60 = global(bool: <class 'bool'>)    ['bool60']
    $60pred = call bool60($58compare_op.7, func=bool60, args=(Var($58compare_op.7, audio.py:1149),), kws=(), vararg=None, varkwarg=None, target=None) ['$58compare_op.7', '$60pred', 'bool60']
    $phi62.0 = x1                            ['$phi62.0', 'x1']
    branch $60pred, 62, 70                   ['$60pred']
label 62:
    $64compare_op.2 = $phi62.0 <= threshold  ['$64compare_op.2', '$phi62.0', 'threshold']
    bool66 = global(bool: <class 'bool'>)    ['bool66']
    $66pred = call bool66($64compare_op.2, func=bool66, args=(Var($64compare_op.2, audio.py:1149),), kws=(), vararg=None, varkwarg=None, target=None) ['$64compare_op.2', '$66pred', 'bool66']
    branch $66pred, 68, 78                   ['$66pred']
label 68:
    jump 74                                  []
label 70:
    jump 78                                  []
label 74:
    x1 = const(int, 0)                       ['x1']
    jump 78                                  []
label 78:
    bool80 = global(bool: <class 'bool'>)    ['bool80']
    $80pred = call bool80(zero_pos, func=bool80, args=(Var(zero_pos, audio.py:1141),), kws=(), vararg=None, varkwarg=None, target=None) ['$80pred', 'bool80', 'zero_pos']
    branch $80pred, 82, 102                  ['$80pred']
label 82:
    $82load_global.0 = global(np: <module 'numpy' from 'C:\\Users\\taskm\\anaconda3\\envs\\taskai\\lib\\site-packages\\numpy\\__init__.py'>) ['$82load_global.0']
    $84load_method.1 = getattr(value=$82load_global.0, attr=signbit) ['$82load_global.0', '$84load_method.1']
    $88call_method.3 = call $84load_method.1(x0, func=$84load_method.1, args=[Var(x0, audio.py:1144)], kws=(), vararg=None, varkwarg=None, target=None) ['$84load_method.1', '$88call_method.3', 'x0']
    $90load_global.4 = global(np: <module 'numpy' from 'C:\\Users\\taskm\\anaconda3\\envs\\taskai\\lib\\site-packages\\numpy\\__init__.py'>) ['$90load_global.4']
    $92load_method.5 = getattr(value=$90load_global.4, attr=signbit) ['$90load_global.4', '$92load_method.5']
    $96call_method.7 = call $92load_method.5(x1, func=$92load_method.5, args=[Var(x1, audio.py:1148)], kws=(), vararg=None, varkwarg=None, target=None) ['$92load_method.5', '$96call_method.7', 'x1']
    $98compare_op.8 = $88call_method.3 != $96call_method.7 ['$88call_method.3', '$96call_method.7', '$98compare_op.8']
    $100return_value.9 = cast(value=$98compare_op.8) ['$100return_value.9', '$98compare_op.8']
    return $100return_value.9                ['$100return_value.9']
label 102:
    $102load_global.0 = global(np: <module 'numpy' from 'C:\\Users\\taskm\\anaconda3\\envs\\taskai\\lib\\site-packages\\numpy\\__init__.py'>) ['$102load_global.0']
    $104load_method.1 = getattr(value=$102load_global.0, attr=sign) ['$102load_global.0', '$104load_method.1']
    $108call_method.3 = call $104load_method.1(x0, func=$104load_method.1, args=[Var(x0, audio.py:1144)], kws=(), vararg=None, varkwarg=None, target=None) ['$104load_method.1', '$108call_method.3', 'x0']
    $110load_global.4 = global(np: <module 'numpy' from 'C:\\Users\\taskm\\anaconda3\\envs\\taskai\\lib\\site-packages\\numpy\\__init__.py'>) ['$110load_global.4']
    $112load_method.5 = getattr(value=$110load_global.4, attr=sign) ['$110load_global.4', '$112load_method.5']
    $116call_method.7 = call $112load_method.5(x1, func=$112load_method.5, args=[Var(x1, audio.py:1148)], kws=(), vararg=None, varkwarg=None, target=None) ['$112load_method.5', '$116call_method.7', 'x1']
    $118compare_op.8 = $108call_method.3 != $116call_method.7 ['$108call_method.3', '$116call_method.7', '$118compare_op.8']
    $120return_value.9 = cast(value=$118compare_op.8) ['$118compare_op.8', '$120return_value.9']
    return $120return_value.9                ['$120return_value.9']

DEBUG:numba.core.byteflow:bytecode dump:
>          0	NOP(arg=None, lineno=1039)
           2	LOAD_FAST(arg=0, lineno=1042)
           4	LOAD_CONST(arg=1, lineno=1042)
           6	BINARY_SUBSCR(arg=None, lineno=1042)
           8	LOAD_FAST(arg=0, lineno=1042)
          10	LOAD_CONST(arg=2, lineno=1042)
          12	BINARY_SUBSCR(arg=None, lineno=1042)
          14	COMPARE_OP(arg=4, lineno=1042)
          16	LOAD_FAST(arg=0, lineno=1042)
          18	LOAD_CONST(arg=1, lineno=1042)
          20	BINARY_SUBSCR(arg=None, lineno=1042)
          22	LOAD_FAST(arg=0, lineno=1042)
          24	LOAD_CONST(arg=3, lineno=1042)
          26	BINARY_SUBSCR(arg=None, lineno=1042)
          28	COMPARE_OP(arg=5, lineno=1042)
          30	BINARY_AND(arg=None, lineno=1042)
          32	RETURN_VALUE(arg=None, lineno=1042)
DEBUG:numba.core.byteflow:pending: deque([State(pc_initial=0 nstack_initial=0)])
DEBUG:numba.core.byteflow:stack: []
DEBUG:numba.core.byteflow:state.pc_initial: State(pc_initial=0 nstack_initial=0)
DEBUG:numba.core.byteflow:dispatch pc=0, inst=NOP(arg=None, lineno=1039)
DEBUG:numba.core.byteflow:stack []
DEBUG:numba.core.byteflow:dispatch pc=2, inst=LOAD_FAST(arg=0, lineno=1042)
DEBUG:numba.core.byteflow:stack []
DEBUG:numba.core.byteflow:dispatch pc=4, inst=LOAD_CONST(arg=1, lineno=1042)
DEBUG:numba.core.byteflow:stack ['$x2.0']
DEBUG:numba.core.byteflow:dispatch pc=6, inst=BINARY_SUBSCR(arg=None, lineno=1042)
DEBUG:numba.core.byteflow:stack ['$x2.0', '$const4.1']
DEBUG:numba.core.byteflow:dispatch pc=8, inst=LOAD_FAST(arg=0, lineno=1042)
DEBUG:numba.core.byteflow:stack ['$6binary_subscr.2']
DEBUG:numba.core.byteflow:dispatch pc=10, inst=LOAD_CONST(arg=2, lineno=1042)
DEBUG:numba.core.byteflow:stack ['$6binary_subscr.2', '$x8.3']
DEBUG:numba.core.byteflow:dispatch pc=12, inst=BINARY_SUBSCR(arg=None, lineno=1042)
DEBUG:numba.core.byteflow:stack ['$6binary_subscr.2', '$x8.3', '$const10.4']
DEBUG:numba.core.byteflow:dispatch pc=14, inst=COMPARE_OP(arg=4, lineno=1042)
DEBUG:numba.core.byteflow:stack ['$6binary_subscr.2', '$12binary_subscr.5']
DEBUG:numba.core.byteflow:dispatch pc=16, inst=LOAD_FAST(arg=0, lineno=1042)
DEBUG:numba.core.byteflow:stack ['$14compare_op.6']
DEBUG:numba.core.byteflow:dispatch pc=18, inst=LOAD_CONST(arg=1, lineno=1042)
DEBUG:numba.core.byteflow:stack ['$14compare_op.6', '$x16.7']
DEBUG:numba.core.byteflow:dispatch pc=20, inst=BINARY_SUBSCR(arg=None, lineno=1042)
DEBUG:numba.core.byteflow:stack ['$14compare_op.6', '$x16.7', '$const18.8']
DEBUG:numba.core.byteflow:dispatch pc=22, inst=LOAD_FAST(arg=0, lineno=1042)
DEBUG:numba.core.byteflow:stack ['$14compare_op.6', '$20binary_subscr.9']
DEBUG:numba.core.byteflow:dispatch pc=24, inst=LOAD_CONST(arg=3, lineno=1042)
DEBUG:numba.core.byteflow:stack ['$14compare_op.6', '$20binary_subscr.9', '$x22.10']
DEBUG:numba.core.byteflow:dispatch pc=26, inst=BINARY_SUBSCR(arg=None, lineno=1042)
DEBUG:numba.core.byteflow:stack ['$14compare_op.6', '$20binary_subscr.9', '$x22.10', '$const24.11']
DEBUG:numba.core.byteflow:dispatch pc=28, inst=COMPARE_OP(arg=5, lineno=1042)
DEBUG:numba.core.byteflow:stack ['$14compare_op.6', '$20binary_subscr.9', '$26binary_subscr.12']
DEBUG:numba.core.byteflow:dispatch pc=30, inst=BINARY_AND(arg=None, lineno=1042)
DEBUG:numba.core.byteflow:stack ['$14compare_op.6', '$28compare_op.13']
DEBUG:numba.core.byteflow:dispatch pc=32, inst=RETURN_VALUE(arg=None, lineno=1042)
DEBUG:numba.core.byteflow:stack ['$30binary_and.14']
DEBUG:numba.core.byteflow:end state. edges=[]
DEBUG:numba.core.byteflow:-------------------------Prune PHIs-------------------------
DEBUG:numba.core.byteflow:Used_phis: defaultdict(<class 'set'>, {State(pc_initial=0 nstack_initial=0): set()})
DEBUG:numba.core.byteflow:defmap: {}
DEBUG:numba.core.byteflow:phismap: defaultdict(<class 'set'>, {})
DEBUG:numba.core.byteflow:changing phismap: defaultdict(<class 'set'>, {})
DEBUG:numba.core.byteflow:keep phismap: {}
DEBUG:numba.core.byteflow:new_out: defaultdict(<class 'dict'>, {})
DEBUG:numba.core.byteflow:----------------------DONE Prune PHIs-----------------------
DEBUG:numba.core.byteflow:block_infos State(pc_initial=0 nstack_initial=0):
AdaptBlockInfo(insts=((0, {}), (2, {'res': '$x2.0'}), (4, {'res': '$const4.1'}), (6, {'index': '$const4.1', 'target': '$x2.0', 'res': '$6binary_subscr.2'}), (8, {'res': '$x8.3'}), (10, {'res': '$const10.4'}), (12, {'index': '$const10.4', 'target': '$x8.3', 'res': '$12binary_subscr.5'}), (14, {'lhs': '$6binary_subscr.2', 'rhs': '$12binary_subscr.5', 'res': '$14compare_op.6'}), (16, {'res': '$x16.7'}), (18, {'res': '$const18.8'}), (20, {'index': '$const18.8', 'target': '$x16.7', 'res': '$20binary_subscr.9'}), (22, {'res': '$x22.10'}), (24, {'res': '$const24.11'}), (26, {'index': '$const24.11', 'target': '$x22.10', 'res': '$26binary_subscr.12'}), (28, {'lhs': '$20binary_subscr.9', 'rhs': '$26binary_subscr.12', 'res': '$28compare_op.13'}), (30, {'lhs': '$14compare_op.6', 'rhs': '$28compare_op.13', 'res': '$30binary_and.14'}), (32, {'retval': '$30binary_and.14', 'castval': '$32return_value.15'})), outgoing_phis={}, blockstack=(), active_try_block=None, outgoing_edgepushed={})
DEBUG:numba.core.interpreter:label 0:
    x = arg(0, name=x)                       ['x']
    $const4.1 = const(int, 0)                ['$const4.1']
    $6binary_subscr.2 = getitem(value=x, index=$const4.1, fn=<built-in function getitem>) ['$6binary_subscr.2', '$const4.1', 'x']
    $const10.4 = const(int, -1)              ['$const10.4']
    $12binary_subscr.5 = getitem(value=x, index=$const10.4, fn=<built-in function getitem>) ['$12binary_subscr.5', '$const10.4', 'x']
    $14compare_op.6 = $6binary_subscr.2 > $12binary_subscr.5 ['$12binary_subscr.5', '$14compare_op.6', '$6binary_subscr.2']
    $const18.8 = const(int, 0)               ['$const18.8']
    $20binary_subscr.9 = getitem(value=x, index=$const18.8, fn=<built-in function getitem>) ['$20binary_subscr.9', '$const18.8', 'x']
    $const24.11 = const(int, 1)              ['$const24.11']
    $26binary_subscr.12 = getitem(value=x, index=$const24.11, fn=<built-in function getitem>) ['$26binary_subscr.12', '$const24.11', 'x']
    $28compare_op.13 = $20binary_subscr.9 >= $26binary_subscr.12 ['$20binary_subscr.9', '$26binary_subscr.12', '$28compare_op.13']
    $30binary_and.14 = $14compare_op.6 & $28compare_op.13 ['$14compare_op.6', '$28compare_op.13', '$30binary_and.14']
    $32return_value.15 = cast(value=$30binary_and.14) ['$30binary_and.14', '$32return_value.15']
    return $32return_value.15                ['$32return_value.15']

DEBUG:numba.core.byteflow:bytecode dump:
>          0	NOP(arg=None, lineno=1045)
           2	LOAD_FAST(arg=0, lineno=1048)
           4	LOAD_CONST(arg=1, lineno=1048)
           6	BINARY_SUBSCR(arg=None, lineno=1048)
           8	LOAD_FAST(arg=0, lineno=1048)
          10	LOAD_CONST(arg=2, lineno=1048)
          12	BINARY_SUBSCR(arg=None, lineno=1048)
          14	COMPARE_OP(arg=0, lineno=1048)
          16	LOAD_FAST(arg=0, lineno=1048)
          18	LOAD_CONST(arg=1, lineno=1048)
          20	BINARY_SUBSCR(arg=None, lineno=1048)
          22	LOAD_FAST(arg=0, lineno=1048)
          24	LOAD_CONST(arg=3, lineno=1048)
          26	BINARY_SUBSCR(arg=None, lineno=1048)
          28	COMPARE_OP(arg=1, lineno=1048)
          30	BINARY_AND(arg=None, lineno=1048)
          32	RETURN_VALUE(arg=None, lineno=1048)
DEBUG:numba.core.byteflow:pending: deque([State(pc_initial=0 nstack_initial=0)])
DEBUG:numba.core.byteflow:stack: []
DEBUG:numba.core.byteflow:state.pc_initial: State(pc_initial=0 nstack_initial=0)
DEBUG:numba.core.byteflow:dispatch pc=0, inst=NOP(arg=None, lineno=1045)
DEBUG:numba.core.byteflow:stack []
DEBUG:numba.core.byteflow:dispatch pc=2, inst=LOAD_FAST(arg=0, lineno=1048)
DEBUG:numba.core.byteflow:stack []
DEBUG:numba.core.byteflow:dispatch pc=4, inst=LOAD_CONST(arg=1, lineno=1048)
DEBUG:numba.core.byteflow:stack ['$x2.0']
DEBUG:numba.core.byteflow:dispatch pc=6, inst=BINARY_SUBSCR(arg=None, lineno=1048)
DEBUG:numba.core.byteflow:stack ['$x2.0', '$const4.1']
DEBUG:numba.core.byteflow:dispatch pc=8, inst=LOAD_FAST(arg=0, lineno=1048)
DEBUG:numba.core.byteflow:stack ['$6binary_subscr.2']
DEBUG:numba.core.byteflow:dispatch pc=10, inst=LOAD_CONST(arg=2, lineno=1048)
DEBUG:numba.core.byteflow:stack ['$6binary_subscr.2', '$x8.3']
DEBUG:numba.core.byteflow:dispatch pc=12, inst=BINARY_SUBSCR(arg=None, lineno=1048)
DEBUG:numba.core.byteflow:stack ['$6binary_subscr.2', '$x8.3', '$const10.4']
DEBUG:numba.core.byteflow:dispatch pc=14, inst=COMPARE_OP(arg=0, lineno=1048)
DEBUG:numba.core.byteflow:stack ['$6binary_subscr.2', '$12binary_subscr.5']
DEBUG:numba.core.byteflow:dispatch pc=16, inst=LOAD_FAST(arg=0, lineno=1048)
DEBUG:numba.core.byteflow:stack ['$14compare_op.6']
DEBUG:numba.core.byteflow:dispatch pc=18, inst=LOAD_CONST(arg=1, lineno=1048)
DEBUG:numba.core.byteflow:stack ['$14compare_op.6', '$x16.7']
DEBUG:numba.core.byteflow:dispatch pc=20, inst=BINARY_SUBSCR(arg=None, lineno=1048)
DEBUG:numba.core.byteflow:stack ['$14compare_op.6', '$x16.7', '$const18.8']
DEBUG:numba.core.byteflow:dispatch pc=22, inst=LOAD_FAST(arg=0, lineno=1048)
DEBUG:numba.core.byteflow:stack ['$14compare_op.6', '$20binary_subscr.9']
DEBUG:numba.core.byteflow:dispatch pc=24, inst=LOAD_CONST(arg=3, lineno=1048)
DEBUG:numba.core.byteflow:stack ['$14compare_op.6', '$20binary_subscr.9', '$x22.10']
DEBUG:numba.core.byteflow:dispatch pc=26, inst=BINARY_SUBSCR(arg=None, lineno=1048)
DEBUG:numba.core.byteflow:stack ['$14compare_op.6', '$20binary_subscr.9', '$x22.10', '$const24.11']
DEBUG:numba.core.byteflow:dispatch pc=28, inst=COMPARE_OP(arg=1, lineno=1048)
DEBUG:numba.core.byteflow:stack ['$14compare_op.6', '$20binary_subscr.9', '$26binary_subscr.12']
DEBUG:numba.core.byteflow:dispatch pc=30, inst=BINARY_AND(arg=None, lineno=1048)
DEBUG:numba.core.byteflow:stack ['$14compare_op.6', '$28compare_op.13']
DEBUG:numba.core.byteflow:dispatch pc=32, inst=RETURN_VALUE(arg=None, lineno=1048)
DEBUG:numba.core.byteflow:stack ['$30binary_and.14']
DEBUG:numba.core.byteflow:end state. edges=[]
DEBUG:numba.core.byteflow:-------------------------Prune PHIs-------------------------
DEBUG:numba.core.byteflow:Used_phis: defaultdict(<class 'set'>, {State(pc_initial=0 nstack_initial=0): set()})
DEBUG:numba.core.byteflow:defmap: {}
DEBUG:numba.core.byteflow:phismap: defaultdict(<class 'set'>, {})
DEBUG:numba.core.byteflow:changing phismap: defaultdict(<class 'set'>, {})
DEBUG:numba.core.byteflow:keep phismap: {}
DEBUG:numba.core.byteflow:new_out: defaultdict(<class 'dict'>, {})
DEBUG:numba.core.byteflow:----------------------DONE Prune PHIs-----------------------
DEBUG:numba.core.byteflow:block_infos State(pc_initial=0 nstack_initial=0):
AdaptBlockInfo(insts=((0, {}), (2, {'res': '$x2.0'}), (4, {'res': '$const4.1'}), (6, {'index': '$const4.1', 'target': '$x2.0', 'res': '$6binary_subscr.2'}), (8, {'res': '$x8.3'}), (10, {'res': '$const10.4'}), (12, {'index': '$const10.4', 'target': '$x8.3', 'res': '$12binary_subscr.5'}), (14, {'lhs': '$6binary_subscr.2', 'rhs': '$12binary_subscr.5', 'res': '$14compare_op.6'}), (16, {'res': '$x16.7'}), (18, {'res': '$const18.8'}), (20, {'index': '$const18.8', 'target': '$x16.7', 'res': '$20binary_subscr.9'}), (22, {'res': '$x22.10'}), (24, {'res': '$const24.11'}), (26, {'index': '$const24.11', 'target': '$x22.10', 'res': '$26binary_subscr.12'}), (28, {'lhs': '$20binary_subscr.9', 'rhs': '$26binary_subscr.12', 'res': '$28compare_op.13'}), (30, {'lhs': '$14compare_op.6', 'rhs': '$28compare_op.13', 'res': '$30binary_and.14'}), (32, {'retval': '$30binary_and.14', 'castval': '$32return_value.15'})), outgoing_phis={}, blockstack=(), active_try_block=None, outgoing_edgepushed={})
DEBUG:numba.core.interpreter:label 0:
    x = arg(0, name=x)                       ['x']
    $const4.1 = const(int, 0)                ['$const4.1']
    $6binary_subscr.2 = getitem(value=x, index=$const4.1, fn=<built-in function getitem>) ['$6binary_subscr.2', '$const4.1', 'x']
    $const10.4 = const(int, -1)              ['$const10.4']
    $12binary_subscr.5 = getitem(value=x, index=$const10.4, fn=<built-in function getitem>) ['$12binary_subscr.5', '$const10.4', 'x']
    $14compare_op.6 = $6binary_subscr.2 < $12binary_subscr.5 ['$12binary_subscr.5', '$14compare_op.6', '$6binary_subscr.2']
    $const18.8 = const(int, 0)               ['$const18.8']
    $20binary_subscr.9 = getitem(value=x, index=$const18.8, fn=<built-in function getitem>) ['$20binary_subscr.9', '$const18.8', 'x']
    $const24.11 = const(int, 1)              ['$const24.11']
    $26binary_subscr.12 = getitem(value=x, index=$const24.11, fn=<built-in function getitem>) ['$26binary_subscr.12', '$const24.11', 'x']
    $28compare_op.13 = $20binary_subscr.9 <= $26binary_subscr.12 ['$20binary_subscr.9', '$26binary_subscr.12', '$28compare_op.13']
    $30binary_and.14 = $14compare_op.6 & $28compare_op.13 ['$14compare_op.6', '$28compare_op.13', '$30binary_and.14']
    $32return_value.15 = cast(value=$30binary_and.14) ['$30binary_and.14', '$32return_value.15']
    return $32return_value.15                ['$32return_value.15']

DEBUG:httpx:load_ssl_context verify=True cert=None trust_env=True http2=False
DEBUG:httpx:load_verify_locations cafile='C:\\Users\\taskm\\anaconda3\\Library\\ssl\\cacert.pem'
DEBUG:httpx:load_ssl_context verify=True cert=None trust_env=True http2=False
DEBUG:httpx:load_verify_locations cafile='C:\\Users\\taskm\\anaconda3\\Library\\ssl\\cacert.pem'
DEBUG:httpx:load_ssl_context verify=True cert=None trust_env=True http2=False
DEBUG:httpx:load_verify_locations cafile='C:\\Users\\taskm\\anaconda3\\Library\\ssl\\cacert.pem'
DEBUG:httpx:load_ssl_context verify=True cert=None trust_env=True http2=False
DEBUG:httpx:load_verify_locations cafile='C:\\Users\\taskm\\anaconda3\\Library\\ssl\\cacert.pem'
DEBUG:httpx:load_ssl_context verify=True cert=None trust_env=True http2=False
DEBUG:httpx:load_verify_locations cafile='C:\\Users\\taskm\\anaconda3\\Library\\ssl\\cacert.pem'
DEBUG:httpx:load_ssl_context verify=True cert=None trust_env=True http2=False
DEBUG:httpx:load_verify_locations cafile='C:\\Users\\taskm\\anaconda3\\Library\\ssl\\cacert.pem'
ERROR:root:An error occurred:
Traceback (most recent call last):
  File "D:\github\chappymedium\chappie.py", line 184, in <module>
    window = ChappieGUI()
  File "D:\github\chappymedium\chappie.py", line 94, in __init__
    self.file_manager = FileManager()
NameError: name 'FileManager' is not defined
ERROR:root:An error occurred:
Traceback (most recent call last):
  File "D:\github\chappymedium\chappie.py", line 184, in <module>
    window = ChappieGUI()
  File "D:\github\chappymedium\chappie.py", line 94, in __init__
    self.file_manager = FileManager()
NameError: name 'FileManager' is not defined
ERROR:root:An error occurred:
Traceback (most recent call last):
  File "D:\github\chappymedium\chappie.py", line 205, in <module>
    window = ChappieGUI()
  File "D:\github\chappymedium\chappie.py", line 116, in __init__
    self.chapter_manager = ChapterManager()
NameError: name 'ChapterManager' is not defined
ERROR:root:An error occurred:
Traceback (most recent call last):
  File "D:\github\chappymedium\chappie.py", line 203, in <module>
    window = ChappieGUI()
  File "D:\github\chappymedium\chappie.py", line 114, in __init__
    self.chapter_manager = ChapterManager()
NameError: name 'ChapterManager' is not defined
DEBUG:httpx:load_ssl_context verify=True cert=None trust_env=True http2=False
DEBUG:httpx:load_verify_locations cafile='C:\\Users\\taskm\\anaconda3\\Library\\ssl\\cacert.pem'
DEBUG:httpx:load_ssl_context verify=True cert=None trust_env=True http2=False
DEBUG:httpx:load_verify_locations cafile='C:\\Users\\taskm\\anaconda3\\Library\\ssl\\cacert.pem'
DEBUG:numba.core.byteflow:bytecode dump:
>          0	NOP(arg=None, lineno=1141)
           2	LOAD_FAST(arg=0, lineno=1144)
           4	LOAD_CONST(arg=1, lineno=1144)
           6	BINARY_SUBSCR(arg=None, lineno=1144)
           8	STORE_FAST(arg=3, lineno=1144)
          10	LOAD_FAST(arg=1, lineno=1145)
          12	UNARY_NEGATIVE(arg=None, lineno=1145)
          14	LOAD_FAST(arg=3, lineno=1145)
          16	DUP_TOP(arg=None, lineno=1145)
          18	ROT_THREE(arg=None, lineno=1145)
          20	COMPARE_OP(arg=1, lineno=1145)
          22	POP_JUMP_IF_FALSE(arg=32, lineno=1145)
          24	LOAD_FAST(arg=1, lineno=1145)
          26	COMPARE_OP(arg=1, lineno=1145)
          28	POP_JUMP_IF_FALSE(arg=40, lineno=1145)
          30	JUMP_FORWARD(arg=4, lineno=1145)
>         32	POP_TOP(arg=None, lineno=1145)
          34	JUMP_FORWARD(arg=4, lineno=1145)
>         36	LOAD_CONST(arg=1, lineno=1146)
          38	STORE_FAST(arg=3, lineno=1146)
>         40	LOAD_FAST(arg=0, lineno=1148)
          42	LOAD_CONST(arg=2, lineno=1148)
          44	BINARY_SUBSCR(arg=None, lineno=1148)
          46	STORE_FAST(arg=4, lineno=1148)
          48	LOAD_FAST(arg=1, lineno=1149)
          50	UNARY_NEGATIVE(arg=None, lineno=1149)
          52	LOAD_FAST(arg=4, lineno=1149)
          54	DUP_TOP(arg=None, lineno=1149)
          56	ROT_THREE(arg=None, lineno=1149)
          58	COMPARE_OP(arg=1, lineno=1149)
          60	POP_JUMP_IF_FALSE(arg=70, lineno=1149)
          62	LOAD_FAST(arg=1, lineno=1149)
          64	COMPARE_OP(arg=1, lineno=1149)
          66	POP_JUMP_IF_FALSE(arg=78, lineno=1149)
          68	JUMP_FORWARD(arg=4, lineno=1149)
>         70	POP_TOP(arg=None, lineno=1149)
          72	JUMP_FORWARD(arg=4, lineno=1149)
>         74	LOAD_CONST(arg=1, lineno=1150)
          76	STORE_FAST(arg=4, lineno=1150)
>         78	LOAD_FAST(arg=2, lineno=1152)
          80	POP_JUMP_IF_FALSE(arg=102, lineno=1152)
          82	LOAD_GLOBAL(arg=0, lineno=1153)
          84	LOAD_METHOD(arg=1, lineno=1153)
          86	LOAD_FAST(arg=3, lineno=1153)
          88	CALL_METHOD(arg=1, lineno=1153)
          90	LOAD_GLOBAL(arg=0, lineno=1153)
          92	LOAD_METHOD(arg=1, lineno=1153)
          94	LOAD_FAST(arg=4, lineno=1153)
          96	CALL_METHOD(arg=1, lineno=1153)
          98	COMPARE_OP(arg=3, lineno=1153)
         100	RETURN_VALUE(arg=None, lineno=1153)
>        102	LOAD_GLOBAL(arg=0, lineno=1155)
         104	LOAD_METHOD(arg=2, lineno=1155)
         106	LOAD_FAST(arg=3, lineno=1155)
         108	CALL_METHOD(arg=1, lineno=1155)
         110	LOAD_GLOBAL(arg=0, lineno=1155)
         112	LOAD_METHOD(arg=2, lineno=1155)
         114	LOAD_FAST(arg=4, lineno=1155)
         116	CALL_METHOD(arg=1, lineno=1155)
         118	COMPARE_OP(arg=3, lineno=1155)
         120	RETURN_VALUE(arg=None, lineno=1155)
         122	LOAD_CONST(arg=3, lineno=1155)
         124	RETURN_VALUE(arg=None, lineno=1155)
DEBUG:numba.core.byteflow:pending: deque([State(pc_initial=0 nstack_initial=0)])
DEBUG:numba.core.byteflow:stack: []
DEBUG:numba.core.byteflow:state.pc_initial: State(pc_initial=0 nstack_initial=0)
DEBUG:numba.core.byteflow:dispatch pc=0, inst=NOP(arg=None, lineno=1141)
DEBUG:numba.core.byteflow:stack []
DEBUG:numba.core.byteflow:dispatch pc=2, inst=LOAD_FAST(arg=0, lineno=1144)
DEBUG:numba.core.byteflow:stack []
DEBUG:numba.core.byteflow:dispatch pc=4, inst=LOAD_CONST(arg=1, lineno=1144)
DEBUG:numba.core.byteflow:stack ['$x2.0']
DEBUG:numba.core.byteflow:dispatch pc=6, inst=BINARY_SUBSCR(arg=None, lineno=1144)
DEBUG:numba.core.byteflow:stack ['$x2.0', '$const4.1']
DEBUG:numba.core.byteflow:dispatch pc=8, inst=STORE_FAST(arg=3, lineno=1144)
DEBUG:numba.core.byteflow:stack ['$6binary_subscr.2']
DEBUG:numba.core.byteflow:dispatch pc=10, inst=LOAD_FAST(arg=1, lineno=1145)
DEBUG:numba.core.byteflow:stack []
DEBUG:numba.core.byteflow:dispatch pc=12, inst=UNARY_NEGATIVE(arg=None, lineno=1145)
DEBUG:numba.core.byteflow:stack ['$threshold10.3']
DEBUG:numba.core.byteflow:dispatch pc=14, inst=LOAD_FAST(arg=3, lineno=1145)
DEBUG:numba.core.byteflow:stack ['$12unary_negative.4']
DEBUG:numba.core.byteflow:dispatch pc=16, inst=DUP_TOP(arg=None, lineno=1145)
DEBUG:numba.core.byteflow:stack ['$12unary_negative.4', '$x014.5']
DEBUG:numba.core.byteflow:dispatch pc=18, inst=ROT_THREE(arg=None, lineno=1145)
DEBUG:numba.core.byteflow:stack ['$12unary_negative.4', '$x014.5', '$16dup_top.6']
DEBUG:numba.core.byteflow:dispatch pc=20, inst=COMPARE_OP(arg=1, lineno=1145)
DEBUG:numba.core.byteflow:stack ['$16dup_top.6', '$12unary_negative.4', '$x014.5']
DEBUG:numba.core.byteflow:dispatch pc=22, inst=POP_JUMP_IF_FALSE(arg=32, lineno=1145)
DEBUG:numba.core.byteflow:stack ['$16dup_top.6', '$20compare_op.7']
DEBUG:numba.core.byteflow:end state. edges=[Edge(pc=24, stack=('$16dup_top.6',), blockstack=(), npush=0), Edge(pc=32, stack=('$16dup_top.6',), blockstack=(), npush=0)]
DEBUG:numba.core.byteflow:pending: deque([State(pc_initial=24 nstack_initial=1), State(pc_initial=32 nstack_initial=1)])
DEBUG:numba.core.byteflow:stack: ['$phi24.0']
DEBUG:numba.core.byteflow:state.pc_initial: State(pc_initial=24 nstack_initial=1)
DEBUG:numba.core.byteflow:dispatch pc=24, inst=LOAD_FAST(arg=1, lineno=1145)
DEBUG:numba.core.byteflow:stack ['$phi24.0']
DEBUG:numba.core.byteflow:dispatch pc=26, inst=COMPARE_OP(arg=1, lineno=1145)
DEBUG:numba.core.byteflow:stack ['$phi24.0', '$threshold24.1']
DEBUG:numba.core.byteflow:dispatch pc=28, inst=POP_JUMP_IF_FALSE(arg=40, lineno=1145)
DEBUG:numba.core.byteflow:stack ['$26compare_op.2']
DEBUG:numba.core.byteflow:end state. edges=[Edge(pc=30, stack=(), blockstack=(), npush=0), Edge(pc=40, stack=(), blockstack=(), npush=0)]
DEBUG:numba.core.byteflow:pending: deque([State(pc_initial=32 nstack_initial=1), State(pc_initial=30 nstack_initial=0), State(pc_initial=40 nstack_initial=0)])
DEBUG:numba.core.byteflow:stack: ['$phi32.0']
DEBUG:numba.core.byteflow:state.pc_initial: State(pc_initial=32 nstack_initial=1)
DEBUG:numba.core.byteflow:dispatch pc=32, inst=POP_TOP(arg=None, lineno=1145)
DEBUG:numba.core.byteflow:stack ['$phi32.0']
DEBUG:numba.core.byteflow:dispatch pc=34, inst=JUMP_FORWARD(arg=4, lineno=1145)
DEBUG:numba.core.byteflow:stack []
DEBUG:numba.core.byteflow:end state. edges=[Edge(pc=40, stack=(), blockstack=(), npush=0)]
DEBUG:numba.core.byteflow:pending: deque([State(pc_initial=30 nstack_initial=0), State(pc_initial=40 nstack_initial=0), State(pc_initial=40 nstack_initial=0)])
DEBUG:numba.core.byteflow:stack: []
DEBUG:numba.core.byteflow:state.pc_initial: State(pc_initial=30 nstack_initial=0)
DEBUG:numba.core.byteflow:dispatch pc=30, inst=JUMP_FORWARD(arg=4, lineno=1145)
DEBUG:numba.core.byteflow:stack []
DEBUG:numba.core.byteflow:end state. edges=[Edge(pc=36, stack=(), blockstack=(), npush=0)]
DEBUG:numba.core.byteflow:pending: deque([State(pc_initial=40 nstack_initial=0), State(pc_initial=40 nstack_initial=0), State(pc_initial=36 nstack_initial=0)])
DEBUG:numba.core.byteflow:stack: []
DEBUG:numba.core.byteflow:state.pc_initial: State(pc_initial=40 nstack_initial=0)
DEBUG:numba.core.byteflow:dispatch pc=40, inst=LOAD_FAST(arg=0, lineno=1148)
DEBUG:numba.core.byteflow:stack []
DEBUG:numba.core.byteflow:dispatch pc=42, inst=LOAD_CONST(arg=2, lineno=1148)
DEBUG:numba.core.byteflow:stack ['$x40.0']
DEBUG:numba.core.byteflow:dispatch pc=44, inst=BINARY_SUBSCR(arg=None, lineno=1148)
DEBUG:numba.core.byteflow:stack ['$x40.0', '$const42.1']
DEBUG:numba.core.byteflow:dispatch pc=46, inst=STORE_FAST(arg=4, lineno=1148)
DEBUG:numba.core.byteflow:stack ['$44binary_subscr.2']
DEBUG:numba.core.byteflow:dispatch pc=48, inst=LOAD_FAST(arg=1, lineno=1149)
DEBUG:numba.core.byteflow:stack []
DEBUG:numba.core.byteflow:dispatch pc=50, inst=UNARY_NEGATIVE(arg=None, lineno=1149)
DEBUG:numba.core.byteflow:stack ['$threshold48.3']
DEBUG:numba.core.byteflow:dispatch pc=52, inst=LOAD_FAST(arg=4, lineno=1149)
DEBUG:numba.core.byteflow:stack ['$50unary_negative.4']
DEBUG:numba.core.byteflow:dispatch pc=54, inst=DUP_TOP(arg=None, lineno=1149)
DEBUG:numba.core.byteflow:stack ['$50unary_negative.4', '$x152.5']
DEBUG:numba.core.byteflow:dispatch pc=56, inst=ROT_THREE(arg=None, lineno=1149)
DEBUG:numba.core.byteflow:stack ['$50unary_negative.4', '$x152.5', '$54dup_top.6']
DEBUG:numba.core.byteflow:dispatch pc=58, inst=COMPARE_OP(arg=1, lineno=1149)
DEBUG:numba.core.byteflow:stack ['$54dup_top.6', '$50unary_negative.4', '$x152.5']
DEBUG:numba.core.byteflow:dispatch pc=60, inst=POP_JUMP_IF_FALSE(arg=70, lineno=1149)
DEBUG:numba.core.byteflow:stack ['$54dup_top.6', '$58compare_op.7']
DEBUG:numba.core.byteflow:end state. edges=[Edge(pc=62, stack=('$54dup_top.6',), blockstack=(), npush=0), Edge(pc=70, stack=('$54dup_top.6',), blockstack=(), npush=0)]
DEBUG:numba.core.byteflow:pending: deque([State(pc_initial=40 nstack_initial=0), State(pc_initial=36 nstack_initial=0), State(pc_initial=62 nstack_initial=1), State(pc_initial=70 nstack_initial=1)])
DEBUG:numba.core.byteflow:pending: deque([State(pc_initial=36 nstack_initial=0), State(pc_initial=62 nstack_initial=1), State(pc_initial=70 nstack_initial=1)])
DEBUG:numba.core.byteflow:stack: []
DEBUG:numba.core.byteflow:state.pc_initial: State(pc_initial=36 nstack_initial=0)
DEBUG:numba.core.byteflow:dispatch pc=36, inst=LOAD_CONST(arg=1, lineno=1146)
DEBUG:numba.core.byteflow:stack []
DEBUG:numba.core.byteflow:dispatch pc=38, inst=STORE_FAST(arg=3, lineno=1146)
DEBUG:numba.core.byteflow:stack ['$const36.0']
DEBUG:numba.core.byteflow:end state. edges=[Edge(pc=40, stack=(), blockstack=(), npush=0)]
DEBUG:numba.core.byteflow:pending: deque([State(pc_initial=62 nstack_initial=1), State(pc_initial=70 nstack_initial=1), State(pc_initial=40 nstack_initial=0)])
DEBUG:numba.core.byteflow:stack: ['$phi62.0']
DEBUG:numba.core.byteflow:state.pc_initial: State(pc_initial=62 nstack_initial=1)
DEBUG:numba.core.byteflow:dispatch pc=62, inst=LOAD_FAST(arg=1, lineno=1149)
DEBUG:numba.core.byteflow:stack ['$phi62.0']
DEBUG:numba.core.byteflow:dispatch pc=64, inst=COMPARE_OP(arg=1, lineno=1149)
DEBUG:numba.core.byteflow:stack ['$phi62.0', '$threshold62.1']
DEBUG:numba.core.byteflow:dispatch pc=66, inst=POP_JUMP_IF_FALSE(arg=78, lineno=1149)
DEBUG:numba.core.byteflow:stack ['$64compare_op.2']
DEBUG:numba.core.byteflow:end state. edges=[Edge(pc=68, stack=(), blockstack=(), npush=0), Edge(pc=78, stack=(), blockstack=(), npush=0)]
DEBUG:numba.core.byteflow:pending: deque([State(pc_initial=70 nstack_initial=1), State(pc_initial=40 nstack_initial=0), State(pc_initial=68 nstack_initial=0), State(pc_initial=78 nstack_initial=0)])
DEBUG:numba.core.byteflow:stack: ['$phi70.0']
DEBUG:numba.core.byteflow:state.pc_initial: State(pc_initial=70 nstack_initial=1)
DEBUG:numba.core.byteflow:dispatch pc=70, inst=POP_TOP(arg=None, lineno=1149)
DEBUG:numba.core.byteflow:stack ['$phi70.0']
DEBUG:numba.core.byteflow:dispatch pc=72, inst=JUMP_FORWARD(arg=4, lineno=1149)
DEBUG:numba.core.byteflow:stack []
DEBUG:numba.core.byteflow:end state. edges=[Edge(pc=78, stack=(), blockstack=(), npush=0)]
DEBUG:numba.core.byteflow:pending: deque([State(pc_initial=40 nstack_initial=0), State(pc_initial=68 nstack_initial=0), State(pc_initial=78 nstack_initial=0), State(pc_initial=78 nstack_initial=0)])
DEBUG:numba.core.byteflow:pending: deque([State(pc_initial=68 nstack_initial=0), State(pc_initial=78 nstack_initial=0), State(pc_initial=78 nstack_initial=0)])
DEBUG:numba.core.byteflow:stack: []
DEBUG:numba.core.byteflow:state.pc_initial: State(pc_initial=68 nstack_initial=0)
DEBUG:numba.core.byteflow:dispatch pc=68, inst=JUMP_FORWARD(arg=4, lineno=1149)
DEBUG:numba.core.byteflow:stack []
DEBUG:numba.core.byteflow:end state. edges=[Edge(pc=74, stack=(), blockstack=(), npush=0)]
DEBUG:numba.core.byteflow:pending: deque([State(pc_initial=78 nstack_initial=0), State(pc_initial=78 nstack_initial=0), State(pc_initial=74 nstack_initial=0)])
DEBUG:numba.core.byteflow:stack: []
DEBUG:numba.core.byteflow:state.pc_initial: State(pc_initial=78 nstack_initial=0)
DEBUG:numba.core.byteflow:dispatch pc=78, inst=LOAD_FAST(arg=2, lineno=1152)
DEBUG:numba.core.byteflow:stack []
DEBUG:numba.core.byteflow:dispatch pc=80, inst=POP_JUMP_IF_FALSE(arg=102, lineno=1152)
DEBUG:numba.core.byteflow:stack ['$zero_pos78.0']
DEBUG:numba.core.byteflow:end state. edges=[Edge(pc=82, stack=(), blockstack=(), npush=0), Edge(pc=102, stack=(), blockstack=(), npush=0)]
DEBUG:numba.core.byteflow:pending: deque([State(pc_initial=78 nstack_initial=0), State(pc_initial=74 nstack_initial=0), State(pc_initial=82 nstack_initial=0), State(pc_initial=102 nstack_initial=0)])
DEBUG:numba.core.byteflow:pending: deque([State(pc_initial=74 nstack_initial=0), State(pc_initial=82 nstack_initial=0), State(pc_initial=102 nstack_initial=0)])
DEBUG:numba.core.byteflow:stack: []
DEBUG:numba.core.byteflow:state.pc_initial: State(pc_initial=74 nstack_initial=0)
DEBUG:numba.core.byteflow:dispatch pc=74, inst=LOAD_CONST(arg=1, lineno=1150)
DEBUG:numba.core.byteflow:stack []
DEBUG:numba.core.byteflow:dispatch pc=76, inst=STORE_FAST(arg=4, lineno=1150)
DEBUG:numba.core.byteflow:stack ['$const74.0']
DEBUG:numba.core.byteflow:end state. edges=[Edge(pc=78, stack=(), blockstack=(), npush=0)]
DEBUG:numba.core.byteflow:pending: deque([State(pc_initial=82 nstack_initial=0), State(pc_initial=102 nstack_initial=0), State(pc_initial=78 nstack_initial=0)])
DEBUG:numba.core.byteflow:stack: []
DEBUG:numba.core.byteflow:state.pc_initial: State(pc_initial=82 nstack_initial=0)
DEBUG:numba.core.byteflow:dispatch pc=82, inst=LOAD_GLOBAL(arg=0, lineno=1153)
DEBUG:numba.core.byteflow:stack []
DEBUG:numba.core.byteflow:dispatch pc=84, inst=LOAD_METHOD(arg=1, lineno=1153)
DEBUG:numba.core.byteflow:stack ['$82load_global.0']
DEBUG:numba.core.byteflow:dispatch pc=86, inst=LOAD_FAST(arg=3, lineno=1153)
DEBUG:numba.core.byteflow:stack ['$84load_method.1']
DEBUG:numba.core.byteflow:dispatch pc=88, inst=CALL_METHOD(arg=1, lineno=1153)
DEBUG:numba.core.byteflow:stack ['$84load_method.1', '$x086.2']
DEBUG:numba.core.byteflow:dispatch pc=90, inst=LOAD_GLOBAL(arg=0, lineno=1153)
DEBUG:numba.core.byteflow:stack ['$88call_method.3']
DEBUG:numba.core.byteflow:dispatch pc=92, inst=LOAD_METHOD(arg=1, lineno=1153)
DEBUG:numba.core.byteflow:stack ['$88call_method.3', '$90load_global.4']
DEBUG:numba.core.byteflow:dispatch pc=94, inst=LOAD_FAST(arg=4, lineno=1153)
DEBUG:numba.core.byteflow:stack ['$88call_method.3', '$92load_method.5']
DEBUG:numba.core.byteflow:dispatch pc=96, inst=CALL_METHOD(arg=1, lineno=1153)
DEBUG:numba.core.byteflow:stack ['$88call_method.3', '$92load_method.5', '$x194.6']
DEBUG:numba.core.byteflow:dispatch pc=98, inst=COMPARE_OP(arg=3, lineno=1153)
DEBUG:numba.core.byteflow:stack ['$88call_method.3', '$96call_method.7']
DEBUG:numba.core.byteflow:dispatch pc=100, inst=RETURN_VALUE(arg=None, lineno=1153)
DEBUG:numba.core.byteflow:stack ['$98compare_op.8']
DEBUG:numba.core.byteflow:end state. edges=[]
DEBUG:numba.core.byteflow:pending: deque([State(pc_initial=102 nstack_initial=0), State(pc_initial=78 nstack_initial=0)])
DEBUG:numba.core.byteflow:stack: []
DEBUG:numba.core.byteflow:state.pc_initial: State(pc_initial=102 nstack_initial=0)
DEBUG:numba.core.byteflow:dispatch pc=102, inst=LOAD_GLOBAL(arg=0, lineno=1155)
DEBUG:numba.core.byteflow:stack []
DEBUG:numba.core.byteflow:dispatch pc=104, inst=LOAD_METHOD(arg=2, lineno=1155)
DEBUG:numba.core.byteflow:stack ['$102load_global.0']
DEBUG:numba.core.byteflow:dispatch pc=106, inst=LOAD_FAST(arg=3, lineno=1155)
DEBUG:numba.core.byteflow:stack ['$104load_method.1']
DEBUG:numba.core.byteflow:dispatch pc=108, inst=CALL_METHOD(arg=1, lineno=1155)
DEBUG:numba.core.byteflow:stack ['$104load_method.1', '$x0106.2']
DEBUG:numba.core.byteflow:dispatch pc=110, inst=LOAD_GLOBAL(arg=0, lineno=1155)
DEBUG:numba.core.byteflow:stack ['$108call_method.3']
DEBUG:numba.core.byteflow:dispatch pc=112, inst=LOAD_METHOD(arg=2, lineno=1155)
DEBUG:numba.core.byteflow:stack ['$108call_method.3', '$110load_global.4']
DEBUG:numba.core.byteflow:dispatch pc=114, inst=LOAD_FAST(arg=4, lineno=1155)
DEBUG:numba.core.byteflow:stack ['$108call_method.3', '$112load_method.5']
DEBUG:numba.core.byteflow:dispatch pc=116, inst=CALL_METHOD(arg=1, lineno=1155)
DEBUG:numba.core.byteflow:stack ['$108call_method.3', '$112load_method.5', '$x1114.6']
DEBUG:numba.core.byteflow:dispatch pc=118, inst=COMPARE_OP(arg=3, lineno=1155)
DEBUG:numba.core.byteflow:stack ['$108call_method.3', '$116call_method.7']
DEBUG:numba.core.byteflow:dispatch pc=120, inst=RETURN_VALUE(arg=None, lineno=1155)
DEBUG:numba.core.byteflow:stack ['$118compare_op.8']
DEBUG:numba.core.byteflow:end state. edges=[]
DEBUG:numba.core.byteflow:pending: deque([State(pc_initial=78 nstack_initial=0)])
DEBUG:numba.core.byteflow:-------------------------Prune PHIs-------------------------
DEBUG:numba.core.byteflow:Used_phis: defaultdict(<class 'set'>,
            {State(pc_initial=0 nstack_initial=0): set(),
             State(pc_initial=24 nstack_initial=1): {'$phi24.0'},
             State(pc_initial=30 nstack_initial=0): set(),
             State(pc_initial=32 nstack_initial=1): set(),
             State(pc_initial=36 nstack_initial=0): set(),
             State(pc_initial=40 nstack_initial=0): set(),
             State(pc_initial=62 nstack_initial=1): {'$phi62.0'},
             State(pc_initial=68 nstack_initial=0): set(),
             State(pc_initial=70 nstack_initial=1): set(),
             State(pc_initial=74 nstack_initial=0): set(),
             State(pc_initial=78 nstack_initial=0): set(),
             State(pc_initial=82 nstack_initial=0): set(),
             State(pc_initial=102 nstack_initial=0): set()})
DEBUG:numba.core.byteflow:defmap: {'$phi24.0': State(pc_initial=0 nstack_initial=0),
 '$phi32.0': State(pc_initial=0 nstack_initial=0),
 '$phi62.0': State(pc_initial=40 nstack_initial=0),
 '$phi70.0': State(pc_initial=40 nstack_initial=0)}
DEBUG:numba.core.byteflow:phismap: defaultdict(<class 'set'>,
            {'$phi24.0': {('$16dup_top.6',
                           State(pc_initial=0 nstack_initial=0))},
             '$phi32.0': {('$16dup_top.6',
                           State(pc_initial=0 nstack_initial=0))},
             '$phi62.0': {('$54dup_top.6',
                           State(pc_initial=40 nstack_initial=0))},
             '$phi70.0': {('$54dup_top.6',
                           State(pc_initial=40 nstack_initial=0))}})
DEBUG:numba.core.byteflow:changing phismap: defaultdict(<class 'set'>,
            {'$phi24.0': {('$16dup_top.6',
                           State(pc_initial=0 nstack_initial=0))},
             '$phi32.0': {('$16dup_top.6',
                           State(pc_initial=0 nstack_initial=0))},
             '$phi62.0': {('$54dup_top.6',
                           State(pc_initial=40 nstack_initial=0))},
             '$phi70.0': {('$54dup_top.6',
                           State(pc_initial=40 nstack_initial=0))}})
DEBUG:numba.core.byteflow:keep phismap: {'$phi24.0': {('$16dup_top.6', State(pc_initial=0 nstack_initial=0))},
 '$phi62.0': {('$54dup_top.6', State(pc_initial=40 nstack_initial=0))}}
DEBUG:numba.core.byteflow:new_out: defaultdict(<class 'dict'>,
            {State(pc_initial=0 nstack_initial=0): {'$phi24.0': '$16dup_top.6'},
             State(pc_initial=40 nstack_initial=0): {'$phi62.0': '$54dup_top.6'}})
DEBUG:numba.core.byteflow:----------------------DONE Prune PHIs-----------------------
DEBUG:numba.core.byteflow:block_infos State(pc_initial=0 nstack_initial=0):
AdaptBlockInfo(insts=((0, {}), (2, {'res': '$x2.0'}), (4, {'res': '$const4.1'}), (6, {'index': '$const4.1', 'target': '$x2.0', 'res': '$6binary_subscr.2'}), (8, {'value': '$6binary_subscr.2'}), (10, {'res': '$threshold10.3'}), (12, {'value': '$threshold10.3', 'res': '$12unary_negative.4'}), (14, {'res': '$x014.5'}), (16, {'orig': ['$x014.5'], 'duped': ['$16dup_top.6']}), (20, {'lhs': '$12unary_negative.4', 'rhs': '$x014.5', 'res': '$20compare_op.7'}), (22, {'pred': '$20compare_op.7'})), outgoing_phis={'$phi24.0': '$16dup_top.6'}, blockstack=(), active_try_block=None, outgoing_edgepushed={24: ('$16dup_top.6',), 32: ('$16dup_top.6',)})
DEBUG:numba.core.byteflow:block_infos State(pc_initial=24 nstack_initial=1):
AdaptBlockInfo(insts=((24, {'res': '$threshold24.1'}), (26, {'lhs': '$phi24.0', 'rhs': '$threshold24.1', 'res': '$26compare_op.2'}), (28, {'pred': '$26compare_op.2'})), outgoing_phis={}, blockstack=(), active_try_block=None, outgoing_edgepushed={30: (), 40: ()})
DEBUG:numba.core.byteflow:block_infos State(pc_initial=30 nstack_initial=0):
AdaptBlockInfo(insts=((30, {}),), outgoing_phis={}, blockstack=(), active_try_block=None, outgoing_edgepushed={36: ()})
DEBUG:numba.core.byteflow:block_infos State(pc_initial=32 nstack_initial=1):
AdaptBlockInfo(insts=((34, {}),), outgoing_phis={}, blockstack=(), active_try_block=None, outgoing_edgepushed={40: ()})
DEBUG:numba.core.byteflow:block_infos State(pc_initial=36 nstack_initial=0):
AdaptBlockInfo(insts=((36, {'res': '$const36.0'}), (38, {'value': '$const36.0'})), outgoing_phis={}, blockstack=(), active_try_block=None, outgoing_edgepushed={40: ()})
DEBUG:numba.core.byteflow:block_infos State(pc_initial=40 nstack_initial=0):
AdaptBlockInfo(insts=((40, {'res': '$x40.0'}), (42, {'res': '$const42.1'}), (44, {'index': '$const42.1', 'target': '$x40.0', 'res': '$44binary_subscr.2'}), (46, {'value': '$44binary_subscr.2'}), (48, {'res': '$threshold48.3'}), (50, {'value': '$threshold48.3', 'res': '$50unary_negative.4'}), (52, {'res': '$x152.5'}), (54, {'orig': ['$x152.5'], 'duped': ['$54dup_top.6']}), (58, {'lhs': '$50unary_negative.4', 'rhs': '$x152.5', 'res': '$58compare_op.7'}), (60, {'pred': '$58compare_op.7'})), outgoing_phis={'$phi62.0': '$54dup_top.6'}, blockstack=(), active_try_block=None, outgoing_edgepushed={62: ('$54dup_top.6',), 70: ('$54dup_top.6',)})
DEBUG:numba.core.byteflow:block_infos State(pc_initial=62 nstack_initial=1):
AdaptBlockInfo(insts=((62, {'res': '$threshold62.1'}), (64, {'lhs': '$phi62.0', 'rhs': '$threshold62.1', 'res': '$64compare_op.2'}), (66, {'pred': '$64compare_op.2'})), outgoing_phis={}, blockstack=(), active_try_block=None, outgoing_edgepushed={68: (), 78: ()})
DEBUG:numba.core.byteflow:block_infos State(pc_initial=68 nstack_initial=0):
AdaptBlockInfo(insts=((68, {}),), outgoing_phis={}, blockstack=(), active_try_block=None, outgoing_edgepushed={74: ()})
DEBUG:numba.core.byteflow:block_infos State(pc_initial=70 nstack_initial=1):
AdaptBlockInfo(insts=((72, {}),), outgoing_phis={}, blockstack=(), active_try_block=None, outgoing_edgepushed={78: ()})
DEBUG:numba.core.byteflow:block_infos State(pc_initial=74 nstack_initial=0):
AdaptBlockInfo(insts=((74, {'res': '$const74.0'}), (76, {'value': '$const74.0'})), outgoing_phis={}, blockstack=(), active_try_block=None, outgoing_edgepushed={78: ()})
DEBUG:numba.core.byteflow:block_infos State(pc_initial=78 nstack_initial=0):
AdaptBlockInfo(insts=((78, {'res': '$zero_pos78.0'}), (80, {'pred': '$zero_pos78.0'})), outgoing_phis={}, blockstack=(), active_try_block=None, outgoing_edgepushed={82: (), 102: ()})
DEBUG:numba.core.byteflow:block_infos State(pc_initial=82 nstack_initial=0):
AdaptBlockInfo(insts=((82, {'res': '$82load_global.0'}), (84, {'item': '$82load_global.0', 'res': '$84load_method.1'}), (86, {'res': '$x086.2'}), (88, {'func': '$84load_method.1', 'args': ['$x086.2'], 'res': '$88call_method.3'}), (90, {'res': '$90load_global.4'}), (92, {'item': '$90load_global.4', 'res': '$92load_method.5'}), (94, {'res': '$x194.6'}), (96, {'func': '$92load_method.5', 'args': ['$x194.6'], 'res': '$96call_method.7'}), (98, {'lhs': '$88call_method.3', 'rhs': '$96call_method.7', 'res': '$98compare_op.8'}), (100, {'retval': '$98compare_op.8', 'castval': '$100return_value.9'})), outgoing_phis={}, blockstack=(), active_try_block=None, outgoing_edgepushed={})
DEBUG:numba.core.byteflow:block_infos State(pc_initial=102 nstack_initial=0):
AdaptBlockInfo(insts=((102, {'res': '$102load_global.0'}), (104, {'item': '$102load_global.0', 'res': '$104load_method.1'}), (106, {'res': '$x0106.2'}), (108, {'func': '$104load_method.1', 'args': ['$x0106.2'], 'res': '$108call_method.3'}), (110, {'res': '$110load_global.4'}), (112, {'item': '$110load_global.4', 'res': '$112load_method.5'}), (114, {'res': '$x1114.6'}), (116, {'func': '$112load_method.5', 'args': ['$x1114.6'], 'res': '$116call_method.7'}), (118, {'lhs': '$108call_method.3', 'rhs': '$116call_method.7', 'res': '$118compare_op.8'}), (120, {'retval': '$118compare_op.8', 'castval': '$120return_value.9'})), outgoing_phis={}, blockstack=(), active_try_block=None, outgoing_edgepushed={})
DEBUG:numba.core.interpreter:label 0:
    x = arg(0, name=x)                       ['x']
    threshold = arg(1, name=threshold)       ['threshold']
    zero_pos = arg(2, name=zero_pos)         ['zero_pos']
    $const4.1 = const(int, 0)                ['$const4.1']
    x0 = getitem(value=x, index=$const4.1, fn=<built-in function getitem>) ['$const4.1', 'x', 'x0']
    $12unary_negative.4 = unary(fn=<built-in function neg>, value=threshold) ['$12unary_negative.4', 'threshold']
    $20compare_op.7 = $12unary_negative.4 <= x0 ['$12unary_negative.4', '$20compare_op.7', 'x0']
    bool22 = global(bool: <class 'bool'>)    ['bool22']
    $22pred = call bool22($20compare_op.7, func=bool22, args=(Var($20compare_op.7, audio.py:1145),), kws=(), vararg=None, varkwarg=None, target=None) ['$20compare_op.7', '$22pred', 'bool22']
    $phi24.0 = x0                            ['$phi24.0', 'x0']
    branch $22pred, 24, 32                   ['$22pred']
label 24:
    $26compare_op.2 = $phi24.0 <= threshold  ['$26compare_op.2', '$phi24.0', 'threshold']
    bool28 = global(bool: <class 'bool'>)    ['bool28']
    $28pred = call bool28($26compare_op.2, func=bool28, args=(Var($26compare_op.2, audio.py:1145),), kws=(), vararg=None, varkwarg=None, target=None) ['$26compare_op.2', '$28pred', 'bool28']
    branch $28pred, 30, 40                   ['$28pred']
label 30:
    jump 36                                  []
label 32:
    jump 40                                  []
label 36:
    x0 = const(int, 0)                       ['x0']
    jump 40                                  []
label 40:
    $const42.1 = const(int, -1)              ['$const42.1']
    x1 = getitem(value=x, index=$const42.1, fn=<built-in function getitem>) ['$const42.1', 'x', 'x1']
    $50unary_negative.4 = unary(fn=<built-in function neg>, value=threshold) ['$50unary_negative.4', 'threshold']
    $58compare_op.7 = $50unary_negative.4 <= x1 ['$50unary_negative.4', '$58compare_op.7', 'x1']
    bool60 = global(bool: <class 'bool'>)    ['bool60']
    $60pred = call bool60($58compare_op.7, func=bool60, args=(Var($58compare_op.7, audio.py:1149),), kws=(), vararg=None, varkwarg=None, target=None) ['$58compare_op.7', '$60pred', 'bool60']
    $phi62.0 = x1                            ['$phi62.0', 'x1']
    branch $60pred, 62, 70                   ['$60pred']
label 62:
    $64compare_op.2 = $phi62.0 <= threshold  ['$64compare_op.2', '$phi62.0', 'threshold']
    bool66 = global(bool: <class 'bool'>)    ['bool66']
    $66pred = call bool66($64compare_op.2, func=bool66, args=(Var($64compare_op.2, audio.py:1149),), kws=(), vararg=None, varkwarg=None, target=None) ['$64compare_op.2', '$66pred', 'bool66']
    branch $66pred, 68, 78                   ['$66pred']
label 68:
    jump 74                                  []
label 70:
    jump 78                                  []
label 74:
    x1 = const(int, 0)                       ['x1']
    jump 78                                  []
label 78:
    bool80 = global(bool: <class 'bool'>)    ['bool80']
    $80pred = call bool80(zero_pos, func=bool80, args=(Var(zero_pos, audio.py:1141),), kws=(), vararg=None, varkwarg=None, target=None) ['$80pred', 'bool80', 'zero_pos']
    branch $80pred, 82, 102                  ['$80pred']
label 82:
    $82load_global.0 = global(np: <module 'numpy' from 'C:\\Users\\taskm\\anaconda3\\envs\\taskai\\lib\\site-packages\\numpy\\__init__.py'>) ['$82load_global.0']
    $84load_method.1 = getattr(value=$82load_global.0, attr=signbit) ['$82load_global.0', '$84load_method.1']
    $88call_method.3 = call $84load_method.1(x0, func=$84load_method.1, args=[Var(x0, audio.py:1144)], kws=(), vararg=None, varkwarg=None, target=None) ['$84load_method.1', '$88call_method.3', 'x0']
    $90load_global.4 = global(np: <module 'numpy' from 'C:\\Users\\taskm\\anaconda3\\envs\\taskai\\lib\\site-packages\\numpy\\__init__.py'>) ['$90load_global.4']
    $92load_method.5 = getattr(value=$90load_global.4, attr=signbit) ['$90load_global.4', '$92load_method.5']
    $96call_method.7 = call $92load_method.5(x1, func=$92load_method.5, args=[Var(x1, audio.py:1148)], kws=(), vararg=None, varkwarg=None, target=None) ['$92load_method.5', '$96call_method.7', 'x1']
    $98compare_op.8 = $88call_method.3 != $96call_method.7 ['$88call_method.3', '$96call_method.7', '$98compare_op.8']
    $100return_value.9 = cast(value=$98compare_op.8) ['$100return_value.9', '$98compare_op.8']
    return $100return_value.9                ['$100return_value.9']
label 102:
    $102load_global.0 = global(np: <module 'numpy' from 'C:\\Users\\taskm\\anaconda3\\envs\\taskai\\lib\\site-packages\\numpy\\__init__.py'>) ['$102load_global.0']
    $104load_method.1 = getattr(value=$102load_global.0, attr=sign) ['$102load_global.0', '$104load_method.1']
    $108call_method.3 = call $104load_method.1(x0, func=$104load_method.1, args=[Var(x0, audio.py:1144)], kws=(), vararg=None, varkwarg=None, target=None) ['$104load_method.1', '$108call_method.3', 'x0']
    $110load_global.4 = global(np: <module 'numpy' from 'C:\\Users\\taskm\\anaconda3\\envs\\taskai\\lib\\site-packages\\numpy\\__init__.py'>) ['$110load_global.4']
    $112load_method.5 = getattr(value=$110load_global.4, attr=sign) ['$110load_global.4', '$112load_method.5']
    $116call_method.7 = call $112load_method.5(x1, func=$112load_method.5, args=[Var(x1, audio.py:1148)], kws=(), vararg=None, varkwarg=None, target=None) ['$112load_method.5', '$116call_method.7', 'x1']
    $118compare_op.8 = $108call_method.3 != $116call_method.7 ['$108call_method.3', '$116call_method.7', '$118compare_op.8']
    $120return_value.9 = cast(value=$118compare_op.8) ['$118compare_op.8', '$120return_value.9']
    return $120return_value.9                ['$120return_value.9']

DEBUG:numba.core.byteflow:bytecode dump:
>          0	NOP(arg=None, lineno=1039)
           2	LOAD_FAST(arg=0, lineno=1042)
           4	LOAD_CONST(arg=1, lineno=1042)
           6	BINARY_SUBSCR(arg=None, lineno=1042)
           8	LOAD_FAST(arg=0, lineno=1042)
          10	LOAD_CONST(arg=2, lineno=1042)
          12	BINARY_SUBSCR(arg=None, lineno=1042)
          14	COMPARE_OP(arg=4, lineno=1042)
          16	LOAD_FAST(arg=0, lineno=1042)
          18	LOAD_CONST(arg=1, lineno=1042)
          20	BINARY_SUBSCR(arg=None, lineno=1042)
          22	LOAD_FAST(arg=0, lineno=1042)
          24	LOAD_CONST(arg=3, lineno=1042)
          26	BINARY_SUBSCR(arg=None, lineno=1042)
          28	COMPARE_OP(arg=5, lineno=1042)
          30	BINARY_AND(arg=None, lineno=1042)
          32	RETURN_VALUE(arg=None, lineno=1042)
DEBUG:numba.core.byteflow:pending: deque([State(pc_initial=0 nstack_initial=0)])
DEBUG:numba.core.byteflow:stack: []
DEBUG:numba.core.byteflow:state.pc_initial: State(pc_initial=0 nstack_initial=0)
DEBUG:numba.core.byteflow:dispatch pc=0, inst=NOP(arg=None, lineno=1039)
DEBUG:numba.core.byteflow:stack []
DEBUG:numba.core.byteflow:dispatch pc=2, inst=LOAD_FAST(arg=0, lineno=1042)
DEBUG:numba.core.byteflow:stack []
DEBUG:numba.core.byteflow:dispatch pc=4, inst=LOAD_CONST(arg=1, lineno=1042)
DEBUG:numba.core.byteflow:stack ['$x2.0']
DEBUG:numba.core.byteflow:dispatch pc=6, inst=BINARY_SUBSCR(arg=None, lineno=1042)
DEBUG:numba.core.byteflow:stack ['$x2.0', '$const4.1']
DEBUG:numba.core.byteflow:dispatch pc=8, inst=LOAD_FAST(arg=0, lineno=1042)
DEBUG:numba.core.byteflow:stack ['$6binary_subscr.2']
DEBUG:numba.core.byteflow:dispatch pc=10, inst=LOAD_CONST(arg=2, lineno=1042)
DEBUG:numba.core.byteflow:stack ['$6binary_subscr.2', '$x8.3']
DEBUG:numba.core.byteflow:dispatch pc=12, inst=BINARY_SUBSCR(arg=None, lineno=1042)
DEBUG:numba.core.byteflow:stack ['$6binary_subscr.2', '$x8.3', '$const10.4']
DEBUG:numba.core.byteflow:dispatch pc=14, inst=COMPARE_OP(arg=4, lineno=1042)
DEBUG:numba.core.byteflow:stack ['$6binary_subscr.2', '$12binary_subscr.5']
DEBUG:numba.core.byteflow:dispatch pc=16, inst=LOAD_FAST(arg=0, lineno=1042)
DEBUG:numba.core.byteflow:stack ['$14compare_op.6']
DEBUG:numba.core.byteflow:dispatch pc=18, inst=LOAD_CONST(arg=1, lineno=1042)
DEBUG:numba.core.byteflow:stack ['$14compare_op.6', '$x16.7']
DEBUG:numba.core.byteflow:dispatch pc=20, inst=BINARY_SUBSCR(arg=None, lineno=1042)
DEBUG:numba.core.byteflow:stack ['$14compare_op.6', '$x16.7', '$const18.8']
DEBUG:numba.core.byteflow:dispatch pc=22, inst=LOAD_FAST(arg=0, lineno=1042)
DEBUG:numba.core.byteflow:stack ['$14compare_op.6', '$20binary_subscr.9']
DEBUG:numba.core.byteflow:dispatch pc=24, inst=LOAD_CONST(arg=3, lineno=1042)
DEBUG:numba.core.byteflow:stack ['$14compare_op.6', '$20binary_subscr.9', '$x22.10']
DEBUG:numba.core.byteflow:dispatch pc=26, inst=BINARY_SUBSCR(arg=None, lineno=1042)
DEBUG:numba.core.byteflow:stack ['$14compare_op.6', '$20binary_subscr.9', '$x22.10', '$const24.11']
DEBUG:numba.core.byteflow:dispatch pc=28, inst=COMPARE_OP(arg=5, lineno=1042)
DEBUG:numba.core.byteflow:stack ['$14compare_op.6', '$20binary_subscr.9', '$26binary_subscr.12']
DEBUG:numba.core.byteflow:dispatch pc=30, inst=BINARY_AND(arg=None, lineno=1042)
DEBUG:numba.core.byteflow:stack ['$14compare_op.6', '$28compare_op.13']
DEBUG:numba.core.byteflow:dispatch pc=32, inst=RETURN_VALUE(arg=None, lineno=1042)
DEBUG:numba.core.byteflow:stack ['$30binary_and.14']
DEBUG:numba.core.byteflow:end state. edges=[]
DEBUG:numba.core.byteflow:-------------------------Prune PHIs-------------------------
DEBUG:numba.core.byteflow:Used_phis: defaultdict(<class 'set'>, {State(pc_initial=0 nstack_initial=0): set()})
DEBUG:numba.core.byteflow:defmap: {}
DEBUG:numba.core.byteflow:phismap: defaultdict(<class 'set'>, {})
DEBUG:numba.core.byteflow:changing phismap: defaultdict(<class 'set'>, {})
DEBUG:numba.core.byteflow:keep phismap: {}
DEBUG:numba.core.byteflow:new_out: defaultdict(<class 'dict'>, {})
DEBUG:numba.core.byteflow:----------------------DONE Prune PHIs-----------------------
DEBUG:numba.core.byteflow:block_infos State(pc_initial=0 nstack_initial=0):
AdaptBlockInfo(insts=((0, {}), (2, {'res': '$x2.0'}), (4, {'res': '$const4.1'}), (6, {'index': '$const4.1', 'target': '$x2.0', 'res': '$6binary_subscr.2'}), (8, {'res': '$x8.3'}), (10, {'res': '$const10.4'}), (12, {'index': '$const10.4', 'target': '$x8.3', 'res': '$12binary_subscr.5'}), (14, {'lhs': '$6binary_subscr.2', 'rhs': '$12binary_subscr.5', 'res': '$14compare_op.6'}), (16, {'res': '$x16.7'}), (18, {'res': '$const18.8'}), (20, {'index': '$const18.8', 'target': '$x16.7', 'res': '$20binary_subscr.9'}), (22, {'res': '$x22.10'}), (24, {'res': '$const24.11'}), (26, {'index': '$const24.11', 'target': '$x22.10', 'res': '$26binary_subscr.12'}), (28, {'lhs': '$20binary_subscr.9', 'rhs': '$26binary_subscr.12', 'res': '$28compare_op.13'}), (30, {'lhs': '$14compare_op.6', 'rhs': '$28compare_op.13', 'res': '$30binary_and.14'}), (32, {'retval': '$30binary_and.14', 'castval': '$32return_value.15'})), outgoing_phis={}, blockstack=(), active_try_block=None, outgoing_edgepushed={})
DEBUG:numba.core.interpreter:label 0:
    x = arg(0, name=x)                       ['x']
    $const4.1 = const(int, 0)                ['$const4.1']
    $6binary_subscr.2 = getitem(value=x, index=$const4.1, fn=<built-in function getitem>) ['$6binary_subscr.2', '$const4.1', 'x']
    $const10.4 = const(int, -1)              ['$const10.4']
    $12binary_subscr.5 = getitem(value=x, index=$const10.4, fn=<built-in function getitem>) ['$12binary_subscr.5', '$const10.4', 'x']
    $14compare_op.6 = $6binary_subscr.2 > $12binary_subscr.5 ['$12binary_subscr.5', '$14compare_op.6', '$6binary_subscr.2']
    $const18.8 = const(int, 0)               ['$const18.8']
    $20binary_subscr.9 = getitem(value=x, index=$const18.8, fn=<built-in function getitem>) ['$20binary_subscr.9', '$const18.8', 'x']
    $const24.11 = const(int, 1)              ['$const24.11']
    $26binary_subscr.12 = getitem(value=x, index=$const24.11, fn=<built-in function getitem>) ['$26binary_subscr.12', '$const24.11', 'x']
    $28compare_op.13 = $20binary_subscr.9 >= $26binary_subscr.12 ['$20binary_subscr.9', '$26binary_subscr.12', '$28compare_op.13']
    $30binary_and.14 = $14compare_op.6 & $28compare_op.13 ['$14compare_op.6', '$28compare_op.13', '$30binary_and.14']
    $32return_value.15 = cast(value=$30binary_and.14) ['$30binary_and.14', '$32return_value.15']
    return $32return_value.15                ['$32return_value.15']

DEBUG:numba.core.byteflow:bytecode dump:
>          0	NOP(arg=None, lineno=1045)
           2	LOAD_FAST(arg=0, lineno=1048)
           4	LOAD_CONST(arg=1, lineno=1048)
           6	BINARY_SUBSCR(arg=None, lineno=1048)
           8	LOAD_FAST(arg=0, lineno=1048)
          10	LOAD_CONST(arg=2, lineno=1048)
          12	BINARY_SUBSCR(arg=None, lineno=1048)
          14	COMPARE_OP(arg=0, lineno=1048)
          16	LOAD_FAST(arg=0, lineno=1048)
          18	LOAD_CONST(arg=1, lineno=1048)
          20	BINARY_SUBSCR(arg=None, lineno=1048)
          22	LOAD_FAST(arg=0, lineno=1048)
          24	LOAD_CONST(arg=3, lineno=1048)
          26	BINARY_SUBSCR(arg=None, lineno=1048)
          28	COMPARE_OP(arg=1, lineno=1048)
          30	BINARY_AND(arg=None, lineno=1048)
          32	RETURN_VALUE(arg=None, lineno=1048)
DEBUG:numba.core.byteflow:pending: deque([State(pc_initial=0 nstack_initial=0)])
DEBUG:numba.core.byteflow:stack: []
DEBUG:numba.core.byteflow:state.pc_initial: State(pc_initial=0 nstack_initial=0)
DEBUG:numba.core.byteflow:dispatch pc=0, inst=NOP(arg=None, lineno=1045)
DEBUG:numba.core.byteflow:stack []
DEBUG:numba.core.byteflow:dispatch pc=2, inst=LOAD_FAST(arg=0, lineno=1048)
DEBUG:numba.core.byteflow:stack []
DEBUG:numba.core.byteflow:dispatch pc=4, inst=LOAD_CONST(arg=1, lineno=1048)
DEBUG:numba.core.byteflow:stack ['$x2.0']
DEBUG:numba.core.byteflow:dispatch pc=6, inst=BINARY_SUBSCR(arg=None, lineno=1048)
DEBUG:numba.core.byteflow:stack ['$x2.0', '$const4.1']
DEBUG:numba.core.byteflow:dispatch pc=8, inst=LOAD_FAST(arg=0, lineno=1048)
DEBUG:numba.core.byteflow:stack ['$6binary_subscr.2']
DEBUG:numba.core.byteflow:dispatch pc=10, inst=LOAD_CONST(arg=2, lineno=1048)
DEBUG:numba.core.byteflow:stack ['$6binary_subscr.2', '$x8.3']
DEBUG:numba.core.byteflow:dispatch pc=12, inst=BINARY_SUBSCR(arg=None, lineno=1048)
DEBUG:numba.core.byteflow:stack ['$6binary_subscr.2', '$x8.3', '$const10.4']
DEBUG:numba.core.byteflow:dispatch pc=14, inst=COMPARE_OP(arg=0, lineno=1048)
DEBUG:numba.core.byteflow:stack ['$6binary_subscr.2', '$12binary_subscr.5']
DEBUG:numba.core.byteflow:dispatch pc=16, inst=LOAD_FAST(arg=0, lineno=1048)
DEBUG:numba.core.byteflow:stack ['$14compare_op.6']
DEBUG:numba.core.byteflow:dispatch pc=18, inst=LOAD_CONST(arg=1, lineno=1048)
DEBUG:numba.core.byteflow:stack ['$14compare_op.6', '$x16.7']
DEBUG:numba.core.byteflow:dispatch pc=20, inst=BINARY_SUBSCR(arg=None, lineno=1048)
DEBUG:numba.core.byteflow:stack ['$14compare_op.6', '$x16.7', '$const18.8']
DEBUG:numba.core.byteflow:dispatch pc=22, inst=LOAD_FAST(arg=0, lineno=1048)
DEBUG:numba.core.byteflow:stack ['$14compare_op.6', '$20binary_subscr.9']
DEBUG:numba.core.byteflow:dispatch pc=24, inst=LOAD_CONST(arg=3, lineno=1048)
DEBUG:numba.core.byteflow:stack ['$14compare_op.6', '$20binary_subscr.9', '$x22.10']
DEBUG:numba.core.byteflow:dispatch pc=26, inst=BINARY_SUBSCR(arg=None, lineno=1048)
DEBUG:numba.core.byteflow:stack ['$14compare_op.6', '$20binary_subscr.9', '$x22.10', '$const24.11']
DEBUG:numba.core.byteflow:dispatch pc=28, inst=COMPARE_OP(arg=1, lineno=1048)
DEBUG:numba.core.byteflow:stack ['$14compare_op.6', '$20binary_subscr.9', '$26binary_subscr.12']
DEBUG:numba.core.byteflow:dispatch pc=30, inst=BINARY_AND(arg=None, lineno=1048)
DEBUG:numba.core.byteflow:stack ['$14compare_op.6', '$28compare_op.13']
DEBUG:numba.core.byteflow:dispatch pc=32, inst=RETURN_VALUE(arg=None, lineno=1048)
DEBUG:numba.core.byteflow:stack ['$30binary_and.14']
DEBUG:numba.core.byteflow:end state. edges=[]
DEBUG:numba.core.byteflow:-------------------------Prune PHIs-------------------------
DEBUG:numba.core.byteflow:Used_phis: defaultdict(<class 'set'>, {State(pc_initial=0 nstack_initial=0): set()})
DEBUG:numba.core.byteflow:defmap: {}
DEBUG:numba.core.byteflow:phismap: defaultdict(<class 'set'>, {})
DEBUG:numba.core.byteflow:changing phismap: defaultdict(<class 'set'>, {})
DEBUG:numba.core.byteflow:keep phismap: {}
DEBUG:numba.core.byteflow:new_out: defaultdict(<class 'dict'>, {})
DEBUG:numba.core.byteflow:----------------------DONE Prune PHIs-----------------------
DEBUG:numba.core.byteflow:block_infos State(pc_initial=0 nstack_initial=0):
AdaptBlockInfo(insts=((0, {}), (2, {'res': '$x2.0'}), (4, {'res': '$const4.1'}), (6, {'index': '$const4.1', 'target': '$x2.0', 'res': '$6binary_subscr.2'}), (8, {'res': '$x8.3'}), (10, {'res': '$const10.4'}), (12, {'index': '$const10.4', 'target': '$x8.3', 'res': '$12binary_subscr.5'}), (14, {'lhs': '$6binary_subscr.2', 'rhs': '$12binary_subscr.5', 'res': '$14compare_op.6'}), (16, {'res': '$x16.7'}), (18, {'res': '$const18.8'}), (20, {'index': '$const18.8', 'target': '$x16.7', 'res': '$20binary_subscr.9'}), (22, {'res': '$x22.10'}), (24, {'res': '$const24.11'}), (26, {'index': '$const24.11', 'target': '$x22.10', 'res': '$26binary_subscr.12'}), (28, {'lhs': '$20binary_subscr.9', 'rhs': '$26binary_subscr.12', 'res': '$28compare_op.13'}), (30, {'lhs': '$14compare_op.6', 'rhs': '$28compare_op.13', 'res': '$30binary_and.14'}), (32, {'retval': '$30binary_and.14', 'castval': '$32return_value.15'})), outgoing_phis={}, blockstack=(), active_try_block=None, outgoing_edgepushed={})
DEBUG:numba.core.interpreter:label 0:
    x = arg(0, name=x)                       ['x']
    $const4.1 = const(int, 0)                ['$const4.1']
    $6binary_subscr.2 = getitem(value=x, index=$const4.1, fn=<built-in function getitem>) ['$6binary_subscr.2', '$const4.1', 'x']
    $const10.4 = const(int, -1)              ['$const10.4']
    $12binary_subscr.5 = getitem(value=x, index=$const10.4, fn=<built-in function getitem>) ['$12binary_subscr.5', '$const10.4', 'x']
    $14compare_op.6 = $6binary_subscr.2 < $12binary_subscr.5 ['$12binary_subscr.5', '$14compare_op.6', '$6binary_subscr.2']
    $const18.8 = const(int, 0)               ['$const18.8']
    $20binary_subscr.9 = getitem(value=x, index=$const18.8, fn=<built-in function getitem>) ['$20binary_subscr.9', '$const18.8', 'x']
    $const24.11 = const(int, 1)              ['$const24.11']
    $26binary_subscr.12 = getitem(value=x, index=$const24.11, fn=<built-in function getitem>) ['$26binary_subscr.12', '$const24.11', 'x']
    $28compare_op.13 = $20binary_subscr.9 <= $26binary_subscr.12 ['$20binary_subscr.9', '$26binary_subscr.12', '$28compare_op.13']
    $30binary_and.14 = $14compare_op.6 & $28compare_op.13 ['$14compare_op.6', '$28compare_op.13', '$30binary_and.14']
    $32return_value.15 = cast(value=$30binary_and.14) ['$30binary_and.14', '$32return_value.15']
    return $32return_value.15                ['$32return_value.15']

ERROR:root:Error processing chapters: not enough values to unpack (expected 3, got 2)
Traceback (most recent call last):
  File "D:\github\chappymedium\chappie.py", line 219, in process_chapters
    result = self.chappie_processor.process_srt(srt_content)
  File "D:\github\chappymedium\chappie_processor.py", line 26, in process_srt
    entries = parse_srt(srt_content)
  File "D:\github\chappymedium\chappie_utils.py", line 17, in parse_srt
    'start': time_to_seconds(time_range[0]),
  File "D:\github\chappymedium\chappie_utils.py", line 30, in time_to_seconds
    h, m, s = time_str.split(':')
ValueError: not enough values to unpack (expected 3, got 2)
ERROR:root:Error processing chapters: not enough values to unpack (expected 3, got 2)
Traceback (most recent call last):
  File "D:\github\chappymedium\chappie.py", line 219, in process_chapters
    result = self.chappie_processor.process_srt(srt_content)
  File "D:\github\chappymedium\chappie_processor.py", line 26, in process_srt
    entries = parse_srt(srt_content)
  File "D:\github\chappymedium\chappie_utils.py", line 17, in parse_srt
    'start': time_to_seconds(time_range[0]),
  File "D:\github\chappymedium\chappie_utils.py", line 30, in time_to_seconds
    h, m, s = time_str.split(':')
ValueError: not enough values to unpack (expected 3, got 2)
DEBUG:openai._base_client:Request options: {'method': 'post', 'url': '/completions', 'files': None, 'json_data': {'model': 'gpt-3.5-turbo-instruct', 'prompt': ["Summarize the following chapter in one sentence: So, long story short is, I've been using AI to build a framework for a union in Battlerap. And I'd like to share it with the culture, I'm working on a video where it kind of breaks things down. But it's just a framework, it's not like, it's more of an idea that could be implemented and even if it's not implemented, portions of it could be either implemented or considered a standard and you have some unique insight. And I would like for you to take a look at the video once it's out. Take a look at the documentation once it's out. And I'll actually reach out to you as well and remind you, hey Lex, could you take a look at this and let's set up an interview and let me talk to you again. And the purpose of me talking to you again would be to get your thoughts about the current version of the framework and then I'm going to revise it as I communicate with you and other people in the Battlerap community in order to iterate it and make it better and better and better over time. Of course, again, using AI in order to do this. That's very exciting. Yo, you said it's exciting. Yo, that's dope. That's dope. Some people just hate anything related to AI, Lex. I don't know what the fuck is wrong with these motherfuckers. I don't mind anything with AI. A lot of people in my life are not in the Battlerap world, so very rarely do I get to geek out about cool, innovative stuff for Battlerap and that's why I'm just so excited. Let me tell you how you're a lead because that's something that I'm a super-duper nerd when it comes to AI, man. My channel is really just a case study in how to use AI in order to take stories and turn them into something else that is really engaging for people. I think that we can use it for families. Even if my channel, your family, like your grandpa, his story was forever for your kids, they're able to watch a story about his life and it's animated or it really looks like him. I'm a really big Battlerap enthusiast, so I decided to use it in that regard. Then on the side, I was developing this union thing. The fact that you said it's exciting really excites me as well. As a thank you, what I want to do is I want to share with you something that I think that your lead could possibly use. That's what I use for music on my page. You may already know about this, but I'm just telling you because I'm a person and I just have a thousand ideas about stuff. I'm super, super- That's what I do all day. I just spill ideas and I can't even catch them all. Yeah. There you go. That's how I am, man. I just tip over and I spill all day. There you go. You know what, Lex? You should use the approach that I use with AI. You should have a voice recorder where you're just spitting out all these ideas. Then you feed that into the AI or you just talk to the AI using some chat GPT or something. You just spit out all these ideas. You're recording all these ideas on your notepad or something like that for an event, for example. You're recording yourself. You're just brainstorming ideas. You're not quite sure about anything, but you tell the AI this stuff and then you tell it, ask me 10 questions so that you can give me 20 great ideas and solutions for all this stuff. Then it'll ask you, even though you spilled your guts about this event, it'll ask you questions in order to give you great advice or to solve your problems. Not that it'll just immediately solve them, but if you go back and forth, man, you can really do some amazing things with especially organizations. Guess what I'm using AI for right now. What are you using it for? Okay. We have a ranking system in our league for a lot of the battlers that are coming up to have basically direction in their career to be like, okay, you have this many wins, this many losses. You're number seven. You can't battle number one, but you could battle number four. Just basically have a general idea for the battlers to know where they're at. What I'm using the AI for is to input their scores and get their overalls, their averages."], 'frequency_penalty': 0, 'logit_bias': {}, 'max_tokens': 256, 'n': 1, 'presence_penalty': 0, 'temperature': 0.7, 'top_p': 1}}
DEBUG:openai._base_client:Sending HTTP Request: POST https://api.openai.com/v1/completions
DEBUG:httpcore.connection:connect_tcp.started host='api.openai.com' port=443 local_address=None timeout=None socket_options=None
DEBUG:httpcore.connection:connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x00000243825D4BB0>
DEBUG:httpcore.connection:start_tls.started ssl_context=<ssl.SSLContext object at 0x00000243FB805EC0> server_hostname='api.openai.com' timeout=None
DEBUG:httpcore.connection:start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x00000243825C38E0>
DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>
DEBUG:httpcore.http11:send_request_headers.complete
DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>
DEBUG:httpcore.http11:send_request_body.complete
DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>
DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Sun, 07 Jul 2024 23:57:38 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-allow-origin', b'*'), (b'Cache-Control', b'no-cache, must-revalidate'), (b'openai-model', b'gpt-3.5-turbo-instruct'), (b'openai-organization', b'machinekingsmedia'), (b'openai-processing-ms', b'3086'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains'), (b'x-ratelimit-limit-requests', b'3500'), (b'x-ratelimit-limit-tokens', b'90000'), (b'x-ratelimit-remaining-requests', b'3499'), (b'x-ratelimit-remaining-tokens', b'88725'), (b'x-ratelimit-reset-requests', b'17ms'), (b'x-ratelimit-reset-tokens', b'849ms'), (b'x-request-id', b'req_648b0f99b1c81f7febfbd27ac8312435'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Set-Cookie', b'__cf_bm=EqBXYpme.hrSlR9YyXE7AcwmuoTPnNUIVCAbvLUy7Fk-1720396658-1.0.1.1-XLfBuHv6dkIbZqyfZvoib8pg28hzvWFuJk1RBkf.BNRKAyT6LcC6py7H0Wmaq0L4FzOxYpDOJsqup14PsAIHjg; path=/; expires=Mon, 08-Jul-24 00:27:38 GMT; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), (b'Set-Cookie', b'_cfuvid=fqtK1aU1WXN5Ro1TN2JV1EX8dkgISDSr643KJ9ex3sc-1720396658927-0.0.1.1-604800000; path=/; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), (b'Server', b'cloudflare'), (b'CF-RAY', b'89fbc7193b160573-IAD'), (b'Content-Encoding', b'br'), (b'alt-svc', b'h3=":443"; ma=86400')])
INFO:httpx:HTTP Request: POST https://api.openai.com/v1/completions "HTTP/1.1 200 OK"
DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>
DEBUG:httpcore.http11:receive_response_body.complete
DEBUG:httpcore.http11:response_closed.started
DEBUG:httpcore.http11:response_closed.complete
DEBUG:openai._base_client:HTTP Response: POST https://api.openai.com/v1/completions "200 OK" Headers([('date', 'Sun, 07 Jul 2024 23:57:38 GMT'), ('content-type', 'application/json'), ('transfer-encoding', 'chunked'), ('connection', 'keep-alive'), ('access-control-allow-origin', '*'), ('cache-control', 'no-cache, must-revalidate'), ('openai-model', 'gpt-3.5-turbo-instruct'), ('openai-organization', 'machinekingsmedia'), ('openai-processing-ms', '3086'), ('openai-version', '2020-10-01'), ('strict-transport-security', 'max-age=31536000; includeSubDomains'), ('x-ratelimit-limit-requests', '3500'), ('x-ratelimit-limit-tokens', '90000'), ('x-ratelimit-remaining-requests', '3499'), ('x-ratelimit-remaining-tokens', '88725'), ('x-ratelimit-reset-requests', '17ms'), ('x-ratelimit-reset-tokens', '849ms'), ('x-request-id', 'req_648b0f99b1c81f7febfbd27ac8312435'), ('cf-cache-status', 'DYNAMIC'), ('set-cookie', '__cf_bm=EqBXYpme.hrSlR9YyXE7AcwmuoTPnNUIVCAbvLUy7Fk-1720396658-1.0.1.1-XLfBuHv6dkIbZqyfZvoib8pg28hzvWFuJk1RBkf.BNRKAyT6LcC6py7H0Wmaq0L4FzOxYpDOJsqup14PsAIHjg; path=/; expires=Mon, 08-Jul-24 00:27:38 GMT; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), ('set-cookie', '_cfuvid=fqtK1aU1WXN5Ro1TN2JV1EX8dkgISDSr643KJ9ex3sc-1720396658927-0.0.1.1-604800000; path=/; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), ('server', 'cloudflare'), ('cf-ray', '89fbc7193b160573-IAD'), ('content-encoding', 'br'), ('alt-svc', 'h3=":443"; ma=86400')])
DEBUG:openai._base_client:request_id: req_648b0f99b1c81f7febfbd27ac8312435
DEBUG:openai._base_client:Request options: {'method': 'post', 'url': '/completions', 'files': None, 'json_data': {'model': 'gpt-3.5-turbo-instruct', 'prompt': ["Summarize the following chapter in one sentence: What's their overall writing score? What's their overall performance score? What's the chances of them having fucking rebuttals in a round type shit? Wow. Have you ever thought about, because you have a roster that I think would be really good for this. Have you ever thought about a role playing, a management sim battle rap game? Yeah. I've never thought about a simulated one, but I've definitely thought of a bunch of different ones. One of my battlers did one of those emulators for D&D and he put us all in it. There's an iBattle Final Fantasy game out there to play with all of us in it, but he just basically made us and changed everybody's name in the game. I think that's pretty cool, simulated battles. It's pretty much like what these kids do on Twitter, right? The impersonation battles, the same thing? No, that's not what I have in mind. Imagine if what you just mentioned before about the percentages of a rebuttal and all that stuff. Imagine if there were a ranking of, you also have a huge audience that interacts with your app. Imagine if they could rank your battlers and their wordplay, their rebuttals, their whatever. You use those rankings like NBA Live does and even maybe adjustments after battles, maybe all their stats change in the game or whatever. I'm telling you this because I'm going to probably do it one day, but if I don't do it, I would love for someone else to actually do it. You have all the tools. You're cutting edge. Your name is freaking iBattle, by the way. I'm just full of ideas. I love battle rap as much as you do. I love helping people as much as you do. I hope to bring what I do on my channel. I want to teach all the leagues how to do what I do. Your channel is really something else, man. It's very... How do I explain this? I also, on the side, I manage pro wrestlers, right? They have a bunch of different ways to get people interested in their matches that we don't do in battle rap. Like, for instance, 2pm call time. You're supposed to get there at 2pm no matter what time your match is and you have to shoot a promo. Right? In wrestling. In wrestling you're talking, right? Yeah, yeah, yeah. Stuff like that, right? And I'm saying that to say that in battle rap we don't have too much of that. You don't get to... You don't have anything about the battler outside of the battle to know about them, to relate to them, to give you any fucking reason to watch them. Right? And I think your channel does that. So, my channel is just a teaser for my vision for battle rap. So, with this idea for this union, right? Imagine if I taught someone else. Like, if there were someone else able to do exactly what I'm doing to be able to take... Let me find a way to make SoulCon and A-Ward, like, to create a promo for them. Even though they don't really have... But what if you don't? What if there's something there already? What if there's a reason for the battle? What's a battle that we have? What's a battle out there that people want that hasn't happened? Let's say, like, T-Rock... Big K... Yeah, yeah, yeah. There you go. T-Rock, Easy to Bounce. Let's say Big K has... Let's say Summer Madness is coming. T-Rock vs. Easy is announced, right? Uh-huh. Outside of their tweets and a promo video from Durez, there's not going to be too much to pull me to the battle, right? Yep. I think your channel has value in that because you can take the already storyline that's there and you can project it onto the audience fluidly without having to force anything, right? Uh-huh. This is why the battle is happening. This is why you should care. And it's a cool-ass fucking way to find out via the AI art, right? Yeah, yeah, yeah."], 'frequency_penalty': 0, 'logit_bias': {}, 'max_tokens': 256, 'n': 1, 'presence_penalty': 0, 'temperature': 0.7, 'top_p': 1}}
DEBUG:openai._base_client:Sending HTTP Request: POST https://api.openai.com/v1/completions
DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>
DEBUG:httpcore.http11:send_request_headers.complete
DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>
DEBUG:httpcore.http11:send_request_body.complete
DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>
DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Sun, 07 Jul 2024 23:57:41 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-allow-origin', b'*'), (b'Cache-Control', b'no-cache, must-revalidate'), (b'openai-model', b'gpt-3.5-turbo-instruct'), (b'openai-organization', b'machinekingsmedia'), (b'openai-processing-ms', b'2524'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains'), (b'x-ratelimit-limit-requests', b'3500'), (b'x-ratelimit-limit-tokens', b'90000'), (b'x-ratelimit-remaining-requests', b'3499'), (b'x-ratelimit-remaining-tokens', b'88813'), (b'x-ratelimit-reset-requests', b'17ms'), (b'x-ratelimit-reset-tokens', b'791ms'), (b'x-request-id', b'req_71a5755108dc033846e9f312ad67e20a'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'89fbc72e683d0573-IAD'), (b'Content-Encoding', b'br'), (b'alt-svc', b'h3=":443"; ma=86400')])
INFO:httpx:HTTP Request: POST https://api.openai.com/v1/completions "HTTP/1.1 200 OK"
DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>
DEBUG:httpcore.http11:receive_response_body.complete
DEBUG:httpcore.http11:response_closed.started
DEBUG:httpcore.http11:response_closed.complete
DEBUG:openai._base_client:HTTP Response: POST https://api.openai.com/v1/completions "200 OK" Headers({'date': 'Sun, 07 Jul 2024 23:57:41 GMT', 'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'access-control-allow-origin': '*', 'cache-control': 'no-cache, must-revalidate', 'openai-model': 'gpt-3.5-turbo-instruct', 'openai-organization': 'machinekingsmedia', 'openai-processing-ms': '2524', 'openai-version': '2020-10-01', 'strict-transport-security': 'max-age=31536000; includeSubDomains', 'x-ratelimit-limit-requests': '3500', 'x-ratelimit-limit-tokens': '90000', 'x-ratelimit-remaining-requests': '3499', 'x-ratelimit-remaining-tokens': '88813', 'x-ratelimit-reset-requests': '17ms', 'x-ratelimit-reset-tokens': '791ms', 'x-request-id': 'req_71a5755108dc033846e9f312ad67e20a', 'cf-cache-status': 'DYNAMIC', 'server': 'cloudflare', 'cf-ray': '89fbc72e683d0573-IAD', 'content-encoding': 'br', 'alt-svc': 'h3=":443"; ma=86400'})
DEBUG:openai._base_client:request_id: req_71a5755108dc033846e9f312ad67e20a
DEBUG:openai._base_client:Request options: {'method': 'post', 'url': '/completions', 'files': None, 'json_data': {'model': 'gpt-3.5-turbo-instruct', 'prompt': ["Summarize the following chapter in one sentence: That would be a dope way to find out about a battle, yeah. Not even about the battle. This is what... A URL should hire you to put that video on their channel on why you should watch the battle. Well, I think that iBattle should have models, like AI models of all their battlers. You never have to have them to come into your studio. If you tell them to take pictures and you go to a website called astrea.ai, you can take pictures of them. That's how I did a lot of my early videos, if you see how I did the Pat State video. Wait a minute, was it the Pat State video? Yeah, the Pat State video, where certain pictures where it really looks like Pat State. It really looks like him. Some of them don't really look like him, but some of them really do. But that same technology, if you have models of all your battlers, you can create amazing promotions. Let's say you want to do something that's like Till Death Do Us Part 5, right? And you want to have them looking like they're in funeral clothes and sad. You never have to have them to come in. You could use AI art really, really easy by just typing the stuff up and you already have the models for them. If the next card is a barbarian card and you want to have them looking like Mad Max or something like that, you can do that as well. I think that would also help bring things to the table. As far as the approach that I used... That's badass. Yeah, that's really badass. Let me tell you another thing. I just told you astrea.ai. I'll send it to you on the... Is that the same site that you can make it for where I can just upload a picture of Real Sick and a picture of City Towers and be like, make these two guys Alien vs. Predator and it'll just do it? No, this doesn't do that. What this does is you upload maybe 10 to 12 pictures as different as possible. This is kind of an older technology as far as AI is concerned, but I think it's perfect for battle rap and for leagues. I can show it to you the next time. I can show it to you on a call if you want to demo it to you. What you would do is you would get 10 pictures of Real Sick doing different things. Or go to his Facebook page or rip some screenshots from Instagram or whatever. Put them in here and then... Who was the other person you said? And City Towers. Get about 10-15 pictures of City Towers. Then, now that you have those two separate, you could do the Alien vs. Predator. Make Real Sick look... Make this man... It's a specific language you have to use here. Yeah, like it's a child, right? No, like it's a token. You wouldn't actually put his name, but on this particular site, it's a token like it's SKS or SKS Man or something like that. But anyway, I can show it to you if you like, but Astrea.ai, if you want to play with it, upload like 10 pictures of yourself. And then in about 30... I think it costs like five bucks. And then in about 30 minutes, it'll return to you a bunch of pictures. I can show you right now True Fo. I was doing some stuff with True Fo a while ago and I... How many pictures do I have? Yeah, if you have an iPhone, I can show you the FaceTime, but it's really good and it totally can be used. Man, fuck it. Hold on. Let me put on... You got iPhone, right? Hold on, hold on. Look at this shit. Look at this shit. That's not really True Fo. That's AI True Fo because I was able to make the model of him based on his pictures. Look at this shit. It looks just like... Yeah, it really does. It really does. There are some imperfections. If you look at the guitar or whatever, it's not as perfect. His fingers look kind of crazy. Yeah, his fingers definitely look crazy. This particular technology but it always is going to try to hide the fingers. It's just so good at giving you a person in multiple looks. That's a completely different look than from what you saw. Look at this. I've seen Twizz put some stuff like this up. Did you see it?"], 'frequency_penalty': 0, 'logit_bias': {}, 'max_tokens': 256, 'n': 1, 'presence_penalty': 0, 'temperature': 0.7, 'top_p': 1}}
DEBUG:openai._base_client:Sending HTTP Request: POST https://api.openai.com/v1/completions
DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>
DEBUG:httpcore.http11:send_request_headers.complete
DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>
DEBUG:httpcore.http11:send_request_body.complete
DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>
DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Sun, 07 Jul 2024 23:57:43 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-allow-origin', b'*'), (b'Cache-Control', b'no-cache, must-revalidate'), (b'openai-model', b'gpt-3.5-turbo-instruct'), (b'openai-organization', b'machinekingsmedia'), (b'openai-processing-ms', b'1723'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains'), (b'x-ratelimit-limit-requests', b'3500'), (b'x-ratelimit-limit-tokens', b'90000'), (b'x-ratelimit-remaining-requests', b'3499'), (b'x-ratelimit-remaining-tokens', b'88756'), (b'x-ratelimit-reset-requests', b'17ms'), (b'x-ratelimit-reset-tokens', b'828ms'), (b'x-request-id', b'req_e65289b7deef0ad8dfa1d16df6c8b50e'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'89fbc73eebbc0573-IAD'), (b'Content-Encoding', b'br'), (b'alt-svc', b'h3=":443"; ma=86400')])
INFO:httpx:HTTP Request: POST https://api.openai.com/v1/completions "HTTP/1.1 200 OK"
DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>
DEBUG:httpcore.http11:receive_response_body.complete
DEBUG:httpcore.http11:response_closed.started
DEBUG:httpcore.http11:response_closed.complete
DEBUG:openai._base_client:HTTP Response: POST https://api.openai.com/v1/completions "200 OK" Headers({'date': 'Sun, 07 Jul 2024 23:57:43 GMT', 'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'access-control-allow-origin': '*', 'cache-control': 'no-cache, must-revalidate', 'openai-model': 'gpt-3.5-turbo-instruct', 'openai-organization': 'machinekingsmedia', 'openai-processing-ms': '1723', 'openai-version': '2020-10-01', 'strict-transport-security': 'max-age=31536000; includeSubDomains', 'x-ratelimit-limit-requests': '3500', 'x-ratelimit-limit-tokens': '90000', 'x-ratelimit-remaining-requests': '3499', 'x-ratelimit-remaining-tokens': '88756', 'x-ratelimit-reset-requests': '17ms', 'x-ratelimit-reset-tokens': '828ms', 'x-request-id': 'req_e65289b7deef0ad8dfa1d16df6c8b50e', 'cf-cache-status': 'DYNAMIC', 'server': 'cloudflare', 'cf-ray': '89fbc73eebbc0573-IAD', 'content-encoding': 'br', 'alt-svc': 'h3=":443"; ma=86400'})
DEBUG:openai._base_client:request_id: req_e65289b7deef0ad8dfa1d16df6c8b50e
DEBUG:openai._base_client:Request options: {'method': 'post', 'url': '/completions', 'files': None, 'json_data': {'model': 'gpt-3.5-turbo-instruct', 'prompt': ["Summarize the following chapter in one sentence: When I was on Facebook, I used to see it. What Twizz does is he uses mid-journey but then he uses face swaps. You see how this doesn't quite look like Homesy. It kind of looks like him but it doesn't quite look like him. If you can get close to him, you can use this technology in here to do a face swap. That's how he does that. It's really... You could do that if you want to do that. Again, I can show you that shit really quick but using a face swap you could... Using a face swap in mid-journey is how you do that. You have to be in Discord and shit like that but that's the way that he does that. That's not the best way to get the best likeness. Since I'm showing you different AIs, let me show you a different one too that you can also use on your cards. It's called Ideogram. What Ideogram is is it's really good at text. Really, really fucking good at text. Let me show you how good at text it is. These are my last ones. Look at this shit. Our Society Battle League. You would have paid a motherfucker to make this a couple years ago. Not even a couple years ago. A couple hours ago. Look at this. Gates of the Guard. Look at that shit. This is top-notch Photoshop work. All I typed in was... I didn't type in all this shit I don't think. I actually didn't type this in. It changed my prompt from an earlier one. You put in something and then it'll change it and make it better or whatever. Sometimes though, if you fuck with it, sometimes it'll mess up shit like that. You see how the G's is fucked up? If you regenerate, it seems like it'll get it right. This one, it got it right. The A's are a little covered up. This shit is free, dog. That's the craziest part of this particular technology. This shit is so good and the fact that it's free is crazy. Whatever you type in, you can use ChatGPT to give you a prompt. Tell it to write me an image prompt for whatever. That's what that is to the right. It's the image prompt. That is the image prompt. This is the prompt and then the magic prompt is what it changes your prompt to or whatever. You see an abstract design with a prominent gate using vibrant colors. See that shit right there? It really follows that shit. Let me also help you with your theme shit. I don't know how closely you fuck with my page, but I got a lot of custom music on my page. Listen to this fucking song. The same way that shit generated pictures, listen to this fucking song called Can't Copy Respect. Listen to this shit. No. No, no, no. The outro to the videos is using this exact same technology. Yes. Yeah, you just type. Check this shit out. Yeah, yeah, yeah. Check this shit out. I write the rhymes because I used to write way back in the days and shit, but check this shit out. Check this shit out. I just typed this shit in. This is what the fuck I typed in. It's URL you can't copy respect time. It's URL you can't copy respect. It's URL you can't copy respect. Let's get it. We remember murder move on them smack DVDs. The leaders of the culture smack Beasley and Pete. It's the URL you can't copy respect. Right? Check out this shit. This is URL... you can't copy respect, this is URL... you can't copy respect... We remember Murder, Moof, Odom, SmackDVD's. The leaders of the culture, Smack, Beasley and V. This is URL... you can't copy respect, this is URL... you can't copy respect. We remember Murder, Moof, Odom, SmackDVD's. The leaders of the culture, Smack, Beasley and V."], 'frequency_penalty': 0, 'logit_bias': {}, 'max_tokens': 256, 'n': 1, 'presence_penalty': 0, 'temperature': 0.7, 'top_p': 1}}
DEBUG:openai._base_client:Sending HTTP Request: POST https://api.openai.com/v1/completions
DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>
DEBUG:httpcore.http11:send_request_headers.complete
DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>
DEBUG:httpcore.http11:send_request_body.complete
DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>
DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Sun, 07 Jul 2024 23:57:44 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-allow-origin', b'*'), (b'Cache-Control', b'no-cache, must-revalidate'), (b'openai-model', b'gpt-3.5-turbo-instruct'), (b'openai-organization', b'machinekingsmedia'), (b'openai-processing-ms', b'1095'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains'), (b'x-ratelimit-limit-requests', b'3500'), (b'x-ratelimit-limit-tokens', b'90000'), (b'x-ratelimit-remaining-requests', b'3499'), (b'x-ratelimit-remaining-tokens', b'88873'), (b'x-ratelimit-reset-requests', b'17ms'), (b'x-ratelimit-reset-tokens', b'750ms'), (b'x-request-id', b'req_b8ed05888fbf0bfa815e8e0c13397e5a'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'89fbc74a592a0573-IAD'), (b'Content-Encoding', b'br'), (b'alt-svc', b'h3=":443"; ma=86400')])
INFO:httpx:HTTP Request: POST https://api.openai.com/v1/completions "HTTP/1.1 200 OK"
DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>
DEBUG:httpcore.http11:receive_response_body.complete
DEBUG:httpcore.http11:response_closed.started
DEBUG:httpcore.http11:response_closed.complete
DEBUG:openai._base_client:HTTP Response: POST https://api.openai.com/v1/completions "200 OK" Headers({'date': 'Sun, 07 Jul 2024 23:57:44 GMT', 'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'access-control-allow-origin': '*', 'cache-control': 'no-cache, must-revalidate', 'openai-model': 'gpt-3.5-turbo-instruct', 'openai-organization': 'machinekingsmedia', 'openai-processing-ms': '1095', 'openai-version': '2020-10-01', 'strict-transport-security': 'max-age=31536000; includeSubDomains', 'x-ratelimit-limit-requests': '3500', 'x-ratelimit-limit-tokens': '90000', 'x-ratelimit-remaining-requests': '3499', 'x-ratelimit-remaining-tokens': '88873', 'x-ratelimit-reset-requests': '17ms', 'x-ratelimit-reset-tokens': '750ms', 'x-request-id': 'req_b8ed05888fbf0bfa815e8e0c13397e5a', 'cf-cache-status': 'DYNAMIC', 'server': 'cloudflare', 'cf-ray': '89fbc74a592a0573-IAD', 'content-encoding': 'br', 'alt-svc': 'h3=":443"; ma=86400'})
DEBUG:openai._base_client:request_id: req_b8ed05888fbf0bfa815e8e0c13397e5a
DEBUG:openai._base_client:Request options: {'method': 'post', 'url': '/completions', 'files': None, 'json_data': {'model': 'gpt-3.5-turbo-instruct', 'prompt': ["Summarize the following chapter in one sentence: We remember Murder, Moof, Odom, SmackDVD's."], 'frequency_penalty': 0, 'logit_bias': {}, 'max_tokens': 256, 'n': 1, 'presence_penalty': 0, 'temperature': 0.7, 'top_p': 1}}
DEBUG:openai._base_client:Sending HTTP Request: POST https://api.openai.com/v1/completions
DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>
DEBUG:httpcore.http11:send_request_headers.complete
DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>
DEBUG:httpcore.http11:send_request_body.complete
DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>
DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Sun, 07 Jul 2024 23:57:45 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-allow-origin', b'*'), (b'Cache-Control', b'no-cache, must-revalidate'), (b'openai-model', b'gpt-3.5-turbo-instruct'), (b'openai-organization', b'machinekingsmedia'), (b'openai-processing-ms', b'436'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains'), (b'x-ratelimit-limit-requests', b'3500'), (b'x-ratelimit-limit-tokens', b'90000'), (b'x-ratelimit-remaining-requests', b'3499'), (b'x-ratelimit-remaining-tokens', b'89721'), (b'x-ratelimit-reset-requests', b'17ms'), (b'x-ratelimit-reset-tokens', b'186ms'), (b'x-request-id', b'req_87e9da198b87849a83c0779dda5ff473'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'89fbc751c9f30573-IAD'), (b'Content-Encoding', b'br'), (b'alt-svc', b'h3=":443"; ma=86400')])
INFO:httpx:HTTP Request: POST https://api.openai.com/v1/completions "HTTP/1.1 200 OK"
DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>
DEBUG:httpcore.http11:receive_response_body.complete
DEBUG:httpcore.http11:response_closed.started
DEBUG:httpcore.http11:response_closed.complete
DEBUG:openai._base_client:HTTP Response: POST https://api.openai.com/v1/completions "200 OK" Headers({'date': 'Sun, 07 Jul 2024 23:57:45 GMT', 'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'access-control-allow-origin': '*', 'cache-control': 'no-cache, must-revalidate', 'openai-model': 'gpt-3.5-turbo-instruct', 'openai-organization': 'machinekingsmedia', 'openai-processing-ms': '436', 'openai-version': '2020-10-01', 'strict-transport-security': 'max-age=31536000; includeSubDomains', 'x-ratelimit-limit-requests': '3500', 'x-ratelimit-limit-tokens': '90000', 'x-ratelimit-remaining-requests': '3499', 'x-ratelimit-remaining-tokens': '89721', 'x-ratelimit-reset-requests': '17ms', 'x-ratelimit-reset-tokens': '186ms', 'x-request-id': 'req_87e9da198b87849a83c0779dda5ff473', 'cf-cache-status': 'DYNAMIC', 'server': 'cloudflare', 'cf-ray': '89fbc751c9f30573-IAD', 'content-encoding': 'br', 'alt-svc': 'h3=":443"; ma=86400'})
DEBUG:openai._base_client:request_id: req_87e9da198b87849a83c0779dda5ff473
DEBUG:openai._base_client:Request options: {'method': 'post', 'url': '/completions', 'files': None, 'json_data': {'model': 'gpt-3.5-turbo-instruct', 'prompt': ["Summarize the following chapter in one sentence: There's definitely a million ways I can integrate this. I can just make trailers, basically. I know he's a busy guy. He's a good guy. He's just busy."], 'frequency_penalty': 0, 'logit_bias': {}, 'max_tokens': 256, 'n': 1, 'presence_penalty': 0, 'temperature': 0.7, 'top_p': 1}}
DEBUG:openai._base_client:Sending HTTP Request: POST https://api.openai.com/v1/completions
DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>
DEBUG:httpcore.http11:send_request_headers.complete
DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>
DEBUG:httpcore.http11:send_request_body.complete
DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>
DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Sun, 07 Jul 2024 23:57:45 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-allow-origin', b'*'), (b'Cache-Control', b'no-cache, must-revalidate'), (b'openai-model', b'gpt-3.5-turbo-instruct'), (b'openai-organization', b'machinekingsmedia'), (b'openai-processing-ms', b'596'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains'), (b'x-ratelimit-limit-requests', b'3500'), (b'x-ratelimit-limit-tokens', b'90000'), (b'x-ratelimit-remaining-requests', b'3499'), (b'x-ratelimit-remaining-tokens', b'89695'), (b'x-ratelimit-reset-requests', b'17ms'), (b'x-ratelimit-reset-tokens', b'203ms'), (b'x-request-id', b'req_b49612740daf2544de9f8dc6a49aeded'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'89fbc755ce3f0573-IAD'), (b'Content-Encoding', b'br'), (b'alt-svc', b'h3=":443"; ma=86400')])
INFO:httpx:HTTP Request: POST https://api.openai.com/v1/completions "HTTP/1.1 200 OK"
DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>
DEBUG:httpcore.http11:receive_response_body.complete
DEBUG:httpcore.http11:response_closed.started
DEBUG:httpcore.http11:response_closed.complete
DEBUG:openai._base_client:HTTP Response: POST https://api.openai.com/v1/completions "200 OK" Headers({'date': 'Sun, 07 Jul 2024 23:57:45 GMT', 'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'access-control-allow-origin': '*', 'cache-control': 'no-cache, must-revalidate', 'openai-model': 'gpt-3.5-turbo-instruct', 'openai-organization': 'machinekingsmedia', 'openai-processing-ms': '596', 'openai-version': '2020-10-01', 'strict-transport-security': 'max-age=31536000; includeSubDomains', 'x-ratelimit-limit-requests': '3500', 'x-ratelimit-limit-tokens': '90000', 'x-ratelimit-remaining-requests': '3499', 'x-ratelimit-remaining-tokens': '89695', 'x-ratelimit-reset-requests': '17ms', 'x-ratelimit-reset-tokens': '203ms', 'x-request-id': 'req_b49612740daf2544de9f8dc6a49aeded', 'cf-cache-status': 'DYNAMIC', 'server': 'cloudflare', 'cf-ray': '89fbc755ce3f0573-IAD', 'content-encoding': 'br', 'alt-svc': 'h3=":443"; ma=86400'})
DEBUG:openai._base_client:request_id: req_b49612740daf2544de9f8dc6a49aeded
DEBUG:openai._base_client:Request options: {'method': 'post', 'url': '/completions', 'files': None, 'json_data': {'model': 'gpt-3.5-turbo-instruct', 'prompt': ["Summarize the following chapter in one sentence: But here's my question. Is this usable stuff? That's crazy. It's called UDIO.COM. Yo, let me show you how easy it is after this last song. I made this shit for the event. Check this shit out, Lex. Yo, music is changing, Lex. Check this shit out. Coming soon. The song sounded like it was AI. What'd you say? The drip song sounded AI. Yeah, it did. But these shits don't. Check this shit out. Coming soon to the Algorithm Institute of Battle Rap. League vs. League. Our Society vs. Gates of the Guard. We don't see these too often. Laughs. What's up, Dre? What's up, Kels? League vs. League. Who you think gonna win? Dre Dennis vs. Kels. Let the games begin. League vs. League. It's time to compete. Our Society and the Gates and the DMV. Yo, my daughter been singing this song nonstop. Yo, I don't know if you got kids. You got kids? And this is small, small break. I have a playlist that I've made for my daughter, teaching her things about life, about the family, about people in the family. And it's a great way to educate a little kid about making the music. And since you're a rapper, since you're a lyricist. Anyway, check this shit out, dog. Check this shit out. Yeah, yeah, yeah, yeah. Coming soon to the Algorithm Institute of Battle Rap. It's the intro. Oh, League vs. League. Our Society vs. Gates of the Garden. We don't see these too often. What's up, Dre? What's up, Kels? Let's go. Uh-huh, uh-huh, uh-huh. See, to the D, black to the C. League vs. League, who you think gonna win? Dre, Dennis vs. Kels, let the games begin. League vs. League, it's time to compete. Our Society and the Gates in the DMV. League vs. League, who you think gonna win? It's League vs. League, let the games begin. League vs. League, it's time to compete. Our Society and the Gates in the DMV. Kels called us soldiers, and Dre did too. Kid Slay answers the phone, and Ace did too. Clumsy had time, Rez Mafia was ready. Lady Conscience said, yeah, and the Saga got ready. Zeke and Cain caught bodies, and Elijah did well. T'Plaso went crazy, and Eunice called an L. P, Black, and Kruger was too close to call. He'll shop his money for Thor's fame against the wall. Bill was in the building, and Dez was on too. Ace no show, but that what he do. Bill was in the building, and Dez was on too. Ace no show, but that what he do. League vs. League, who you think gonna win? Dre, Dennis vs. Kels, let the games begin. Yo, Lex. Yo, Lex. Yo, Lex. Yo, Lex. Ha ha ha ha ha ha. That was incredible, man. Lex. The production value on iBattle shit better go up a little bit, Lex. Ha ha ha ha ha ha. Yo, flyers already look dope as fuck. Yeah, I mean, I guess at this point we don't have any excuses. None. But them trailers, though, you got lyricists on staff. I'm not gonna lie, though. When I first saw your stuff, I was like, how the fuck is he doing this? This is insane. It's not even me. My girl walked by the TV, and she immediately turned and she went, is that drugs? I was like, holy shit. Yo, I forgot. You had drugs on something. He had a name. Something Hellcat. I don't remember. I've booked drugs a ton of times. Oh, Archduke. Yeah, Archduke. What's his whole name? Archduke something. Redcat. Archduke Redcat. Yes. Okay. Okay. Okay. I hope drugs not mad at me, man. I mean, you know, listen, bro, I think what you do is very important. And I think that it's important. If I was a fucking 17-year-old battler coming up and I watched your fucking channel, I would be like, holy shit, I don't want to be like these guys."], 'frequency_penalty': 0, 'logit_bias': {}, 'max_tokens': 256, 'n': 1, 'presence_penalty': 0, 'temperature': 0.7, 'top_p': 1}}
DEBUG:openai._base_client:Sending HTTP Request: POST https://api.openai.com/v1/completions
DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>
DEBUG:httpcore.http11:send_request_headers.complete
DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>
DEBUG:httpcore.http11:send_request_body.complete
DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>
DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Sun, 07 Jul 2024 23:57:49 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-allow-origin', b'*'), (b'Cache-Control', b'no-cache, must-revalidate'), (b'openai-model', b'gpt-3.5-turbo-instruct'), (b'openai-organization', b'machinekingsmedia'), (b'openai-processing-ms', b'2725'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains'), (b'x-ratelimit-limit-requests', b'3500'), (b'x-ratelimit-limit-tokens', b'90000'), (b'x-ratelimit-remaining-requests', b'3499'), (b'x-ratelimit-remaining-tokens', b'88855'), (b'x-ratelimit-reset-requests', b'17ms'), (b'x-ratelimit-reset-tokens', b'763ms'), (b'x-request-id', b'req_32b585f3645e7081e0f41da8180aa1f2'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'89fbc75a4ae10573-IAD'), (b'Content-Encoding', b'br'), (b'alt-svc', b'h3=":443"; ma=86400')])
INFO:httpx:HTTP Request: POST https://api.openai.com/v1/completions "HTTP/1.1 200 OK"
DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>
DEBUG:httpcore.http11:receive_response_body.complete
DEBUG:httpcore.http11:response_closed.started
DEBUG:httpcore.http11:response_closed.complete
DEBUG:openai._base_client:HTTP Response: POST https://api.openai.com/v1/completions "200 OK" Headers({'date': 'Sun, 07 Jul 2024 23:57:49 GMT', 'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'access-control-allow-origin': '*', 'cache-control': 'no-cache, must-revalidate', 'openai-model': 'gpt-3.5-turbo-instruct', 'openai-organization': 'machinekingsmedia', 'openai-processing-ms': '2725', 'openai-version': '2020-10-01', 'strict-transport-security': 'max-age=31536000; includeSubDomains', 'x-ratelimit-limit-requests': '3500', 'x-ratelimit-limit-tokens': '90000', 'x-ratelimit-remaining-requests': '3499', 'x-ratelimit-remaining-tokens': '88855', 'x-ratelimit-reset-requests': '17ms', 'x-ratelimit-reset-tokens': '763ms', 'x-request-id': 'req_32b585f3645e7081e0f41da8180aa1f2', 'cf-cache-status': 'DYNAMIC', 'server': 'cloudflare', 'cf-ray': '89fbc75a4ae10573-IAD', 'content-encoding': 'br', 'alt-svc': 'h3=":443"; ma=86400'})
DEBUG:openai._base_client:request_id: req_32b585f3645e7081e0f41da8180aa1f2
DEBUG:openai._base_client:Request options: {'method': 'post', 'url': '/completions', 'files': None, 'json_data': {'model': 'gpt-3.5-turbo-instruct', 'prompt': ["Summarize the following chapter in one sentence: But that's so necessary, though. That's so necessary because they're probably gonna if they don't watch the video. There's so many lessons in this shit because battle rap has something every week, like every week is something that happens. There's so many lessons and I always try to pull a lesson out of it, too. Lex, I don't know if I don't really be putting people except for the Audi boom situation, but I don't really be putting people. Yeah. There's a lesson there, too. And you know what? I think the most important one is if you blow it up and you got something going on, there's no shame in working a fucking job to don't be an asshole. Don't be doing the wrong thing because you're too cool to work at Home Depot, you know, because now you're in jail and ain't nothing cool about that. Yeah, yeah, yeah. You know, so, you know, I think I think it's pretty fucking awesome. But Lex, you can custom make trailers for your for your battles. Like if you have a big battle coming up and when I tell you that this thing does crazy different genres, I have this thing called the app. This video that I'm working on about the URL app and I also like this is how versatile this shit is, Lex. You can tell this shit."], 'frequency_penalty': 0, 'logit_bias': {}, 'max_tokens': 256, 'n': 1, 'presence_penalty': 0, 'temperature': 0.7, 'top_p': 1}}
DEBUG:openai._base_client:Sending HTTP Request: POST https://api.openai.com/v1/completions
DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>
DEBUG:httpcore.http11:send_request_headers.complete
DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>
DEBUG:httpcore.http11:send_request_body.complete
DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>
DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Sun, 07 Jul 2024 23:57:50 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-allow-origin', b'*'), (b'Cache-Control', b'no-cache, must-revalidate'), (b'openai-model', b'gpt-3.5-turbo-instruct'), (b'openai-organization', b'machinekingsmedia'), (b'openai-processing-ms', b'665'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains'), (b'x-ratelimit-limit-requests', b'3500'), (b'x-ratelimit-limit-tokens', b'90000'), (b'x-ratelimit-remaining-requests', b'3499'), (b'x-ratelimit-remaining-tokens', b'89428'), (b'x-ratelimit-reset-requests', b'17ms'), (b'x-ratelimit-reset-tokens', b'381ms'), (b'x-request-id', b'req_49cf709e5ab92e54f7075a31e2c0345f'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'89fbc76ef8550573-IAD'), (b'Content-Encoding', b'br'), (b'alt-svc', b'h3=":443"; ma=86400')])
INFO:httpx:HTTP Request: POST https://api.openai.com/v1/completions "HTTP/1.1 200 OK"
DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>
DEBUG:httpcore.http11:receive_response_body.complete
DEBUG:httpcore.http11:response_closed.started
DEBUG:httpcore.http11:response_closed.complete
DEBUG:openai._base_client:HTTP Response: POST https://api.openai.com/v1/completions "200 OK" Headers({'date': 'Sun, 07 Jul 2024 23:57:50 GMT', 'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'access-control-allow-origin': '*', 'cache-control': 'no-cache, must-revalidate', 'openai-model': 'gpt-3.5-turbo-instruct', 'openai-organization': 'machinekingsmedia', 'openai-processing-ms': '665', 'openai-version': '2020-10-01', 'strict-transport-security': 'max-age=31536000; includeSubDomains', 'x-ratelimit-limit-requests': '3500', 'x-ratelimit-limit-tokens': '90000', 'x-ratelimit-remaining-requests': '3499', 'x-ratelimit-remaining-tokens': '89428', 'x-ratelimit-reset-requests': '17ms', 'x-ratelimit-reset-tokens': '381ms', 'x-request-id': 'req_49cf709e5ab92e54f7075a31e2c0345f', 'cf-cache-status': 'DYNAMIC', 'server': 'cloudflare', 'cf-ray': '89fbc76ef8550573-IAD', 'content-encoding': 'br', 'alt-svc': 'h3=":443"; ma=86400'})
DEBUG:openai._base_client:request_id: req_49cf709e5ab92e54f7075a31e2c0345f
DEBUG:openai._base_client:Request options: {'method': 'post', 'url': '/completions', 'files': None, 'json_data': {'model': 'gpt-3.5-turbo-instruct', 'prompt': ["Summarize the following chapter in one sentence: Stand up comedian performing at a comedy show, right? And I type there some fucking jokes. You ever hear URL talk about being for the culture? Yeah, for the culture. If the culture comes with a monthly subscription fee. Yeah, it sounds like somebody real. But it can do crazy. I've done birthday stuff for my family and shit like that, but it can do crazy genre. So and again, because you're because you're a rapper, like you should be able to come up with some words to describe some images. You should be able to come up with some words to describe some fucking music genres and you should be able to write lyrics man and be able to come up with some dope trailers and shit too. There you go. There you go. There you go. There you go. So, yeah, I just wanted to make sure that I put you on to the audio to ideogram. I'm going to send you the links just so you have it. Sounds good. Sounds good, man. Next. Thank you for your time. Once I put this union video out, I'm going to reach out to you again, try to connect with you and we're going to talk and I can't wait to hear your input and your ideas and I've already had people tell me that it's not going to work, but it's not about working or not working. It's about designing something like it doesn't have to be a coat that somebody going to put on, but let's design something that if somebody get cold enough they got a coat right there that is already tested and approved by all of the greats in the community and you are one of those greats, man. So again, thank you for your time. I think a union would work with the right people and the right people to take this serious and really benefit from it. Yo, Lex, I used this fucking AI I believe like two years ago. GPT-3 GPT-3, right? I used it to create this system in the middle of the night that I think could end police brutality. Like that was my goal that night to see I used the AI to end police brutality and I feel like I created something that was fucking amazing, right? I'm getting chills even thinking about it. That's the night that I knew that this shit could be used to create anything. If it convinced me personally that this shit that it came up with to this day that it will work and I really think that that shit will work. Like it was very detailed killing of unarmed black men specifically. Sorry. I thought that it could be used to do anything. You want to know the details? Because I got to stand up and walk around telling you this shit. The gist of it is check this shit out. I almost want to pull it up because there's so many details. All right. So imagine that this thing exists. So this is how it works. There's a there's a app called I deserve better app, right? And it starts off just concentrating on stopping or lowering to a certain threshold unarmed black men getting killed. It's a known threshold like this many unarmed black men got to get killed in order for us to go to open it up to other things like we take you take a vote or whatever to open it up to other things. I'm skipping out on some stuff, but I almost want to pull it up while I'm talking but the essence of it was yeah, let me pull it up. Give me one second. Give me one second because it I thought it was I thought it was a fucking work of art man. Like and you did this on GPT-3. Yeah GPT-3 like two years ago. Yeah, it's what you say that I'm really missing out on AI. Yeah, you are man. But it's kind of ironic too because the last question that you asked me before was I know that there's another era coming and I'm looking forward to you know, so so let me tell you there's this game called Project Zomboid that I also used to that I love because it's a zombie simulation game, right? It's like you're looking kind of down at them and it's so detailed like like you get sick if you drink water that that's not boiled out of a lake and it's like super super detail right super detail you you jump over you jump you run through some bushes and you can get a super detail, right? I use the AI to create something called the Zombrain to tell me how to I just tell it a situation and then"], 'frequency_penalty': 0, 'logit_bias': {}, 'max_tokens': 256, 'n': 1, 'presence_penalty': 0, 'temperature': 0.7, 'top_p': 1}}
DEBUG:openai._base_client:Sending HTTP Request: POST https://api.openai.com/v1/completions
DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>
DEBUG:httpcore.http11:send_request_headers.complete
DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>
DEBUG:httpcore.http11:send_request_body.complete
DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>
DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Sun, 07 Jul 2024 23:57:52 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-allow-origin', b'*'), (b'Cache-Control', b'no-cache, must-revalidate'), (b'openai-model', b'gpt-3.5-turbo-instruct'), (b'openai-organization', b'machinekingsmedia'), (b'openai-processing-ms', b'1972'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains'), (b'x-ratelimit-limit-requests', b'3500'), (b'x-ratelimit-limit-tokens', b'90000'), (b'x-ratelimit-remaining-requests', b'3499'), (b'x-ratelimit-remaining-tokens', b'88707'), (b'x-ratelimit-reset-requests', b'17ms'), (b'x-ratelimit-reset-tokens', b'862ms'), (b'x-request-id', b'req_b30deafda236ff58cd2ebc78dedc6b36'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'89fbc773cdeb0573-IAD'), (b'Content-Encoding', b'br'), (b'alt-svc', b'h3=":443"; ma=86400')])
INFO:httpx:HTTP Request: POST https://api.openai.com/v1/completions "HTTP/1.1 200 OK"
DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>
DEBUG:httpcore.http11:receive_response_body.complete
DEBUG:httpcore.http11:response_closed.started
DEBUG:httpcore.http11:response_closed.complete
DEBUG:openai._base_client:HTTP Response: POST https://api.openai.com/v1/completions "200 OK" Headers({'date': 'Sun, 07 Jul 2024 23:57:52 GMT', 'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'access-control-allow-origin': '*', 'cache-control': 'no-cache, must-revalidate', 'openai-model': 'gpt-3.5-turbo-instruct', 'openai-organization': 'machinekingsmedia', 'openai-processing-ms': '1972', 'openai-version': '2020-10-01', 'strict-transport-security': 'max-age=31536000; includeSubDomains', 'x-ratelimit-limit-requests': '3500', 'x-ratelimit-limit-tokens': '90000', 'x-ratelimit-remaining-requests': '3499', 'x-ratelimit-remaining-tokens': '88707', 'x-ratelimit-reset-requests': '17ms', 'x-ratelimit-reset-tokens': '862ms', 'x-request-id': 'req_b30deafda236ff58cd2ebc78dedc6b36', 'cf-cache-status': 'DYNAMIC', 'server': 'cloudflare', 'cf-ray': '89fbc773cdeb0573-IAD', 'content-encoding': 'br', 'alt-svc': 'h3=":443"; ma=86400'})
DEBUG:openai._base_client:request_id: req_b30deafda236ff58cd2ebc78dedc6b36
DEBUG:openai._base_client:Request options: {'method': 'post', 'url': '/completions', 'files': None, 'json_data': {'model': 'gpt-3.5-turbo-instruct', 'prompt': ["Summarize the following chapter in one sentence: it tells me what to do because the AI knew the fucking game it knew how to play the game. And so I was trying to build a guild in order to I wanted it to help me build a guild on discord and the shit did it like I ended up having a guild the guild was fucking awesome. The channel was still on discord and then that's when I knew that it could literally help me lead like I could if I feed it as much data as possible. So now at work what I do is I record meetings and I use another app to record meetings and when you record meetings if you're talking to a battler, right? And you record this and you put this in the AI not only are you able to get notes, but you're able to get you're able to do a lot of take the cognitive load of figuring out answers and and getting solutions like anyway, I want to let me show you this fucking I'm interested man. I want to I want to incorporate as much as I can. So for your business, this is what you should do when you're in a business meeting with your team and all of y'all talking about something that's important record that shit or use some type of AI app or quarter on chat GPT put that into the AI not only can you just put that into the AI, but you can also put that into the AI and you can put that into the AI. Not only can you just pass out notes for everybody. That's regular shit, right? But you can get solutions. You can tell it act as act as our AI brain our I battle brain and give us give us some some ideas that we can that we can possibly implement that we wouldn't have thought of otherwise and like you'll be shocked man. Like it'll it'll really give you some nice stuff. I've talked to the AI on Facebook. Yeah. Yeah, that's nice. That's nice. But but giving it giving it as much context as possible is really the key. So that's why you would give it that meeting because now it understands that you haven't that opera is always telling you the same thing and it's telling you that that kid Slade is doing this and I mean you're telling it that kid Slade is doing that. You're telling it that as an organization. Y'all need to figure out what y'all going to do next month for the card. And this is the third time that has happened where y'all kind of waited to the last minute. Then when you talk to it because information you gave it all of this context and you can interact with it in any way possible. Let's what I like to believe is that we're entering an age where you can interact with information. They used to say that the other age was information age, but now you can interact with information in a whole different way. And what I do at my channel is I take information and I'll let you interact with it by hearing somebody describe it to you like it's a story and then you see images to go along with it. But as a business you can use it to help to give you possible solution to suggest some software that's free that we can use that will save us time you describe a process that you have and then you can tell it give me three suggestions for us to fix this particular process that we have. But even before you if you start to take that approach if you don't take anything from this conversation, let's take this it's really great when you have the the AI to give you answers, but you're never going to get the best answer by just asking a question and then it comes back to you because a lot of times we don't give it enough context. We don't give it the big picture and that's how it works the best. So the best way to do that is to just spit out as much information as possible about a topic and then ask it and I've called this the interview method or interview approach even though that sounds stupid it's an easy way to remember it because you then ask the AI to interview you about the topic in order to do a certain thing. So you say you just had your iBattle meeting right you then fed the transcript to the AI you tell it okay ask me 10 questions about iBattle or about that meeting in order to solve all of our problems you know I'm just pulling something high out of the air or in order to give me great advice in order to help our company and trying to do what we're trying to do. So then it'll ask you questions with the goal of giving you great advice, but it'll ask you these questions powered by the meeting that y'all just had. So that's a lot of power. So this"], 'frequency_penalty': 0, 'logit_bias': {}, 'max_tokens': 256, 'n': 1, 'presence_penalty': 0, 'temperature': 0.7, 'top_p': 1}}
DEBUG:openai._base_client:Sending HTTP Request: POST https://api.openai.com/v1/completions
DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>
DEBUG:httpcore.http11:send_request_headers.complete
DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>
DEBUG:httpcore.http11:send_request_body.complete
DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>
DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Sun, 07 Jul 2024 23:57:53 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-allow-origin', b'*'), (b'Cache-Control', b'no-cache, must-revalidate'), (b'openai-model', b'gpt-3.5-turbo-instruct'), (b'openai-organization', b'machinekingsmedia'), (b'openai-processing-ms', b'682'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains'), (b'x-ratelimit-limit-requests', b'3500'), (b'x-ratelimit-limit-tokens', b'90000'), (b'x-ratelimit-remaining-requests', b'3499'), (b'x-ratelimit-remaining-tokens', b'88643'), (b'x-ratelimit-reset-requests', b'17ms'), (b'x-ratelimit-reset-tokens', b'904ms'), (b'x-request-id', b'req_40e0c221f9c4daabfd16e1c436f17cfa'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'89fbc78508b50573-IAD'), (b'Content-Encoding', b'br'), (b'alt-svc', b'h3=":443"; ma=86400')])
INFO:httpx:HTTP Request: POST https://api.openai.com/v1/completions "HTTP/1.1 200 OK"
DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>
DEBUG:httpcore.http11:receive_response_body.complete
DEBUG:httpcore.http11:response_closed.started
DEBUG:httpcore.http11:response_closed.complete
DEBUG:openai._base_client:HTTP Response: POST https://api.openai.com/v1/completions "200 OK" Headers({'date': 'Sun, 07 Jul 2024 23:57:53 GMT', 'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'access-control-allow-origin': '*', 'cache-control': 'no-cache, must-revalidate', 'openai-model': 'gpt-3.5-turbo-instruct', 'openai-organization': 'machinekingsmedia', 'openai-processing-ms': '682', 'openai-version': '2020-10-01', 'strict-transport-security': 'max-age=31536000; includeSubDomains', 'x-ratelimit-limit-requests': '3500', 'x-ratelimit-limit-tokens': '90000', 'x-ratelimit-remaining-requests': '3499', 'x-ratelimit-remaining-tokens': '88643', 'x-ratelimit-reset-requests': '17ms', 'x-ratelimit-reset-tokens': '904ms', 'x-request-id': 'req_40e0c221f9c4daabfd16e1c436f17cfa', 'cf-cache-status': 'DYNAMIC', 'server': 'cloudflare', 'cf-ray': '89fbc78508b50573-IAD', 'content-encoding': 'br', 'alt-svc': 'h3=":443"; ma=86400'})
DEBUG:openai._base_client:request_id: req_40e0c221f9c4daabfd16e1c436f17cfa
DEBUG:openai._base_client:Request options: {'method': 'post', 'url': '/completions', 'files': None, 'json_data': {'model': 'gpt-3.5-turbo-instruct', 'prompt': ["Summarize the following chapter in one sentence: solution to black man being killed by the police that I got from the AI it is it is an app it starts with an app where this is this is not a great explanation that I'm looking at. All right so people get to follow these cases first of all they get to follow these cases on this particular app. The app transcribes all of the legal proceedings and you're able to skip to certain parts like auto-generated chapters. If you want to see the officer testifying on the stand you can go there and you can do this. The team updates people but the money goes to pay for private detectives, high-powered lawyers and social media experts in order to shift public perception and to gain more information. Let me read that again. The funding is for private detectives, high-powered lawyers and social media experts and this is on the victim side. The unarmed black man that got killed now all of a sudden his family got access to private detectives, high-powered lawyers and social media experts and they get counseling, they get financial support all guided from the membership of this app. So you basically centralize the support system and fortify it. Yes but it gets deeper just like how you got your little rating system for your battlers Lex? A rating system for lawyers mother fucker, check this shit out. A rating system for lawyers. You use the app in order to find lawyers and you got a win-loss record. You got a win-loss record you choose your topic like oh I got a speeding ticket. You need a speeding ticket lawyer? The AI looks through his wins and losses. How many times did he win? How many times did he lose? And whatever like I don't know legal stuff but this stuff is easy for AI to comb through. So what is also used, this app is also used as a ranking system for lawyers. So ideally a lawyer is important for them to have their high priority cases like if they're an expensive lawyer you want to be undefeated on that bitch. You know what I mean? You basically see someone's batting average Exactly, a lawyer's batting average, an attorney and it's easy. Go ahead Before you pay them. Before you pay them, yes Yes, because you never know that shit, right? You never know that shit. And how do you find a lawyer? Wikipedia, like MMA fighters Yo, that's even a better idea, yo, for the idea Wikipedia Lawyers. Yeah, yeah, because the AI can easily scrape the web and then How do we not know your win to loss record of your fucking cases? Why is your record Why is your record sealed and mine's open? There you go. And they get paid so much money and you never know if the Unless they win Unless they win high attention cases, right? Yeah But other than that, it's But those are celebrity lawyers we're talking about I'm talking about, let's say Let's say a regular lawyer A regular lawyer that you got into a car accident And you know that the other dude was in the wrong. You got the dashboard cam And then you see that it says UPS on the fucking truck that hits you. And now you're looking for a lawyer Right now, what you're going to do, go to Yelp or just go to Google. In this world, you will go to this app You will type in what the fuck happened to you The AI will say, oh, you need this type of lawyer. Here goes some lawyers Here go their Wikipedia page. Here go their win-loss records You want this shit arranged by win-losses or money How bad do you want to win? Do you want to hear something real crazy? What you got? One of my battlers is a lawyer His name is Dunt, right? And when he was coming up He was a public defender So basically he's a public defender In Queens. He gets assigned to this kid Right? And the kid basically Turns to him in the courtroom as they Meet each other. And he's like Hey, are you Dunt from iBattle?"], 'frequency_penalty': 0, 'logit_bias': {}, 'max_tokens': 256, 'n': 1, 'presence_penalty': 0, 'temperature': 0.7, 'top_p': 1}}
DEBUG:openai._base_client:Sending HTTP Request: POST https://api.openai.com/v1/completions
DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>
DEBUG:httpcore.http11:send_request_headers.complete
DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>
DEBUG:httpcore.http11:send_request_body.complete
DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>
DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Sun, 07 Jul 2024 23:57:57 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-allow-origin', b'*'), (b'Cache-Control', b'no-cache, must-revalidate'), (b'openai-model', b'gpt-3.5-turbo-instruct'), (b'openai-organization', b'machinekingsmedia'), (b'openai-processing-ms', b'3679'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains'), (b'x-ratelimit-limit-requests', b'3500'), (b'x-ratelimit-limit-tokens', b'90000'), (b'x-ratelimit-remaining-requests', b'3499'), (b'x-ratelimit-remaining-tokens', b'88795'), (b'x-ratelimit-reset-requests', b'17ms'), (b'x-ratelimit-reset-tokens', b'803ms'), (b'x-request-id', b'req_df1c924aa19005a7716142130723457c'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'89fbc789fdc80573-IAD'), (b'Content-Encoding', b'br'), (b'alt-svc', b'h3=":443"; ma=86400')])
INFO:httpx:HTTP Request: POST https://api.openai.com/v1/completions "HTTP/1.1 200 OK"
DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>
DEBUG:httpcore.http11:receive_response_body.complete
DEBUG:httpcore.http11:response_closed.started
DEBUG:httpcore.http11:response_closed.complete
DEBUG:openai._base_client:HTTP Response: POST https://api.openai.com/v1/completions "200 OK" Headers({'date': 'Sun, 07 Jul 2024 23:57:57 GMT', 'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'access-control-allow-origin': '*', 'cache-control': 'no-cache, must-revalidate', 'openai-model': 'gpt-3.5-turbo-instruct', 'openai-organization': 'machinekingsmedia', 'openai-processing-ms': '3679', 'openai-version': '2020-10-01', 'strict-transport-security': 'max-age=31536000; includeSubDomains', 'x-ratelimit-limit-requests': '3500', 'x-ratelimit-limit-tokens': '90000', 'x-ratelimit-remaining-requests': '3499', 'x-ratelimit-remaining-tokens': '88795', 'x-ratelimit-reset-requests': '17ms', 'x-ratelimit-reset-tokens': '803ms', 'x-request-id': 'req_df1c924aa19005a7716142130723457c', 'cf-cache-status': 'DYNAMIC', 'server': 'cloudflare', 'cf-ray': '89fbc789fdc80573-IAD', 'content-encoding': 'br', 'alt-svc': 'h3=":443"; ma=86400'})
DEBUG:openai._base_client:request_id: req_df1c924aa19005a7716142130723457c
DEBUG:openai._base_client:Request options: {'method': 'post', 'url': '/completions', 'files': None, 'json_data': {'model': 'gpt-3.5-turbo-instruct', 'prompt': ["Summarize the following chapter in one sentence: And he's like, yeah, I am And he's like, are you my fucking lawyer? Yo, you should have told me that When I was recording for the story. That's hilarious He tells him yes, and he goes Yo, can you get me on iBattle? I battled He's like, I battle on Coliseum and shit So now, he battles with us His name is Pyrex Jones. He's on the league He's a battler and he got on the league by being defended By Dunt support Yeah I was watching an iBattle battle a couple months ago And I heard somebody They sounded so unique. And I wonder if that's him Let me see He kind of Damn, I can't really say He's got a little bit of hollow in him. I'm not gonna lie But that's It's because he's from Queens. That's why They're just from the same place"], 'frequency_penalty': 0, 'logit_bias': {}, 'max_tokens': 256, 'n': 1, 'presence_penalty': 0, 'temperature': 0.7, 'top_p': 1}}
DEBUG:openai._base_client:Sending HTTP Request: POST https://api.openai.com/v1/completions
DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>
DEBUG:httpcore.http11:send_request_headers.complete
DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>
DEBUG:httpcore.http11:send_request_body.complete
DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>
DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Sun, 07 Jul 2024 23:57:58 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-allow-origin', b'*'), (b'Cache-Control', b'no-cache, must-revalidate'), (b'openai-model', b'gpt-3.5-turbo-instruct'), (b'openai-organization', b'machinekingsmedia'), (b'openai-processing-ms', b'512'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains'), (b'x-ratelimit-limit-requests', b'3500'), (b'x-ratelimit-limit-tokens', b'90000'), (b'x-ratelimit-remaining-requests', b'3499'), (b'x-ratelimit-remaining-tokens', b'89548'), (b'x-ratelimit-reset-requests', b'17ms'), (b'x-ratelimit-reset-tokens', b'300ms'), (b'x-request-id', b'req_621a970e2ccb61f96980e321b864c885'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'89fbc7a1bdb70573-IAD'), (b'Content-Encoding', b'br'), (b'alt-svc', b'h3=":443"; ma=86400')])
INFO:httpx:HTTP Request: POST https://api.openai.com/v1/completions "HTTP/1.1 200 OK"
DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>
DEBUG:httpcore.http11:receive_response_body.complete
DEBUG:httpcore.http11:response_closed.started
DEBUG:httpcore.http11:response_closed.complete
DEBUG:openai._base_client:HTTP Response: POST https://api.openai.com/v1/completions "200 OK" Headers({'date': 'Sun, 07 Jul 2024 23:57:58 GMT', 'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'access-control-allow-origin': '*', 'cache-control': 'no-cache, must-revalidate', 'openai-model': 'gpt-3.5-turbo-instruct', 'openai-organization': 'machinekingsmedia', 'openai-processing-ms': '512', 'openai-version': '2020-10-01', 'strict-transport-security': 'max-age=31536000; includeSubDomains', 'x-ratelimit-limit-requests': '3500', 'x-ratelimit-limit-tokens': '90000', 'x-ratelimit-remaining-requests': '3499', 'x-ratelimit-remaining-tokens': '89548', 'x-ratelimit-reset-requests': '17ms', 'x-ratelimit-reset-tokens': '300ms', 'x-request-id': 'req_621a970e2ccb61f96980e321b864c885', 'cf-cache-status': 'DYNAMIC', 'server': 'cloudflare', 'cf-ray': '89fbc7a1bdb70573-IAD', 'content-encoding': 'br', 'alt-svc': 'h3=":443"; ma=86400'})
DEBUG:openai._base_client:request_id: req_621a970e2ccb61f96980e321b864c885
DEBUG:openai._base_client:Request options: {'method': 'post', 'url': '/completions', 'files': None, 'json_data': {'model': 'gpt-3.5-turbo-instruct', 'prompt': ["Summarize the following chapter in one sentence: We have a lot of those I've been doing this for a long time I have one where it was like"], 'frequency_penalty': 0, 'logit_bias': {}, 'max_tokens': 256, 'n': 1, 'presence_penalty': 0, 'temperature': 0.7, 'top_p': 1}}
DEBUG:openai._base_client:Sending HTTP Request: POST https://api.openai.com/v1/completions
DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>
DEBUG:httpcore.http11:send_request_headers.complete
DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>
DEBUG:httpcore.http11:send_request_body.complete
DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>
DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Sun, 07 Jul 2024 23:57:59 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-allow-origin', b'*'), (b'Cache-Control', b'no-cache, must-revalidate'), (b'openai-model', b'gpt-3.5-turbo-instruct'), (b'openai-organization', b'machinekingsmedia'), (b'openai-processing-ms', b'422'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains'), (b'x-ratelimit-limit-requests', b'3500'), (b'x-ratelimit-limit-tokens', b'90000'), (b'x-ratelimit-remaining-requests', b'3499'), (b'x-ratelimit-remaining-tokens', b'89710'), (b'x-ratelimit-reset-requests', b'17ms'), (b'x-ratelimit-reset-tokens', b'193ms'), (b'x-request-id', b'req_bcfa6c8f6d40bc90c8674e38b0d3190e'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'89fbc7a59a7c0573-IAD'), (b'Content-Encoding', b'br'), (b'alt-svc', b'h3=":443"; ma=86400')])
INFO:httpx:HTTP Request: POST https://api.openai.com/v1/completions "HTTP/1.1 200 OK"
DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>
DEBUG:httpcore.http11:receive_response_body.complete
DEBUG:httpcore.http11:response_closed.started
DEBUG:httpcore.http11:response_closed.complete
DEBUG:openai._base_client:HTTP Response: POST https://api.openai.com/v1/completions "200 OK" Headers({'date': 'Sun, 07 Jul 2024 23:57:59 GMT', 'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'access-control-allow-origin': '*', 'cache-control': 'no-cache, must-revalidate', 'openai-model': 'gpt-3.5-turbo-instruct', 'openai-organization': 'machinekingsmedia', 'openai-processing-ms': '422', 'openai-version': '2020-10-01', 'strict-transport-security': 'max-age=31536000; includeSubDomains', 'x-ratelimit-limit-requests': '3500', 'x-ratelimit-limit-tokens': '90000', 'x-ratelimit-remaining-requests': '3499', 'x-ratelimit-remaining-tokens': '89710', 'x-ratelimit-reset-requests': '17ms', 'x-ratelimit-reset-tokens': '193ms', 'x-request-id': 'req_bcfa6c8f6d40bc90c8674e38b0d3190e', 'cf-cache-status': 'DYNAMIC', 'server': 'cloudflare', 'cf-ray': '89fbc7a59a7c0573-IAD', 'content-encoding': 'br', 'alt-svc': 'h3=":443"; ma=86400'})
DEBUG:openai._base_client:request_id: req_bcfa6c8f6d40bc90c8674e38b0d3190e
DEBUG:openai._base_client:Request options: {'method': 'post', 'url': '/completions', 'files': None, 'json_data': {'model': 'gpt-3.5-turbo-instruct', 'prompt': ["Provide a brief summary of the following transcript: 1\n00:00:00,000 --> 00:00:14,680\nSo, long story short is, I've been using AI to build a framework for a union in Battlerap.\n\n2\n00:00:14,680 --> 00:00:21,040\nAnd I'd like to share it with the culture, I'm working on a video where it kind of breaks\n\n3\n00:00:21,040 --> 00:00:23,040\nthings down.\n\n4\n00:00:23,040 --> 00:00:29,840\nBut it's just a framework, it's not like, it's more of an idea that could be implemented\n\n5\n00:00:29,840 --> 00:00:35,240\nand even if it's not implemented, portions of it could be either implemented or considered\n\n6\n00:00:35,240 --> 00:00:42,040\na standard and you have some unique insight.\n\n7\n00:00:42,040 --> 00:00:47,160\nAnd I would like for you to take a look at the video once it's out.\n\n8\n00:00:47,160 --> 00:00:50,080\nTake a look at the documentation once it's out.\n\n9\n00:00:50,080 --> 00:00:56,759\nAnd I'll actually reach out to you as well and remind you, hey Lex, could you take a\n\n10\n00:00:56,759 --> 00:01:01,680\nlook at this and let's set up an interview and let me talk to you again.\n\n11\n00:01:01,680 --> 00:01:07,599\nAnd the purpose of me talking to you again would be to get your thoughts about the current\n\n12\n00:01:07,599 --> 00:01:13,599\nversion of the framework and then I'm going to revise it as I communicate with you and\n\n13\n00:01:13,599 --> 00:01:18,559\nother people in the Battlerap community in order to iterate it and make it better and\n\n14\n00:01:18,559 --> 00:01:20,239\nbetter and better over time.\n\n15\n00:01:20,239 --> 00:01:23,480\nOf course, again, using AI in order to do this.\n\n16\n00:01:23,480 --> 00:01:25,480\nThat's very exciting.\n\n17\n00:01:26,199 --> 00:01:27,199\nYo, you said it's exciting.\n\n18\n00:01:27,199 --> 00:01:28,199\nYo, that's dope.\n\n19\n00:01:28,199 --> 00:01:29,199\nThat's dope.\n\n20\n00:01:29,199 --> 00:01:32,879\nSome people just hate anything related to AI, Lex.\n\n21\n00:01:32,879 --> 00:01:35,639\nI don't know what the fuck is wrong with these motherfuckers.\n\n22\n00:01:35,639 --> 00:01:37,440\nI don't mind anything with AI.\n\n23\n00:01:37,440 --> 00:01:44,519\nA lot of people in my life are not in the Battlerap world, so very rarely do I get to\n\n24\n00:01:44,519 --> 00:01:50,839\ngeek out about cool, innovative stuff for Battlerap and that's why I'm just so excited.\n\n25\n00:01:51,360 --> 00:01:56,919\nLet me tell you how you're a lead because that's something that I'm a super-duper nerd\n\n26\n00:01:56,919 --> 00:01:59,000\nwhen it comes to AI, man.\n\n27\n00:01:59,000 --> 00:02:08,199\nMy channel is really just a case study in how to use AI in order to take stories and\n\n28\n00:02:08,199 --> 00:02:13,839\nturn them into something else that is really engaging for people.\n\n29\n00:02:13,839 --> 00:02:16,839\nI think that we can use it for families.\n\n30\n00:02:16,839 --> 00:02:23,880\nEven if my channel, your family, like your grandpa, his story was forever for your kids,\n\n31\n00:02:23,880 --> 00:02:31,479\nthey're able to watch a story about his life and it's animated or it really looks like\n\n32\n00:02:31,479 --> 00:02:32,479\nhim.\n\n33\n00:02:32,479 --> 00:02:39,479\nI'm a really big Battlerap enthusiast, so I decided to use it in that regard.\n\n34\n00:02:39,479 --> 00:02:43,960\nThen on the side, I was developing this union thing.\n\n35\n00:02:43,960 --> 00:02:48,240\nThe fact that you said it's exciting really excites me as well.\n\n36\n00:02:48,240 --> 00:02:57,600\nAs a thank you, what I want to do is I want to share with you something that I think that\n\n37\n00:02:57,600 --> 00:03:02,160\nyour lead could possibly use.\n\n38\n00:03:02,160 --> 00:03:04,320\nThat's what I use for music on my page.\n\n39\n00:03:04,320 --> 00:03:09,360\nYou may already know about this, but I'm just telling you because I'm a person and I just\n\n40\n00:03:09,360 --> 00:03:11,360\nhave a thousand ideas about stuff.\n\n41\n00:03:11,360 --> 00:03:12,360\nI'm super, super-\n\n42\n00:03:12,360 --> 00:03:13,360\nThat's what I do all day.\n\n43\n00:03:13,759 --> 00:03:15,759\nI just spill ideas and I can't even catch them all.\n\n44\n00:03:15,759 --> 00:03:16,759\nYeah.\n\n45\n00:03:16,759 --> 00:03:17,759\nThere you go.\n\n46\n00:03:17,759 --> 00:03:18,759\nThat's how I am, man.\n\n47\n00:03:18,759 --> 00:03:21,759\nI just tip over and I spill all day.\n\n48\n00:03:21,759 --> 00:03:22,759\nThere you go.\n\n49\n00:03:22,759 --> 00:03:23,759\nYou know what, Lex?\n\n50\n00:03:23,759 --> 00:03:27,960\nYou should use the approach that I use with AI.\n\n51\n00:03:27,960 --> 00:03:31,759\nYou should have a voice recorder where you're just spitting out all these ideas.\n\n52\n00:03:31,759 --> 00:03:38,759\nThen you feed that into the AI or you just talk to the AI using some chat GPT or something.\n\n53\n00:03:38,759 --> 00:03:40,679\nYou just spit out all these ideas.\n\n54\n00:03:41,080 --> 00:03:46,080\nYou're recording all these ideas on your notepad or something like that for an event, for example.\n\n55\n00:03:46,080 --> 00:03:47,080\nYou're recording yourself.\n\n56\n00:03:47,080 --> 00:03:49,080\nYou're just brainstorming ideas.\n\n57\n00:03:49,080 --> 00:03:53,679\nYou're not quite sure about anything, but you tell the AI this stuff and then you tell\n\n58\n00:03:53,679 --> 00:04:01,080\nit, ask me 10 questions so that you can give me 20 great ideas and solutions for all this\n\n59\n00:04:01,080 --> 00:04:02,080\nstuff.\n\n60\n00:04:02,080 --> 00:04:07,479\nThen it'll ask you, even though you spilled your guts about this event, it'll ask you\n\n61\n00:04:07,479 --> 00:04:13,279\nquestions in order to give you great advice or to solve your problems.\n\n62\n00:04:13,279 --> 00:04:18,279\nNot that it'll just immediately solve them, but if you go back and forth, man, you can\n\n63\n00:04:18,279 --> 00:04:23,279\nreally do some amazing things with especially organizations.\n\n64\n00:04:23,279 --> 00:04:26,279\nGuess what I'm using AI for right now.\n\n65\n00:04:26,279 --> 00:04:27,279\nWhat are you using it for?\n\n66\n00:04:27,279 --> 00:04:30,279\nOkay.\n\n67\n00:04:31,079 --> 00:04:38,079\nWe have a ranking system in our league for a lot of the battlers that are coming up to\n\n68\n00:04:38,079 --> 00:04:45,079\nhave basically direction in their career to be like, okay, you have this many wins, this\n\n69\n00:04:45,079 --> 00:04:46,079\nmany losses.\n\n70\n00:04:46,079 --> 00:04:49,079\nYou're number seven.\n\n71\n00:04:49,079 --> 00:04:54,079\nYou can't battle number one, but you could battle number four.\n\n72\n00:04:54,079 --> 00:04:57,079\nJust basically have a general idea for the battlers to know where they're at.\n\n73\n00:04:57,880 --> 00:05:06,880\nWhat I'm using the AI for is to input their scores and get their overalls, their averages.\n\n74\n00:05:06,880 --> 00:05:09,880\nWhat's their overall writing score?\n\n75\n00:05:09,880 --> 00:05:12,880\nWhat's their overall performance score?\n\n76\n00:05:12,880 --> 00:05:17,880\nWhat's the chances of them having fucking rebuttals in a round type shit?\n\n77\n00:05:17,880 --> 00:05:20,880\nWow.\n\n78\n00:05:21,679 --> 00:05:26,679\nHave you ever thought about, because you have a roster that I think would be really\n\n79\n00:05:26,679 --> 00:05:30,679\ngood for this.\n\n80\n00:05:30,679 --> 00:05:43,679\nHave you ever thought about a role playing, a management sim battle rap game?\n\n81\n00:05:43,679 --> 00:05:46,679\nYeah.\n\n82\n00:05:47,480 --> 00:05:53,480\nI've never thought about a simulated one, but I've definitely thought of a bunch of\n\n83\n00:05:53,480 --> 00:05:54,480\ndifferent ones.\n\n84\n00:05:54,480 --> 00:05:59,480\nOne of my battlers did one of those emulators for D&D and he put us all in it.\n\n85\n00:05:59,480 --> 00:06:05,480\nThere's an iBattle Final Fantasy game out there to play with all of us in it, but he\n\n86\n00:06:05,480 --> 00:06:09,480\njust basically made us and changed everybody's name in the game.\n\n87\n00:06:09,480 --> 00:06:15,480\nI think that's pretty cool, simulated battles.\n\n88\n00:06:16,279 --> 00:06:18,279\nIt's pretty much like what these kids do on Twitter, right?\n\n89\n00:06:18,279 --> 00:06:20,279\nThe impersonation battles, the same thing?\n\n90\n00:06:20,279 --> 00:06:22,279\nNo, that's not what I have in mind.\n\n91\n00:06:22,279 --> 00:06:32,279\nImagine if what you just mentioned before about the percentages of a rebuttal and all\n\n92\n00:06:32,279 --> 00:06:33,279\nthat stuff.\n\n93\n00:06:33,279 --> 00:06:40,279\nImagine if there were a ranking of, you also have a huge audience that interacts with your\n\n94\n00:06:40,279 --> 00:06:41,279\napp.\n\n95\n00:06:42,079 --> 00:06:48,079\nImagine if they could rank your battlers and their wordplay, their rebuttals, their\n\n96\n00:06:48,079 --> 00:06:49,079\nwhatever.\n\n97\n00:06:49,079 --> 00:06:56,079\nYou use those rankings like NBA Live does and even maybe adjustments after battles,\n\n98\n00:06:56,079 --> 00:06:59,079\nmaybe all their stats change in the game or whatever.\n\n99\n00:06:59,079 --> 00:07:06,079\nI'm telling you this because I'm going to probably do it one day, but if I don't do\n\n100\n00:07:06,079 --> 00:07:09,079\nit, I would love for someone else to actually do it.\n\n101\n00:07:09,880 --> 00:07:10,880\nYou have all the tools.\n\n102\n00:07:10,880 --> 00:07:11,880\nYou're cutting edge.\n\n103\n00:07:11,880 --> 00:07:14,880\nYour name is freaking iBattle, by the way.\n\n104\n00:07:14,880 --> 00:07:16,880\nI'm just full of ideas.\n\n105\n00:07:16,880 --> 00:07:18,880\nI love battle rap as much as you do.\n\n106\n00:07:18,880 --> 00:07:20,880\nI love helping people as much as you do.\n\n107\n00:07:20,880 --> 00:07:24,880\nI hope to bring what I do on my channel.\n\n108\n00:07:24,880 --> 00:07:27,880\nI want to teach all the leagues how to do what I do.\n\n109\n00:07:27,880 --> 00:07:33,880\nYour channel is really something else, man.\n\n110\n00:07:33,880 --> 00:07:34,880\nIt's very...\n\n111\n00:07:36,880 --> 00:07:38,880\nHow do I explain this?\n\n112\n00:07:39,679 --> 00:07:45,679\nI also, on the side, I manage pro wrestlers, right?\n\n113\n00:07:49,679 --> 00:07:53,679\nThey have a bunch of different ways to get people interested in their matches that we\n\n114\n00:07:53,679 --> 00:07:54,679\ndon't do in battle rap.\n\n115\n00:07:54,679 --> 00:07:57,679\nLike, for instance, 2pm call time.\n\n116\n00:07:57,679 --> 00:08:00,679\nYou're supposed to get there at 2pm no matter what time your match is and you have to shoot\n\n117\n00:08:00,679 --> 00:08:01,679\na promo.\n\n118\n00:08:01,679 --> 00:08:02,679\nRight?\n\n119\n00:08:02,679 --> 00:08:03,679\nIn wrestling.\n\n120\n00:08:03,679 --> 00:08:04,679\nIn wrestling you're talking, right?\n\n121\n00:08:04,679 --> 00:08:06,679\nYeah, yeah, yeah.\n\n122\n00:08:07,480 --> 00:08:09,480\nStuff like that, right?\n\n123\n00:08:11,480 --> 00:08:17,480\nAnd I'm saying that to say that in battle rap we don't have too much of that.\n\n124\n00:08:17,480 --> 00:08:18,480\nYou don't get to...\n\n125\n00:08:18,480 --> 00:08:23,480\nYou don't have anything about the battler outside of the battle to know about them,\n\n126\n00:08:23,480 --> 00:08:27,480\nto relate to them, to give you any fucking reason to watch them.\n\n127\n00:08:28,480 --> 00:08:29,480\nRight?\n\n128\n00:08:29,480 --> 00:08:32,479\nAnd I think your channel does that.\n\n129\n00:08:33,280 --> 00:08:38,280\nSo, my channel is just a teaser for my vision for battle rap.\n\n130\n00:08:38,280 --> 00:08:41,280\nSo, with this idea for this union, right?\n\n131\n00:08:41,280 --> 00:08:47,280\nImagine if I taught someone else.\n\n132\n00:08:47,280 --> 00:08:52,280\nLike, if there were someone else able to do exactly what I'm doing to be able to take...\n\n133\n00:08:52,280 --> 00:09:01,280\nLet me find a way to make SoulCon and A-Ward, like, to create a promo for them.\n\n134\n00:09:02,080 --> 00:09:03,080\nEven though they don't really have...\n\n135\n00:09:03,080 --> 00:09:05,080\nBut what if you don't?\n\n136\n00:09:05,080 --> 00:09:07,080\nWhat if there's something there already?\n\n137\n00:09:09,080 --> 00:09:12,080\nWhat if there's a reason for the battle?\n\n138\n00:09:12,080 --> 00:09:14,080\nWhat's a battle that we have?\n\n139\n00:09:14,080 --> 00:09:18,080\nWhat's a battle out there that people want that hasn't happened?\n\n140\n00:09:18,080 --> 00:09:20,080\nLet's say, like, T-Rock...\n\n141\n00:09:20,080 --> 00:09:21,080\nBig K...\n\n142\n00:09:21,080 --> 00:09:22,080\nYeah, yeah, yeah.\n\n143\n00:09:22,080 --> 00:09:23,080\nThere you go.\n\n144\n00:09:23,080 --> 00:09:24,080\nT-Rock, Easy to Bounce.\n\n145\n00:09:24,080 --> 00:09:25,080\nLet's say Big K has...\n\n146\n00:09:25,080 --> 00:09:28,080\nLet's say Summer Madness is coming.\n\n147\n00:09:28,080 --> 00:09:30,080\nT-Rock vs. Easy is announced, right?\n\n148\n00:09:30,080 --> 00:09:31,080\nUh-huh.\n\n149\n00:09:31,880 --> 00:09:36,880\nOutside of their tweets and a promo video from Durez,\n\n150\n00:09:36,880 --> 00:09:40,880\nthere's not going to be too much to pull me to the battle, right?\n\n151\n00:09:40,880 --> 00:09:41,880\nYep.\n\n152\n00:09:41,880 --> 00:09:44,880\nI think your channel has value in that\n\n153\n00:09:44,880 --> 00:09:48,880\nbecause you can take the already storyline that's there\n\n154\n00:09:48,880 --> 00:09:52,880\nand you can project it onto the audience fluidly\n\n155\n00:09:52,880 --> 00:09:55,880\nwithout having to force anything, right?\n\n156\n00:09:55,880 --> 00:09:56,880\nUh-huh.\n\n157\n00:09:56,880 --> 00:09:58,880\nThis is why the battle is happening.\n\n158\n00:09:58,880 --> 00:10:00,880\nThis is why you should care.\n\n159\n00:10:01,679 --> 00:10:04,679\nAnd it's a cool-ass fucking way to find out\n\n160\n00:10:04,679 --> 00:10:06,679\nvia the AI art, right?\n\n161\n00:10:06,679 --> 00:10:07,679\nYeah, yeah, yeah.\n\n162\n00:10:07,679 --> 00:10:11,679\nThat would be a dope way to find out about a battle, yeah.\n\n163\n00:10:11,679 --> 00:10:13,679\nNot even about the battle.\n\n164\n00:10:13,679 --> 00:10:16,679\nThis is what...\n\n165\n00:10:16,679 --> 00:10:20,679\nA URL should hire you to put that video on their channel\n\n166\n00:10:20,679 --> 00:10:22,679\non why you should watch the battle.\n\n167\n00:10:22,679 --> 00:10:26,679\nWell, I think that iBattle should have models,\n\n168\n00:10:26,679 --> 00:10:29,679\nlike AI models of all their battlers.\n\n169\n00:10:30,479 --> 00:10:34,479\nYou never have to have them to come into your studio.\n\n170\n00:10:34,479 --> 00:10:36,479\nIf you tell them to take pictures\n\n171\n00:10:36,479 --> 00:10:39,479\nand you go to a website called astrea.ai,\n\n172\n00:10:39,479 --> 00:10:41,479\nyou can take pictures of them.\n\n173\n00:10:41,479 --> 00:10:44,479\nThat's how I did a lot of my early videos,\n\n174\n00:10:44,479 --> 00:10:47,479\nif you see how I did the Pat State video.\n\n175\n00:10:47,479 --> 00:10:50,479\nWait a minute, was it the Pat State video?\n\n176\n00:10:50,479 --> 00:10:52,479\nYeah, the Pat State video,\n\n177\n00:10:52,479 --> 00:10:56,479\nwhere certain pictures where it really looks like Pat State.\n\n178\n00:10:56,479 --> 00:10:58,479\nIt really looks like him.\n\n179\n00:10:59,280 --> 00:11:02,280\nSome of them don't really look like him,\n\n180\n00:11:02,280 --> 00:11:04,280\nbut some of them really do.\n\n181\n00:11:04,280 --> 00:11:06,280\nBut that same technology,\n\n182\n00:11:06,280 --> 00:11:09,280\nif you have models of all your battlers,\n\n183\n00:11:09,280 --> 00:11:12,280\nyou can create amazing promotions.\n\n184\n00:11:12,280 --> 00:11:15,280\nLet's say you want to do something that's like\n\n185\n00:11:15,280 --> 00:11:18,280\nTill Death Do Us Part 5, right?\n\n186\n00:11:18,280 --> 00:11:20,280\nAnd you want to have them looking like\n\n187\n00:11:20,280 --> 00:11:22,280\nthey're in funeral clothes and sad.\n\n188\n00:11:22,280 --> 00:11:24,280\nYou never have to have them to come in.\n\n189\n00:11:24,280 --> 00:11:27,280\nYou could use AI art really, really easy\n\n190\n00:11:27,280 --> 00:11:29,280\nby just typing the stuff up\n\n191\n00:11:29,280 --> 00:11:31,280\nand you already have the models for them.\n\n192\n00:11:31,280 --> 00:11:33,280\nIf the next card is a barbarian card\n\n193\n00:11:33,280 --> 00:11:35,280\nand you want to have them looking like\n\n194\n00:11:35,280 --> 00:11:37,280\nMad Max or something like that,\n\n195\n00:11:37,280 --> 00:11:39,280\nyou can do that as well.\n\n196\n00:11:39,280 --> 00:11:41,280\nI think that would also help\n\n197\n00:11:41,280 --> 00:11:43,280\nbring things to the table.\n\n198\n00:11:43,280 --> 00:11:45,280\nAs far as the approach that I used...\n\n199\n00:11:45,280 --> 00:11:47,280\nThat's badass.\n\n200\n00:11:47,280 --> 00:11:50,280\nYeah, that's really badass.\n\n201\n00:11:50,280 --> 00:11:52,280\nLet me tell you another thing.\n\n202\n00:11:52,280 --> 00:11:54,280\nI just told you astrea.ai.\n\n203\n00:11:54,280 --> 00:11:56,280\nI'll send it to you on the...\n\n204\n00:11:56,280 --> 00:11:58,280\nIs that the same site\n\n205\n00:11:58,280 --> 00:12:00,280\nthat you can make it for\n\n206\n00:12:00,280 --> 00:12:02,280\nwhere I can just upload a picture of Real Sick\n\n207\n00:12:02,280 --> 00:12:04,280\nand a picture of City Towers\n\n208\n00:12:04,280 --> 00:12:06,280\nand be like, make these two guys\n\n209\n00:12:06,280 --> 00:12:08,280\nAlien vs. Predator and it'll just do it?\n\n210\n00:12:08,280 --> 00:12:10,280\nNo, this doesn't do that.\n\n211\n00:12:10,280 --> 00:12:12,280\nWhat this does is\n\n212\n00:12:12,280 --> 00:12:16,280\nyou upload maybe 10 to 12 pictures\n\n213\n00:12:16,280 --> 00:12:19,280\nas different as possible.\n\n214\n00:12:19,280 --> 00:12:21,280\nThis is kind of an older technology\n\n215\n00:12:21,280 --> 00:12:23,280\nas far as AI is concerned,\n\n216\n00:12:23,280 --> 00:12:25,280\nbut I think it's perfect for battle rap\n\n217\n00:12:25,280 --> 00:12:27,280\nand for leagues.\n\n218\n00:12:27,280 --> 00:12:29,280\nI can show it to you the next time.\n\n219\n00:12:29,280 --> 00:12:31,280\nI can show it to you on a call\n\n220\n00:12:31,280 --> 00:12:33,280\nif you want to demo it to you.\n\n221\n00:12:33,280 --> 00:12:35,280\nWhat you would do is\n\n222\n00:12:35,280 --> 00:12:37,280\nyou would get 10 pictures of Real Sick\n\n223\n00:12:37,280 --> 00:12:39,280\ndoing different things.\n\n224\n00:12:39,280 --> 00:12:41,280\nOr go to his Facebook page\n\n225\n00:12:41,280 --> 00:12:43,280\nor rip some screenshots\n\n226\n00:12:43,280 --> 00:12:46,280\nfrom Instagram or whatever.\n\n227\n00:12:46,280 --> 00:12:48,280\nPut them in here and then...\n\n228\n00:12:48,280 --> 00:12:50,280\nWho was the other person you said?\n\n229\n00:12:50,280 --> 00:12:52,280\nAnd City Towers.\n\n230\n00:12:52,280 --> 00:12:55,280\nGet about 10-15 pictures of City Towers.\n\n231\n00:12:55,280 --> 00:12:58,280\nThen, now that you have those two separate,\n\n232\n00:12:58,280 --> 00:13:01,280\nyou could do the Alien vs. Predator.\n\n233\n00:13:01,280 --> 00:13:03,280\nMake Real Sick look...\n\n234\n00:13:03,280 --> 00:13:05,280\nMake this man...\n\n235\n00:13:05,280 --> 00:13:07,280\nIt's a specific language\n\n236\n00:13:07,280 --> 00:13:09,280\nyou have to use here.\n\n237\n00:13:09,280 --> 00:13:11,280\nYeah, like it's a child, right?\n\n238\n00:13:11,280 --> 00:13:13,280\nNo, like it's a token.\n\n239\n00:13:13,280 --> 00:13:15,280\nYou wouldn't actually put his name,\n\n240\n00:13:15,280 --> 00:13:17,280\nbut on this particular site,\n\n241\n00:13:17,280 --> 00:13:19,280\nit's a token like it's SKS\n\n242\n00:13:20,280 --> 00:13:23,280\nor SKS Man or something like that.\n\n243\n00:13:23,280 --> 00:13:25,280\nBut anyway, I can show it to you\n\n244\n00:13:25,280 --> 00:13:27,280\nif you like, but Astrea.ai,\n\n245\n00:13:27,280 --> 00:13:29,280\nif you want to play with it,\n\n246\n00:13:29,280 --> 00:13:32,280\nupload like 10 pictures of yourself.\n\n247\n00:13:32,280 --> 00:13:34,280\nAnd then in about 30...\n\n248\n00:13:34,280 --> 00:13:36,280\nI think it costs like five bucks.\n\n249\n00:13:36,280 --> 00:13:38,280\nAnd then in about 30 minutes,\n\n250\n00:13:38,280 --> 00:13:40,280\nit'll return to you a bunch of pictures.\n\n251\n00:13:40,280 --> 00:13:43,280\nI can show you right now True Fo.\n\n252\n00:13:43,280 --> 00:13:45,280\nI was doing some stuff with True Fo\n\n253\n00:13:45,280 --> 00:13:47,280\na while ago and I...\n\n254\n00:13:47,280 --> 00:13:49,280\nHow many pictures do I have?\n\n255\n00:13:50,280 --> 00:13:52,280\nYeah, if you have an iPhone,\n\n256\n00:13:52,280 --> 00:13:54,280\nI can show you the FaceTime,\n\n257\n00:13:54,280 --> 00:13:56,280\nbut it's really good\n\n258\n00:13:56,280 --> 00:13:58,280\nand it totally can be used.\n\n259\n00:13:58,280 --> 00:14:00,280\nMan, fuck it. Hold on. Let me put on...\n\n260\n00:14:00,280 --> 00:14:02,280\nYou got iPhone, right?\n\n261\n00:14:02,280 --> 00:14:04,280\nHold on, hold on.\n\n262\n00:14:04,280 --> 00:14:06,280\nLook at this shit.\n\n263\n00:14:09,280 --> 00:14:11,280\nLook at this shit.\n\n264\n00:14:11,280 --> 00:14:13,280\nThat's not really True Fo.\n\n265\n00:14:13,280 --> 00:14:15,280\nThat's AI True Fo\n\n266\n00:14:15,280 --> 00:14:17,280\nbecause I was able to make\n\n267\n00:14:17,280 --> 00:14:19,280\nthe model of him\n\n268\n00:14:19,280 --> 00:14:21,280\nbased on his pictures.\n\n269\n00:14:21,280 --> 00:14:23,280\nLook at this shit.\n\n270\n00:14:25,280 --> 00:14:27,280\nIt looks just like...\n\n271\n00:14:27,280 --> 00:14:29,280\nYeah, it really does.\n\n272\n00:14:29,280 --> 00:14:31,280\nIt really does.\n\n273\n00:14:31,280 --> 00:14:33,280\nThere are some imperfections.\n\n274\n00:14:33,280 --> 00:14:35,280\nIf you look at the guitar or whatever,\n\n275\n00:14:35,280 --> 00:14:37,280\nit's not as perfect.\n\n276\n00:14:37,280 --> 00:14:39,280\nHis fingers look kind of crazy.\n\n277\n00:14:39,280 --> 00:14:41,280\nYeah, his fingers definitely look crazy.\n\n278\n00:14:41,280 --> 00:14:43,280\nThis particular technology\n\n279\n00:14:44,280 --> 00:14:46,280\nbut it always\n\n280\n00:14:46,280 --> 00:14:48,280\nis going to try to hide the fingers.\n\n281\n00:14:48,280 --> 00:14:50,280\nIt's just so good\n\n282\n00:14:50,280 --> 00:14:53,280\nat giving you a person\n\n283\n00:14:53,280 --> 00:14:55,280\nin multiple looks.\n\n284\n00:14:55,280 --> 00:14:57,280\nThat's a completely different look\n\n285\n00:14:57,280 --> 00:14:59,280\nthan from what you saw.\n\n286\n00:15:03,280 --> 00:15:05,280\nLook at this.\n\n287\n00:15:05,280 --> 00:15:07,280\nI've seen Twizz put some stuff like this up.\n\n288\n00:15:07,280 --> 00:15:09,280\nDid you see it?\n\n289\n00:15:09,280 --> 00:15:11,280\nWhen I was on Facebook,\n\n290\n00:15:11,280 --> 00:15:13,280\nI used to see it.\n\n291\n00:15:13,280 --> 00:15:15,280\nWhat Twizz does\n\n292\n00:15:15,280 --> 00:15:17,280\nis he uses\n\n293\n00:15:17,280 --> 00:15:19,280\nmid-journey\n\n294\n00:15:19,280 --> 00:15:21,280\nbut then he uses face swaps.\n\n295\n00:15:21,280 --> 00:15:23,280\nYou see how this doesn't quite look like\n\n296\n00:15:23,280 --> 00:15:25,280\nHomesy. It kind of looks like him\n\n297\n00:15:25,280 --> 00:15:27,280\nbut it doesn't quite look like him.\n\n298\n00:15:27,280 --> 00:15:29,280\nIf you can get close to him,\n\n299\n00:15:29,280 --> 00:15:31,280\nyou can use this technology in here\n\n300\n00:15:31,280 --> 00:15:33,280\nto do a face swap.\n\n301\n00:15:33,280 --> 00:15:35,280\nThat's how he does that.\n\n302\n00:15:35,280 --> 00:15:37,280\nIt's really...\n\n303\n00:15:37,280 --> 00:15:39,280\nYou could do that\n\n304\n00:15:39,280 --> 00:15:41,280\nif you want to do that.\n\n305\n00:15:41,280 --> 00:15:43,280\nAgain, I can show you that shit really quick\n\n306\n00:15:43,280 --> 00:15:45,280\nbut using a face swap\n\n307\n00:15:45,280 --> 00:15:47,280\nyou could...\n\n308\n00:15:47,280 --> 00:15:49,280\nUsing a face swap\n\n309\n00:15:49,280 --> 00:15:51,280\nin mid-journey is how you do that.\n\n310\n00:15:51,280 --> 00:15:53,280\nYou have to be in Discord and shit like that\n\n311\n00:15:53,280 --> 00:15:55,280\nbut that's the way that he does that.\n\n312\n00:15:57,280 --> 00:15:59,280\nThat's not the best way\n\n313\n00:15:59,280 --> 00:16:01,280\nto get the best likeness.\n\n314\n00:16:01,280 --> 00:16:03,280\nSince I'm showing you different AIs,\n\n315\n00:16:03,280 --> 00:16:05,280\nlet me show you a different one too\n\n316\n00:16:05,280 --> 00:16:07,280\nthat you can also use on your cards.\n\n317\n00:16:07,280 --> 00:16:09,280\nIt's called Ideogram.\n\n318\n00:16:09,280 --> 00:16:11,280\nWhat Ideogram is\n\n319\n00:16:11,280 --> 00:16:13,280\nis it's really good at text.\n\n320\n00:16:13,280 --> 00:16:15,280\nReally, really\n\n321\n00:16:15,280 --> 00:16:17,280\nfucking good at text.\n\n322\n00:16:17,280 --> 00:16:19,280\nLet me show you how good\n\n323\n00:16:19,280 --> 00:16:21,280\nat text it is. These are my last ones.\n\n324\n00:16:21,280 --> 00:16:23,280\nLook at this shit.\n\n325\n00:16:23,280 --> 00:16:25,280\nOur Society Battle League.\n\n326\n00:16:25,280 --> 00:16:27,280\nYou would have paid a motherfucker\n\n327\n00:16:27,280 --> 00:16:29,280\nto make this a couple years ago.\n\n328\n00:16:29,280 --> 00:16:31,280\nNot even a couple years ago.\n\n329\n00:16:31,280 --> 00:16:33,280\nA couple hours ago.\n\n330\n00:16:33,280 --> 00:16:35,280\nLook at this.\n\n331\n00:16:35,280 --> 00:16:37,280\nGates of the Guard. Look at that shit.\n\n332\n00:16:37,280 --> 00:16:39,280\nThis is top-notch\n\n333\n00:16:39,280 --> 00:16:41,280\nPhotoshop work.\n\n334\n00:16:41,280 --> 00:16:43,280\nAll I typed in was...\n\n335\n00:16:43,280 --> 00:16:45,280\nI didn't type in all this shit\n\n336\n00:16:45,280 --> 00:16:47,280\nI don't think.\n\n337\n00:16:47,280 --> 00:16:49,280\nI actually didn't type this in.\n\n338\n00:16:49,280 --> 00:16:51,280\nIt changed my prompt\n\n339\n00:16:51,280 --> 00:16:53,280\nfrom an earlier one.\n\n340\n00:16:53,280 --> 00:16:55,280\nYou put in\n\n341\n00:16:55,280 --> 00:16:57,280\nsomething and then it'll\n\n342\n00:16:57,280 --> 00:16:59,280\nchange it and\n\n343\n00:16:59,280 --> 00:17:01,280\nmake it better or whatever.\n\n344\n00:17:01,280 --> 00:17:03,280\nSometimes though, if you\n\n345\n00:17:03,280 --> 00:17:05,280\nfuck with it, sometimes it'll mess up shit like that.\n\n346\n00:17:05,280 --> 00:17:07,280\nYou see how the G's is fucked up?\n\n347\n00:17:07,280 --> 00:17:09,280\nIf you\n\n348\n00:17:09,280 --> 00:17:11,280\nregenerate, it seems like it'll\n\n349\n00:17:11,280 --> 00:17:13,280\nget it right.\n\n350\n00:17:13,280 --> 00:17:15,280\nThis one, it got it right.\n\n351\n00:17:15,280 --> 00:17:17,280\nThe A's are a little covered up.\n\n352\n00:17:17,280 --> 00:17:19,280\nThis shit is free, dog.\n\n353\n00:17:19,280 --> 00:17:21,280\nThat's the craziest part of this particular\n\n354\n00:17:21,280 --> 00:17:23,280\ntechnology.\n\n355\n00:17:23,280 --> 00:17:25,280\nThis shit is so good and the fact that it's free\n\n356\n00:17:25,280 --> 00:17:27,280\nis crazy.\n\n357\n00:17:27,280 --> 00:17:29,280\nWhatever you type in,\n\n358\n00:17:29,280 --> 00:17:31,280\nyou can use ChatGPT\n\n359\n00:17:31,280 --> 00:17:33,280\nto give you a prompt.\n\n360\n00:17:33,280 --> 00:17:35,280\nTell it to write me an image prompt\n\n361\n00:17:35,280 --> 00:17:37,280\nfor whatever.\n\n362\n00:17:37,280 --> 00:17:39,280\nThat's what that is to the right.\n\n363\n00:17:39,280 --> 00:17:41,280\nIt's the image prompt.\n\n364\n00:17:41,280 --> 00:17:43,280\nThat is the image prompt.\n\n365\n00:17:43,280 --> 00:17:45,280\nThis is the prompt and then the magic prompt\n\n366\n00:17:45,280 --> 00:17:47,280\nis what it changes\n\n367\n00:17:47,280 --> 00:17:49,280\nyour prompt to or whatever.\n\n368\n00:17:49,280 --> 00:17:51,280\nYou see an abstract design\n\n369\n00:17:51,280 --> 00:17:53,280\nwith a prominent gate\n\n370\n00:17:53,280 --> 00:17:55,280\nusing vibrant colors.\n\n371\n00:17:59,280 --> 00:18:01,280\nSee that shit right there?\n\n372\n00:18:03,280 --> 00:18:05,280\nIt really follows that shit.\n\n373\n00:18:05,280 --> 00:18:07,280\nLet me also help you with your theme shit.\n\n374\n00:18:07,280 --> 00:18:09,280\nI don't know how closely you\n\n375\n00:18:09,280 --> 00:18:11,280\nfuck with my page, but\n\n376\n00:18:11,280 --> 00:18:13,280\nI got a lot of custom music on my page.\n\n377\n00:18:17,280 --> 00:18:19,280\nListen to this fucking song.\n\n378\n00:18:19,280 --> 00:18:21,280\nThe same way that shit\n\n379\n00:18:21,280 --> 00:18:23,280\ngenerated pictures,\n\n380\n00:18:23,280 --> 00:18:25,280\nlisten to this fucking song called\n\n381\n00:18:25,280 --> 00:18:27,280\nCan't Copy Respect.\n\n382\n00:18:27,280 --> 00:18:29,280\nListen to this shit.\n\n383\n00:18:33,280 --> 00:18:35,280\nNo.\n\n384\n00:18:35,280 --> 00:18:37,280\nNo, no, no.\n\n385\n00:18:37,280 --> 00:18:39,280\nThe outro to the videos is using this exact same\n\n386\n00:18:39,280 --> 00:18:41,280\ntechnology. Yes.\n\n387\n00:18:41,280 --> 00:18:43,280\nYeah, you just type.\n\n388\n00:18:43,280 --> 00:18:45,280\nCheck this shit out.\n\n389\n00:18:47,280 --> 00:18:49,280\nYeah, yeah, yeah.\n\n390\n00:18:49,280 --> 00:18:51,280\nCheck this shit out.\n\n391\n00:18:51,280 --> 00:18:53,280\nI write the rhymes because I used to write way back in the days\n\n392\n00:18:53,280 --> 00:18:55,280\nand shit, but check this shit out.\n\n393\n00:18:55,280 --> 00:18:57,280\nCheck this shit out.\n\n394\n00:18:57,280 --> 00:18:59,280\nI just typed this shit in.\n\n395\n00:18:59,280 --> 00:19:01,280\nThis is what the fuck I typed in.\n\n396\n00:19:01,280 --> 00:19:03,280\nIt's URL you can't copy respect time.\n\n397\n00:19:03,280 --> 00:19:05,280\nIt's URL you can't copy respect.\n\n398\n00:19:05,280 --> 00:19:07,280\nIt's URL you can't copy respect.\n\n399\n00:19:07,280 --> 00:19:09,280\nLet's get it.\n\n400\n00:19:09,280 --> 00:19:11,280\nWe remember murder move on them smack DVDs.\n\n401\n00:19:11,280 --> 00:19:13,280\nThe leaders of the culture\n\n402\n00:19:13,280 --> 00:19:15,280\nsmack Beasley and Pete.\n\n403\n00:19:15,280 --> 00:19:17,280\nIt's the URL you can't copy respect.\n\n404\n00:19:17,280 --> 00:19:19,280\nRight?\n\n405\n00:19:19,280 --> 00:19:21,280\nCheck out this shit.\n\n406\n00:19:21,280 --> 00:19:31,280\nThis is URL... you can't copy respect, this is URL... you can't copy respect...\n\n407\n00:19:31,280 --> 00:19:34,280\nWe remember Murder, Moof, Odom, SmackDVD's.\n\n408\n00:19:34,280 --> 00:19:37,280\nThe leaders of the culture, Smack, Beasley and V.\n\n409\n00:19:37,280 --> 00:19:46,280\nThis is URL... you can't copy respect, this is URL... you can't copy respect.\n\n410\n00:19:47,280 --> 00:19:50,280\nWe remember Murder, Moof, Odom, SmackDVD's.\n\n411\n00:19:50,280 --> 00:19:54,280\nThe leaders of the culture, Smack, Beasley and V.\n\n412\n00:20:16,280 --> 00:20:19,280\nWe remember Murder, Moof, Odom, SmackDVD's.\n\n413\n00:20:46,280 --> 00:21:06,280\nThere's definitely a million ways I can integrate this. I can just make trailers, basically.\n\n414\n00:21:06,280 --> 00:21:18,280\nI know he's a busy guy. He's a good guy. He's just busy.\n\n415\n00:21:36,280 --> 00:21:46,280\nBut here's my question. Is this usable stuff?\n\n416\n00:21:46,280 --> 00:21:48,280\nThat's crazy.\n\n417\n00:21:48,280 --> 00:21:50,280\nIt's called UDIO.COM.\n\n418\n00:21:50,280 --> 00:21:55,280\nYo, let me show you how easy it is after this last song.\n\n419\n00:21:55,280 --> 00:22:02,280\nI made this shit for the event. Check this shit out, Lex.\n\n420\n00:22:02,280 --> 00:22:06,280\nYo, music is changing, Lex. Check this shit out.\n\n421\n00:22:08,280 --> 00:22:09,280\nComing soon.\n\n422\n00:22:09,280 --> 00:22:11,280\nThe song sounded like it was AI.\n\n423\n00:22:11,280 --> 00:22:12,280\nWhat'd you say?\n\n424\n00:22:12,280 --> 00:22:14,280\nThe drip song sounded AI.\n\n425\n00:22:14,280 --> 00:22:16,280\nYeah, it did. But these shits don't.\n\n426\n00:22:16,280 --> 00:22:17,280\nCheck this shit out.\n\n427\n00:22:17,280 --> 00:22:19,280\nComing soon to the Algorithm Institute of Battle Rap.\n\n428\n00:22:19,280 --> 00:22:22,280\nLeague vs. League. Our Society vs. Gates of the Guard.\n\n429\n00:22:22,280 --> 00:22:24,280\nWe don't see these too often.\n\n430\n00:22:24,280 --> 00:22:25,280\nLaughs.\n\n431\n00:22:25,280 --> 00:22:26,280\nWhat's up, Dre?\n\n432\n00:22:26,280 --> 00:22:27,280\nWhat's up, Kels?\n\n433\n00:22:27,280 --> 00:22:28,280\nLeague vs. League.\n\n434\n00:22:28,280 --> 00:22:29,280\nWho you think gonna win?\n\n435\n00:22:29,280 --> 00:22:31,280\nDre Dennis vs. Kels.\n\n436\n00:22:31,280 --> 00:22:32,280\nLet the games begin.\n\n437\n00:22:32,280 --> 00:22:33,280\nLeague vs. League.\n\n438\n00:22:33,280 --> 00:22:34,280\nIt's time to compete.\n\n439\n00:22:34,280 --> 00:22:37,280\nOur Society and the Gates and the DMV.\n\n440\n00:22:37,280 --> 00:22:42,280\nYo, my daughter been singing this song nonstop.\n\n441\n00:22:42,280 --> 00:22:44,280\nYo, I don't know if you got kids.\n\n442\n00:22:44,280 --> 00:22:45,280\nYou got kids?\n\n443\n00:22:45,280 --> 00:22:48,280\nAnd this is small, small break.\n\n444\n00:22:48,280 --> 00:22:51,280\nI have a playlist that I've made for my daughter,\n\n445\n00:22:51,280 --> 00:22:54,280\nteaching her things about life,\n\n446\n00:22:54,280 --> 00:22:56,280\nabout the family,\n\n447\n00:22:56,280 --> 00:22:59,280\nabout people in the family.\n\n448\n00:22:59,280 --> 00:23:03,280\nAnd it's a great way to educate a little kid\n\n449\n00:23:03,280 --> 00:23:04,280\nabout making the music.\n\n450\n00:23:04,280 --> 00:23:06,280\nAnd since you're a rapper,\n\n451\n00:23:06,280 --> 00:23:09,280\nsince you're a lyricist.\n\n452\n00:23:09,280 --> 00:23:11,280\nAnyway, check this shit out, dog.\n\n453\n00:23:11,280 --> 00:23:12,280\nCheck this shit out.\n\n454\n00:23:13,280 --> 00:23:17,280\nYeah, yeah, yeah, yeah.\n\n455\n00:23:17,280 --> 00:23:20,280\nComing soon to the Algorithm Institute of Battle Rap.\n\n456\n00:23:20,280 --> 00:23:22,280\nIt's the intro.\n\n457\n00:23:22,280 --> 00:23:25,280\nOh, League vs. League.\n\n458\n00:23:25,280 --> 00:23:30,280\nOur Society vs. Gates of the Garden.\n\n459\n00:23:30,280 --> 00:23:35,280\nWe don't see these too often.\n\n460\n00:23:35,280 --> 00:23:37,280\nWhat's up, Dre?\n\n461\n00:23:37,280 --> 00:23:39,280\nWhat's up, Kels?\n\n462\n00:23:39,280 --> 00:23:40,280\nLet's go.\n\n463\n00:23:41,280 --> 00:23:43,280\nUh-huh, uh-huh, uh-huh.\n\n464\n00:23:43,280 --> 00:23:45,280\nSee, to the D, black to the C.\n\n465\n00:23:45,280 --> 00:23:48,280\nLeague vs. League, who you think gonna win?\n\n466\n00:23:48,280 --> 00:23:52,280\nDre, Dennis vs. Kels, let the games begin.\n\n467\n00:23:52,280 --> 00:23:55,280\nLeague vs. League, it's time to compete.\n\n468\n00:23:55,280 --> 00:23:58,280\nOur Society and the Gates in the DMV.\n\n469\n00:23:58,280 --> 00:24:01,280\nLeague vs. League, who you think gonna win?\n\n470\n00:24:01,280 --> 00:24:04,280\nIt's League vs. League, let the games begin.\n\n471\n00:24:04,280 --> 00:24:07,280\nLeague vs. League, it's time to compete.\n\n472\n00:24:07,280 --> 00:24:11,280\nOur Society and the Gates in the DMV.\n\n473\n00:24:11,280 --> 00:24:14,280\nKels called us soldiers, and Dre did too.\n\n474\n00:24:14,280 --> 00:24:17,280\nKid Slay answers the phone, and Ace did too.\n\n475\n00:24:17,280 --> 00:24:20,280\nClumsy had time, Rez Mafia was ready.\n\n476\n00:24:20,280 --> 00:24:24,280\nLady Conscience said, yeah, and the Saga got ready.\n\n477\n00:24:24,280 --> 00:24:27,280\nZeke and Cain caught bodies, and Elijah did well.\n\n478\n00:24:27,280 --> 00:24:30,280\nT'Plaso went crazy, and Eunice called an L.\n\n479\n00:24:30,280 --> 00:24:33,280\nP, Black, and Kruger was too close to call.\n\n480\n00:24:34,280 --> 00:24:37,280\nHe'll shop his money for Thor's fame against the wall.\n\n481\n00:24:37,280 --> 00:24:40,280\nBill was in the building, and Dez was on too.\n\n482\n00:24:40,280 --> 00:24:43,280\nAce no show, but that what he do.\n\n483\n00:24:43,280 --> 00:24:46,280\nBill was in the building, and Dez was on too.\n\n484\n00:24:46,280 --> 00:24:49,280\nAce no show, but that what he do.\n\n485\n00:24:49,280 --> 00:24:52,280\nLeague vs. League, who you think gonna win?\n\n486\n00:24:52,280 --> 00:24:56,280\nDre, Dennis vs. Kels, let the games begin.\n\n487\n00:24:56,280 --> 00:24:59,280\nYo, Lex. Yo, Lex. Yo, Lex.\n\n488\n00:24:59,280 --> 00:25:02,280\nYo, Lex.\n\n489\n00:25:02,280 --> 00:25:05,280\nHa ha ha ha ha ha.\n\n490\n00:25:05,280 --> 00:25:08,280\nThat was incredible, man.\n\n491\n00:25:08,280 --> 00:25:11,280\nLex. The production value\n\n492\n00:25:11,280 --> 00:25:14,280\non iBattle shit better go up a little bit, Lex.\n\n493\n00:25:14,280 --> 00:25:17,280\nHa ha ha ha ha ha.\n\n494\n00:25:17,280 --> 00:25:20,280\nYo, flyers already look dope as fuck.\n\n495\n00:25:20,280 --> 00:25:23,280\nYeah, I mean, I guess at this point we don't have any excuses.\n\n496\n00:25:23,280 --> 00:25:26,280\nNone. But them trailers,\n\n497\n00:25:26,280 --> 00:25:29,280\nthough, you got lyricists on staff.\n\n498\n00:25:29,280 --> 00:25:32,280\nI'm not gonna lie, though.\n\n499\n00:25:32,280 --> 00:25:35,280\nWhen I first saw your stuff,\n\n500\n00:25:35,280 --> 00:25:38,280\nI was like, how the fuck is he doing this?\n\n501\n00:25:38,280 --> 00:25:41,280\nThis is insane. It's not even me.\n\n502\n00:25:41,280 --> 00:25:44,280\nMy girl walked by the TV,\n\n503\n00:25:44,280 --> 00:25:47,280\nand she immediately turned and she went,\n\n504\n00:25:47,280 --> 00:25:50,280\nis that drugs?\n\n505\n00:25:50,280 --> 00:25:53,280\nI was like, holy shit.\n\n506\n00:25:53,280 --> 00:25:56,280\nYo, I forgot. You had drugs on\n\n507\n00:25:56,280 --> 00:25:59,280\nsomething. He had a name.\n\n508\n00:25:59,280 --> 00:26:02,280\nSomething Hellcat.\n\n509\n00:26:02,280 --> 00:26:05,280\nI don't remember. I've booked drugs a ton of times. Oh, Archduke.\n\n510\n00:26:05,280 --> 00:26:08,280\nYeah, Archduke. What's his whole name? Archduke something.\n\n511\n00:26:08,280 --> 00:26:11,280\nRedcat. Archduke Redcat.\n\n512\n00:26:11,280 --> 00:26:14,280\nYes. Okay. Okay. Okay.\n\n513\n00:26:14,280 --> 00:26:17,280\nI hope drugs not mad at me, man.\n\n514\n00:26:17,280 --> 00:26:20,280\nI mean, you know,\n\n515\n00:26:20,280 --> 00:26:23,280\nlisten, bro, I think what you do is very important.\n\n516\n00:26:23,280 --> 00:26:26,280\nAnd I think that\n\n517\n00:26:26,280 --> 00:26:29,280\nit's important. If I was a fucking 17-year-old\n\n518\n00:26:29,280 --> 00:26:32,280\nbattler coming up and I watched your\n\n519\n00:26:32,280 --> 00:26:35,280\nfucking channel, I would be like, holy shit, I don't want to be like\n\n520\n00:26:35,280 --> 00:26:38,280\nthese guys.\n\n521\n00:26:38,280 --> 00:26:41,280\nBut that's so necessary, though.\n\n522\n00:26:41,280 --> 00:26:44,280\nThat's so necessary because they're probably\n\n523\n00:26:44,280 --> 00:26:47,280\ngonna if they don't watch the video.\n\n524\n00:26:47,280 --> 00:26:50,280\nThere's so many lessons in this shit because battle rap has something\n\n525\n00:26:50,280 --> 00:26:53,280\nevery week, like every week is something that happens.\n\n526\n00:26:53,280 --> 00:26:56,280\nThere's so many lessons and I always try to pull\n\n527\n00:26:56,280 --> 00:26:59,280\na lesson out of it, too. Lex, I don't know if I don't\n\n528\n00:26:59,280 --> 00:27:02,280\nreally be putting people except for the Audi boom situation, but I\n\n529\n00:27:02,280 --> 00:27:05,280\ndon't really be putting people. Yeah.\n\n530\n00:27:05,280 --> 00:27:08,280\nThere's a lesson there, too. And you know what?\n\n531\n00:27:08,280 --> 00:27:11,280\nI think the most important one is if you blow it up and you got\n\n532\n00:27:11,280 --> 00:27:14,280\nsomething going on, there's no shame in working a fucking\n\n533\n00:27:14,280 --> 00:27:17,280\njob to don't be an asshole. Don't be\n\n534\n00:27:17,280 --> 00:27:20,280\ndoing the wrong thing because you're too cool to work\n\n535\n00:27:20,280 --> 00:27:23,280\nat Home Depot, you know, because now you're in jail\n\n536\n00:27:23,280 --> 00:27:26,280\nand ain't nothing cool about that. Yeah, yeah, yeah.\n\n537\n00:27:26,280 --> 00:27:29,280\nYou know, so, you know, I think I think\n\n538\n00:27:29,280 --> 00:27:32,280\nit's pretty fucking awesome. But Lex, you can custom\n\n539\n00:27:32,280 --> 00:27:35,280\nmake trailers for your for your battles. Like if you have\n\n540\n00:27:35,280 --> 00:27:38,280\na big battle coming up and when I tell you that this\n\n541\n00:27:38,280 --> 00:27:41,280\nthing does crazy different genres,\n\n542\n00:27:41,280 --> 00:27:44,280\nI have this thing called the app.\n\n543\n00:27:44,280 --> 00:27:47,280\nThis video that I'm working on about the URL app\n\n544\n00:27:47,280 --> 00:27:50,280\nand I\n\n545\n00:27:50,280 --> 00:27:53,280\nalso like this is how versatile this shit is, Lex.\n\n546\n00:27:53,280 --> 00:27:56,280\nYou can tell this shit.\n\n547\n00:28:02,280 --> 00:28:05,280\nStand up comedian performing at\n\n548\n00:28:05,280 --> 00:28:08,280\na comedy show, right?\n\n549\n00:28:08,280 --> 00:28:11,280\nAnd I type there some fucking jokes.\n\n550\n00:28:12,280 --> 00:28:15,280\nYou ever hear URL talk about\n\n551\n00:28:15,280 --> 00:28:18,280\nbeing for the culture? Yeah, for the culture.\n\n552\n00:28:18,280 --> 00:28:21,280\nIf the culture comes with a monthly subscription\n\n553\n00:28:21,280 --> 00:28:24,280\nfee.\n\n554\n00:28:26,280 --> 00:28:29,280\nYeah, it sounds like\n\n555\n00:28:29,280 --> 00:28:32,280\nsomebody real.\n\n556\n00:28:32,280 --> 00:28:35,280\nBut it can do crazy. I've done\n\n557\n00:28:35,280 --> 00:28:38,280\nbirthday stuff for my family and shit like that, but it can do\n\n558\n00:28:38,280 --> 00:28:41,280\ncrazy genre. So and again, because\n\n559\n00:28:41,280 --> 00:28:44,280\nyou're because you're a rapper, like\n\n560\n00:28:44,280 --> 00:28:47,280\nyou should be able to come up with some words to describe some images.\n\n561\n00:28:47,280 --> 00:28:50,280\nYou should be able to come up with some words to describe some fucking\n\n562\n00:28:50,280 --> 00:28:53,280\nmusic genres and you should be able to write lyrics\n\n563\n00:28:53,280 --> 00:28:56,280\nman and be able to come up with some dope trailers and shit too.\n\n564\n00:28:59,280 --> 00:29:02,280\nThere you go. There you go. There you go. There you go.\n\n565\n00:29:02,280 --> 00:29:05,280\nSo, yeah, I just wanted to make sure that I put you\n\n566\n00:29:05,280 --> 00:29:08,280\non to the audio to ideogram. I'm going to send you the links\n\n567\n00:29:08,280 --> 00:29:11,280\njust so you have it.\n\n568\n00:29:11,280 --> 00:29:14,280\nSounds good. Sounds good, man.\n\n569\n00:29:14,280 --> 00:29:17,280\nNext. Thank you for your time. Once I put this union video\n\n570\n00:29:17,280 --> 00:29:20,280\nout, I'm going to reach out to you again, try to connect with you\n\n571\n00:29:20,280 --> 00:29:23,280\nand we're going to talk and I can't\n\n572\n00:29:23,280 --> 00:29:26,280\nwait to hear your input and your ideas and\n\n573\n00:29:26,280 --> 00:29:29,280\nI've already had people\n\n574\n00:29:29,280 --> 00:29:32,280\ntell me that it's not going to work, but it's not about\n\n575\n00:29:33,280 --> 00:29:36,280\nworking or not working. It's about designing something\n\n576\n00:29:36,280 --> 00:29:39,280\nlike it doesn't have to be a coat that somebody going to put on, but\n\n577\n00:29:39,280 --> 00:29:42,280\nlet's design something that if somebody get cold enough\n\n578\n00:29:42,280 --> 00:29:45,280\nthey got a coat right there that is already\n\n579\n00:29:45,280 --> 00:29:48,280\ntested and approved by all of the\n\n580\n00:29:48,280 --> 00:29:51,280\ngreats in the community and you are one of those greats, man. So\n\n581\n00:29:51,280 --> 00:29:54,280\nagain, thank you for your time.\n\n582\n00:29:54,280 --> 00:29:57,280\nI think a union would work with the right people and the right people\n\n583\n00:29:57,280 --> 00:30:00,280\nto take this serious and really benefit from it.\n\n584\n00:30:00,280 --> 00:30:03,280\nYo, Lex, I used this fucking AI\n\n585\n00:30:03,280 --> 00:30:06,280\nI believe like two years ago.\n\n586\n00:30:06,280 --> 00:30:09,280\nGPT-3\n\n587\n00:30:09,280 --> 00:30:12,280\nGPT-3, right? I used it to\n\n588\n00:30:12,280 --> 00:30:15,280\ncreate this system in the\n\n589\n00:30:15,280 --> 00:30:18,280\nmiddle of the night that I think could end police brutality.\n\n590\n00:30:18,280 --> 00:30:21,280\nLike that was my goal that night to see\n\n591\n00:30:21,280 --> 00:30:24,280\nI used the AI to end police brutality and I feel like I created\n\n592\n00:30:24,280 --> 00:30:27,280\nsomething that was fucking amazing, right?\n\n593\n00:30:27,280 --> 00:30:30,280\nI'm getting chills even thinking about it. That's the night that I\n\n594\n00:30:30,280 --> 00:30:33,280\nknew that this shit could be used to create\n\n595\n00:30:33,280 --> 00:30:36,280\nanything. If it convinced me\n\n596\n00:30:36,280 --> 00:30:39,280\npersonally that this shit that it came up\n\n597\n00:30:39,280 --> 00:30:42,280\nwith to this day that it will work and I really think\n\n598\n00:30:42,280 --> 00:30:45,280\nthat that shit will work. Like it was very detailed\n\n599\n00:30:45,280 --> 00:30:48,280\nkilling of unarmed black men specifically. Sorry.\n\n600\n00:30:49,280 --> 00:30:52,280\nI thought that it could be used to do anything.\n\n601\n00:30:52,280 --> 00:30:55,280\nYou want to know the details? Because I got to stand up and walk around\n\n602\n00:30:55,280 --> 00:30:58,280\ntelling you this shit.\n\n603\n00:30:58,280 --> 00:31:01,280\nThe gist of it is\n\n604\n00:31:01,280 --> 00:31:04,280\ncheck this shit out. I almost want to pull it up\n\n605\n00:31:04,280 --> 00:31:07,280\nbecause there's so many details. All right. So imagine\n\n606\n00:31:07,280 --> 00:31:10,280\nthat this thing exists. So this is how it works.\n\n607\n00:31:10,280 --> 00:31:13,280\nThere's a\n\n608\n00:31:13,280 --> 00:31:16,280\nthere's a app called\n\n609\n00:31:16,280 --> 00:31:19,280\nI deserve better app, right?\n\n610\n00:31:19,280 --> 00:31:22,280\nAnd it starts off just\n\n611\n00:31:22,280 --> 00:31:25,280\nconcentrating on\n\n612\n00:31:25,280 --> 00:31:28,280\nstopping or lowering to a certain threshold\n\n613\n00:31:28,280 --> 00:31:31,280\nunarmed black men getting killed.\n\n614\n00:31:31,280 --> 00:31:34,280\nIt's a known threshold like this many unarmed black men\n\n615\n00:31:34,280 --> 00:31:37,280\ngot to get killed in order for us to\n\n616\n00:31:37,280 --> 00:31:40,280\ngo to open it up to other things\n\n617\n00:31:40,280 --> 00:31:43,280\nlike we take you take a vote or whatever to open it up to other\n\n618\n00:31:43,280 --> 00:31:46,280\nthings. I'm skipping out on some stuff, but I almost want to pull it up\n\n619\n00:31:46,280 --> 00:31:49,280\nwhile I'm talking but the essence of it was\n\n620\n00:31:49,280 --> 00:31:52,280\nyeah, let me pull it up. Give me one second. Give me one second because\n\n621\n00:31:52,280 --> 00:31:55,280\nit I thought it was I thought it was a fucking work of art\n\n622\n00:31:55,280 --> 00:31:58,280\nman. Like and you did\n\n623\n00:31:58,280 --> 00:32:01,280\nthis on GPT-3. Yeah\n\n624\n00:32:01,280 --> 00:32:04,280\nGPT-3 like two years ago.\n\n625\n00:32:04,280 --> 00:32:07,280\nYeah, it's\n\n626\n00:32:08,280 --> 00:32:11,280\nwhat you say\n\n627\n00:32:11,280 --> 00:32:14,280\nthat I'm really missing out on AI. Yeah, you are man.\n\n628\n00:32:14,280 --> 00:32:17,280\nBut it's kind of ironic too because\n\n629\n00:32:17,280 --> 00:32:20,280\nthe last question that you asked me before was\n\n630\n00:32:20,280 --> 00:32:23,280\nI know that there's another era coming and I'm looking\n\n631\n00:32:23,280 --> 00:32:26,280\nforward to you know, so\n\n632\n00:32:26,280 --> 00:32:29,280\nso let me tell you there's this game called Project\n\n633\n00:32:29,280 --> 00:32:32,280\nZomboid that I also used to that I love because\n\n634\n00:32:32,280 --> 00:32:35,280\nit's a zombie simulation game, right? It's like you're looking\n\n635\n00:32:35,280 --> 00:32:38,280\nkind of down at them and it's so detailed like\n\n636\n00:32:38,280 --> 00:32:41,280\nlike you get sick if you drink water that\n\n637\n00:32:41,280 --> 00:32:44,280\nthat's not boiled out of a lake and it's like super\n\n638\n00:32:44,280 --> 00:32:47,280\nsuper detail right super detail you\n\n639\n00:32:47,280 --> 00:32:50,280\nyou jump over you jump you run through some bushes\n\n640\n00:32:50,280 --> 00:32:53,280\nand you can get a super detail, right?\n\n641\n00:32:53,280 --> 00:32:56,280\nI use the AI to create something called\n\n642\n00:32:56,280 --> 00:32:59,280\nthe Zombrain to tell me how\n\n643\n00:32:59,280 --> 00:33:02,280\nto I just tell it a situation and then\n\n644\n00:33:02,280 --> 00:33:05,280\nit tells me what to do because the AI knew the fucking game\n\n645\n00:33:05,280 --> 00:33:08,280\nit knew how to play the game. And so\n\n646\n00:33:08,280 --> 00:33:11,280\nI was trying to build a guild in order to\n\n647\n00:33:11,280 --> 00:33:14,280\nI wanted it to help me build a guild on discord\n\n648\n00:33:14,280 --> 00:33:17,280\nand the shit did it like I ended up having a guild\n\n649\n00:33:17,280 --> 00:33:20,280\nthe guild was fucking awesome. The channel was still on discord\n\n650\n00:33:20,280 --> 00:33:23,280\nand then that's when I knew that it could literally\n\n651\n00:33:23,280 --> 00:33:26,280\nhelp me lead like I could\n\n652\n00:33:26,280 --> 00:33:29,280\nif I feed it as much data as possible. So now at\n\n653\n00:33:29,280 --> 00:33:32,280\nwork what I do is I record meetings\n\n654\n00:33:32,280 --> 00:33:35,280\nand I use another app to record meetings and when you record\n\n655\n00:33:35,280 --> 00:33:38,280\nmeetings if you're talking to a battler, right?\n\n656\n00:33:38,280 --> 00:33:41,280\nAnd you record this and you put this in the\n\n657\n00:33:41,280 --> 00:33:44,280\nAI not only are you able to get notes, but you're able to\n\n658\n00:33:44,280 --> 00:33:47,280\nget you're able to do a lot of take the cognitive load\n\n659\n00:33:47,280 --> 00:33:50,280\nof figuring out answers and\n\n660\n00:33:50,280 --> 00:33:53,280\nand getting solutions like\n\n661\n00:33:53,280 --> 00:33:56,280\nanyway, I want to let me show you this fucking\n\n662\n00:33:56,280 --> 00:33:59,280\nI'm interested man. I want to\n\n663\n00:33:59,280 --> 00:34:02,280\nI want to incorporate as much as I can. So for your\n\n664\n00:34:02,280 --> 00:34:05,280\nbusiness, this is what you should do when you're in a business\n\n665\n00:34:05,280 --> 00:34:08,280\nmeeting with your team and all of y'all talking about\n\n666\n00:34:08,280 --> 00:34:11,280\nsomething that's important record that shit or use\n\n667\n00:34:11,280 --> 00:34:14,280\nsome type of AI app or quarter on chat GPT\n\n668\n00:34:14,280 --> 00:34:17,280\nput that into the AI not only\n\n669\n00:34:17,280 --> 00:34:20,280\ncan you just put that into the AI, but you can also\n\n670\n00:34:20,280 --> 00:34:23,280\nput that into the AI and you can\n\n671\n00:34:23,280 --> 00:34:26,280\nput that into the AI. Not only can you just\n\n672\n00:34:26,280 --> 00:34:29,280\npass out notes for everybody. That's regular shit, right?\n\n673\n00:34:29,280 --> 00:34:32,280\nBut you can get solutions. You can tell it act\n\n674\n00:34:32,280 --> 00:34:35,280\nas act as our\n\n675\n00:34:35,280 --> 00:34:38,280\nAI brain our I battle brain\n\n676\n00:34:38,280 --> 00:34:41,280\nand give us give us\n\n677\n00:34:41,280 --> 00:34:44,280\nsome some ideas that we can\n\n678\n00:34:44,280 --> 00:34:47,280\nthat we can possibly implement that we wouldn't have thought of\n\n679\n00:34:47,280 --> 00:34:50,280\notherwise and like you'll\n\n680\n00:34:50,280 --> 00:34:53,280\nbe shocked man. Like it'll it'll really give you some nice stuff.\n\n681\n00:34:53,280 --> 00:34:56,280\nI've talked to\n\n682\n00:34:56,280 --> 00:34:59,280\nthe AI on Facebook. Yeah.\n\n683\n00:34:59,280 --> 00:35:02,280\nYeah, that's nice. That's nice. But\n\n684\n00:35:02,280 --> 00:35:05,280\nbut giving it giving it as much\n\n685\n00:35:05,280 --> 00:35:08,280\ncontext as possible is really the key.\n\n686\n00:35:08,280 --> 00:35:11,280\nSo that's why you would give it that meeting because now it\n\n687\n00:35:11,280 --> 00:35:14,280\nunderstands that you haven't\n\n688\n00:35:14,280 --> 00:35:17,280\nthat opera is always telling you the same thing\n\n689\n00:35:17,280 --> 00:35:20,280\nand it's telling you that that kid Slade is doing\n\n690\n00:35:20,280 --> 00:35:23,280\nthis and I mean you're telling it that kid Slade\n\n691\n00:35:23,280 --> 00:35:26,280\nis doing that. You're telling it that as an organization. Y'all\n\n692\n00:35:26,280 --> 00:35:29,280\nneed to figure out what y'all going to do\n\n693\n00:35:29,280 --> 00:35:32,280\nnext month for the card. And this is the third time that has happened\n\n694\n00:35:32,280 --> 00:35:35,280\nwhere y'all kind of waited to the last minute. Then when you talk\n\n695\n00:35:35,280 --> 00:35:38,280\nto it because information\n\n696\n00:35:38,280 --> 00:35:41,280\nyou gave it all of this context and you\n\n697\n00:35:41,280 --> 00:35:44,280\ncan interact with it in any way possible. Let's\n\n698\n00:35:44,280 --> 00:35:47,280\nwhat I like to believe is that we're entering an age\n\n699\n00:35:47,280 --> 00:35:50,280\nwhere you can interact with information. They used to say\n\n700\n00:35:50,280 --> 00:35:53,280\nthat the other age was information age, but\n\n701\n00:35:53,280 --> 00:35:56,280\nnow you can interact with information in a whole different way.\n\n702\n00:35:56,280 --> 00:35:59,280\nAnd what I do at my channel is I take information\n\n703\n00:35:59,280 --> 00:36:02,280\nand I'll let you interact with it by\n\n704\n00:36:02,280 --> 00:36:05,280\nhearing somebody describe it to you like it's a story\n\n705\n00:36:05,280 --> 00:36:08,280\nand then you see images to go along with it.\n\n706\n00:36:08,280 --> 00:36:11,280\nBut as a business you can use\n\n707\n00:36:11,280 --> 00:36:14,280\nit to help to give you possible\n\n708\n00:36:14,280 --> 00:36:17,280\nsolution to suggest some software that's\n\n709\n00:36:17,280 --> 00:36:20,280\nfree that we can use that will save us time\n\n710\n00:36:20,280 --> 00:36:23,280\nyou\n\n711\n00:36:23,280 --> 00:36:26,280\ndescribe a process that you have and then you can tell it\n\n712\n00:36:26,280 --> 00:36:29,280\ngive me three suggestions\n\n713\n00:36:29,280 --> 00:36:32,280\nfor us to fix this particular process\n\n714\n00:36:32,280 --> 00:36:35,280\nthat we have. But even before you\n\n715\n00:36:35,280 --> 00:36:38,280\nif you start to take that approach if you don't take anything from\n\n716\n00:36:38,280 --> 00:36:41,280\nthis conversation, let's take this\n\n717\n00:36:41,280 --> 00:36:44,280\nit's really great when you have the\n\n718\n00:36:44,280 --> 00:36:47,280\nthe AI to give you answers, but you're never going to\n\n719\n00:36:47,280 --> 00:36:50,280\nget the best answer by just asking a question\n\n720\n00:36:50,280 --> 00:36:53,280\nand then it comes back to you because a lot of times we don't give it\n\n721\n00:36:53,280 --> 00:36:56,280\nenough context. We don't give it the big picture and that's\n\n722\n00:36:56,280 --> 00:36:59,280\nhow it works the best. So the best way to do that\n\n723\n00:36:59,280 --> 00:37:02,280\nis to just spit out as much information as possible\n\n724\n00:37:02,280 --> 00:37:05,280\nabout a topic and then ask it\n\n725\n00:37:06,280 --> 00:37:09,280\nand I've called this the interview method or interview approach\n\n726\n00:37:09,280 --> 00:37:12,280\neven though that sounds stupid it's an easy way to remember it because\n\n727\n00:37:12,280 --> 00:37:15,280\nyou then ask the AI to interview you about the\n\n728\n00:37:15,280 --> 00:37:18,280\ntopic in order to do a certain thing. So\n\n729\n00:37:18,280 --> 00:37:21,280\nyou say you just had your iBattle meeting right\n\n730\n00:37:21,280 --> 00:37:24,280\nyou then fed the transcript to the AI\n\n731\n00:37:24,280 --> 00:37:27,280\nyou tell it okay ask me\n\n732\n00:37:27,280 --> 00:37:30,280\n10 questions about iBattle\n\n733\n00:37:30,280 --> 00:37:33,280\nor about that meeting in order to\n\n734\n00:37:33,280 --> 00:37:36,280\nsolve all of our problems you know I'm just\n\n735\n00:37:36,280 --> 00:37:39,280\npulling something high out of the air or in order to give me great\n\n736\n00:37:39,280 --> 00:37:42,280\nadvice in order to help our company\n\n737\n00:37:42,280 --> 00:37:45,280\nand trying to do what we're trying to do. So then it'll\n\n738\n00:37:45,280 --> 00:37:48,280\nask you questions with the goal\n\n739\n00:37:48,280 --> 00:37:51,280\nof giving you great advice, but it'll\n\n740\n00:37:51,280 --> 00:37:54,280\nask you these questions powered by the\n\n741\n00:37:54,280 --> 00:37:57,280\nmeeting that y'all just had. So that's\n\n742\n00:37:57,280 --> 00:38:00,280\na lot of power.\n\n743\n00:38:00,280 --> 00:38:03,280\nSo this\n\n744\n00:38:03,280 --> 00:38:06,280\nsolution to\n\n745\n00:38:06,280 --> 00:38:09,280\nblack man being killed\n\n746\n00:38:09,280 --> 00:38:12,280\nby the police that I got from the AI\n\n747\n00:38:12,280 --> 00:38:15,280\nit is\n\n748\n00:38:15,280 --> 00:38:18,280\nit is\n\n749\n00:38:18,280 --> 00:38:21,280\nan app\n\n750\n00:38:21,280 --> 00:38:24,280\nit starts with an app where\n\n751\n00:38:24,280 --> 00:38:27,280\nthis is\n\n752\n00:38:27,280 --> 00:38:30,280\nthis is not a great explanation that I'm looking at.\n\n753\n00:38:30,280 --> 00:38:33,280\nAll right so\n\n754\n00:38:33,280 --> 00:38:36,280\npeople get to follow\n\n755\n00:38:36,280 --> 00:38:39,280\nthese cases first of all they get to follow these cases on\n\n756\n00:38:39,280 --> 00:38:42,280\nthis particular app. The app transcribes\n\n757\n00:38:42,280 --> 00:38:45,280\nall of the\n\n758\n00:38:45,280 --> 00:38:48,280\nlegal proceedings and you're able to skip\n\n759\n00:38:48,280 --> 00:38:51,280\nto certain parts like auto-generated chapters. If you\n\n760\n00:38:51,280 --> 00:38:54,280\nwant to see the officer testifying on the stand you can go there\n\n761\n00:38:54,280 --> 00:38:57,280\nand you can do this. The team updates people\n\n762\n00:38:57,280 --> 00:39:00,280\nbut the money goes to pay for private\n\n763\n00:39:00,280 --> 00:39:03,280\ndetectives, high-powered lawyers and social media\n\n764\n00:39:03,280 --> 00:39:06,280\nexperts in order to shift public\n\n765\n00:39:06,280 --> 00:39:09,280\nperception and to gain more information. Let me read that again.\n\n766\n00:39:09,280 --> 00:39:12,280\nThe funding\n\n767\n00:39:12,280 --> 00:39:15,280\nis for private detectives, high-powered lawyers\n\n768\n00:39:15,280 --> 00:39:18,280\nand social media experts and this is on\n\n769\n00:39:18,280 --> 00:39:21,280\nthe victim side. The\n\n770\n00:39:22,280 --> 00:39:25,280\nunarmed black man that got killed now all of a sudden\n\n771\n00:39:25,280 --> 00:39:28,280\nhis family got access to private detectives, high-powered lawyers\n\n772\n00:39:28,280 --> 00:39:31,280\nand social media experts and they get\n\n773\n00:39:31,280 --> 00:39:34,280\ncounseling, they get financial support\n\n774\n00:39:34,280 --> 00:39:37,280\nall guided from the membership of this app.\n\n775\n00:39:37,280 --> 00:39:40,280\nSo you basically centralize the support system\n\n776\n00:39:40,280 --> 00:39:43,280\nand fortify it. Yes\n\n777\n00:39:43,280 --> 00:39:46,280\nbut it gets deeper\n\n778\n00:39:46,280 --> 00:39:49,280\njust like how you got your little rating system for your battlers\n\n779\n00:39:49,280 --> 00:39:52,280\nLex? A rating system for lawyers\n\n780\n00:39:52,280 --> 00:39:55,280\nmother fucker, check this shit out. A rating system\n\n781\n00:39:55,280 --> 00:39:58,280\nfor lawyers. You use the app in order to\n\n782\n00:39:58,280 --> 00:40:01,280\nfind lawyers and you got a win-loss\n\n783\n00:40:01,280 --> 00:40:04,280\nrecord. You got a win-loss record\n\n784\n00:40:04,280 --> 00:40:07,280\nyou choose your topic like oh\n\n785\n00:40:07,280 --> 00:40:10,280\nI got a speeding ticket. You need a\n\n786\n00:40:10,280 --> 00:40:13,280\nspeeding ticket lawyer? The AI looks\n\n787\n00:40:13,280 --> 00:40:16,280\nthrough his wins and losses. How many times did he\n\n788\n00:40:16,280 --> 00:40:19,280\nwin? How many times did he lose? And whatever like I don't know\n\n789\n00:40:19,280 --> 00:40:22,280\nlegal stuff but this stuff is easy for AI to\n\n790\n00:40:22,280 --> 00:40:25,280\ncomb through. So what is also\n\n791\n00:40:25,280 --> 00:40:28,280\nused, this app is also used as a ranking\n\n792\n00:40:28,280 --> 00:40:31,280\nsystem for lawyers. So ideally a lawyer\n\n793\n00:40:31,280 --> 00:40:34,280\nis important for them to have\n\n794\n00:40:34,280 --> 00:40:37,280\ntheir high priority cases like\n\n795\n00:40:37,280 --> 00:40:40,280\nif they're an expensive lawyer you want to be\n\n796\n00:40:40,280 --> 00:40:43,280\nundefeated on that bitch. You know what I mean?\n\n797\n00:40:44,280 --> 00:40:47,280\nYou basically see someone's batting average\n\n798\n00:40:47,280 --> 00:40:50,280\nExactly, a lawyer's batting average, an attorney\n\n799\n00:40:50,280 --> 00:40:53,280\nand it's easy. Go ahead\n\n800\n00:40:53,280 --> 00:40:56,280\nBefore you pay them. Before you pay them, yes\n\n801\n00:40:56,280 --> 00:40:59,280\nYes, because you never know that shit, right?\n\n802\n00:40:59,280 --> 00:41:02,280\nYou never know that shit. And how do you find\n\n803\n00:41:02,280 --> 00:41:05,280\na lawyer? Wikipedia, like MMA fighters\n\n804\n00:41:05,280 --> 00:41:08,280\nYo, that's even a better\n\n805\n00:41:08,280 --> 00:41:11,280\nidea, yo, for the idea\n\n806\n00:41:12,280 --> 00:41:15,280\nWikipedia\n\n807\n00:41:15,280 --> 00:41:18,280\nLawyers. Yeah, yeah, because the AI can easily\n\n808\n00:41:18,280 --> 00:41:21,280\nscrape the web and then\n\n809\n00:41:21,280 --> 00:41:24,280\nHow do we not know your win to\n\n810\n00:41:24,280 --> 00:41:27,280\nloss record of your fucking cases? Why is your record\n\n811\n00:41:27,280 --> 00:41:30,280\nWhy is your record sealed and mine's open?\n\n812\n00:41:32,280 --> 00:41:35,280\nThere you go. And they get paid so much\n\n813\n00:41:35,280 --> 00:41:38,280\nmoney and you never know if the\n\n814\n00:41:38,280 --> 00:41:41,280\nUnless they win\n\n815\n00:41:41,280 --> 00:41:44,280\nUnless they win\n\n816\n00:41:44,280 --> 00:41:47,280\nhigh attention cases, right? Yeah\n\n817\n00:41:47,280 --> 00:41:50,280\nBut other than that, it's\n\n818\n00:41:50,280 --> 00:41:53,280\nBut those are celebrity lawyers we're talking about\n\n819\n00:41:53,280 --> 00:41:56,280\nI'm talking about, let's say\n\n820\n00:41:56,280 --> 00:41:59,280\nLet's say a regular lawyer\n\n821\n00:41:59,280 --> 00:42:02,280\nA regular lawyer that\n\n822\n00:42:02,280 --> 00:42:05,280\nyou got into a car accident\n\n823\n00:42:05,280 --> 00:42:08,280\nAnd you know that the other dude was in the wrong. You got the dashboard cam\n\n824\n00:42:08,280 --> 00:42:11,280\nAnd then you see that it says UPS on the fucking\n\n825\n00:42:11,280 --> 00:42:14,280\ntruck that hits you. And now you're looking for a lawyer\n\n826\n00:42:14,280 --> 00:42:17,280\nRight now, what you're going to do, go to Yelp or just go to\n\n827\n00:42:17,280 --> 00:42:20,280\nGoogle. In this world, you will go to this app\n\n828\n00:42:20,280 --> 00:42:23,280\nYou will type in what the fuck happened to you\n\n829\n00:42:23,280 --> 00:42:26,280\nThe AI will say, oh, you need this type of lawyer. Here goes some lawyers\n\n830\n00:42:26,280 --> 00:42:29,280\nHere go their Wikipedia page. Here go their win-loss records\n\n831\n00:42:29,280 --> 00:42:32,280\nYou want this shit arranged by win-losses or money\n\n832\n00:42:32,280 --> 00:42:35,280\nHow bad do you want to win?\n\n833\n00:42:35,280 --> 00:42:38,280\nDo you want to hear something real crazy?\n\n834\n00:42:38,280 --> 00:42:41,280\nWhat you got? One of my battlers is a lawyer\n\n835\n00:42:41,280 --> 00:42:44,280\nHis name is Dunt, right? And when he was coming up\n\n836\n00:42:44,280 --> 00:42:47,280\nHe was a public defender\n\n837\n00:42:47,280 --> 00:42:50,280\nSo basically he's a public defender\n\n838\n00:42:50,280 --> 00:42:53,280\nIn Queens. He gets assigned to this kid\n\n839\n00:42:53,280 --> 00:42:56,280\nRight? And the kid basically\n\n840\n00:42:56,280 --> 00:42:59,280\nTurns to him in the courtroom as they\n\n841\n00:42:59,280 --> 00:43:02,280\nMeet each other. And he's like\n\n842\n00:43:02,280 --> 00:43:05,280\nHey, are you Dunt from iBattle?\n\n843\n00:43:05,280 --> 00:43:08,280\nAnd he's like, yeah, I am\n\n844\n00:43:08,280 --> 00:43:11,280\nAnd he's like, are you my fucking lawyer?\n\n845\n00:43:11,280 --> 00:43:14,280\nYo, you should have told me that\n\n846\n00:43:14,280 --> 00:43:17,280\nWhen I was recording for the story. That's hilarious\n\n847\n00:43:17,280 --> 00:43:20,280\nHe tells him yes, and he goes\n\n848\n00:43:20,280 --> 00:43:23,280\nYo, can you get me on iBattle? I battled\n\n849\n00:43:23,280 --> 00:43:26,280\nHe's like, I battle on Coliseum and shit\n\n850\n00:43:26,280 --> 00:43:29,280\nSo now, he battles with us\n\n851\n00:43:29,280 --> 00:43:32,280\nHis name is Pyrex Jones. He's on the league\n\n852\n00:43:32,280 --> 00:43:35,280\nHe's a battler and he got on the league by being defended\n\n853\n00:43:35,280 --> 00:43:38,280\nBy Dunt support\n\n854\n00:43:38,280 --> 00:43:41,280\nYeah\n\n855\n00:43:42,280 --> 00:43:45,280\nI was watching an iBattle battle a couple months ago\n\n856\n00:43:45,280 --> 00:43:48,280\nAnd I heard somebody\n\n857\n00:43:48,280 --> 00:43:51,280\nThey sounded so unique. And I wonder if that's him\n\n858\n00:43:51,280 --> 00:43:54,280\nLet me see\n\n859\n00:43:54,280 --> 00:43:57,280\nHe kind of\n\n860\n00:43:57,280 --> 00:44:00,280\nDamn, I can't really say\n\n861\n00:44:00,280 --> 00:44:03,280\nHe's got a little bit of hollow in him. I'm not gonna lie\n\n862\n00:44:03,280 --> 00:44:06,280\nBut that's\n\n863\n00:44:06,280 --> 00:44:09,280\nIt's because he's from Queens. That's why\n\n864\n00:44:09,280 --> 00:44:12,280\nThey're just from the same place\n\n865\n00:44:22,280 --> 00:44:25,280\nWe have a lot of those\n\n866\n00:44:25,280 --> 00:44:28,280\nI've been doing this for a long time\n\n867\n00:44:28,280 --> 00:44:31,280\nI have one where it was like\n\n\n"], 'frequency_penalty': 0, 'logit_bias': {}, 'max_tokens': 256, 'n': 1, 'presence_penalty': 0, 'temperature': 0.7, 'top_p': 1}}
DEBUG:openai._base_client:Sending HTTP Request: POST https://api.openai.com/v1/completions
DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>
DEBUG:httpcore.http11:send_request_headers.complete
DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>
DEBUG:httpcore.http11:send_request_body.complete
DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>
DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 400, b'Bad Request', [(b'Date', b'Sun, 07 Jul 2024 23:57:59 GMT'), (b'Content-Type', b'application/json'), (b'Content-Length', b'296'), (b'Connection', b'keep-alive'), (b'access-control-allow-origin', b'*'), (b'openai-model', b'gpt-3.5-turbo-instruct'), (b'openai-organization', b'machinekingsmedia'), (b'openai-processing-ms', b'51'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains'), (b'x-ratelimit-limit-requests', b'3500'), (b'x-ratelimit-limit-tokens', b'90000'), (b'x-ratelimit-remaining-requests', b'3499'), (b'x-ratelimit-remaining-tokens', b'73924'), (b'x-ratelimit-reset-requests', b'17ms'), (b'x-ratelimit-reset-tokens', b'10.716s'), (b'x-request-id', b'req_0b8345efba9c26f772f62c90bc73d44d'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'89fbc7abf9450573-IAD'), (b'alt-svc', b'h3=":443"; ma=86400')])
INFO:httpx:HTTP Request: POST https://api.openai.com/v1/completions "HTTP/1.1 400 Bad Request"
DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>
DEBUG:httpcore.http11:receive_response_body.complete
DEBUG:httpcore.http11:response_closed.started
DEBUG:httpcore.http11:response_closed.complete
DEBUG:openai._base_client:HTTP Response: POST https://api.openai.com/v1/completions "400 Bad Request" Headers({'date': 'Sun, 07 Jul 2024 23:57:59 GMT', 'content-type': 'application/json', 'content-length': '296', 'connection': 'keep-alive', 'access-control-allow-origin': '*', 'openai-model': 'gpt-3.5-turbo-instruct', 'openai-organization': 'machinekingsmedia', 'openai-processing-ms': '51', 'openai-version': '2020-10-01', 'strict-transport-security': 'max-age=31536000; includeSubDomains', 'x-ratelimit-limit-requests': '3500', 'x-ratelimit-limit-tokens': '90000', 'x-ratelimit-remaining-requests': '3499', 'x-ratelimit-remaining-tokens': '73924', 'x-ratelimit-reset-requests': '17ms', 'x-ratelimit-reset-tokens': '10.716s', 'x-request-id': 'req_0b8345efba9c26f772f62c90bc73d44d', 'cf-cache-status': 'DYNAMIC', 'server': 'cloudflare', 'cf-ray': '89fbc7abf9450573-IAD', 'alt-svc': 'h3=":443"; ma=86400'})
DEBUG:openai._base_client:request_id: req_0b8345efba9c26f772f62c90bc73d44d
DEBUG:openai._base_client:Encountered httpx.HTTPStatusError
Traceback (most recent call last):
  File "C:\Users\taskm\anaconda3\envs\taskai\lib\site-packages\openai\_base_client.py", line 1020, in _request
    response.raise_for_status()
  File "C:\Users\taskm\anaconda3\envs\taskai\lib\site-packages\httpx\_models.py", line 761, in raise_for_status
    raise HTTPStatusError(message, request=request, response=self)
httpx.HTTPStatusError: Client error '400 Bad Request' for url 'https://api.openai.com/v1/completions'
For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/400
DEBUG:openai._base_client:Not retrying
DEBUG:openai._base_client:Re-raising status error
ERROR:root:Error processing chapters: Error code: 400 - {'error': {'message': "This model's maximum context length is 4097 tokens, however you requested 25028 tokens (24772 in your prompt; 256 for the completion). Please reduce your prompt; or completion length.", 'type': 'invalid_request_error', 'param': None, 'code': None}}
Traceback (most recent call last):
  File "D:\github\chappymedium\chappie.py", line 219, in process_chapters
    result = self.chappie_processor.process_srt(srt_content)
  File "D:\github\chappymedium\chappie_processor.py", line 33, in process_srt
    overall_summary = self._generate_overall_summary(srt_content)
  File "D:\github\chappymedium\chappie_processor.py", line 112, in _generate_overall_summary
    return chain.run(transcript=srt_content)
  File "C:\Users\taskm\anaconda3\envs\taskai\lib\site-packages\langchain_core\_api\deprecation.py", line 168, in warning_emitting_wrapper
    return wrapped(*args, **kwargs)
  File "C:\Users\taskm\anaconda3\envs\taskai\lib\site-packages\langchain\chains\base.py", line 605, in run
    return self(kwargs, callbacks=callbacks, tags=tags, metadata=metadata)[
  File "C:\Users\taskm\anaconda3\envs\taskai\lib\site-packages\langchain_core\_api\deprecation.py", line 168, in warning_emitting_wrapper
    return wrapped(*args, **kwargs)
  File "C:\Users\taskm\anaconda3\envs\taskai\lib\site-packages\langchain\chains\base.py", line 383, in __call__
    return self.invoke(
  File "C:\Users\taskm\anaconda3\envs\taskai\lib\site-packages\langchain\chains\base.py", line 166, in invoke
    raise e
  File "C:\Users\taskm\anaconda3\envs\taskai\lib\site-packages\langchain\chains\base.py", line 156, in invoke
    self._call(inputs, run_manager=run_manager)
  File "C:\Users\taskm\anaconda3\envs\taskai\lib\site-packages\langchain\chains\llm.py", line 127, in _call
    response = self.generate([inputs], run_manager=run_manager)
  File "C:\Users\taskm\anaconda3\envs\taskai\lib\site-packages\langchain\chains\llm.py", line 139, in generate
    return self.llm.generate_prompt(
  File "C:\Users\taskm\anaconda3\envs\taskai\lib\site-packages\langchain_core\language_models\llms.py", line 633, in generate_prompt
    return self.generate(prompt_strings, stop=stop, callbacks=callbacks, **kwargs)
  File "C:\Users\taskm\anaconda3\envs\taskai\lib\site-packages\langchain_core\language_models\llms.py", line 803, in generate
    output = self._generate_helper(
  File "C:\Users\taskm\anaconda3\envs\taskai\lib\site-packages\langchain_core\language_models\llms.py", line 670, in _generate_helper
    raise e
  File "C:\Users\taskm\anaconda3\envs\taskai\lib\site-packages\langchain_core\language_models\llms.py", line 657, in _generate_helper
    self._generate(
  File "C:\Users\taskm\anaconda3\envs\taskai\lib\site-packages\langchain_openai\llms\base.py", line 353, in _generate
    response = self.client.create(prompt=_prompts, **params)
  File "C:\Users\taskm\anaconda3\envs\taskai\lib\site-packages\openai\_utils\_utils.py", line 277, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\taskm\anaconda3\envs\taskai\lib\site-packages\openai\resources\completions.py", line 528, in create
    return self._post(
  File "C:\Users\taskm\anaconda3\envs\taskai\lib\site-packages\openai\_base_client.py", line 1261, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "C:\Users\taskm\anaconda3\envs\taskai\lib\site-packages\openai\_base_client.py", line 942, in request
    return self._request(
  File "C:\Users\taskm\anaconda3\envs\taskai\lib\site-packages\openai\_base_client.py", line 1041, in _request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "This model's maximum context length is 4097 tokens, however you requested 25028 tokens (24772 in your prompt; 256 for the completion). Please reduce your prompt; or completion length.", 'type': 'invalid_request_error', 'param': None, 'code': None}}
ERROR:root:Error processing chapters: not enough values to unpack (expected 3, got 2)
Traceback (most recent call last):
  File "D:\github\chappymedium\chappie.py", line 219, in process_chapters
    result = self.chappie_processor.process_srt(srt_content)
  File "D:\github\chappymedium\chappie_processor.py", line 26, in process_srt
    entries = parse_srt(srt_content)
  File "D:\github\chappymedium\chappie_utils.py", line 17, in parse_srt
    'start': time_to_seconds(time_range[0]),
  File "D:\github\chappymedium\chappie_utils.py", line 30, in time_to_seconds
    h, m, s = time_str.split(':')
ValueError: not enough values to unpack (expected 3, got 2)
ERROR:root:Error processing chapters: not enough values to unpack (expected 3, got 2)
Traceback (most recent call last):
  File "D:\github\chappymedium\chappie.py", line 219, in process_chapters
    result = self.chappie_processor.process_srt(srt_content)
  File "D:\github\chappymedium\chappie_processor.py", line 26, in process_srt
    entries = parse_srt(srt_content)
  File "D:\github\chappymedium\chappie_utils.py", line 17, in parse_srt
    'start': time_to_seconds(time_range[0]),
  File "D:\github\chappymedium\chappie_utils.py", line 30, in time_to_seconds
    h, m, s = time_str.split(':')
ValueError: not enough values to unpack (expected 3, got 2)
ERROR:root:Error processing chapters: not enough values to unpack (expected 3, got 2)
Traceback (most recent call last):
  File "D:\github\chappymedium\chappie.py", line 219, in process_chapters
    result = self.chappie_processor.process_srt(srt_content)
  File "D:\github\chappymedium\chappie_processor.py", line 26, in process_srt
    entries = parse_srt(srt_content)
  File "D:\github\chappymedium\chappie_utils.py", line 17, in parse_srt
    'start': time_to_seconds(time_range[0]),
  File "D:\github\chappymedium\chappie_utils.py", line 30, in time_to_seconds
    h, m, s = time_str.split(':')
ValueError: not enough values to unpack (expected 3, got 2)
DEBUG:httpx:load_ssl_context verify=True cert=None trust_env=True http2=False
DEBUG:httpx:load_verify_locations cafile='C:\\Users\\taskm\\anaconda3\\Library\\ssl\\cacert.pem'
DEBUG:httpx:load_ssl_context verify=True cert=None trust_env=True http2=False
DEBUG:httpx:load_verify_locations cafile='C:\\Users\\taskm\\anaconda3\\Library\\ssl\\cacert.pem'
DEBUG:numba.core.byteflow:bytecode dump:
>          0	NOP(arg=None, lineno=1141)
           2	LOAD_FAST(arg=0, lineno=1144)
           4	LOAD_CONST(arg=1, lineno=1144)
           6	BINARY_SUBSCR(arg=None, lineno=1144)
           8	STORE_FAST(arg=3, lineno=1144)
          10	LOAD_FAST(arg=1, lineno=1145)
          12	UNARY_NEGATIVE(arg=None, lineno=1145)
          14	LOAD_FAST(arg=3, lineno=1145)
          16	DUP_TOP(arg=None, lineno=1145)
          18	ROT_THREE(arg=None, lineno=1145)
          20	COMPARE_OP(arg=1, lineno=1145)
          22	POP_JUMP_IF_FALSE(arg=32, lineno=1145)
          24	LOAD_FAST(arg=1, lineno=1145)
          26	COMPARE_OP(arg=1, lineno=1145)
          28	POP_JUMP_IF_FALSE(arg=40, lineno=1145)
          30	JUMP_FORWARD(arg=4, lineno=1145)
>         32	POP_TOP(arg=None, lineno=1145)
          34	JUMP_FORWARD(arg=4, lineno=1145)
>         36	LOAD_CONST(arg=1, lineno=1146)
          38	STORE_FAST(arg=3, lineno=1146)
>         40	LOAD_FAST(arg=0, lineno=1148)
          42	LOAD_CONST(arg=2, lineno=1148)
          44	BINARY_SUBSCR(arg=None, lineno=1148)
          46	STORE_FAST(arg=4, lineno=1148)
          48	LOAD_FAST(arg=1, lineno=1149)
          50	UNARY_NEGATIVE(arg=None, lineno=1149)
          52	LOAD_FAST(arg=4, lineno=1149)
          54	DUP_TOP(arg=None, lineno=1149)
          56	ROT_THREE(arg=None, lineno=1149)
          58	COMPARE_OP(arg=1, lineno=1149)
          60	POP_JUMP_IF_FALSE(arg=70, lineno=1149)
          62	LOAD_FAST(arg=1, lineno=1149)
          64	COMPARE_OP(arg=1, lineno=1149)
          66	POP_JUMP_IF_FALSE(arg=78, lineno=1149)
          68	JUMP_FORWARD(arg=4, lineno=1149)
>         70	POP_TOP(arg=None, lineno=1149)
          72	JUMP_FORWARD(arg=4, lineno=1149)
>         74	LOAD_CONST(arg=1, lineno=1150)
          76	STORE_FAST(arg=4, lineno=1150)
>         78	LOAD_FAST(arg=2, lineno=1152)
          80	POP_JUMP_IF_FALSE(arg=102, lineno=1152)
          82	LOAD_GLOBAL(arg=0, lineno=1153)
          84	LOAD_METHOD(arg=1, lineno=1153)
          86	LOAD_FAST(arg=3, lineno=1153)
          88	CALL_METHOD(arg=1, lineno=1153)
          90	LOAD_GLOBAL(arg=0, lineno=1153)
          92	LOAD_METHOD(arg=1, lineno=1153)
          94	LOAD_FAST(arg=4, lineno=1153)
          96	CALL_METHOD(arg=1, lineno=1153)
          98	COMPARE_OP(arg=3, lineno=1153)
         100	RETURN_VALUE(arg=None, lineno=1153)
>        102	LOAD_GLOBAL(arg=0, lineno=1155)
         104	LOAD_METHOD(arg=2, lineno=1155)
         106	LOAD_FAST(arg=3, lineno=1155)
         108	CALL_METHOD(arg=1, lineno=1155)
         110	LOAD_GLOBAL(arg=0, lineno=1155)
         112	LOAD_METHOD(arg=2, lineno=1155)
         114	LOAD_FAST(arg=4, lineno=1155)
         116	CALL_METHOD(arg=1, lineno=1155)
         118	COMPARE_OP(arg=3, lineno=1155)
         120	RETURN_VALUE(arg=None, lineno=1155)
         122	LOAD_CONST(arg=3, lineno=1155)
         124	RETURN_VALUE(arg=None, lineno=1155)
DEBUG:numba.core.byteflow:pending: deque([State(pc_initial=0 nstack_initial=0)])
DEBUG:numba.core.byteflow:stack: []
DEBUG:numba.core.byteflow:state.pc_initial: State(pc_initial=0 nstack_initial=0)
DEBUG:numba.core.byteflow:dispatch pc=0, inst=NOP(arg=None, lineno=1141)
DEBUG:numba.core.byteflow:stack []
DEBUG:numba.core.byteflow:dispatch pc=2, inst=LOAD_FAST(arg=0, lineno=1144)
DEBUG:numba.core.byteflow:stack []
DEBUG:numba.core.byteflow:dispatch pc=4, inst=LOAD_CONST(arg=1, lineno=1144)
DEBUG:numba.core.byteflow:stack ['$x2.0']
DEBUG:numba.core.byteflow:dispatch pc=6, inst=BINARY_SUBSCR(arg=None, lineno=1144)
DEBUG:numba.core.byteflow:stack ['$x2.0', '$const4.1']
DEBUG:numba.core.byteflow:dispatch pc=8, inst=STORE_FAST(arg=3, lineno=1144)
DEBUG:numba.core.byteflow:stack ['$6binary_subscr.2']
DEBUG:numba.core.byteflow:dispatch pc=10, inst=LOAD_FAST(arg=1, lineno=1145)
DEBUG:numba.core.byteflow:stack []
DEBUG:numba.core.byteflow:dispatch pc=12, inst=UNARY_NEGATIVE(arg=None, lineno=1145)
DEBUG:numba.core.byteflow:stack ['$threshold10.3']
DEBUG:numba.core.byteflow:dispatch pc=14, inst=LOAD_FAST(arg=3, lineno=1145)
DEBUG:numba.core.byteflow:stack ['$12unary_negative.4']
DEBUG:numba.core.byteflow:dispatch pc=16, inst=DUP_TOP(arg=None, lineno=1145)
DEBUG:numba.core.byteflow:stack ['$12unary_negative.4', '$x014.5']
DEBUG:numba.core.byteflow:dispatch pc=18, inst=ROT_THREE(arg=None, lineno=1145)
DEBUG:numba.core.byteflow:stack ['$12unary_negative.4', '$x014.5', '$16dup_top.6']
DEBUG:numba.core.byteflow:dispatch pc=20, inst=COMPARE_OP(arg=1, lineno=1145)
DEBUG:numba.core.byteflow:stack ['$16dup_top.6', '$12unary_negative.4', '$x014.5']
DEBUG:numba.core.byteflow:dispatch pc=22, inst=POP_JUMP_IF_FALSE(arg=32, lineno=1145)
DEBUG:numba.core.byteflow:stack ['$16dup_top.6', '$20compare_op.7']
DEBUG:numba.core.byteflow:end state. edges=[Edge(pc=24, stack=('$16dup_top.6',), blockstack=(), npush=0), Edge(pc=32, stack=('$16dup_top.6',), blockstack=(), npush=0)]
DEBUG:numba.core.byteflow:pending: deque([State(pc_initial=24 nstack_initial=1), State(pc_initial=32 nstack_initial=1)])
DEBUG:numba.core.byteflow:stack: ['$phi24.0']
DEBUG:numba.core.byteflow:state.pc_initial: State(pc_initial=24 nstack_initial=1)
DEBUG:numba.core.byteflow:dispatch pc=24, inst=LOAD_FAST(arg=1, lineno=1145)
DEBUG:numba.core.byteflow:stack ['$phi24.0']
DEBUG:numba.core.byteflow:dispatch pc=26, inst=COMPARE_OP(arg=1, lineno=1145)
DEBUG:numba.core.byteflow:stack ['$phi24.0', '$threshold24.1']
DEBUG:numba.core.byteflow:dispatch pc=28, inst=POP_JUMP_IF_FALSE(arg=40, lineno=1145)
DEBUG:numba.core.byteflow:stack ['$26compare_op.2']
DEBUG:numba.core.byteflow:end state. edges=[Edge(pc=30, stack=(), blockstack=(), npush=0), Edge(pc=40, stack=(), blockstack=(), npush=0)]
DEBUG:numba.core.byteflow:pending: deque([State(pc_initial=32 nstack_initial=1), State(pc_initial=30 nstack_initial=0), State(pc_initial=40 nstack_initial=0)])
DEBUG:numba.core.byteflow:stack: ['$phi32.0']
DEBUG:numba.core.byteflow:state.pc_initial: State(pc_initial=32 nstack_initial=1)
DEBUG:numba.core.byteflow:dispatch pc=32, inst=POP_TOP(arg=None, lineno=1145)
DEBUG:numba.core.byteflow:stack ['$phi32.0']
DEBUG:numba.core.byteflow:dispatch pc=34, inst=JUMP_FORWARD(arg=4, lineno=1145)
DEBUG:numba.core.byteflow:stack []
DEBUG:numba.core.byteflow:end state. edges=[Edge(pc=40, stack=(), blockstack=(), npush=0)]
DEBUG:numba.core.byteflow:pending: deque([State(pc_initial=30 nstack_initial=0), State(pc_initial=40 nstack_initial=0), State(pc_initial=40 nstack_initial=0)])
DEBUG:numba.core.byteflow:stack: []
DEBUG:numba.core.byteflow:state.pc_initial: State(pc_initial=30 nstack_initial=0)
DEBUG:numba.core.byteflow:dispatch pc=30, inst=JUMP_FORWARD(arg=4, lineno=1145)
DEBUG:numba.core.byteflow:stack []
DEBUG:numba.core.byteflow:end state. edges=[Edge(pc=36, stack=(), blockstack=(), npush=0)]
DEBUG:numba.core.byteflow:pending: deque([State(pc_initial=40 nstack_initial=0), State(pc_initial=40 nstack_initial=0), State(pc_initial=36 nstack_initial=0)])
DEBUG:numba.core.byteflow:stack: []
DEBUG:numba.core.byteflow:state.pc_initial: State(pc_initial=40 nstack_initial=0)
DEBUG:numba.core.byteflow:dispatch pc=40, inst=LOAD_FAST(arg=0, lineno=1148)
DEBUG:numba.core.byteflow:stack []
DEBUG:numba.core.byteflow:dispatch pc=42, inst=LOAD_CONST(arg=2, lineno=1148)
DEBUG:numba.core.byteflow:stack ['$x40.0']
DEBUG:numba.core.byteflow:dispatch pc=44, inst=BINARY_SUBSCR(arg=None, lineno=1148)
DEBUG:numba.core.byteflow:stack ['$x40.0', '$const42.1']
DEBUG:numba.core.byteflow:dispatch pc=46, inst=STORE_FAST(arg=4, lineno=1148)
DEBUG:numba.core.byteflow:stack ['$44binary_subscr.2']
DEBUG:numba.core.byteflow:dispatch pc=48, inst=LOAD_FAST(arg=1, lineno=1149)
DEBUG:numba.core.byteflow:stack []
DEBUG:numba.core.byteflow:dispatch pc=50, inst=UNARY_NEGATIVE(arg=None, lineno=1149)
DEBUG:numba.core.byteflow:stack ['$threshold48.3']
DEBUG:numba.core.byteflow:dispatch pc=52, inst=LOAD_FAST(arg=4, lineno=1149)
DEBUG:numba.core.byteflow:stack ['$50unary_negative.4']
DEBUG:numba.core.byteflow:dispatch pc=54, inst=DUP_TOP(arg=None, lineno=1149)
DEBUG:numba.core.byteflow:stack ['$50unary_negative.4', '$x152.5']
DEBUG:numba.core.byteflow:dispatch pc=56, inst=ROT_THREE(arg=None, lineno=1149)
DEBUG:numba.core.byteflow:stack ['$50unary_negative.4', '$x152.5', '$54dup_top.6']
DEBUG:numba.core.byteflow:dispatch pc=58, inst=COMPARE_OP(arg=1, lineno=1149)
DEBUG:numba.core.byteflow:stack ['$54dup_top.6', '$50unary_negative.4', '$x152.5']
DEBUG:numba.core.byteflow:dispatch pc=60, inst=POP_JUMP_IF_FALSE(arg=70, lineno=1149)
DEBUG:numba.core.byteflow:stack ['$54dup_top.6', '$58compare_op.7']
DEBUG:numba.core.byteflow:end state. edges=[Edge(pc=62, stack=('$54dup_top.6',), blockstack=(), npush=0), Edge(pc=70, stack=('$54dup_top.6',), blockstack=(), npush=0)]
DEBUG:numba.core.byteflow:pending: deque([State(pc_initial=40 nstack_initial=0), State(pc_initial=36 nstack_initial=0), State(pc_initial=62 nstack_initial=1), State(pc_initial=70 nstack_initial=1)])
DEBUG:numba.core.byteflow:pending: deque([State(pc_initial=36 nstack_initial=0), State(pc_initial=62 nstack_initial=1), State(pc_initial=70 nstack_initial=1)])
DEBUG:numba.core.byteflow:stack: []
DEBUG:numba.core.byteflow:state.pc_initial: State(pc_initial=36 nstack_initial=0)
DEBUG:numba.core.byteflow:dispatch pc=36, inst=LOAD_CONST(arg=1, lineno=1146)
DEBUG:numba.core.byteflow:stack []
DEBUG:numba.core.byteflow:dispatch pc=38, inst=STORE_FAST(arg=3, lineno=1146)
DEBUG:numba.core.byteflow:stack ['$const36.0']
DEBUG:numba.core.byteflow:end state. edges=[Edge(pc=40, stack=(), blockstack=(), npush=0)]
DEBUG:numba.core.byteflow:pending: deque([State(pc_initial=62 nstack_initial=1), State(pc_initial=70 nstack_initial=1), State(pc_initial=40 nstack_initial=0)])
DEBUG:numba.core.byteflow:stack: ['$phi62.0']
DEBUG:numba.core.byteflow:state.pc_initial: State(pc_initial=62 nstack_initial=1)
DEBUG:numba.core.byteflow:dispatch pc=62, inst=LOAD_FAST(arg=1, lineno=1149)
DEBUG:numba.core.byteflow:stack ['$phi62.0']
DEBUG:numba.core.byteflow:dispatch pc=64, inst=COMPARE_OP(arg=1, lineno=1149)
DEBUG:numba.core.byteflow:stack ['$phi62.0', '$threshold62.1']
DEBUG:numba.core.byteflow:dispatch pc=66, inst=POP_JUMP_IF_FALSE(arg=78, lineno=1149)
DEBUG:numba.core.byteflow:stack ['$64compare_op.2']
DEBUG:numba.core.byteflow:end state. edges=[Edge(pc=68, stack=(), blockstack=(), npush=0), Edge(pc=78, stack=(), blockstack=(), npush=0)]
DEBUG:numba.core.byteflow:pending: deque([State(pc_initial=70 nstack_initial=1), State(pc_initial=40 nstack_initial=0), State(pc_initial=68 nstack_initial=0), State(pc_initial=78 nstack_initial=0)])
DEBUG:numba.core.byteflow:stack: ['$phi70.0']
DEBUG:numba.core.byteflow:state.pc_initial: State(pc_initial=70 nstack_initial=1)
DEBUG:numba.core.byteflow:dispatch pc=70, inst=POP_TOP(arg=None, lineno=1149)
DEBUG:numba.core.byteflow:stack ['$phi70.0']
DEBUG:numba.core.byteflow:dispatch pc=72, inst=JUMP_FORWARD(arg=4, lineno=1149)
DEBUG:numba.core.byteflow:stack []
DEBUG:numba.core.byteflow:end state. edges=[Edge(pc=78, stack=(), blockstack=(), npush=0)]
DEBUG:numba.core.byteflow:pending: deque([State(pc_initial=40 nstack_initial=0), State(pc_initial=68 nstack_initial=0), State(pc_initial=78 nstack_initial=0), State(pc_initial=78 nstack_initial=0)])
DEBUG:numba.core.byteflow:pending: deque([State(pc_initial=68 nstack_initial=0), State(pc_initial=78 nstack_initial=0), State(pc_initial=78 nstack_initial=0)])
DEBUG:numba.core.byteflow:stack: []
DEBUG:numba.core.byteflow:state.pc_initial: State(pc_initial=68 nstack_initial=0)
DEBUG:numba.core.byteflow:dispatch pc=68, inst=JUMP_FORWARD(arg=4, lineno=1149)
DEBUG:numba.core.byteflow:stack []
DEBUG:numba.core.byteflow:end state. edges=[Edge(pc=74, stack=(), blockstack=(), npush=0)]
DEBUG:numba.core.byteflow:pending: deque([State(pc_initial=78 nstack_initial=0), State(pc_initial=78 nstack_initial=0), State(pc_initial=74 nstack_initial=0)])
DEBUG:numba.core.byteflow:stack: []
DEBUG:numba.core.byteflow:state.pc_initial: State(pc_initial=78 nstack_initial=0)
DEBUG:numba.core.byteflow:dispatch pc=78, inst=LOAD_FAST(arg=2, lineno=1152)
DEBUG:numba.core.byteflow:stack []
DEBUG:numba.core.byteflow:dispatch pc=80, inst=POP_JUMP_IF_FALSE(arg=102, lineno=1152)
DEBUG:numba.core.byteflow:stack ['$zero_pos78.0']
DEBUG:numba.core.byteflow:end state. edges=[Edge(pc=82, stack=(), blockstack=(), npush=0), Edge(pc=102, stack=(), blockstack=(), npush=0)]
DEBUG:numba.core.byteflow:pending: deque([State(pc_initial=78 nstack_initial=0), State(pc_initial=74 nstack_initial=0), State(pc_initial=82 nstack_initial=0), State(pc_initial=102 nstack_initial=0)])
DEBUG:numba.core.byteflow:pending: deque([State(pc_initial=74 nstack_initial=0), State(pc_initial=82 nstack_initial=0), State(pc_initial=102 nstack_initial=0)])
DEBUG:numba.core.byteflow:stack: []
DEBUG:numba.core.byteflow:state.pc_initial: State(pc_initial=74 nstack_initial=0)
DEBUG:numba.core.byteflow:dispatch pc=74, inst=LOAD_CONST(arg=1, lineno=1150)
DEBUG:numba.core.byteflow:stack []
DEBUG:numba.core.byteflow:dispatch pc=76, inst=STORE_FAST(arg=4, lineno=1150)
DEBUG:numba.core.byteflow:stack ['$const74.0']
DEBUG:numba.core.byteflow:end state. edges=[Edge(pc=78, stack=(), blockstack=(), npush=0)]
DEBUG:numba.core.byteflow:pending: deque([State(pc_initial=82 nstack_initial=0), State(pc_initial=102 nstack_initial=0), State(pc_initial=78 nstack_initial=0)])
DEBUG:numba.core.byteflow:stack: []
DEBUG:numba.core.byteflow:state.pc_initial: State(pc_initial=82 nstack_initial=0)
DEBUG:numba.core.byteflow:dispatch pc=82, inst=LOAD_GLOBAL(arg=0, lineno=1153)
DEBUG:numba.core.byteflow:stack []
DEBUG:numba.core.byteflow:dispatch pc=84, inst=LOAD_METHOD(arg=1, lineno=1153)
DEBUG:numba.core.byteflow:stack ['$82load_global.0']
DEBUG:numba.core.byteflow:dispatch pc=86, inst=LOAD_FAST(arg=3, lineno=1153)
DEBUG:numba.core.byteflow:stack ['$84load_method.1']
DEBUG:numba.core.byteflow:dispatch pc=88, inst=CALL_METHOD(arg=1, lineno=1153)
DEBUG:numba.core.byteflow:stack ['$84load_method.1', '$x086.2']
DEBUG:numba.core.byteflow:dispatch pc=90, inst=LOAD_GLOBAL(arg=0, lineno=1153)
DEBUG:numba.core.byteflow:stack ['$88call_method.3']
DEBUG:numba.core.byteflow:dispatch pc=92, inst=LOAD_METHOD(arg=1, lineno=1153)
DEBUG:numba.core.byteflow:stack ['$88call_method.3', '$90load_global.4']
DEBUG:numba.core.byteflow:dispatch pc=94, inst=LOAD_FAST(arg=4, lineno=1153)
DEBUG:numba.core.byteflow:stack ['$88call_method.3', '$92load_method.5']
DEBUG:numba.core.byteflow:dispatch pc=96, inst=CALL_METHOD(arg=1, lineno=1153)
DEBUG:numba.core.byteflow:stack ['$88call_method.3', '$92load_method.5', '$x194.6']
DEBUG:numba.core.byteflow:dispatch pc=98, inst=COMPARE_OP(arg=3, lineno=1153)
DEBUG:numba.core.byteflow:stack ['$88call_method.3', '$96call_method.7']
DEBUG:numba.core.byteflow:dispatch pc=100, inst=RETURN_VALUE(arg=None, lineno=1153)
DEBUG:numba.core.byteflow:stack ['$98compare_op.8']
DEBUG:numba.core.byteflow:end state. edges=[]
DEBUG:numba.core.byteflow:pending: deque([State(pc_initial=102 nstack_initial=0), State(pc_initial=78 nstack_initial=0)])
DEBUG:numba.core.byteflow:stack: []
DEBUG:numba.core.byteflow:state.pc_initial: State(pc_initial=102 nstack_initial=0)
DEBUG:numba.core.byteflow:dispatch pc=102, inst=LOAD_GLOBAL(arg=0, lineno=1155)
DEBUG:numba.core.byteflow:stack []
DEBUG:numba.core.byteflow:dispatch pc=104, inst=LOAD_METHOD(arg=2, lineno=1155)
DEBUG:numba.core.byteflow:stack ['$102load_global.0']
DEBUG:numba.core.byteflow:dispatch pc=106, inst=LOAD_FAST(arg=3, lineno=1155)
DEBUG:numba.core.byteflow:stack ['$104load_method.1']
DEBUG:numba.core.byteflow:dispatch pc=108, inst=CALL_METHOD(arg=1, lineno=1155)
DEBUG:numba.core.byteflow:stack ['$104load_method.1', '$x0106.2']
DEBUG:numba.core.byteflow:dispatch pc=110, inst=LOAD_GLOBAL(arg=0, lineno=1155)
DEBUG:numba.core.byteflow:stack ['$108call_method.3']
DEBUG:numba.core.byteflow:dispatch pc=112, inst=LOAD_METHOD(arg=2, lineno=1155)
DEBUG:numba.core.byteflow:stack ['$108call_method.3', '$110load_global.4']
DEBUG:numba.core.byteflow:dispatch pc=114, inst=LOAD_FAST(arg=4, lineno=1155)
DEBUG:numba.core.byteflow:stack ['$108call_method.3', '$112load_method.5']
DEBUG:numba.core.byteflow:dispatch pc=116, inst=CALL_METHOD(arg=1, lineno=1155)
DEBUG:numba.core.byteflow:stack ['$108call_method.3', '$112load_method.5', '$x1114.6']
DEBUG:numba.core.byteflow:dispatch pc=118, inst=COMPARE_OP(arg=3, lineno=1155)
DEBUG:numba.core.byteflow:stack ['$108call_method.3', '$116call_method.7']
DEBUG:numba.core.byteflow:dispatch pc=120, inst=RETURN_VALUE(arg=None, lineno=1155)
DEBUG:numba.core.byteflow:stack ['$118compare_op.8']
DEBUG:numba.core.byteflow:end state. edges=[]
DEBUG:numba.core.byteflow:pending: deque([State(pc_initial=78 nstack_initial=0)])
DEBUG:numba.core.byteflow:-------------------------Prune PHIs-------------------------
DEBUG:numba.core.byteflow:Used_phis: defaultdict(<class 'set'>,
            {State(pc_initial=0 nstack_initial=0): set(),
             State(pc_initial=24 nstack_initial=1): {'$phi24.0'},
             State(pc_initial=30 nstack_initial=0): set(),
             State(pc_initial=32 nstack_initial=1): set(),
             State(pc_initial=36 nstack_initial=0): set(),
             State(pc_initial=40 nstack_initial=0): set(),
             State(pc_initial=62 nstack_initial=1): {'$phi62.0'},
             State(pc_initial=68 nstack_initial=0): set(),
             State(pc_initial=70 nstack_initial=1): set(),
             State(pc_initial=74 nstack_initial=0): set(),
             State(pc_initial=78 nstack_initial=0): set(),
             State(pc_initial=82 nstack_initial=0): set(),
             State(pc_initial=102 nstack_initial=0): set()})
DEBUG:numba.core.byteflow:defmap: {'$phi24.0': State(pc_initial=0 nstack_initial=0),
 '$phi32.0': State(pc_initial=0 nstack_initial=0),
 '$phi62.0': State(pc_initial=40 nstack_initial=0),
 '$phi70.0': State(pc_initial=40 nstack_initial=0)}
DEBUG:numba.core.byteflow:phismap: defaultdict(<class 'set'>,
            {'$phi24.0': {('$16dup_top.6',
                           State(pc_initial=0 nstack_initial=0))},
             '$phi32.0': {('$16dup_top.6',
                           State(pc_initial=0 nstack_initial=0))},
             '$phi62.0': {('$54dup_top.6',
                           State(pc_initial=40 nstack_initial=0))},
             '$phi70.0': {('$54dup_top.6',
                           State(pc_initial=40 nstack_initial=0))}})
DEBUG:numba.core.byteflow:changing phismap: defaultdict(<class 'set'>,
            {'$phi24.0': {('$16dup_top.6',
                           State(pc_initial=0 nstack_initial=0))},
             '$phi32.0': {('$16dup_top.6',
                           State(pc_initial=0 nstack_initial=0))},
             '$phi62.0': {('$54dup_top.6',
                           State(pc_initial=40 nstack_initial=0))},
             '$phi70.0': {('$54dup_top.6',
                           State(pc_initial=40 nstack_initial=0))}})
DEBUG:numba.core.byteflow:keep phismap: {'$phi24.0': {('$16dup_top.6', State(pc_initial=0 nstack_initial=0))},
 '$phi62.0': {('$54dup_top.6', State(pc_initial=40 nstack_initial=0))}}
DEBUG:numba.core.byteflow:new_out: defaultdict(<class 'dict'>,
            {State(pc_initial=0 nstack_initial=0): {'$phi24.0': '$16dup_top.6'},
             State(pc_initial=40 nstack_initial=0): {'$phi62.0': '$54dup_top.6'}})
DEBUG:numba.core.byteflow:----------------------DONE Prune PHIs-----------------------
DEBUG:numba.core.byteflow:block_infos State(pc_initial=0 nstack_initial=0):
AdaptBlockInfo(insts=((0, {}), (2, {'res': '$x2.0'}), (4, {'res': '$const4.1'}), (6, {'index': '$const4.1', 'target': '$x2.0', 'res': '$6binary_subscr.2'}), (8, {'value': '$6binary_subscr.2'}), (10, {'res': '$threshold10.3'}), (12, {'value': '$threshold10.3', 'res': '$12unary_negative.4'}), (14, {'res': '$x014.5'}), (16, {'orig': ['$x014.5'], 'duped': ['$16dup_top.6']}), (20, {'lhs': '$12unary_negative.4', 'rhs': '$x014.5', 'res': '$20compare_op.7'}), (22, {'pred': '$20compare_op.7'})), outgoing_phis={'$phi24.0': '$16dup_top.6'}, blockstack=(), active_try_block=None, outgoing_edgepushed={24: ('$16dup_top.6',), 32: ('$16dup_top.6',)})
DEBUG:numba.core.byteflow:block_infos State(pc_initial=24 nstack_initial=1):
AdaptBlockInfo(insts=((24, {'res': '$threshold24.1'}), (26, {'lhs': '$phi24.0', 'rhs': '$threshold24.1', 'res': '$26compare_op.2'}), (28, {'pred': '$26compare_op.2'})), outgoing_phis={}, blockstack=(), active_try_block=None, outgoing_edgepushed={30: (), 40: ()})
DEBUG:numba.core.byteflow:block_infos State(pc_initial=30 nstack_initial=0):
AdaptBlockInfo(insts=((30, {}),), outgoing_phis={}, blockstack=(), active_try_block=None, outgoing_edgepushed={36: ()})
DEBUG:numba.core.byteflow:block_infos State(pc_initial=32 nstack_initial=1):
AdaptBlockInfo(insts=((34, {}),), outgoing_phis={}, blockstack=(), active_try_block=None, outgoing_edgepushed={40: ()})
DEBUG:numba.core.byteflow:block_infos State(pc_initial=36 nstack_initial=0):
AdaptBlockInfo(insts=((36, {'res': '$const36.0'}), (38, {'value': '$const36.0'})), outgoing_phis={}, blockstack=(), active_try_block=None, outgoing_edgepushed={40: ()})
DEBUG:numba.core.byteflow:block_infos State(pc_initial=40 nstack_initial=0):
AdaptBlockInfo(insts=((40, {'res': '$x40.0'}), (42, {'res': '$const42.1'}), (44, {'index': '$const42.1', 'target': '$x40.0', 'res': '$44binary_subscr.2'}), (46, {'value': '$44binary_subscr.2'}), (48, {'res': '$threshold48.3'}), (50, {'value': '$threshold48.3', 'res': '$50unary_negative.4'}), (52, {'res': '$x152.5'}), (54, {'orig': ['$x152.5'], 'duped': ['$54dup_top.6']}), (58, {'lhs': '$50unary_negative.4', 'rhs': '$x152.5', 'res': '$58compare_op.7'}), (60, {'pred': '$58compare_op.7'})), outgoing_phis={'$phi62.0': '$54dup_top.6'}, blockstack=(), active_try_block=None, outgoing_edgepushed={62: ('$54dup_top.6',), 70: ('$54dup_top.6',)})
DEBUG:numba.core.byteflow:block_infos State(pc_initial=62 nstack_initial=1):
AdaptBlockInfo(insts=((62, {'res': '$threshold62.1'}), (64, {'lhs': '$phi62.0', 'rhs': '$threshold62.1', 'res': '$64compare_op.2'}), (66, {'pred': '$64compare_op.2'})), outgoing_phis={}, blockstack=(), active_try_block=None, outgoing_edgepushed={68: (), 78: ()})
DEBUG:numba.core.byteflow:block_infos State(pc_initial=68 nstack_initial=0):
AdaptBlockInfo(insts=((68, {}),), outgoing_phis={}, blockstack=(), active_try_block=None, outgoing_edgepushed={74: ()})
DEBUG:numba.core.byteflow:block_infos State(pc_initial=70 nstack_initial=1):
AdaptBlockInfo(insts=((72, {}),), outgoing_phis={}, blockstack=(), active_try_block=None, outgoing_edgepushed={78: ()})
DEBUG:numba.core.byteflow:block_infos State(pc_initial=74 nstack_initial=0):
AdaptBlockInfo(insts=((74, {'res': '$const74.0'}), (76, {'value': '$const74.0'})), outgoing_phis={}, blockstack=(), active_try_block=None, outgoing_edgepushed={78: ()})
DEBUG:numba.core.byteflow:block_infos State(pc_initial=78 nstack_initial=0):
AdaptBlockInfo(insts=((78, {'res': '$zero_pos78.0'}), (80, {'pred': '$zero_pos78.0'})), outgoing_phis={}, blockstack=(), active_try_block=None, outgoing_edgepushed={82: (), 102: ()})
DEBUG:numba.core.byteflow:block_infos State(pc_initial=82 nstack_initial=0):
AdaptBlockInfo(insts=((82, {'res': '$82load_global.0'}), (84, {'item': '$82load_global.0', 'res': '$84load_method.1'}), (86, {'res': '$x086.2'}), (88, {'func': '$84load_method.1', 'args': ['$x086.2'], 'res': '$88call_method.3'}), (90, {'res': '$90load_global.4'}), (92, {'item': '$90load_global.4', 'res': '$92load_method.5'}), (94, {'res': '$x194.6'}), (96, {'func': '$92load_method.5', 'args': ['$x194.6'], 'res': '$96call_method.7'}), (98, {'lhs': '$88call_method.3', 'rhs': '$96call_method.7', 'res': '$98compare_op.8'}), (100, {'retval': '$98compare_op.8', 'castval': '$100return_value.9'})), outgoing_phis={}, blockstack=(), active_try_block=None, outgoing_edgepushed={})
DEBUG:numba.core.byteflow:block_infos State(pc_initial=102 nstack_initial=0):
AdaptBlockInfo(insts=((102, {'res': '$102load_global.0'}), (104, {'item': '$102load_global.0', 'res': '$104load_method.1'}), (106, {'res': '$x0106.2'}), (108, {'func': '$104load_method.1', 'args': ['$x0106.2'], 'res': '$108call_method.3'}), (110, {'res': '$110load_global.4'}), (112, {'item': '$110load_global.4', 'res': '$112load_method.5'}), (114, {'res': '$x1114.6'}), (116, {'func': '$112load_method.5', 'args': ['$x1114.6'], 'res': '$116call_method.7'}), (118, {'lhs': '$108call_method.3', 'rhs': '$116call_method.7', 'res': '$118compare_op.8'}), (120, {'retval': '$118compare_op.8', 'castval': '$120return_value.9'})), outgoing_phis={}, blockstack=(), active_try_block=None, outgoing_edgepushed={})
DEBUG:numba.core.interpreter:label 0:
    x = arg(0, name=x)                       ['x']
    threshold = arg(1, name=threshold)       ['threshold']
    zero_pos = arg(2, name=zero_pos)         ['zero_pos']
    $const4.1 = const(int, 0)                ['$const4.1']
    x0 = getitem(value=x, index=$const4.1, fn=<built-in function getitem>) ['$const4.1', 'x', 'x0']
    $12unary_negative.4 = unary(fn=<built-in function neg>, value=threshold) ['$12unary_negative.4', 'threshold']
    $20compare_op.7 = $12unary_negative.4 <= x0 ['$12unary_negative.4', '$20compare_op.7', 'x0']
    bool22 = global(bool: <class 'bool'>)    ['bool22']
    $22pred = call bool22($20compare_op.7, func=bool22, args=(Var($20compare_op.7, audio.py:1145),), kws=(), vararg=None, varkwarg=None, target=None) ['$20compare_op.7', '$22pred', 'bool22']
    $phi24.0 = x0                            ['$phi24.0', 'x0']
    branch $22pred, 24, 32                   ['$22pred']
label 24:
    $26compare_op.2 = $phi24.0 <= threshold  ['$26compare_op.2', '$phi24.0', 'threshold']
    bool28 = global(bool: <class 'bool'>)    ['bool28']
    $28pred = call bool28($26compare_op.2, func=bool28, args=(Var($26compare_op.2, audio.py:1145),), kws=(), vararg=None, varkwarg=None, target=None) ['$26compare_op.2', '$28pred', 'bool28']
    branch $28pred, 30, 40                   ['$28pred']
label 30:
    jump 36                                  []
label 32:
    jump 40                                  []
label 36:
    x0 = const(int, 0)                       ['x0']
    jump 40                                  []
label 40:
    $const42.1 = const(int, -1)              ['$const42.1']
    x1 = getitem(value=x, index=$const42.1, fn=<built-in function getitem>) ['$const42.1', 'x', 'x1']
    $50unary_negative.4 = unary(fn=<built-in function neg>, value=threshold) ['$50unary_negative.4', 'threshold']
    $58compare_op.7 = $50unary_negative.4 <= x1 ['$50unary_negative.4', '$58compare_op.7', 'x1']
    bool60 = global(bool: <class 'bool'>)    ['bool60']
    $60pred = call bool60($58compare_op.7, func=bool60, args=(Var($58compare_op.7, audio.py:1149),), kws=(), vararg=None, varkwarg=None, target=None) ['$58compare_op.7', '$60pred', 'bool60']
    $phi62.0 = x1                            ['$phi62.0', 'x1']
    branch $60pred, 62, 70                   ['$60pred']
label 62:
    $64compare_op.2 = $phi62.0 <= threshold  ['$64compare_op.2', '$phi62.0', 'threshold']
    bool66 = global(bool: <class 'bool'>)    ['bool66']
    $66pred = call bool66($64compare_op.2, func=bool66, args=(Var($64compare_op.2, audio.py:1149),), kws=(), vararg=None, varkwarg=None, target=None) ['$64compare_op.2', '$66pred', 'bool66']
    branch $66pred, 68, 78                   ['$66pred']
label 68:
    jump 74                                  []
label 70:
    jump 78                                  []
label 74:
    x1 = const(int, 0)                       ['x1']
    jump 78                                  []
label 78:
    bool80 = global(bool: <class 'bool'>)    ['bool80']
    $80pred = call bool80(zero_pos, func=bool80, args=(Var(zero_pos, audio.py:1141),), kws=(), vararg=None, varkwarg=None, target=None) ['$80pred', 'bool80', 'zero_pos']
    branch $80pred, 82, 102                  ['$80pred']
label 82:
    $82load_global.0 = global(np: <module 'numpy' from 'C:\\Users\\taskm\\anaconda3\\envs\\taskai\\lib\\site-packages\\numpy\\__init__.py'>) ['$82load_global.0']
    $84load_method.1 = getattr(value=$82load_global.0, attr=signbit) ['$82load_global.0', '$84load_method.1']
    $88call_method.3 = call $84load_method.1(x0, func=$84load_method.1, args=[Var(x0, audio.py:1144)], kws=(), vararg=None, varkwarg=None, target=None) ['$84load_method.1', '$88call_method.3', 'x0']
    $90load_global.4 = global(np: <module 'numpy' from 'C:\\Users\\taskm\\anaconda3\\envs\\taskai\\lib\\site-packages\\numpy\\__init__.py'>) ['$90load_global.4']
    $92load_method.5 = getattr(value=$90load_global.4, attr=signbit) ['$90load_global.4', '$92load_method.5']
    $96call_method.7 = call $92load_method.5(x1, func=$92load_method.5, args=[Var(x1, audio.py:1148)], kws=(), vararg=None, varkwarg=None, target=None) ['$92load_method.5', '$96call_method.7', 'x1']
    $98compare_op.8 = $88call_method.3 != $96call_method.7 ['$88call_method.3', '$96call_method.7', '$98compare_op.8']
    $100return_value.9 = cast(value=$98compare_op.8) ['$100return_value.9', '$98compare_op.8']
    return $100return_value.9                ['$100return_value.9']
label 102:
    $102load_global.0 = global(np: <module 'numpy' from 'C:\\Users\\taskm\\anaconda3\\envs\\taskai\\lib\\site-packages\\numpy\\__init__.py'>) ['$102load_global.0']
    $104load_method.1 = getattr(value=$102load_global.0, attr=sign) ['$102load_global.0', '$104load_method.1']
    $108call_method.3 = call $104load_method.1(x0, func=$104load_method.1, args=[Var(x0, audio.py:1144)], kws=(), vararg=None, varkwarg=None, target=None) ['$104load_method.1', '$108call_method.3', 'x0']
    $110load_global.4 = global(np: <module 'numpy' from 'C:\\Users\\taskm\\anaconda3\\envs\\taskai\\lib\\site-packages\\numpy\\__init__.py'>) ['$110load_global.4']
    $112load_method.5 = getattr(value=$110load_global.4, attr=sign) ['$110load_global.4', '$112load_method.5']
    $116call_method.7 = call $112load_method.5(x1, func=$112load_method.5, args=[Var(x1, audio.py:1148)], kws=(), vararg=None, varkwarg=None, target=None) ['$112load_method.5', '$116call_method.7', 'x1']
    $118compare_op.8 = $108call_method.3 != $116call_method.7 ['$108call_method.3', '$116call_method.7', '$118compare_op.8']
    $120return_value.9 = cast(value=$118compare_op.8) ['$118compare_op.8', '$120return_value.9']
    return $120return_value.9                ['$120return_value.9']

DEBUG:numba.core.byteflow:bytecode dump:
>          0	NOP(arg=None, lineno=1039)
           2	LOAD_FAST(arg=0, lineno=1042)
           4	LOAD_CONST(arg=1, lineno=1042)
           6	BINARY_SUBSCR(arg=None, lineno=1042)
           8	LOAD_FAST(arg=0, lineno=1042)
          10	LOAD_CONST(arg=2, lineno=1042)
          12	BINARY_SUBSCR(arg=None, lineno=1042)
          14	COMPARE_OP(arg=4, lineno=1042)
          16	LOAD_FAST(arg=0, lineno=1042)
          18	LOAD_CONST(arg=1, lineno=1042)
          20	BINARY_SUBSCR(arg=None, lineno=1042)
          22	LOAD_FAST(arg=0, lineno=1042)
          24	LOAD_CONST(arg=3, lineno=1042)
          26	BINARY_SUBSCR(arg=None, lineno=1042)
          28	COMPARE_OP(arg=5, lineno=1042)
          30	BINARY_AND(arg=None, lineno=1042)
          32	RETURN_VALUE(arg=None, lineno=1042)
DEBUG:numba.core.byteflow:pending: deque([State(pc_initial=0 nstack_initial=0)])
DEBUG:numba.core.byteflow:stack: []
DEBUG:numba.core.byteflow:state.pc_initial: State(pc_initial=0 nstack_initial=0)
DEBUG:numba.core.byteflow:dispatch pc=0, inst=NOP(arg=None, lineno=1039)
DEBUG:numba.core.byteflow:stack []
DEBUG:numba.core.byteflow:dispatch pc=2, inst=LOAD_FAST(arg=0, lineno=1042)
DEBUG:numba.core.byteflow:stack []
DEBUG:numba.core.byteflow:dispatch pc=4, inst=LOAD_CONST(arg=1, lineno=1042)
DEBUG:numba.core.byteflow:stack ['$x2.0']
DEBUG:numba.core.byteflow:dispatch pc=6, inst=BINARY_SUBSCR(arg=None, lineno=1042)
DEBUG:numba.core.byteflow:stack ['$x2.0', '$const4.1']
DEBUG:numba.core.byteflow:dispatch pc=8, inst=LOAD_FAST(arg=0, lineno=1042)
DEBUG:numba.core.byteflow:stack ['$6binary_subscr.2']
DEBUG:numba.core.byteflow:dispatch pc=10, inst=LOAD_CONST(arg=2, lineno=1042)
DEBUG:numba.core.byteflow:stack ['$6binary_subscr.2', '$x8.3']
DEBUG:numba.core.byteflow:dispatch pc=12, inst=BINARY_SUBSCR(arg=None, lineno=1042)
DEBUG:numba.core.byteflow:stack ['$6binary_subscr.2', '$x8.3', '$const10.4']
DEBUG:numba.core.byteflow:dispatch pc=14, inst=COMPARE_OP(arg=4, lineno=1042)
DEBUG:numba.core.byteflow:stack ['$6binary_subscr.2', '$12binary_subscr.5']
DEBUG:numba.core.byteflow:dispatch pc=16, inst=LOAD_FAST(arg=0, lineno=1042)
DEBUG:numba.core.byteflow:stack ['$14compare_op.6']
DEBUG:numba.core.byteflow:dispatch pc=18, inst=LOAD_CONST(arg=1, lineno=1042)
DEBUG:numba.core.byteflow:stack ['$14compare_op.6', '$x16.7']
DEBUG:numba.core.byteflow:dispatch pc=20, inst=BINARY_SUBSCR(arg=None, lineno=1042)
DEBUG:numba.core.byteflow:stack ['$14compare_op.6', '$x16.7', '$const18.8']
DEBUG:numba.core.byteflow:dispatch pc=22, inst=LOAD_FAST(arg=0, lineno=1042)
DEBUG:numba.core.byteflow:stack ['$14compare_op.6', '$20binary_subscr.9']
DEBUG:numba.core.byteflow:dispatch pc=24, inst=LOAD_CONST(arg=3, lineno=1042)
DEBUG:numba.core.byteflow:stack ['$14compare_op.6', '$20binary_subscr.9', '$x22.10']
DEBUG:numba.core.byteflow:dispatch pc=26, inst=BINARY_SUBSCR(arg=None, lineno=1042)
DEBUG:numba.core.byteflow:stack ['$14compare_op.6', '$20binary_subscr.9', '$x22.10', '$const24.11']
DEBUG:numba.core.byteflow:dispatch pc=28, inst=COMPARE_OP(arg=5, lineno=1042)
DEBUG:numba.core.byteflow:stack ['$14compare_op.6', '$20binary_subscr.9', '$26binary_subscr.12']
DEBUG:numba.core.byteflow:dispatch pc=30, inst=BINARY_AND(arg=None, lineno=1042)
DEBUG:numba.core.byteflow:stack ['$14compare_op.6', '$28compare_op.13']
DEBUG:numba.core.byteflow:dispatch pc=32, inst=RETURN_VALUE(arg=None, lineno=1042)
DEBUG:numba.core.byteflow:stack ['$30binary_and.14']
DEBUG:numba.core.byteflow:end state. edges=[]
DEBUG:numba.core.byteflow:-------------------------Prune PHIs-------------------------
DEBUG:numba.core.byteflow:Used_phis: defaultdict(<class 'set'>, {State(pc_initial=0 nstack_initial=0): set()})
DEBUG:numba.core.byteflow:defmap: {}
DEBUG:numba.core.byteflow:phismap: defaultdict(<class 'set'>, {})
DEBUG:numba.core.byteflow:changing phismap: defaultdict(<class 'set'>, {})
DEBUG:numba.core.byteflow:keep phismap: {}
DEBUG:numba.core.byteflow:new_out: defaultdict(<class 'dict'>, {})
DEBUG:numba.core.byteflow:----------------------DONE Prune PHIs-----------------------
DEBUG:numba.core.byteflow:block_infos State(pc_initial=0 nstack_initial=0):
AdaptBlockInfo(insts=((0, {}), (2, {'res': '$x2.0'}), (4, {'res': '$const4.1'}), (6, {'index': '$const4.1', 'target': '$x2.0', 'res': '$6binary_subscr.2'}), (8, {'res': '$x8.3'}), (10, {'res': '$const10.4'}), (12, {'index': '$const10.4', 'target': '$x8.3', 'res': '$12binary_subscr.5'}), (14, {'lhs': '$6binary_subscr.2', 'rhs': '$12binary_subscr.5', 'res': '$14compare_op.6'}), (16, {'res': '$x16.7'}), (18, {'res': '$const18.8'}), (20, {'index': '$const18.8', 'target': '$x16.7', 'res': '$20binary_subscr.9'}), (22, {'res': '$x22.10'}), (24, {'res': '$const24.11'}), (26, {'index': '$const24.11', 'target': '$x22.10', 'res': '$26binary_subscr.12'}), (28, {'lhs': '$20binary_subscr.9', 'rhs': '$26binary_subscr.12', 'res': '$28compare_op.13'}), (30, {'lhs': '$14compare_op.6', 'rhs': '$28compare_op.13', 'res': '$30binary_and.14'}), (32, {'retval': '$30binary_and.14', 'castval': '$32return_value.15'})), outgoing_phis={}, blockstack=(), active_try_block=None, outgoing_edgepushed={})
DEBUG:numba.core.interpreter:label 0:
    x = arg(0, name=x)                       ['x']
    $const4.1 = const(int, 0)                ['$const4.1']
    $6binary_subscr.2 = getitem(value=x, index=$const4.1, fn=<built-in function getitem>) ['$6binary_subscr.2', '$const4.1', 'x']
    $const10.4 = const(int, -1)              ['$const10.4']
    $12binary_subscr.5 = getitem(value=x, index=$const10.4, fn=<built-in function getitem>) ['$12binary_subscr.5', '$const10.4', 'x']
    $14compare_op.6 = $6binary_subscr.2 > $12binary_subscr.5 ['$12binary_subscr.5', '$14compare_op.6', '$6binary_subscr.2']
    $const18.8 = const(int, 0)               ['$const18.8']
    $20binary_subscr.9 = getitem(value=x, index=$const18.8, fn=<built-in function getitem>) ['$20binary_subscr.9', '$const18.8', 'x']
    $const24.11 = const(int, 1)              ['$const24.11']
    $26binary_subscr.12 = getitem(value=x, index=$const24.11, fn=<built-in function getitem>) ['$26binary_subscr.12', '$const24.11', 'x']
    $28compare_op.13 = $20binary_subscr.9 >= $26binary_subscr.12 ['$20binary_subscr.9', '$26binary_subscr.12', '$28compare_op.13']
    $30binary_and.14 = $14compare_op.6 & $28compare_op.13 ['$14compare_op.6', '$28compare_op.13', '$30binary_and.14']
    $32return_value.15 = cast(value=$30binary_and.14) ['$30binary_and.14', '$32return_value.15']
    return $32return_value.15                ['$32return_value.15']

DEBUG:numba.core.byteflow:bytecode dump:
>          0	NOP(arg=None, lineno=1045)
           2	LOAD_FAST(arg=0, lineno=1048)
           4	LOAD_CONST(arg=1, lineno=1048)
           6	BINARY_SUBSCR(arg=None, lineno=1048)
           8	LOAD_FAST(arg=0, lineno=1048)
          10	LOAD_CONST(arg=2, lineno=1048)
          12	BINARY_SUBSCR(arg=None, lineno=1048)
          14	COMPARE_OP(arg=0, lineno=1048)
          16	LOAD_FAST(arg=0, lineno=1048)
          18	LOAD_CONST(arg=1, lineno=1048)
          20	BINARY_SUBSCR(arg=None, lineno=1048)
          22	LOAD_FAST(arg=0, lineno=1048)
          24	LOAD_CONST(arg=3, lineno=1048)
          26	BINARY_SUBSCR(arg=None, lineno=1048)
          28	COMPARE_OP(arg=1, lineno=1048)
          30	BINARY_AND(arg=None, lineno=1048)
          32	RETURN_VALUE(arg=None, lineno=1048)
DEBUG:numba.core.byteflow:pending: deque([State(pc_initial=0 nstack_initial=0)])
DEBUG:numba.core.byteflow:stack: []
DEBUG:numba.core.byteflow:state.pc_initial: State(pc_initial=0 nstack_initial=0)
DEBUG:numba.core.byteflow:dispatch pc=0, inst=NOP(arg=None, lineno=1045)
DEBUG:numba.core.byteflow:stack []
DEBUG:numba.core.byteflow:dispatch pc=2, inst=LOAD_FAST(arg=0, lineno=1048)
DEBUG:numba.core.byteflow:stack []
DEBUG:numba.core.byteflow:dispatch pc=4, inst=LOAD_CONST(arg=1, lineno=1048)
DEBUG:numba.core.byteflow:stack ['$x2.0']
DEBUG:numba.core.byteflow:dispatch pc=6, inst=BINARY_SUBSCR(arg=None, lineno=1048)
DEBUG:numba.core.byteflow:stack ['$x2.0', '$const4.1']
DEBUG:numba.core.byteflow:dispatch pc=8, inst=LOAD_FAST(arg=0, lineno=1048)
DEBUG:numba.core.byteflow:stack ['$6binary_subscr.2']
DEBUG:numba.core.byteflow:dispatch pc=10, inst=LOAD_CONST(arg=2, lineno=1048)
DEBUG:numba.core.byteflow:stack ['$6binary_subscr.2', '$x8.3']
DEBUG:numba.core.byteflow:dispatch pc=12, inst=BINARY_SUBSCR(arg=None, lineno=1048)
DEBUG:numba.core.byteflow:stack ['$6binary_subscr.2', '$x8.3', '$const10.4']
DEBUG:numba.core.byteflow:dispatch pc=14, inst=COMPARE_OP(arg=0, lineno=1048)
DEBUG:numba.core.byteflow:stack ['$6binary_subscr.2', '$12binary_subscr.5']
DEBUG:numba.core.byteflow:dispatch pc=16, inst=LOAD_FAST(arg=0, lineno=1048)
DEBUG:numba.core.byteflow:stack ['$14compare_op.6']
DEBUG:numba.core.byteflow:dispatch pc=18, inst=LOAD_CONST(arg=1, lineno=1048)
DEBUG:numba.core.byteflow:stack ['$14compare_op.6', '$x16.7']
DEBUG:numba.core.byteflow:dispatch pc=20, inst=BINARY_SUBSCR(arg=None, lineno=1048)
DEBUG:numba.core.byteflow:stack ['$14compare_op.6', '$x16.7', '$const18.8']
DEBUG:numba.core.byteflow:dispatch pc=22, inst=LOAD_FAST(arg=0, lineno=1048)
DEBUG:numba.core.byteflow:stack ['$14compare_op.6', '$20binary_subscr.9']
DEBUG:numba.core.byteflow:dispatch pc=24, inst=LOAD_CONST(arg=3, lineno=1048)
DEBUG:numba.core.byteflow:stack ['$14compare_op.6', '$20binary_subscr.9', '$x22.10']
DEBUG:numba.core.byteflow:dispatch pc=26, inst=BINARY_SUBSCR(arg=None, lineno=1048)
DEBUG:numba.core.byteflow:stack ['$14compare_op.6', '$20binary_subscr.9', '$x22.10', '$const24.11']
DEBUG:numba.core.byteflow:dispatch pc=28, inst=COMPARE_OP(arg=1, lineno=1048)
DEBUG:numba.core.byteflow:stack ['$14compare_op.6', '$20binary_subscr.9', '$26binary_subscr.12']
DEBUG:numba.core.byteflow:dispatch pc=30, inst=BINARY_AND(arg=None, lineno=1048)
DEBUG:numba.core.byteflow:stack ['$14compare_op.6', '$28compare_op.13']
DEBUG:numba.core.byteflow:dispatch pc=32, inst=RETURN_VALUE(arg=None, lineno=1048)
DEBUG:numba.core.byteflow:stack ['$30binary_and.14']
DEBUG:numba.core.byteflow:end state. edges=[]
DEBUG:numba.core.byteflow:-------------------------Prune PHIs-------------------------
DEBUG:numba.core.byteflow:Used_phis: defaultdict(<class 'set'>, {State(pc_initial=0 nstack_initial=0): set()})
DEBUG:numba.core.byteflow:defmap: {}
DEBUG:numba.core.byteflow:phismap: defaultdict(<class 'set'>, {})
DEBUG:numba.core.byteflow:changing phismap: defaultdict(<class 'set'>, {})
DEBUG:numba.core.byteflow:keep phismap: {}
DEBUG:numba.core.byteflow:new_out: defaultdict(<class 'dict'>, {})
DEBUG:numba.core.byteflow:----------------------DONE Prune PHIs-----------------------
DEBUG:numba.core.byteflow:block_infos State(pc_initial=0 nstack_initial=0):
AdaptBlockInfo(insts=((0, {}), (2, {'res': '$x2.0'}), (4, {'res': '$const4.1'}), (6, {'index': '$const4.1', 'target': '$x2.0', 'res': '$6binary_subscr.2'}), (8, {'res': '$x8.3'}), (10, {'res': '$const10.4'}), (12, {'index': '$const10.4', 'target': '$x8.3', 'res': '$12binary_subscr.5'}), (14, {'lhs': '$6binary_subscr.2', 'rhs': '$12binary_subscr.5', 'res': '$14compare_op.6'}), (16, {'res': '$x16.7'}), (18, {'res': '$const18.8'}), (20, {'index': '$const18.8', 'target': '$x16.7', 'res': '$20binary_subscr.9'}), (22, {'res': '$x22.10'}), (24, {'res': '$const24.11'}), (26, {'index': '$const24.11', 'target': '$x22.10', 'res': '$26binary_subscr.12'}), (28, {'lhs': '$20binary_subscr.9', 'rhs': '$26binary_subscr.12', 'res': '$28compare_op.13'}), (30, {'lhs': '$14compare_op.6', 'rhs': '$28compare_op.13', 'res': '$30binary_and.14'}), (32, {'retval': '$30binary_and.14', 'castval': '$32return_value.15'})), outgoing_phis={}, blockstack=(), active_try_block=None, outgoing_edgepushed={})
DEBUG:numba.core.interpreter:label 0:
    x = arg(0, name=x)                       ['x']
    $const4.1 = const(int, 0)                ['$const4.1']
    $6binary_subscr.2 = getitem(value=x, index=$const4.1, fn=<built-in function getitem>) ['$6binary_subscr.2', '$const4.1', 'x']
    $const10.4 = const(int, -1)              ['$const10.4']
    $12binary_subscr.5 = getitem(value=x, index=$const10.4, fn=<built-in function getitem>) ['$12binary_subscr.5', '$const10.4', 'x']
    $14compare_op.6 = $6binary_subscr.2 < $12binary_subscr.5 ['$12binary_subscr.5', '$14compare_op.6', '$6binary_subscr.2']
    $const18.8 = const(int, 0)               ['$const18.8']
    $20binary_subscr.9 = getitem(value=x, index=$const18.8, fn=<built-in function getitem>) ['$20binary_subscr.9', '$const18.8', 'x']
    $const24.11 = const(int, 1)              ['$const24.11']
    $26binary_subscr.12 = getitem(value=x, index=$const24.11, fn=<built-in function getitem>) ['$26binary_subscr.12', '$const24.11', 'x']
    $28compare_op.13 = $20binary_subscr.9 <= $26binary_subscr.12 ['$20binary_subscr.9', '$26binary_subscr.12', '$28compare_op.13']
    $30binary_and.14 = $14compare_op.6 & $28compare_op.13 ['$14compare_op.6', '$28compare_op.13', '$30binary_and.14']
    $32return_value.15 = cast(value=$30binary_and.14) ['$30binary_and.14', '$32return_value.15']
    return $32return_value.15                ['$32return_value.15']

ERROR:root:Error processing chapters: not enough values to unpack (expected 3, got 2)
Traceback (most recent call last):
  File "D:\github\chappymedium\chappie.py", line 219, in process_chapters
    result = self.chappie_processor.process_srt(srt_content)
  File "D:\github\chappymedium\chappie_processor.py", line 32, in process_srt
    result = self.process_chunk(chunk)
  File "D:\github\chappymedium\chappie_processor.py", line 12, in process_chunk
    entries = parse_srt(chunk)
  File "D:\github\chappymedium\chappie_utils.py", line 17, in parse_srt
    'start': time_to_seconds(time_range[0]),
  File "D:\github\chappymedium\chappie_utils.py", line 30, in time_to_seconds
    h, m, s = time_str.split(':')
ValueError: not enough values to unpack (expected 3, got 2)
ERROR:root:Error processing chapters: not enough values to unpack (expected 3, got 2)
Traceback (most recent call last):
  File "D:\github\chappymedium\chappie.py", line 219, in process_chapters
    result = self.chappie_processor.process_srt(srt_content)
  File "D:\github\chappymedium\chappie_processor.py", line 32, in process_srt
    result = self.process_chunk(chunk)
  File "D:\github\chappymedium\chappie_processor.py", line 12, in process_chunk
    entries = parse_srt(chunk)
  File "D:\github\chappymedium\chappie_utils.py", line 17, in parse_srt
    'start': time_to_seconds(time_range[0]),
  File "D:\github\chappymedium\chappie_utils.py", line 30, in time_to_seconds
    h, m, s = time_str.split(':')
ValueError: not enough values to unpack (expected 3, got 2)
ERROR:root:Error processing chapters: not enough values to unpack (expected 3, got 2)
Traceback (most recent call last):
  File "D:\github\chappymedium\chappie.py", line 219, in process_chapters
    result = self.chappie_processor.process_srt(srt_content)
  File "D:\github\chappymedium\chappie_processor.py", line 32, in process_srt
    result = self.process_chunk(chunk)
  File "D:\github\chappymedium\chappie_processor.py", line 12, in process_chunk
    entries = parse_srt(chunk)
  File "D:\github\chappymedium\chappie_utils.py", line 17, in parse_srt
    'start': time_to_seconds(time_range[0]),
  File "D:\github\chappymedium\chappie_utils.py", line 30, in time_to_seconds
    h, m, s = time_str.split(':')
ValueError: not enough values to unpack (expected 3, got 2)
DEBUG:httpx:load_ssl_context verify=True cert=None trust_env=True http2=False
DEBUG:httpx:load_verify_locations cafile='C:\\Users\\taskm\\anaconda3\\Library\\ssl\\cacert.pem'
DEBUG:httpx:load_ssl_context verify=True cert=None trust_env=True http2=False
DEBUG:httpx:load_verify_locations cafile='C:\\Users\\taskm\\anaconda3\\Library\\ssl\\cacert.pem'
DEBUG:numba.core.byteflow:bytecode dump:
>          0	NOP(arg=None, lineno=1141)
           2	LOAD_FAST(arg=0, lineno=1144)
           4	LOAD_CONST(arg=1, lineno=1144)
           6	BINARY_SUBSCR(arg=None, lineno=1144)
           8	STORE_FAST(arg=3, lineno=1144)
          10	LOAD_FAST(arg=1, lineno=1145)
          12	UNARY_NEGATIVE(arg=None, lineno=1145)
          14	LOAD_FAST(arg=3, lineno=1145)
          16	DUP_TOP(arg=None, lineno=1145)
          18	ROT_THREE(arg=None, lineno=1145)
          20	COMPARE_OP(arg=1, lineno=1145)
          22	POP_JUMP_IF_FALSE(arg=32, lineno=1145)
          24	LOAD_FAST(arg=1, lineno=1145)
          26	COMPARE_OP(arg=1, lineno=1145)
          28	POP_JUMP_IF_FALSE(arg=40, lineno=1145)
          30	JUMP_FORWARD(arg=4, lineno=1145)
>         32	POP_TOP(arg=None, lineno=1145)
          34	JUMP_FORWARD(arg=4, lineno=1145)
>         36	LOAD_CONST(arg=1, lineno=1146)
          38	STORE_FAST(arg=3, lineno=1146)
>         40	LOAD_FAST(arg=0, lineno=1148)
          42	LOAD_CONST(arg=2, lineno=1148)
          44	BINARY_SUBSCR(arg=None, lineno=1148)
          46	STORE_FAST(arg=4, lineno=1148)
          48	LOAD_FAST(arg=1, lineno=1149)
          50	UNARY_NEGATIVE(arg=None, lineno=1149)
          52	LOAD_FAST(arg=4, lineno=1149)
          54	DUP_TOP(arg=None, lineno=1149)
          56	ROT_THREE(arg=None, lineno=1149)
          58	COMPARE_OP(arg=1, lineno=1149)
          60	POP_JUMP_IF_FALSE(arg=70, lineno=1149)
          62	LOAD_FAST(arg=1, lineno=1149)
          64	COMPARE_OP(arg=1, lineno=1149)
          66	POP_JUMP_IF_FALSE(arg=78, lineno=1149)
          68	JUMP_FORWARD(arg=4, lineno=1149)
>         70	POP_TOP(arg=None, lineno=1149)
          72	JUMP_FORWARD(arg=4, lineno=1149)
>         74	LOAD_CONST(arg=1, lineno=1150)
          76	STORE_FAST(arg=4, lineno=1150)
>         78	LOAD_FAST(arg=2, lineno=1152)
          80	POP_JUMP_IF_FALSE(arg=102, lineno=1152)
          82	LOAD_GLOBAL(arg=0, lineno=1153)
          84	LOAD_METHOD(arg=1, lineno=1153)
          86	LOAD_FAST(arg=3, lineno=1153)
          88	CALL_METHOD(arg=1, lineno=1153)
          90	LOAD_GLOBAL(arg=0, lineno=1153)
          92	LOAD_METHOD(arg=1, lineno=1153)
          94	LOAD_FAST(arg=4, lineno=1153)
          96	CALL_METHOD(arg=1, lineno=1153)
          98	COMPARE_OP(arg=3, lineno=1153)
         100	RETURN_VALUE(arg=None, lineno=1153)
>        102	LOAD_GLOBAL(arg=0, lineno=1155)
         104	LOAD_METHOD(arg=2, lineno=1155)
         106	LOAD_FAST(arg=3, lineno=1155)
         108	CALL_METHOD(arg=1, lineno=1155)
         110	LOAD_GLOBAL(arg=0, lineno=1155)
         112	LOAD_METHOD(arg=2, lineno=1155)
         114	LOAD_FAST(arg=4, lineno=1155)
         116	CALL_METHOD(arg=1, lineno=1155)
         118	COMPARE_OP(arg=3, lineno=1155)
         120	RETURN_VALUE(arg=None, lineno=1155)
         122	LOAD_CONST(arg=3, lineno=1155)
         124	RETURN_VALUE(arg=None, lineno=1155)
DEBUG:numba.core.byteflow:pending: deque([State(pc_initial=0 nstack_initial=0)])
DEBUG:numba.core.byteflow:stack: []
DEBUG:numba.core.byteflow:state.pc_initial: State(pc_initial=0 nstack_initial=0)
DEBUG:numba.core.byteflow:dispatch pc=0, inst=NOP(arg=None, lineno=1141)
DEBUG:numba.core.byteflow:stack []
DEBUG:numba.core.byteflow:dispatch pc=2, inst=LOAD_FAST(arg=0, lineno=1144)
DEBUG:numba.core.byteflow:stack []
DEBUG:numba.core.byteflow:dispatch pc=4, inst=LOAD_CONST(arg=1, lineno=1144)
DEBUG:numba.core.byteflow:stack ['$x2.0']
DEBUG:numba.core.byteflow:dispatch pc=6, inst=BINARY_SUBSCR(arg=None, lineno=1144)
DEBUG:numba.core.byteflow:stack ['$x2.0', '$const4.1']
DEBUG:numba.core.byteflow:dispatch pc=8, inst=STORE_FAST(arg=3, lineno=1144)
DEBUG:numba.core.byteflow:stack ['$6binary_subscr.2']
DEBUG:numba.core.byteflow:dispatch pc=10, inst=LOAD_FAST(arg=1, lineno=1145)
DEBUG:numba.core.byteflow:stack []
DEBUG:numba.core.byteflow:dispatch pc=12, inst=UNARY_NEGATIVE(arg=None, lineno=1145)
DEBUG:numba.core.byteflow:stack ['$threshold10.3']
DEBUG:numba.core.byteflow:dispatch pc=14, inst=LOAD_FAST(arg=3, lineno=1145)
DEBUG:numba.core.byteflow:stack ['$12unary_negative.4']
DEBUG:numba.core.byteflow:dispatch pc=16, inst=DUP_TOP(arg=None, lineno=1145)
DEBUG:numba.core.byteflow:stack ['$12unary_negative.4', '$x014.5']
DEBUG:numba.core.byteflow:dispatch pc=18, inst=ROT_THREE(arg=None, lineno=1145)
DEBUG:numba.core.byteflow:stack ['$12unary_negative.4', '$x014.5', '$16dup_top.6']
DEBUG:numba.core.byteflow:dispatch pc=20, inst=COMPARE_OP(arg=1, lineno=1145)
DEBUG:numba.core.byteflow:stack ['$16dup_top.6', '$12unary_negative.4', '$x014.5']
DEBUG:numba.core.byteflow:dispatch pc=22, inst=POP_JUMP_IF_FALSE(arg=32, lineno=1145)
DEBUG:numba.core.byteflow:stack ['$16dup_top.6', '$20compare_op.7']
DEBUG:numba.core.byteflow:end state. edges=[Edge(pc=24, stack=('$16dup_top.6',), blockstack=(), npush=0), Edge(pc=32, stack=('$16dup_top.6',), blockstack=(), npush=0)]
DEBUG:numba.core.byteflow:pending: deque([State(pc_initial=24 nstack_initial=1), State(pc_initial=32 nstack_initial=1)])
DEBUG:numba.core.byteflow:stack: ['$phi24.0']
DEBUG:numba.core.byteflow:state.pc_initial: State(pc_initial=24 nstack_initial=1)
DEBUG:numba.core.byteflow:dispatch pc=24, inst=LOAD_FAST(arg=1, lineno=1145)
DEBUG:numba.core.byteflow:stack ['$phi24.0']
DEBUG:numba.core.byteflow:dispatch pc=26, inst=COMPARE_OP(arg=1, lineno=1145)
DEBUG:numba.core.byteflow:stack ['$phi24.0', '$threshold24.1']
DEBUG:numba.core.byteflow:dispatch pc=28, inst=POP_JUMP_IF_FALSE(arg=40, lineno=1145)
DEBUG:numba.core.byteflow:stack ['$26compare_op.2']
DEBUG:numba.core.byteflow:end state. edges=[Edge(pc=30, stack=(), blockstack=(), npush=0), Edge(pc=40, stack=(), blockstack=(), npush=0)]
DEBUG:numba.core.byteflow:pending: deque([State(pc_initial=32 nstack_initial=1), State(pc_initial=30 nstack_initial=0), State(pc_initial=40 nstack_initial=0)])
DEBUG:numba.core.byteflow:stack: ['$phi32.0']
DEBUG:numba.core.byteflow:state.pc_initial: State(pc_initial=32 nstack_initial=1)
DEBUG:numba.core.byteflow:dispatch pc=32, inst=POP_TOP(arg=None, lineno=1145)
DEBUG:numba.core.byteflow:stack ['$phi32.0']
DEBUG:numba.core.byteflow:dispatch pc=34, inst=JUMP_FORWARD(arg=4, lineno=1145)
DEBUG:numba.core.byteflow:stack []
DEBUG:numba.core.byteflow:end state. edges=[Edge(pc=40, stack=(), blockstack=(), npush=0)]
DEBUG:numba.core.byteflow:pending: deque([State(pc_initial=30 nstack_initial=0), State(pc_initial=40 nstack_initial=0), State(pc_initial=40 nstack_initial=0)])
DEBUG:numba.core.byteflow:stack: []
DEBUG:numba.core.byteflow:state.pc_initial: State(pc_initial=30 nstack_initial=0)
DEBUG:numba.core.byteflow:dispatch pc=30, inst=JUMP_FORWARD(arg=4, lineno=1145)
DEBUG:numba.core.byteflow:stack []
DEBUG:numba.core.byteflow:end state. edges=[Edge(pc=36, stack=(), blockstack=(), npush=0)]
DEBUG:numba.core.byteflow:pending: deque([State(pc_initial=40 nstack_initial=0), State(pc_initial=40 nstack_initial=0), State(pc_initial=36 nstack_initial=0)])
DEBUG:numba.core.byteflow:stack: []
DEBUG:numba.core.byteflow:state.pc_initial: State(pc_initial=40 nstack_initial=0)
DEBUG:numba.core.byteflow:dispatch pc=40, inst=LOAD_FAST(arg=0, lineno=1148)
DEBUG:numba.core.byteflow:stack []
DEBUG:numba.core.byteflow:dispatch pc=42, inst=LOAD_CONST(arg=2, lineno=1148)
DEBUG:numba.core.byteflow:stack ['$x40.0']
DEBUG:numba.core.byteflow:dispatch pc=44, inst=BINARY_SUBSCR(arg=None, lineno=1148)
DEBUG:numba.core.byteflow:stack ['$x40.0', '$const42.1']
DEBUG:numba.core.byteflow:dispatch pc=46, inst=STORE_FAST(arg=4, lineno=1148)
DEBUG:numba.core.byteflow:stack ['$44binary_subscr.2']
DEBUG:numba.core.byteflow:dispatch pc=48, inst=LOAD_FAST(arg=1, lineno=1149)
DEBUG:numba.core.byteflow:stack []
DEBUG:numba.core.byteflow:dispatch pc=50, inst=UNARY_NEGATIVE(arg=None, lineno=1149)
DEBUG:numba.core.byteflow:stack ['$threshold48.3']
DEBUG:numba.core.byteflow:dispatch pc=52, inst=LOAD_FAST(arg=4, lineno=1149)
DEBUG:numba.core.byteflow:stack ['$50unary_negative.4']
DEBUG:numba.core.byteflow:dispatch pc=54, inst=DUP_TOP(arg=None, lineno=1149)
DEBUG:numba.core.byteflow:stack ['$50unary_negative.4', '$x152.5']
DEBUG:numba.core.byteflow:dispatch pc=56, inst=ROT_THREE(arg=None, lineno=1149)
DEBUG:numba.core.byteflow:stack ['$50unary_negative.4', '$x152.5', '$54dup_top.6']
DEBUG:numba.core.byteflow:dispatch pc=58, inst=COMPARE_OP(arg=1, lineno=1149)
DEBUG:numba.core.byteflow:stack ['$54dup_top.6', '$50unary_negative.4', '$x152.5']
DEBUG:numba.core.byteflow:dispatch pc=60, inst=POP_JUMP_IF_FALSE(arg=70, lineno=1149)
DEBUG:numba.core.byteflow:stack ['$54dup_top.6', '$58compare_op.7']
DEBUG:numba.core.byteflow:end state. edges=[Edge(pc=62, stack=('$54dup_top.6',), blockstack=(), npush=0), Edge(pc=70, stack=('$54dup_top.6',), blockstack=(), npush=0)]
DEBUG:numba.core.byteflow:pending: deque([State(pc_initial=40 nstack_initial=0), State(pc_initial=36 nstack_initial=0), State(pc_initial=62 nstack_initial=1), State(pc_initial=70 nstack_initial=1)])
DEBUG:numba.core.byteflow:pending: deque([State(pc_initial=36 nstack_initial=0), State(pc_initial=62 nstack_initial=1), State(pc_initial=70 nstack_initial=1)])
DEBUG:numba.core.byteflow:stack: []
DEBUG:numba.core.byteflow:state.pc_initial: State(pc_initial=36 nstack_initial=0)
DEBUG:numba.core.byteflow:dispatch pc=36, inst=LOAD_CONST(arg=1, lineno=1146)
DEBUG:numba.core.byteflow:stack []
DEBUG:numba.core.byteflow:dispatch pc=38, inst=STORE_FAST(arg=3, lineno=1146)
DEBUG:numba.core.byteflow:stack ['$const36.0']
DEBUG:numba.core.byteflow:end state. edges=[Edge(pc=40, stack=(), blockstack=(), npush=0)]
DEBUG:numba.core.byteflow:pending: deque([State(pc_initial=62 nstack_initial=1), State(pc_initial=70 nstack_initial=1), State(pc_initial=40 nstack_initial=0)])
DEBUG:numba.core.byteflow:stack: ['$phi62.0']
DEBUG:numba.core.byteflow:state.pc_initial: State(pc_initial=62 nstack_initial=1)
DEBUG:numba.core.byteflow:dispatch pc=62, inst=LOAD_FAST(arg=1, lineno=1149)
DEBUG:numba.core.byteflow:stack ['$phi62.0']
DEBUG:numba.core.byteflow:dispatch pc=64, inst=COMPARE_OP(arg=1, lineno=1149)
DEBUG:numba.core.byteflow:stack ['$phi62.0', '$threshold62.1']
DEBUG:numba.core.byteflow:dispatch pc=66, inst=POP_JUMP_IF_FALSE(arg=78, lineno=1149)
DEBUG:numba.core.byteflow:stack ['$64compare_op.2']
DEBUG:numba.core.byteflow:end state. edges=[Edge(pc=68, stack=(), blockstack=(), npush=0), Edge(pc=78, stack=(), blockstack=(), npush=0)]
DEBUG:numba.core.byteflow:pending: deque([State(pc_initial=70 nstack_initial=1), State(pc_initial=40 nstack_initial=0), State(pc_initial=68 nstack_initial=0), State(pc_initial=78 nstack_initial=0)])
DEBUG:numba.core.byteflow:stack: ['$phi70.0']
DEBUG:numba.core.byteflow:state.pc_initial: State(pc_initial=70 nstack_initial=1)
DEBUG:numba.core.byteflow:dispatch pc=70, inst=POP_TOP(arg=None, lineno=1149)
DEBUG:numba.core.byteflow:stack ['$phi70.0']
DEBUG:numba.core.byteflow:dispatch pc=72, inst=JUMP_FORWARD(arg=4, lineno=1149)
DEBUG:numba.core.byteflow:stack []
DEBUG:numba.core.byteflow:end state. edges=[Edge(pc=78, stack=(), blockstack=(), npush=0)]
DEBUG:numba.core.byteflow:pending: deque([State(pc_initial=40 nstack_initial=0), State(pc_initial=68 nstack_initial=0), State(pc_initial=78 nstack_initial=0), State(pc_initial=78 nstack_initial=0)])
DEBUG:numba.core.byteflow:pending: deque([State(pc_initial=68 nstack_initial=0), State(pc_initial=78 nstack_initial=0), State(pc_initial=78 nstack_initial=0)])
DEBUG:numba.core.byteflow:stack: []
DEBUG:numba.core.byteflow:state.pc_initial: State(pc_initial=68 nstack_initial=0)
DEBUG:numba.core.byteflow:dispatch pc=68, inst=JUMP_FORWARD(arg=4, lineno=1149)
DEBUG:numba.core.byteflow:stack []
DEBUG:numba.core.byteflow:end state. edges=[Edge(pc=74, stack=(), blockstack=(), npush=0)]
DEBUG:numba.core.byteflow:pending: deque([State(pc_initial=78 nstack_initial=0), State(pc_initial=78 nstack_initial=0), State(pc_initial=74 nstack_initial=0)])
DEBUG:numba.core.byteflow:stack: []
DEBUG:numba.core.byteflow:state.pc_initial: State(pc_initial=78 nstack_initial=0)
DEBUG:numba.core.byteflow:dispatch pc=78, inst=LOAD_FAST(arg=2, lineno=1152)
DEBUG:numba.core.byteflow:stack []
DEBUG:numba.core.byteflow:dispatch pc=80, inst=POP_JUMP_IF_FALSE(arg=102, lineno=1152)
DEBUG:numba.core.byteflow:stack ['$zero_pos78.0']
DEBUG:numba.core.byteflow:end state. edges=[Edge(pc=82, stack=(), blockstack=(), npush=0), Edge(pc=102, stack=(), blockstack=(), npush=0)]
DEBUG:numba.core.byteflow:pending: deque([State(pc_initial=78 nstack_initial=0), State(pc_initial=74 nstack_initial=0), State(pc_initial=82 nstack_initial=0), State(pc_initial=102 nstack_initial=0)])
DEBUG:numba.core.byteflow:pending: deque([State(pc_initial=74 nstack_initial=0), State(pc_initial=82 nstack_initial=0), State(pc_initial=102 nstack_initial=0)])
DEBUG:numba.core.byteflow:stack: []
DEBUG:numba.core.byteflow:state.pc_initial: State(pc_initial=74 nstack_initial=0)
DEBUG:numba.core.byteflow:dispatch pc=74, inst=LOAD_CONST(arg=1, lineno=1150)
DEBUG:numba.core.byteflow:stack []
DEBUG:numba.core.byteflow:dispatch pc=76, inst=STORE_FAST(arg=4, lineno=1150)
DEBUG:numba.core.byteflow:stack ['$const74.0']
DEBUG:numba.core.byteflow:end state. edges=[Edge(pc=78, stack=(), blockstack=(), npush=0)]
DEBUG:numba.core.byteflow:pending: deque([State(pc_initial=82 nstack_initial=0), State(pc_initial=102 nstack_initial=0), State(pc_initial=78 nstack_initial=0)])
DEBUG:numba.core.byteflow:stack: []
DEBUG:numba.core.byteflow:state.pc_initial: State(pc_initial=82 nstack_initial=0)
DEBUG:numba.core.byteflow:dispatch pc=82, inst=LOAD_GLOBAL(arg=0, lineno=1153)
DEBUG:numba.core.byteflow:stack []
DEBUG:numba.core.byteflow:dispatch pc=84, inst=LOAD_METHOD(arg=1, lineno=1153)
DEBUG:numba.core.byteflow:stack ['$82load_global.0']
DEBUG:numba.core.byteflow:dispatch pc=86, inst=LOAD_FAST(arg=3, lineno=1153)
DEBUG:numba.core.byteflow:stack ['$84load_method.1']
DEBUG:numba.core.byteflow:dispatch pc=88, inst=CALL_METHOD(arg=1, lineno=1153)
DEBUG:numba.core.byteflow:stack ['$84load_method.1', '$x086.2']
DEBUG:numba.core.byteflow:dispatch pc=90, inst=LOAD_GLOBAL(arg=0, lineno=1153)
DEBUG:numba.core.byteflow:stack ['$88call_method.3']
DEBUG:numba.core.byteflow:dispatch pc=92, inst=LOAD_METHOD(arg=1, lineno=1153)
DEBUG:numba.core.byteflow:stack ['$88call_method.3', '$90load_global.4']
DEBUG:numba.core.byteflow:dispatch pc=94, inst=LOAD_FAST(arg=4, lineno=1153)
DEBUG:numba.core.byteflow:stack ['$88call_method.3', '$92load_method.5']
DEBUG:numba.core.byteflow:dispatch pc=96, inst=CALL_METHOD(arg=1, lineno=1153)
DEBUG:numba.core.byteflow:stack ['$88call_method.3', '$92load_method.5', '$x194.6']
DEBUG:numba.core.byteflow:dispatch pc=98, inst=COMPARE_OP(arg=3, lineno=1153)
DEBUG:numba.core.byteflow:stack ['$88call_method.3', '$96call_method.7']
DEBUG:numba.core.byteflow:dispatch pc=100, inst=RETURN_VALUE(arg=None, lineno=1153)
DEBUG:numba.core.byteflow:stack ['$98compare_op.8']
DEBUG:numba.core.byteflow:end state. edges=[]
DEBUG:numba.core.byteflow:pending: deque([State(pc_initial=102 nstack_initial=0), State(pc_initial=78 nstack_initial=0)])
DEBUG:numba.core.byteflow:stack: []
DEBUG:numba.core.byteflow:state.pc_initial: State(pc_initial=102 nstack_initial=0)
DEBUG:numba.core.byteflow:dispatch pc=102, inst=LOAD_GLOBAL(arg=0, lineno=1155)
DEBUG:numba.core.byteflow:stack []
DEBUG:numba.core.byteflow:dispatch pc=104, inst=LOAD_METHOD(arg=2, lineno=1155)
DEBUG:numba.core.byteflow:stack ['$102load_global.0']
DEBUG:numba.core.byteflow:dispatch pc=106, inst=LOAD_FAST(arg=3, lineno=1155)
DEBUG:numba.core.byteflow:stack ['$104load_method.1']
DEBUG:numba.core.byteflow:dispatch pc=108, inst=CALL_METHOD(arg=1, lineno=1155)
DEBUG:numba.core.byteflow:stack ['$104load_method.1', '$x0106.2']
DEBUG:numba.core.byteflow:dispatch pc=110, inst=LOAD_GLOBAL(arg=0, lineno=1155)
DEBUG:numba.core.byteflow:stack ['$108call_method.3']
DEBUG:numba.core.byteflow:dispatch pc=112, inst=LOAD_METHOD(arg=2, lineno=1155)
DEBUG:numba.core.byteflow:stack ['$108call_method.3', '$110load_global.4']
DEBUG:numba.core.byteflow:dispatch pc=114, inst=LOAD_FAST(arg=4, lineno=1155)
DEBUG:numba.core.byteflow:stack ['$108call_method.3', '$112load_method.5']
DEBUG:numba.core.byteflow:dispatch pc=116, inst=CALL_METHOD(arg=1, lineno=1155)
DEBUG:numba.core.byteflow:stack ['$108call_method.3', '$112load_method.5', '$x1114.6']
DEBUG:numba.core.byteflow:dispatch pc=118, inst=COMPARE_OP(arg=3, lineno=1155)
DEBUG:numba.core.byteflow:stack ['$108call_method.3', '$116call_method.7']
DEBUG:numba.core.byteflow:dispatch pc=120, inst=RETURN_VALUE(arg=None, lineno=1155)
DEBUG:numba.core.byteflow:stack ['$118compare_op.8']
DEBUG:numba.core.byteflow:end state. edges=[]
DEBUG:numba.core.byteflow:pending: deque([State(pc_initial=78 nstack_initial=0)])
DEBUG:numba.core.byteflow:-------------------------Prune PHIs-------------------------
DEBUG:numba.core.byteflow:Used_phis: defaultdict(<class 'set'>,
            {State(pc_initial=0 nstack_initial=0): set(),
             State(pc_initial=24 nstack_initial=1): {'$phi24.0'},
             State(pc_initial=30 nstack_initial=0): set(),
             State(pc_initial=32 nstack_initial=1): set(),
             State(pc_initial=36 nstack_initial=0): set(),
             State(pc_initial=40 nstack_initial=0): set(),
             State(pc_initial=62 nstack_initial=1): {'$phi62.0'},
             State(pc_initial=68 nstack_initial=0): set(),
             State(pc_initial=70 nstack_initial=1): set(),
             State(pc_initial=74 nstack_initial=0): set(),
             State(pc_initial=78 nstack_initial=0): set(),
             State(pc_initial=82 nstack_initial=0): set(),
             State(pc_initial=102 nstack_initial=0): set()})
DEBUG:numba.core.byteflow:defmap: {'$phi24.0': State(pc_initial=0 nstack_initial=0),
 '$phi32.0': State(pc_initial=0 nstack_initial=0),
 '$phi62.0': State(pc_initial=40 nstack_initial=0),
 '$phi70.0': State(pc_initial=40 nstack_initial=0)}
DEBUG:numba.core.byteflow:phismap: defaultdict(<class 'set'>,
            {'$phi24.0': {('$16dup_top.6',
                           State(pc_initial=0 nstack_initial=0))},
             '$phi32.0': {('$16dup_top.6',
                           State(pc_initial=0 nstack_initial=0))},
             '$phi62.0': {('$54dup_top.6',
                           State(pc_initial=40 nstack_initial=0))},
             '$phi70.0': {('$54dup_top.6',
                           State(pc_initial=40 nstack_initial=0))}})
DEBUG:numba.core.byteflow:changing phismap: defaultdict(<class 'set'>,
            {'$phi24.0': {('$16dup_top.6',
                           State(pc_initial=0 nstack_initial=0))},
             '$phi32.0': {('$16dup_top.6',
                           State(pc_initial=0 nstack_initial=0))},
             '$phi62.0': {('$54dup_top.6',
                           State(pc_initial=40 nstack_initial=0))},
             '$phi70.0': {('$54dup_top.6',
                           State(pc_initial=40 nstack_initial=0))}})
DEBUG:numba.core.byteflow:keep phismap: {'$phi24.0': {('$16dup_top.6', State(pc_initial=0 nstack_initial=0))},
 '$phi62.0': {('$54dup_top.6', State(pc_initial=40 nstack_initial=0))}}
DEBUG:numba.core.byteflow:new_out: defaultdict(<class 'dict'>,
            {State(pc_initial=0 nstack_initial=0): {'$phi24.0': '$16dup_top.6'},
             State(pc_initial=40 nstack_initial=0): {'$phi62.0': '$54dup_top.6'}})
DEBUG:numba.core.byteflow:----------------------DONE Prune PHIs-----------------------
DEBUG:numba.core.byteflow:block_infos State(pc_initial=0 nstack_initial=0):
AdaptBlockInfo(insts=((0, {}), (2, {'res': '$x2.0'}), (4, {'res': '$const4.1'}), (6, {'index': '$const4.1', 'target': '$x2.0', 'res': '$6binary_subscr.2'}), (8, {'value': '$6binary_subscr.2'}), (10, {'res': '$threshold10.3'}), (12, {'value': '$threshold10.3', 'res': '$12unary_negative.4'}), (14, {'res': '$x014.5'}), (16, {'orig': ['$x014.5'], 'duped': ['$16dup_top.6']}), (20, {'lhs': '$12unary_negative.4', 'rhs': '$x014.5', 'res': '$20compare_op.7'}), (22, {'pred': '$20compare_op.7'})), outgoing_phis={'$phi24.0': '$16dup_top.6'}, blockstack=(), active_try_block=None, outgoing_edgepushed={24: ('$16dup_top.6',), 32: ('$16dup_top.6',)})
DEBUG:numba.core.byteflow:block_infos State(pc_initial=24 nstack_initial=1):
AdaptBlockInfo(insts=((24, {'res': '$threshold24.1'}), (26, {'lhs': '$phi24.0', 'rhs': '$threshold24.1', 'res': '$26compare_op.2'}), (28, {'pred': '$26compare_op.2'})), outgoing_phis={}, blockstack=(), active_try_block=None, outgoing_edgepushed={30: (), 40: ()})
DEBUG:numba.core.byteflow:block_infos State(pc_initial=30 nstack_initial=0):
AdaptBlockInfo(insts=((30, {}),), outgoing_phis={}, blockstack=(), active_try_block=None, outgoing_edgepushed={36: ()})
DEBUG:numba.core.byteflow:block_infos State(pc_initial=32 nstack_initial=1):
AdaptBlockInfo(insts=((34, {}),), outgoing_phis={}, blockstack=(), active_try_block=None, outgoing_edgepushed={40: ()})
DEBUG:numba.core.byteflow:block_infos State(pc_initial=36 nstack_initial=0):
AdaptBlockInfo(insts=((36, {'res': '$const36.0'}), (38, {'value': '$const36.0'})), outgoing_phis={}, blockstack=(), active_try_block=None, outgoing_edgepushed={40: ()})
DEBUG:numba.core.byteflow:block_infos State(pc_initial=40 nstack_initial=0):
AdaptBlockInfo(insts=((40, {'res': '$x40.0'}), (42, {'res': '$const42.1'}), (44, {'index': '$const42.1', 'target': '$x40.0', 'res': '$44binary_subscr.2'}), (46, {'value': '$44binary_subscr.2'}), (48, {'res': '$threshold48.3'}), (50, {'value': '$threshold48.3', 'res': '$50unary_negative.4'}), (52, {'res': '$x152.5'}), (54, {'orig': ['$x152.5'], 'duped': ['$54dup_top.6']}), (58, {'lhs': '$50unary_negative.4', 'rhs': '$x152.5', 'res': '$58compare_op.7'}), (60, {'pred': '$58compare_op.7'})), outgoing_phis={'$phi62.0': '$54dup_top.6'}, blockstack=(), active_try_block=None, outgoing_edgepushed={62: ('$54dup_top.6',), 70: ('$54dup_top.6',)})
DEBUG:numba.core.byteflow:block_infos State(pc_initial=62 nstack_initial=1):
AdaptBlockInfo(insts=((62, {'res': '$threshold62.1'}), (64, {'lhs': '$phi62.0', 'rhs': '$threshold62.1', 'res': '$64compare_op.2'}), (66, {'pred': '$64compare_op.2'})), outgoing_phis={}, blockstack=(), active_try_block=None, outgoing_edgepushed={68: (), 78: ()})
DEBUG:numba.core.byteflow:block_infos State(pc_initial=68 nstack_initial=0):
AdaptBlockInfo(insts=((68, {}),), outgoing_phis={}, blockstack=(), active_try_block=None, outgoing_edgepushed={74: ()})
DEBUG:numba.core.byteflow:block_infos State(pc_initial=70 nstack_initial=1):
AdaptBlockInfo(insts=((72, {}),), outgoing_phis={}, blockstack=(), active_try_block=None, outgoing_edgepushed={78: ()})
DEBUG:numba.core.byteflow:block_infos State(pc_initial=74 nstack_initial=0):
AdaptBlockInfo(insts=((74, {'res': '$const74.0'}), (76, {'value': '$const74.0'})), outgoing_phis={}, blockstack=(), active_try_block=None, outgoing_edgepushed={78: ()})
DEBUG:numba.core.byteflow:block_infos State(pc_initial=78 nstack_initial=0):
AdaptBlockInfo(insts=((78, {'res': '$zero_pos78.0'}), (80, {'pred': '$zero_pos78.0'})), outgoing_phis={}, blockstack=(), active_try_block=None, outgoing_edgepushed={82: (), 102: ()})
DEBUG:numba.core.byteflow:block_infos State(pc_initial=82 nstack_initial=0):
AdaptBlockInfo(insts=((82, {'res': '$82load_global.0'}), (84, {'item': '$82load_global.0', 'res': '$84load_method.1'}), (86, {'res': '$x086.2'}), (88, {'func': '$84load_method.1', 'args': ['$x086.2'], 'res': '$88call_method.3'}), (90, {'res': '$90load_global.4'}), (92, {'item': '$90load_global.4', 'res': '$92load_method.5'}), (94, {'res': '$x194.6'}), (96, {'func': '$92load_method.5', 'args': ['$x194.6'], 'res': '$96call_method.7'}), (98, {'lhs': '$88call_method.3', 'rhs': '$96call_method.7', 'res': '$98compare_op.8'}), (100, {'retval': '$98compare_op.8', 'castval': '$100return_value.9'})), outgoing_phis={}, blockstack=(), active_try_block=None, outgoing_edgepushed={})
DEBUG:numba.core.byteflow:block_infos State(pc_initial=102 nstack_initial=0):
AdaptBlockInfo(insts=((102, {'res': '$102load_global.0'}), (104, {'item': '$102load_global.0', 'res': '$104load_method.1'}), (106, {'res': '$x0106.2'}), (108, {'func': '$104load_method.1', 'args': ['$x0106.2'], 'res': '$108call_method.3'}), (110, {'res': '$110load_global.4'}), (112, {'item': '$110load_global.4', 'res': '$112load_method.5'}), (114, {'res': '$x1114.6'}), (116, {'func': '$112load_method.5', 'args': ['$x1114.6'], 'res': '$116call_method.7'}), (118, {'lhs': '$108call_method.3', 'rhs': '$116call_method.7', 'res': '$118compare_op.8'}), (120, {'retval': '$118compare_op.8', 'castval': '$120return_value.9'})), outgoing_phis={}, blockstack=(), active_try_block=None, outgoing_edgepushed={})
DEBUG:numba.core.interpreter:label 0:
    x = arg(0, name=x)                       ['x']
    threshold = arg(1, name=threshold)       ['threshold']
    zero_pos = arg(2, name=zero_pos)         ['zero_pos']
    $const4.1 = const(int, 0)                ['$const4.1']
    x0 = getitem(value=x, index=$const4.1, fn=<built-in function getitem>) ['$const4.1', 'x', 'x0']
    $12unary_negative.4 = unary(fn=<built-in function neg>, value=threshold) ['$12unary_negative.4', 'threshold']
    $20compare_op.7 = $12unary_negative.4 <= x0 ['$12unary_negative.4', '$20compare_op.7', 'x0']
    bool22 = global(bool: <class 'bool'>)    ['bool22']
    $22pred = call bool22($20compare_op.7, func=bool22, args=(Var($20compare_op.7, audio.py:1145),), kws=(), vararg=None, varkwarg=None, target=None) ['$20compare_op.7', '$22pred', 'bool22']
    $phi24.0 = x0                            ['$phi24.0', 'x0']
    branch $22pred, 24, 32                   ['$22pred']
label 24:
    $26compare_op.2 = $phi24.0 <= threshold  ['$26compare_op.2', '$phi24.0', 'threshold']
    bool28 = global(bool: <class 'bool'>)    ['bool28']
    $28pred = call bool28($26compare_op.2, func=bool28, args=(Var($26compare_op.2, audio.py:1145),), kws=(), vararg=None, varkwarg=None, target=None) ['$26compare_op.2', '$28pred', 'bool28']
    branch $28pred, 30, 40                   ['$28pred']
label 30:
    jump 36                                  []
label 32:
    jump 40                                  []
label 36:
    x0 = const(int, 0)                       ['x0']
    jump 40                                  []
label 40:
    $const42.1 = const(int, -1)              ['$const42.1']
    x1 = getitem(value=x, index=$const42.1, fn=<built-in function getitem>) ['$const42.1', 'x', 'x1']
    $50unary_negative.4 = unary(fn=<built-in function neg>, value=threshold) ['$50unary_negative.4', 'threshold']
    $58compare_op.7 = $50unary_negative.4 <= x1 ['$50unary_negative.4', '$58compare_op.7', 'x1']
    bool60 = global(bool: <class 'bool'>)    ['bool60']
    $60pred = call bool60($58compare_op.7, func=bool60, args=(Var($58compare_op.7, audio.py:1149),), kws=(), vararg=None, varkwarg=None, target=None) ['$58compare_op.7', '$60pred', 'bool60']
    $phi62.0 = x1                            ['$phi62.0', 'x1']
    branch $60pred, 62, 70                   ['$60pred']
label 62:
    $64compare_op.2 = $phi62.0 <= threshold  ['$64compare_op.2', '$phi62.0', 'threshold']
    bool66 = global(bool: <class 'bool'>)    ['bool66']
    $66pred = call bool66($64compare_op.2, func=bool66, args=(Var($64compare_op.2, audio.py:1149),), kws=(), vararg=None, varkwarg=None, target=None) ['$64compare_op.2', '$66pred', 'bool66']
    branch $66pred, 68, 78                   ['$66pred']
label 68:
    jump 74                                  []
label 70:
    jump 78                                  []
label 74:
    x1 = const(int, 0)                       ['x1']
    jump 78                                  []
label 78:
    bool80 = global(bool: <class 'bool'>)    ['bool80']
    $80pred = call bool80(zero_pos, func=bool80, args=(Var(zero_pos, audio.py:1141),), kws=(), vararg=None, varkwarg=None, target=None) ['$80pred', 'bool80', 'zero_pos']
    branch $80pred, 82, 102                  ['$80pred']
label 82:
    $82load_global.0 = global(np: <module 'numpy' from 'C:\\Users\\taskm\\anaconda3\\envs\\taskai\\lib\\site-packages\\numpy\\__init__.py'>) ['$82load_global.0']
    $84load_method.1 = getattr(value=$82load_global.0, attr=signbit) ['$82load_global.0', '$84load_method.1']
    $88call_method.3 = call $84load_method.1(x0, func=$84load_method.1, args=[Var(x0, audio.py:1144)], kws=(), vararg=None, varkwarg=None, target=None) ['$84load_method.1', '$88call_method.3', 'x0']
    $90load_global.4 = global(np: <module 'numpy' from 'C:\\Users\\taskm\\anaconda3\\envs\\taskai\\lib\\site-packages\\numpy\\__init__.py'>) ['$90load_global.4']
    $92load_method.5 = getattr(value=$90load_global.4, attr=signbit) ['$90load_global.4', '$92load_method.5']
    $96call_method.7 = call $92load_method.5(x1, func=$92load_method.5, args=[Var(x1, audio.py:1148)], kws=(), vararg=None, varkwarg=None, target=None) ['$92load_method.5', '$96call_method.7', 'x1']
    $98compare_op.8 = $88call_method.3 != $96call_method.7 ['$88call_method.3', '$96call_method.7', '$98compare_op.8']
    $100return_value.9 = cast(value=$98compare_op.8) ['$100return_value.9', '$98compare_op.8']
    return $100return_value.9                ['$100return_value.9']
label 102:
    $102load_global.0 = global(np: <module 'numpy' from 'C:\\Users\\taskm\\anaconda3\\envs\\taskai\\lib\\site-packages\\numpy\\__init__.py'>) ['$102load_global.0']
    $104load_method.1 = getattr(value=$102load_global.0, attr=sign) ['$102load_global.0', '$104load_method.1']
    $108call_method.3 = call $104load_method.1(x0, func=$104load_method.1, args=[Var(x0, audio.py:1144)], kws=(), vararg=None, varkwarg=None, target=None) ['$104load_method.1', '$108call_method.3', 'x0']
    $110load_global.4 = global(np: <module 'numpy' from 'C:\\Users\\taskm\\anaconda3\\envs\\taskai\\lib\\site-packages\\numpy\\__init__.py'>) ['$110load_global.4']
    $112load_method.5 = getattr(value=$110load_global.4, attr=sign) ['$110load_global.4', '$112load_method.5']
    $116call_method.7 = call $112load_method.5(x1, func=$112load_method.5, args=[Var(x1, audio.py:1148)], kws=(), vararg=None, varkwarg=None, target=None) ['$112load_method.5', '$116call_method.7', 'x1']
    $118compare_op.8 = $108call_method.3 != $116call_method.7 ['$108call_method.3', '$116call_method.7', '$118compare_op.8']
    $120return_value.9 = cast(value=$118compare_op.8) ['$118compare_op.8', '$120return_value.9']
    return $120return_value.9                ['$120return_value.9']

DEBUG:numba.core.byteflow:bytecode dump:
>          0	NOP(arg=None, lineno=1039)
           2	LOAD_FAST(arg=0, lineno=1042)
           4	LOAD_CONST(arg=1, lineno=1042)
           6	BINARY_SUBSCR(arg=None, lineno=1042)
           8	LOAD_FAST(arg=0, lineno=1042)
          10	LOAD_CONST(arg=2, lineno=1042)
          12	BINARY_SUBSCR(arg=None, lineno=1042)
          14	COMPARE_OP(arg=4, lineno=1042)
          16	LOAD_FAST(arg=0, lineno=1042)
          18	LOAD_CONST(arg=1, lineno=1042)
          20	BINARY_SUBSCR(arg=None, lineno=1042)
          22	LOAD_FAST(arg=0, lineno=1042)
          24	LOAD_CONST(arg=3, lineno=1042)
          26	BINARY_SUBSCR(arg=None, lineno=1042)
          28	COMPARE_OP(arg=5, lineno=1042)
          30	BINARY_AND(arg=None, lineno=1042)
          32	RETURN_VALUE(arg=None, lineno=1042)
DEBUG:numba.core.byteflow:pending: deque([State(pc_initial=0 nstack_initial=0)])
DEBUG:numba.core.byteflow:stack: []
DEBUG:numba.core.byteflow:state.pc_initial: State(pc_initial=0 nstack_initial=0)
DEBUG:numba.core.byteflow:dispatch pc=0, inst=NOP(arg=None, lineno=1039)
DEBUG:numba.core.byteflow:stack []
DEBUG:numba.core.byteflow:dispatch pc=2, inst=LOAD_FAST(arg=0, lineno=1042)
DEBUG:numba.core.byteflow:stack []
DEBUG:numba.core.byteflow:dispatch pc=4, inst=LOAD_CONST(arg=1, lineno=1042)
DEBUG:numba.core.byteflow:stack ['$x2.0']
DEBUG:numba.core.byteflow:dispatch pc=6, inst=BINARY_SUBSCR(arg=None, lineno=1042)
DEBUG:numba.core.byteflow:stack ['$x2.0', '$const4.1']
DEBUG:numba.core.byteflow:dispatch pc=8, inst=LOAD_FAST(arg=0, lineno=1042)
DEBUG:numba.core.byteflow:stack ['$6binary_subscr.2']
DEBUG:numba.core.byteflow:dispatch pc=10, inst=LOAD_CONST(arg=2, lineno=1042)
DEBUG:numba.core.byteflow:stack ['$6binary_subscr.2', '$x8.3']
DEBUG:numba.core.byteflow:dispatch pc=12, inst=BINARY_SUBSCR(arg=None, lineno=1042)
DEBUG:numba.core.byteflow:stack ['$6binary_subscr.2', '$x8.3', '$const10.4']
DEBUG:numba.core.byteflow:dispatch pc=14, inst=COMPARE_OP(arg=4, lineno=1042)
DEBUG:numba.core.byteflow:stack ['$6binary_subscr.2', '$12binary_subscr.5']
DEBUG:numba.core.byteflow:dispatch pc=16, inst=LOAD_FAST(arg=0, lineno=1042)
DEBUG:numba.core.byteflow:stack ['$14compare_op.6']
DEBUG:numba.core.byteflow:dispatch pc=18, inst=LOAD_CONST(arg=1, lineno=1042)
DEBUG:numba.core.byteflow:stack ['$14compare_op.6', '$x16.7']
DEBUG:numba.core.byteflow:dispatch pc=20, inst=BINARY_SUBSCR(arg=None, lineno=1042)
DEBUG:numba.core.byteflow:stack ['$14compare_op.6', '$x16.7', '$const18.8']
DEBUG:numba.core.byteflow:dispatch pc=22, inst=LOAD_FAST(arg=0, lineno=1042)
DEBUG:numba.core.byteflow:stack ['$14compare_op.6', '$20binary_subscr.9']
DEBUG:numba.core.byteflow:dispatch pc=24, inst=LOAD_CONST(arg=3, lineno=1042)
DEBUG:numba.core.byteflow:stack ['$14compare_op.6', '$20binary_subscr.9', '$x22.10']
DEBUG:numba.core.byteflow:dispatch pc=26, inst=BINARY_SUBSCR(arg=None, lineno=1042)
DEBUG:numba.core.byteflow:stack ['$14compare_op.6', '$20binary_subscr.9', '$x22.10', '$const24.11']
DEBUG:numba.core.byteflow:dispatch pc=28, inst=COMPARE_OP(arg=5, lineno=1042)
DEBUG:numba.core.byteflow:stack ['$14compare_op.6', '$20binary_subscr.9', '$26binary_subscr.12']
DEBUG:numba.core.byteflow:dispatch pc=30, inst=BINARY_AND(arg=None, lineno=1042)
DEBUG:numba.core.byteflow:stack ['$14compare_op.6', '$28compare_op.13']
DEBUG:numba.core.byteflow:dispatch pc=32, inst=RETURN_VALUE(arg=None, lineno=1042)
DEBUG:numba.core.byteflow:stack ['$30binary_and.14']
DEBUG:numba.core.byteflow:end state. edges=[]
DEBUG:numba.core.byteflow:-------------------------Prune PHIs-------------------------
DEBUG:numba.core.byteflow:Used_phis: defaultdict(<class 'set'>, {State(pc_initial=0 nstack_initial=0): set()})
DEBUG:numba.core.byteflow:defmap: {}
DEBUG:numba.core.byteflow:phismap: defaultdict(<class 'set'>, {})
DEBUG:numba.core.byteflow:changing phismap: defaultdict(<class 'set'>, {})
DEBUG:numba.core.byteflow:keep phismap: {}
DEBUG:numba.core.byteflow:new_out: defaultdict(<class 'dict'>, {})
DEBUG:numba.core.byteflow:----------------------DONE Prune PHIs-----------------------
DEBUG:numba.core.byteflow:block_infos State(pc_initial=0 nstack_initial=0):
AdaptBlockInfo(insts=((0, {}), (2, {'res': '$x2.0'}), (4, {'res': '$const4.1'}), (6, {'index': '$const4.1', 'target': '$x2.0', 'res': '$6binary_subscr.2'}), (8, {'res': '$x8.3'}), (10, {'res': '$const10.4'}), (12, {'index': '$const10.4', 'target': '$x8.3', 'res': '$12binary_subscr.5'}), (14, {'lhs': '$6binary_subscr.2', 'rhs': '$12binary_subscr.5', 'res': '$14compare_op.6'}), (16, {'res': '$x16.7'}), (18, {'res': '$const18.8'}), (20, {'index': '$const18.8', 'target': '$x16.7', 'res': '$20binary_subscr.9'}), (22, {'res': '$x22.10'}), (24, {'res': '$const24.11'}), (26, {'index': '$const24.11', 'target': '$x22.10', 'res': '$26binary_subscr.12'}), (28, {'lhs': '$20binary_subscr.9', 'rhs': '$26binary_subscr.12', 'res': '$28compare_op.13'}), (30, {'lhs': '$14compare_op.6', 'rhs': '$28compare_op.13', 'res': '$30binary_and.14'}), (32, {'retval': '$30binary_and.14', 'castval': '$32return_value.15'})), outgoing_phis={}, blockstack=(), active_try_block=None, outgoing_edgepushed={})
DEBUG:numba.core.interpreter:label 0:
    x = arg(0, name=x)                       ['x']
    $const4.1 = const(int, 0)                ['$const4.1']
    $6binary_subscr.2 = getitem(value=x, index=$const4.1, fn=<built-in function getitem>) ['$6binary_subscr.2', '$const4.1', 'x']
    $const10.4 = const(int, -1)              ['$const10.4']
    $12binary_subscr.5 = getitem(value=x, index=$const10.4, fn=<built-in function getitem>) ['$12binary_subscr.5', '$const10.4', 'x']
    $14compare_op.6 = $6binary_subscr.2 > $12binary_subscr.5 ['$12binary_subscr.5', '$14compare_op.6', '$6binary_subscr.2']
    $const18.8 = const(int, 0)               ['$const18.8']
    $20binary_subscr.9 = getitem(value=x, index=$const18.8, fn=<built-in function getitem>) ['$20binary_subscr.9', '$const18.8', 'x']
    $const24.11 = const(int, 1)              ['$const24.11']
    $26binary_subscr.12 = getitem(value=x, index=$const24.11, fn=<built-in function getitem>) ['$26binary_subscr.12', '$const24.11', 'x']
    $28compare_op.13 = $20binary_subscr.9 >= $26binary_subscr.12 ['$20binary_subscr.9', '$26binary_subscr.12', '$28compare_op.13']
    $30binary_and.14 = $14compare_op.6 & $28compare_op.13 ['$14compare_op.6', '$28compare_op.13', '$30binary_and.14']
    $32return_value.15 = cast(value=$30binary_and.14) ['$30binary_and.14', '$32return_value.15']
    return $32return_value.15                ['$32return_value.15']

DEBUG:numba.core.byteflow:bytecode dump:
>          0	NOP(arg=None, lineno=1045)
           2	LOAD_FAST(arg=0, lineno=1048)
           4	LOAD_CONST(arg=1, lineno=1048)
           6	BINARY_SUBSCR(arg=None, lineno=1048)
           8	LOAD_FAST(arg=0, lineno=1048)
          10	LOAD_CONST(arg=2, lineno=1048)
          12	BINARY_SUBSCR(arg=None, lineno=1048)
          14	COMPARE_OP(arg=0, lineno=1048)
          16	LOAD_FAST(arg=0, lineno=1048)
          18	LOAD_CONST(arg=1, lineno=1048)
          20	BINARY_SUBSCR(arg=None, lineno=1048)
          22	LOAD_FAST(arg=0, lineno=1048)
          24	LOAD_CONST(arg=3, lineno=1048)
          26	BINARY_SUBSCR(arg=None, lineno=1048)
          28	COMPARE_OP(arg=1, lineno=1048)
          30	BINARY_AND(arg=None, lineno=1048)
          32	RETURN_VALUE(arg=None, lineno=1048)
DEBUG:numba.core.byteflow:pending: deque([State(pc_initial=0 nstack_initial=0)])
DEBUG:numba.core.byteflow:stack: []
DEBUG:numba.core.byteflow:state.pc_initial: State(pc_initial=0 nstack_initial=0)
DEBUG:numba.core.byteflow:dispatch pc=0, inst=NOP(arg=None, lineno=1045)
DEBUG:numba.core.byteflow:stack []
DEBUG:numba.core.byteflow:dispatch pc=2, inst=LOAD_FAST(arg=0, lineno=1048)
DEBUG:numba.core.byteflow:stack []
DEBUG:numba.core.byteflow:dispatch pc=4, inst=LOAD_CONST(arg=1, lineno=1048)
DEBUG:numba.core.byteflow:stack ['$x2.0']
DEBUG:numba.core.byteflow:dispatch pc=6, inst=BINARY_SUBSCR(arg=None, lineno=1048)
DEBUG:numba.core.byteflow:stack ['$x2.0', '$const4.1']
DEBUG:numba.core.byteflow:dispatch pc=8, inst=LOAD_FAST(arg=0, lineno=1048)
DEBUG:numba.core.byteflow:stack ['$6binary_subscr.2']
DEBUG:numba.core.byteflow:dispatch pc=10, inst=LOAD_CONST(arg=2, lineno=1048)
DEBUG:numba.core.byteflow:stack ['$6binary_subscr.2', '$x8.3']
DEBUG:numba.core.byteflow:dispatch pc=12, inst=BINARY_SUBSCR(arg=None, lineno=1048)
DEBUG:numba.core.byteflow:stack ['$6binary_subscr.2', '$x8.3', '$const10.4']
DEBUG:numba.core.byteflow:dispatch pc=14, inst=COMPARE_OP(arg=0, lineno=1048)
DEBUG:numba.core.byteflow:stack ['$6binary_subscr.2', '$12binary_subscr.5']
DEBUG:numba.core.byteflow:dispatch pc=16, inst=LOAD_FAST(arg=0, lineno=1048)
DEBUG:numba.core.byteflow:stack ['$14compare_op.6']
DEBUG:numba.core.byteflow:dispatch pc=18, inst=LOAD_CONST(arg=1, lineno=1048)
DEBUG:numba.core.byteflow:stack ['$14compare_op.6', '$x16.7']
DEBUG:numba.core.byteflow:dispatch pc=20, inst=BINARY_SUBSCR(arg=None, lineno=1048)
DEBUG:numba.core.byteflow:stack ['$14compare_op.6', '$x16.7', '$const18.8']
DEBUG:numba.core.byteflow:dispatch pc=22, inst=LOAD_FAST(arg=0, lineno=1048)
DEBUG:numba.core.byteflow:stack ['$14compare_op.6', '$20binary_subscr.9']
DEBUG:numba.core.byteflow:dispatch pc=24, inst=LOAD_CONST(arg=3, lineno=1048)
DEBUG:numba.core.byteflow:stack ['$14compare_op.6', '$20binary_subscr.9', '$x22.10']
DEBUG:numba.core.byteflow:dispatch pc=26, inst=BINARY_SUBSCR(arg=None, lineno=1048)
DEBUG:numba.core.byteflow:stack ['$14compare_op.6', '$20binary_subscr.9', '$x22.10', '$const24.11']
DEBUG:numba.core.byteflow:dispatch pc=28, inst=COMPARE_OP(arg=1, lineno=1048)
DEBUG:numba.core.byteflow:stack ['$14compare_op.6', '$20binary_subscr.9', '$26binary_subscr.12']
DEBUG:numba.core.byteflow:dispatch pc=30, inst=BINARY_AND(arg=None, lineno=1048)
DEBUG:numba.core.byteflow:stack ['$14compare_op.6', '$28compare_op.13']
DEBUG:numba.core.byteflow:dispatch pc=32, inst=RETURN_VALUE(arg=None, lineno=1048)
DEBUG:numba.core.byteflow:stack ['$30binary_and.14']
DEBUG:numba.core.byteflow:end state. edges=[]
DEBUG:numba.core.byteflow:-------------------------Prune PHIs-------------------------
DEBUG:numba.core.byteflow:Used_phis: defaultdict(<class 'set'>, {State(pc_initial=0 nstack_initial=0): set()})
DEBUG:numba.core.byteflow:defmap: {}
DEBUG:numba.core.byteflow:phismap: defaultdict(<class 'set'>, {})
DEBUG:numba.core.byteflow:changing phismap: defaultdict(<class 'set'>, {})
DEBUG:numba.core.byteflow:keep phismap: {}
DEBUG:numba.core.byteflow:new_out: defaultdict(<class 'dict'>, {})
DEBUG:numba.core.byteflow:----------------------DONE Prune PHIs-----------------------
DEBUG:numba.core.byteflow:block_infos State(pc_initial=0 nstack_initial=0):
AdaptBlockInfo(insts=((0, {}), (2, {'res': '$x2.0'}), (4, {'res': '$const4.1'}), (6, {'index': '$const4.1', 'target': '$x2.0', 'res': '$6binary_subscr.2'}), (8, {'res': '$x8.3'}), (10, {'res': '$const10.4'}), (12, {'index': '$const10.4', 'target': '$x8.3', 'res': '$12binary_subscr.5'}), (14, {'lhs': '$6binary_subscr.2', 'rhs': '$12binary_subscr.5', 'res': '$14compare_op.6'}), (16, {'res': '$x16.7'}), (18, {'res': '$const18.8'}), (20, {'index': '$const18.8', 'target': '$x16.7', 'res': '$20binary_subscr.9'}), (22, {'res': '$x22.10'}), (24, {'res': '$const24.11'}), (26, {'index': '$const24.11', 'target': '$x22.10', 'res': '$26binary_subscr.12'}), (28, {'lhs': '$20binary_subscr.9', 'rhs': '$26binary_subscr.12', 'res': '$28compare_op.13'}), (30, {'lhs': '$14compare_op.6', 'rhs': '$28compare_op.13', 'res': '$30binary_and.14'}), (32, {'retval': '$30binary_and.14', 'castval': '$32return_value.15'})), outgoing_phis={}, blockstack=(), active_try_block=None, outgoing_edgepushed={})
DEBUG:numba.core.interpreter:label 0:
    x = arg(0, name=x)                       ['x']
    $const4.1 = const(int, 0)                ['$const4.1']
    $6binary_subscr.2 = getitem(value=x, index=$const4.1, fn=<built-in function getitem>) ['$6binary_subscr.2', '$const4.1', 'x']
    $const10.4 = const(int, -1)              ['$const10.4']
    $12binary_subscr.5 = getitem(value=x, index=$const10.4, fn=<built-in function getitem>) ['$12binary_subscr.5', '$const10.4', 'x']
    $14compare_op.6 = $6binary_subscr.2 < $12binary_subscr.5 ['$12binary_subscr.5', '$14compare_op.6', '$6binary_subscr.2']
    $const18.8 = const(int, 0)               ['$const18.8']
    $20binary_subscr.9 = getitem(value=x, index=$const18.8, fn=<built-in function getitem>) ['$20binary_subscr.9', '$const18.8', 'x']
    $const24.11 = const(int, 1)              ['$const24.11']
    $26binary_subscr.12 = getitem(value=x, index=$const24.11, fn=<built-in function getitem>) ['$26binary_subscr.12', '$const24.11', 'x']
    $28compare_op.13 = $20binary_subscr.9 <= $26binary_subscr.12 ['$20binary_subscr.9', '$26binary_subscr.12', '$28compare_op.13']
    $30binary_and.14 = $14compare_op.6 & $28compare_op.13 ['$14compare_op.6', '$28compare_op.13', '$30binary_and.14']
    $32return_value.15 = cast(value=$30binary_and.14) ['$30binary_and.14', '$32return_value.15']
    return $32return_value.15                ['$32return_value.15']

ERROR:root:Error processing chapters: not enough values to unpack (expected 2, got 1)
Traceback (most recent call last):
  File "D:\github\chappymedium\chappie_utils.py", line 19, in time_to_seconds
    h, m, s = time_str.split(':')
ValueError: not enough values to unpack (expected 3, got 2)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\github\chappymedium\chappie.py", line 240, in process_chapters
    result = self.chappie_processor.process_srt(srt_content)
  File "D:\github\chappymedium\chappie_processor.py", line 14, in process_srt
    entries = parse_srt(srt_content)
  File "D:\github\chappymedium\chappie_utils.py", line 11, in parse_srt
    'start': time_to_seconds(time_range[0]),
  File "D:\github\chappymedium\chappie_utils.py", line 25, in time_to_seconds
    s, ms = s.split(',')
ValueError: not enough values to unpack (expected 2, got 1)
ERROR:root:Error processing chapters: not enough values to unpack (expected 2, got 1)
Traceback (most recent call last):
  File "D:\github\chappymedium\chappie_utils.py", line 19, in time_to_seconds
    h, m, s = time_str.split(':')
ValueError: not enough values to unpack (expected 3, got 2)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\github\chappymedium\chappie.py", line 240, in process_chapters
    result = self.chappie_processor.process_srt(srt_content)
  File "D:\github\chappymedium\chappie_processor.py", line 14, in process_srt
    entries = parse_srt(srt_content)
  File "D:\github\chappymedium\chappie_utils.py", line 11, in parse_srt
    'start': time_to_seconds(time_range[0]),
  File "D:\github\chappymedium\chappie_utils.py", line 25, in time_to_seconds
    s, ms = s.split(',')
ValueError: not enough values to unpack (expected 2, got 1)
ERROR:root:Error processing chapters: not enough values to unpack (expected 2, got 1)
Traceback (most recent call last):
  File "D:\github\chappymedium\chappie_utils.py", line 19, in time_to_seconds
    h, m, s = time_str.split(':')
ValueError: not enough values to unpack (expected 3, got 2)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\github\chappymedium\chappie.py", line 240, in process_chapters
    result = self.chappie_processor.process_srt(srt_content)
  File "D:\github\chappymedium\chappie_processor.py", line 14, in process_srt
    entries = parse_srt(srt_content)
  File "D:\github\chappymedium\chappie_utils.py", line 11, in parse_srt
    'start': time_to_seconds(time_range[0]),
  File "D:\github\chappymedium\chappie_utils.py", line 25, in time_to_seconds
    s, ms = s.split(',')
ValueError: not enough values to unpack (expected 2, got 1)
DEBUG:numba.core.byteflow:bytecode dump:
>          0	NOP(arg=None, lineno=1141)
           2	LOAD_FAST(arg=0, lineno=1144)
           4	LOAD_CONST(arg=1, lineno=1144)
           6	BINARY_SUBSCR(arg=None, lineno=1144)
           8	STORE_FAST(arg=3, lineno=1144)
          10	LOAD_FAST(arg=1, lineno=1145)
          12	UNARY_NEGATIVE(arg=None, lineno=1145)
          14	LOAD_FAST(arg=3, lineno=1145)
          16	DUP_TOP(arg=None, lineno=1145)
          18	ROT_THREE(arg=None, lineno=1145)
          20	COMPARE_OP(arg=1, lineno=1145)
          22	POP_JUMP_IF_FALSE(arg=32, lineno=1145)
          24	LOAD_FAST(arg=1, lineno=1145)
          26	COMPARE_OP(arg=1, lineno=1145)
          28	POP_JUMP_IF_FALSE(arg=40, lineno=1145)
          30	JUMP_FORWARD(arg=4, lineno=1145)
>         32	POP_TOP(arg=None, lineno=1145)
          34	JUMP_FORWARD(arg=4, lineno=1145)
>         36	LOAD_CONST(arg=1, lineno=1146)
          38	STORE_FAST(arg=3, lineno=1146)
>         40	LOAD_FAST(arg=0, lineno=1148)
          42	LOAD_CONST(arg=2, lineno=1148)
          44	BINARY_SUBSCR(arg=None, lineno=1148)
          46	STORE_FAST(arg=4, lineno=1148)
          48	LOAD_FAST(arg=1, lineno=1149)
          50	UNARY_NEGATIVE(arg=None, lineno=1149)
          52	LOAD_FAST(arg=4, lineno=1149)
          54	DUP_TOP(arg=None, lineno=1149)
          56	ROT_THREE(arg=None, lineno=1149)
          58	COMPARE_OP(arg=1, lineno=1149)
          60	POP_JUMP_IF_FALSE(arg=70, lineno=1149)
          62	LOAD_FAST(arg=1, lineno=1149)
          64	COMPARE_OP(arg=1, lineno=1149)
          66	POP_JUMP_IF_FALSE(arg=78, lineno=1149)
          68	JUMP_FORWARD(arg=4, lineno=1149)
>         70	POP_TOP(arg=None, lineno=1149)
          72	JUMP_FORWARD(arg=4, lineno=1149)
>         74	LOAD_CONST(arg=1, lineno=1150)
          76	STORE_FAST(arg=4, lineno=1150)
>         78	LOAD_FAST(arg=2, lineno=1152)
          80	POP_JUMP_IF_FALSE(arg=102, lineno=1152)
          82	LOAD_GLOBAL(arg=0, lineno=1153)
          84	LOAD_METHOD(arg=1, lineno=1153)
          86	LOAD_FAST(arg=3, lineno=1153)
          88	CALL_METHOD(arg=1, lineno=1153)
          90	LOAD_GLOBAL(arg=0, lineno=1153)
          92	LOAD_METHOD(arg=1, lineno=1153)
          94	LOAD_FAST(arg=4, lineno=1153)
          96	CALL_METHOD(arg=1, lineno=1153)
          98	COMPARE_OP(arg=3, lineno=1153)
         100	RETURN_VALUE(arg=None, lineno=1153)
>        102	LOAD_GLOBAL(arg=0, lineno=1155)
         104	LOAD_METHOD(arg=2, lineno=1155)
         106	LOAD_FAST(arg=3, lineno=1155)
         108	CALL_METHOD(arg=1, lineno=1155)
         110	LOAD_GLOBAL(arg=0, lineno=1155)
         112	LOAD_METHOD(arg=2, lineno=1155)
         114	LOAD_FAST(arg=4, lineno=1155)
         116	CALL_METHOD(arg=1, lineno=1155)
         118	COMPARE_OP(arg=3, lineno=1155)
         120	RETURN_VALUE(arg=None, lineno=1155)
         122	LOAD_CONST(arg=3, lineno=1155)
         124	RETURN_VALUE(arg=None, lineno=1155)
DEBUG:numba.core.byteflow:pending: deque([State(pc_initial=0 nstack_initial=0)])
DEBUG:numba.core.byteflow:stack: []
DEBUG:numba.core.byteflow:state.pc_initial: State(pc_initial=0 nstack_initial=0)
DEBUG:numba.core.byteflow:dispatch pc=0, inst=NOP(arg=None, lineno=1141)
DEBUG:numba.core.byteflow:stack []
DEBUG:numba.core.byteflow:dispatch pc=2, inst=LOAD_FAST(arg=0, lineno=1144)
DEBUG:numba.core.byteflow:stack []
DEBUG:numba.core.byteflow:dispatch pc=4, inst=LOAD_CONST(arg=1, lineno=1144)
DEBUG:numba.core.byteflow:stack ['$x2.0']
DEBUG:numba.core.byteflow:dispatch pc=6, inst=BINARY_SUBSCR(arg=None, lineno=1144)
DEBUG:numba.core.byteflow:stack ['$x2.0', '$const4.1']
DEBUG:numba.core.byteflow:dispatch pc=8, inst=STORE_FAST(arg=3, lineno=1144)
DEBUG:numba.core.byteflow:stack ['$6binary_subscr.2']
DEBUG:numba.core.byteflow:dispatch pc=10, inst=LOAD_FAST(arg=1, lineno=1145)
DEBUG:numba.core.byteflow:stack []
DEBUG:numba.core.byteflow:dispatch pc=12, inst=UNARY_NEGATIVE(arg=None, lineno=1145)
DEBUG:numba.core.byteflow:stack ['$threshold10.3']
DEBUG:numba.core.byteflow:dispatch pc=14, inst=LOAD_FAST(arg=3, lineno=1145)
DEBUG:numba.core.byteflow:stack ['$12unary_negative.4']
DEBUG:numba.core.byteflow:dispatch pc=16, inst=DUP_TOP(arg=None, lineno=1145)
DEBUG:numba.core.byteflow:stack ['$12unary_negative.4', '$x014.5']
DEBUG:numba.core.byteflow:dispatch pc=18, inst=ROT_THREE(arg=None, lineno=1145)
DEBUG:numba.core.byteflow:stack ['$12unary_negative.4', '$x014.5', '$16dup_top.6']
DEBUG:numba.core.byteflow:dispatch pc=20, inst=COMPARE_OP(arg=1, lineno=1145)
DEBUG:numba.core.byteflow:stack ['$16dup_top.6', '$12unary_negative.4', '$x014.5']
DEBUG:numba.core.byteflow:dispatch pc=22, inst=POP_JUMP_IF_FALSE(arg=32, lineno=1145)
DEBUG:numba.core.byteflow:stack ['$16dup_top.6', '$20compare_op.7']
DEBUG:numba.core.byteflow:end state. edges=[Edge(pc=24, stack=('$16dup_top.6',), blockstack=(), npush=0), Edge(pc=32, stack=('$16dup_top.6',), blockstack=(), npush=0)]
DEBUG:numba.core.byteflow:pending: deque([State(pc_initial=24 nstack_initial=1), State(pc_initial=32 nstack_initial=1)])
DEBUG:numba.core.byteflow:stack: ['$phi24.0']
DEBUG:numba.core.byteflow:state.pc_initial: State(pc_initial=24 nstack_initial=1)
DEBUG:numba.core.byteflow:dispatch pc=24, inst=LOAD_FAST(arg=1, lineno=1145)
DEBUG:numba.core.byteflow:stack ['$phi24.0']
DEBUG:numba.core.byteflow:dispatch pc=26, inst=COMPARE_OP(arg=1, lineno=1145)
DEBUG:numba.core.byteflow:stack ['$phi24.0', '$threshold24.1']
DEBUG:numba.core.byteflow:dispatch pc=28, inst=POP_JUMP_IF_FALSE(arg=40, lineno=1145)
DEBUG:numba.core.byteflow:stack ['$26compare_op.2']
DEBUG:numba.core.byteflow:end state. edges=[Edge(pc=30, stack=(), blockstack=(), npush=0), Edge(pc=40, stack=(), blockstack=(), npush=0)]
DEBUG:numba.core.byteflow:pending: deque([State(pc_initial=32 nstack_initial=1), State(pc_initial=30 nstack_initial=0), State(pc_initial=40 nstack_initial=0)])
DEBUG:numba.core.byteflow:stack: ['$phi32.0']
DEBUG:numba.core.byteflow:state.pc_initial: State(pc_initial=32 nstack_initial=1)
DEBUG:numba.core.byteflow:dispatch pc=32, inst=POP_TOP(arg=None, lineno=1145)
DEBUG:numba.core.byteflow:stack ['$phi32.0']
DEBUG:numba.core.byteflow:dispatch pc=34, inst=JUMP_FORWARD(arg=4, lineno=1145)
DEBUG:numba.core.byteflow:stack []
DEBUG:numba.core.byteflow:end state. edges=[Edge(pc=40, stack=(), blockstack=(), npush=0)]
DEBUG:numba.core.byteflow:pending: deque([State(pc_initial=30 nstack_initial=0), State(pc_initial=40 nstack_initial=0), State(pc_initial=40 nstack_initial=0)])
DEBUG:numba.core.byteflow:stack: []
DEBUG:numba.core.byteflow:state.pc_initial: State(pc_initial=30 nstack_initial=0)
DEBUG:numba.core.byteflow:dispatch pc=30, inst=JUMP_FORWARD(arg=4, lineno=1145)
DEBUG:numba.core.byteflow:stack []
DEBUG:numba.core.byteflow:end state. edges=[Edge(pc=36, stack=(), blockstack=(), npush=0)]
DEBUG:numba.core.byteflow:pending: deque([State(pc_initial=40 nstack_initial=0), State(pc_initial=40 nstack_initial=0), State(pc_initial=36 nstack_initial=0)])
DEBUG:numba.core.byteflow:stack: []
DEBUG:numba.core.byteflow:state.pc_initial: State(pc_initial=40 nstack_initial=0)
DEBUG:numba.core.byteflow:dispatch pc=40, inst=LOAD_FAST(arg=0, lineno=1148)
DEBUG:numba.core.byteflow:stack []
DEBUG:numba.core.byteflow:dispatch pc=42, inst=LOAD_CONST(arg=2, lineno=1148)
DEBUG:numba.core.byteflow:stack ['$x40.0']
DEBUG:numba.core.byteflow:dispatch pc=44, inst=BINARY_SUBSCR(arg=None, lineno=1148)
DEBUG:numba.core.byteflow:stack ['$x40.0', '$const42.1']
DEBUG:numba.core.byteflow:dispatch pc=46, inst=STORE_FAST(arg=4, lineno=1148)
DEBUG:numba.core.byteflow:stack ['$44binary_subscr.2']
DEBUG:numba.core.byteflow:dispatch pc=48, inst=LOAD_FAST(arg=1, lineno=1149)
DEBUG:numba.core.byteflow:stack []
DEBUG:numba.core.byteflow:dispatch pc=50, inst=UNARY_NEGATIVE(arg=None, lineno=1149)
DEBUG:numba.core.byteflow:stack ['$threshold48.3']
DEBUG:numba.core.byteflow:dispatch pc=52, inst=LOAD_FAST(arg=4, lineno=1149)
DEBUG:numba.core.byteflow:stack ['$50unary_negative.4']
DEBUG:numba.core.byteflow:dispatch pc=54, inst=DUP_TOP(arg=None, lineno=1149)
DEBUG:numba.core.byteflow:stack ['$50unary_negative.4', '$x152.5']
DEBUG:numba.core.byteflow:dispatch pc=56, inst=ROT_THREE(arg=None, lineno=1149)
DEBUG:numba.core.byteflow:stack ['$50unary_negative.4', '$x152.5', '$54dup_top.6']
DEBUG:numba.core.byteflow:dispatch pc=58, inst=COMPARE_OP(arg=1, lineno=1149)
DEBUG:numba.core.byteflow:stack ['$54dup_top.6', '$50unary_negative.4', '$x152.5']
DEBUG:numba.core.byteflow:dispatch pc=60, inst=POP_JUMP_IF_FALSE(arg=70, lineno=1149)
DEBUG:numba.core.byteflow:stack ['$54dup_top.6', '$58compare_op.7']
DEBUG:numba.core.byteflow:end state. edges=[Edge(pc=62, stack=('$54dup_top.6',), blockstack=(), npush=0), Edge(pc=70, stack=('$54dup_top.6',), blockstack=(), npush=0)]
DEBUG:numba.core.byteflow:pending: deque([State(pc_initial=40 nstack_initial=0), State(pc_initial=36 nstack_initial=0), State(pc_initial=62 nstack_initial=1), State(pc_initial=70 nstack_initial=1)])
DEBUG:numba.core.byteflow:pending: deque([State(pc_initial=36 nstack_initial=0), State(pc_initial=62 nstack_initial=1), State(pc_initial=70 nstack_initial=1)])
DEBUG:numba.core.byteflow:stack: []
DEBUG:numba.core.byteflow:state.pc_initial: State(pc_initial=36 nstack_initial=0)
DEBUG:numba.core.byteflow:dispatch pc=36, inst=LOAD_CONST(arg=1, lineno=1146)
DEBUG:numba.core.byteflow:stack []
DEBUG:numba.core.byteflow:dispatch pc=38, inst=STORE_FAST(arg=3, lineno=1146)
DEBUG:numba.core.byteflow:stack ['$const36.0']
DEBUG:numba.core.byteflow:end state. edges=[Edge(pc=40, stack=(), blockstack=(), npush=0)]
DEBUG:numba.core.byteflow:pending: deque([State(pc_initial=62 nstack_initial=1), State(pc_initial=70 nstack_initial=1), State(pc_initial=40 nstack_initial=0)])
DEBUG:numba.core.byteflow:stack: ['$phi62.0']
DEBUG:numba.core.byteflow:state.pc_initial: State(pc_initial=62 nstack_initial=1)
DEBUG:numba.core.byteflow:dispatch pc=62, inst=LOAD_FAST(arg=1, lineno=1149)
DEBUG:numba.core.byteflow:stack ['$phi62.0']
DEBUG:numba.core.byteflow:dispatch pc=64, inst=COMPARE_OP(arg=1, lineno=1149)
DEBUG:numba.core.byteflow:stack ['$phi62.0', '$threshold62.1']
DEBUG:numba.core.byteflow:dispatch pc=66, inst=POP_JUMP_IF_FALSE(arg=78, lineno=1149)
DEBUG:numba.core.byteflow:stack ['$64compare_op.2']
DEBUG:numba.core.byteflow:end state. edges=[Edge(pc=68, stack=(), blockstack=(), npush=0), Edge(pc=78, stack=(), blockstack=(), npush=0)]
DEBUG:numba.core.byteflow:pending: deque([State(pc_initial=70 nstack_initial=1), State(pc_initial=40 nstack_initial=0), State(pc_initial=68 nstack_initial=0), State(pc_initial=78 nstack_initial=0)])
DEBUG:numba.core.byteflow:stack: ['$phi70.0']
DEBUG:numba.core.byteflow:state.pc_initial: State(pc_initial=70 nstack_initial=1)
DEBUG:numba.core.byteflow:dispatch pc=70, inst=POP_TOP(arg=None, lineno=1149)
DEBUG:numba.core.byteflow:stack ['$phi70.0']
DEBUG:numba.core.byteflow:dispatch pc=72, inst=JUMP_FORWARD(arg=4, lineno=1149)
DEBUG:numba.core.byteflow:stack []
DEBUG:numba.core.byteflow:end state. edges=[Edge(pc=78, stack=(), blockstack=(), npush=0)]
DEBUG:numba.core.byteflow:pending: deque([State(pc_initial=40 nstack_initial=0), State(pc_initial=68 nstack_initial=0), State(pc_initial=78 nstack_initial=0), State(pc_initial=78 nstack_initial=0)])
DEBUG:numba.core.byteflow:pending: deque([State(pc_initial=68 nstack_initial=0), State(pc_initial=78 nstack_initial=0), State(pc_initial=78 nstack_initial=0)])
DEBUG:numba.core.byteflow:stack: []
DEBUG:numba.core.byteflow:state.pc_initial: State(pc_initial=68 nstack_initial=0)
DEBUG:numba.core.byteflow:dispatch pc=68, inst=JUMP_FORWARD(arg=4, lineno=1149)
DEBUG:numba.core.byteflow:stack []
DEBUG:numba.core.byteflow:end state. edges=[Edge(pc=74, stack=(), blockstack=(), npush=0)]
DEBUG:numba.core.byteflow:pending: deque([State(pc_initial=78 nstack_initial=0), State(pc_initial=78 nstack_initial=0), State(pc_initial=74 nstack_initial=0)])
DEBUG:numba.core.byteflow:stack: []
DEBUG:numba.core.byteflow:state.pc_initial: State(pc_initial=78 nstack_initial=0)
DEBUG:numba.core.byteflow:dispatch pc=78, inst=LOAD_FAST(arg=2, lineno=1152)
DEBUG:numba.core.byteflow:stack []
DEBUG:numba.core.byteflow:dispatch pc=80, inst=POP_JUMP_IF_FALSE(arg=102, lineno=1152)
DEBUG:numba.core.byteflow:stack ['$zero_pos78.0']
DEBUG:numba.core.byteflow:end state. edges=[Edge(pc=82, stack=(), blockstack=(), npush=0), Edge(pc=102, stack=(), blockstack=(), npush=0)]
DEBUG:numba.core.byteflow:pending: deque([State(pc_initial=78 nstack_initial=0), State(pc_initial=74 nstack_initial=0), State(pc_initial=82 nstack_initial=0), State(pc_initial=102 nstack_initial=0)])
DEBUG:numba.core.byteflow:pending: deque([State(pc_initial=74 nstack_initial=0), State(pc_initial=82 nstack_initial=0), State(pc_initial=102 nstack_initial=0)])
DEBUG:numba.core.byteflow:stack: []
DEBUG:numba.core.byteflow:state.pc_initial: State(pc_initial=74 nstack_initial=0)
DEBUG:numba.core.byteflow:dispatch pc=74, inst=LOAD_CONST(arg=1, lineno=1150)
DEBUG:numba.core.byteflow:stack []
DEBUG:numba.core.byteflow:dispatch pc=76, inst=STORE_FAST(arg=4, lineno=1150)
DEBUG:numba.core.byteflow:stack ['$const74.0']
DEBUG:numba.core.byteflow:end state. edges=[Edge(pc=78, stack=(), blockstack=(), npush=0)]
DEBUG:numba.core.byteflow:pending: deque([State(pc_initial=82 nstack_initial=0), State(pc_initial=102 nstack_initial=0), State(pc_initial=78 nstack_initial=0)])
DEBUG:numba.core.byteflow:stack: []
DEBUG:numba.core.byteflow:state.pc_initial: State(pc_initial=82 nstack_initial=0)
DEBUG:numba.core.byteflow:dispatch pc=82, inst=LOAD_GLOBAL(arg=0, lineno=1153)
DEBUG:numba.core.byteflow:stack []
DEBUG:numba.core.byteflow:dispatch pc=84, inst=LOAD_METHOD(arg=1, lineno=1153)
DEBUG:numba.core.byteflow:stack ['$82load_global.0']
DEBUG:numba.core.byteflow:dispatch pc=86, inst=LOAD_FAST(arg=3, lineno=1153)
DEBUG:numba.core.byteflow:stack ['$84load_method.1']
DEBUG:numba.core.byteflow:dispatch pc=88, inst=CALL_METHOD(arg=1, lineno=1153)
DEBUG:numba.core.byteflow:stack ['$84load_method.1', '$x086.2']
DEBUG:numba.core.byteflow:dispatch pc=90, inst=LOAD_GLOBAL(arg=0, lineno=1153)
DEBUG:numba.core.byteflow:stack ['$88call_method.3']
DEBUG:numba.core.byteflow:dispatch pc=92, inst=LOAD_METHOD(arg=1, lineno=1153)
DEBUG:numba.core.byteflow:stack ['$88call_method.3', '$90load_global.4']
DEBUG:numba.core.byteflow:dispatch pc=94, inst=LOAD_FAST(arg=4, lineno=1153)
DEBUG:numba.core.byteflow:stack ['$88call_method.3', '$92load_method.5']
DEBUG:numba.core.byteflow:dispatch pc=96, inst=CALL_METHOD(arg=1, lineno=1153)
DEBUG:numba.core.byteflow:stack ['$88call_method.3', '$92load_method.5', '$x194.6']
DEBUG:numba.core.byteflow:dispatch pc=98, inst=COMPARE_OP(arg=3, lineno=1153)
DEBUG:numba.core.byteflow:stack ['$88call_method.3', '$96call_method.7']
DEBUG:numba.core.byteflow:dispatch pc=100, inst=RETURN_VALUE(arg=None, lineno=1153)
DEBUG:numba.core.byteflow:stack ['$98compare_op.8']
DEBUG:numba.core.byteflow:end state. edges=[]
DEBUG:numba.core.byteflow:pending: deque([State(pc_initial=102 nstack_initial=0), State(pc_initial=78 nstack_initial=0)])
DEBUG:numba.core.byteflow:stack: []
DEBUG:numba.core.byteflow:state.pc_initial: State(pc_initial=102 nstack_initial=0)
DEBUG:numba.core.byteflow:dispatch pc=102, inst=LOAD_GLOBAL(arg=0, lineno=1155)
DEBUG:numba.core.byteflow:stack []
DEBUG:numba.core.byteflow:dispatch pc=104, inst=LOAD_METHOD(arg=2, lineno=1155)
DEBUG:numba.core.byteflow:stack ['$102load_global.0']
DEBUG:numba.core.byteflow:dispatch pc=106, inst=LOAD_FAST(arg=3, lineno=1155)
DEBUG:numba.core.byteflow:stack ['$104load_method.1']
DEBUG:numba.core.byteflow:dispatch pc=108, inst=CALL_METHOD(arg=1, lineno=1155)
DEBUG:numba.core.byteflow:stack ['$104load_method.1', '$x0106.2']
DEBUG:numba.core.byteflow:dispatch pc=110, inst=LOAD_GLOBAL(arg=0, lineno=1155)
DEBUG:numba.core.byteflow:stack ['$108call_method.3']
DEBUG:numba.core.byteflow:dispatch pc=112, inst=LOAD_METHOD(arg=2, lineno=1155)
DEBUG:numba.core.byteflow:stack ['$108call_method.3', '$110load_global.4']
DEBUG:numba.core.byteflow:dispatch pc=114, inst=LOAD_FAST(arg=4, lineno=1155)
DEBUG:numba.core.byteflow:stack ['$108call_method.3', '$112load_method.5']
DEBUG:numba.core.byteflow:dispatch pc=116, inst=CALL_METHOD(arg=1, lineno=1155)
DEBUG:numba.core.byteflow:stack ['$108call_method.3', '$112load_method.5', '$x1114.6']
DEBUG:numba.core.byteflow:dispatch pc=118, inst=COMPARE_OP(arg=3, lineno=1155)
DEBUG:numba.core.byteflow:stack ['$108call_method.3', '$116call_method.7']
DEBUG:numba.core.byteflow:dispatch pc=120, inst=RETURN_VALUE(arg=None, lineno=1155)
DEBUG:numba.core.byteflow:stack ['$118compare_op.8']
DEBUG:numba.core.byteflow:end state. edges=[]
DEBUG:numba.core.byteflow:pending: deque([State(pc_initial=78 nstack_initial=0)])
DEBUG:numba.core.byteflow:-------------------------Prune PHIs-------------------------
DEBUG:numba.core.byteflow:Used_phis: defaultdict(<class 'set'>,
            {State(pc_initial=0 nstack_initial=0): set(),
             State(pc_initial=24 nstack_initial=1): {'$phi24.0'},
             State(pc_initial=30 nstack_initial=0): set(),
             State(pc_initial=32 nstack_initial=1): set(),
             State(pc_initial=36 nstack_initial=0): set(),
             State(pc_initial=40 nstack_initial=0): set(),
             State(pc_initial=62 nstack_initial=1): {'$phi62.0'},
             State(pc_initial=68 nstack_initial=0): set(),
             State(pc_initial=70 nstack_initial=1): set(),
             State(pc_initial=74 nstack_initial=0): set(),
             State(pc_initial=78 nstack_initial=0): set(),
             State(pc_initial=82 nstack_initial=0): set(),
             State(pc_initial=102 nstack_initial=0): set()})
DEBUG:numba.core.byteflow:defmap: {'$phi24.0': State(pc_initial=0 nstack_initial=0),
 '$phi32.0': State(pc_initial=0 nstack_initial=0),
 '$phi62.0': State(pc_initial=40 nstack_initial=0),
 '$phi70.0': State(pc_initial=40 nstack_initial=0)}
DEBUG:numba.core.byteflow:phismap: defaultdict(<class 'set'>,
            {'$phi24.0': {('$16dup_top.6',
                           State(pc_initial=0 nstack_initial=0))},
             '$phi32.0': {('$16dup_top.6',
                           State(pc_initial=0 nstack_initial=0))},
             '$phi62.0': {('$54dup_top.6',
                           State(pc_initial=40 nstack_initial=0))},
             '$phi70.0': {('$54dup_top.6',
                           State(pc_initial=40 nstack_initial=0))}})
DEBUG:numba.core.byteflow:changing phismap: defaultdict(<class 'set'>,
            {'$phi24.0': {('$16dup_top.6',
                           State(pc_initial=0 nstack_initial=0))},
             '$phi32.0': {('$16dup_top.6',
                           State(pc_initial=0 nstack_initial=0))},
             '$phi62.0': {('$54dup_top.6',
                           State(pc_initial=40 nstack_initial=0))},
             '$phi70.0': {('$54dup_top.6',
                           State(pc_initial=40 nstack_initial=0))}})
DEBUG:numba.core.byteflow:keep phismap: {'$phi24.0': {('$16dup_top.6', State(pc_initial=0 nstack_initial=0))},
 '$phi62.0': {('$54dup_top.6', State(pc_initial=40 nstack_initial=0))}}
DEBUG:numba.core.byteflow:new_out: defaultdict(<class 'dict'>,
            {State(pc_initial=0 nstack_initial=0): {'$phi24.0': '$16dup_top.6'},
             State(pc_initial=40 nstack_initial=0): {'$phi62.0': '$54dup_top.6'}})
DEBUG:numba.core.byteflow:----------------------DONE Prune PHIs-----------------------
DEBUG:numba.core.byteflow:block_infos State(pc_initial=0 nstack_initial=0):
AdaptBlockInfo(insts=((0, {}), (2, {'res': '$x2.0'}), (4, {'res': '$const4.1'}), (6, {'index': '$const4.1', 'target': '$x2.0', 'res': '$6binary_subscr.2'}), (8, {'value': '$6binary_subscr.2'}), (10, {'res': '$threshold10.3'}), (12, {'value': '$threshold10.3', 'res': '$12unary_negative.4'}), (14, {'res': '$x014.5'}), (16, {'orig': ['$x014.5'], 'duped': ['$16dup_top.6']}), (20, {'lhs': '$12unary_negative.4', 'rhs': '$x014.5', 'res': '$20compare_op.7'}), (22, {'pred': '$20compare_op.7'})), outgoing_phis={'$phi24.0': '$16dup_top.6'}, blockstack=(), active_try_block=None, outgoing_edgepushed={24: ('$16dup_top.6',), 32: ('$16dup_top.6',)})
DEBUG:numba.core.byteflow:block_infos State(pc_initial=24 nstack_initial=1):
AdaptBlockInfo(insts=((24, {'res': '$threshold24.1'}), (26, {'lhs': '$phi24.0', 'rhs': '$threshold24.1', 'res': '$26compare_op.2'}), (28, {'pred': '$26compare_op.2'})), outgoing_phis={}, blockstack=(), active_try_block=None, outgoing_edgepushed={30: (), 40: ()})
DEBUG:numba.core.byteflow:block_infos State(pc_initial=30 nstack_initial=0):
AdaptBlockInfo(insts=((30, {}),), outgoing_phis={}, blockstack=(), active_try_block=None, outgoing_edgepushed={36: ()})
DEBUG:numba.core.byteflow:block_infos State(pc_initial=32 nstack_initial=1):
AdaptBlockInfo(insts=((34, {}),), outgoing_phis={}, blockstack=(), active_try_block=None, outgoing_edgepushed={40: ()})
DEBUG:numba.core.byteflow:block_infos State(pc_initial=36 nstack_initial=0):
AdaptBlockInfo(insts=((36, {'res': '$const36.0'}), (38, {'value': '$const36.0'})), outgoing_phis={}, blockstack=(), active_try_block=None, outgoing_edgepushed={40: ()})
DEBUG:numba.core.byteflow:block_infos State(pc_initial=40 nstack_initial=0):
AdaptBlockInfo(insts=((40, {'res': '$x40.0'}), (42, {'res': '$const42.1'}), (44, {'index': '$const42.1', 'target': '$x40.0', 'res': '$44binary_subscr.2'}), (46, {'value': '$44binary_subscr.2'}), (48, {'res': '$threshold48.3'}), (50, {'value': '$threshold48.3', 'res': '$50unary_negative.4'}), (52, {'res': '$x152.5'}), (54, {'orig': ['$x152.5'], 'duped': ['$54dup_top.6']}), (58, {'lhs': '$50unary_negative.4', 'rhs': '$x152.5', 'res': '$58compare_op.7'}), (60, {'pred': '$58compare_op.7'})), outgoing_phis={'$phi62.0': '$54dup_top.6'}, blockstack=(), active_try_block=None, outgoing_edgepushed={62: ('$54dup_top.6',), 70: ('$54dup_top.6',)})
DEBUG:numba.core.byteflow:block_infos State(pc_initial=62 nstack_initial=1):
AdaptBlockInfo(insts=((62, {'res': '$threshold62.1'}), (64, {'lhs': '$phi62.0', 'rhs': '$threshold62.1', 'res': '$64compare_op.2'}), (66, {'pred': '$64compare_op.2'})), outgoing_phis={}, blockstack=(), active_try_block=None, outgoing_edgepushed={68: (), 78: ()})
DEBUG:numba.core.byteflow:block_infos State(pc_initial=68 nstack_initial=0):
AdaptBlockInfo(insts=((68, {}),), outgoing_phis={}, blockstack=(), active_try_block=None, outgoing_edgepushed={74: ()})
DEBUG:numba.core.byteflow:block_infos State(pc_initial=70 nstack_initial=1):
AdaptBlockInfo(insts=((72, {}),), outgoing_phis={}, blockstack=(), active_try_block=None, outgoing_edgepushed={78: ()})
DEBUG:numba.core.byteflow:block_infos State(pc_initial=74 nstack_initial=0):
AdaptBlockInfo(insts=((74, {'res': '$const74.0'}), (76, {'value': '$const74.0'})), outgoing_phis={}, blockstack=(), active_try_block=None, outgoing_edgepushed={78: ()})
DEBUG:numba.core.byteflow:block_infos State(pc_initial=78 nstack_initial=0):
AdaptBlockInfo(insts=((78, {'res': '$zero_pos78.0'}), (80, {'pred': '$zero_pos78.0'})), outgoing_phis={}, blockstack=(), active_try_block=None, outgoing_edgepushed={82: (), 102: ()})
DEBUG:numba.core.byteflow:block_infos State(pc_initial=82 nstack_initial=0):
AdaptBlockInfo(insts=((82, {'res': '$82load_global.0'}), (84, {'item': '$82load_global.0', 'res': '$84load_method.1'}), (86, {'res': '$x086.2'}), (88, {'func': '$84load_method.1', 'args': ['$x086.2'], 'res': '$88call_method.3'}), (90, {'res': '$90load_global.4'}), (92, {'item': '$90load_global.4', 'res': '$92load_method.5'}), (94, {'res': '$x194.6'}), (96, {'func': '$92load_method.5', 'args': ['$x194.6'], 'res': '$96call_method.7'}), (98, {'lhs': '$88call_method.3', 'rhs': '$96call_method.7', 'res': '$98compare_op.8'}), (100, {'retval': '$98compare_op.8', 'castval': '$100return_value.9'})), outgoing_phis={}, blockstack=(), active_try_block=None, outgoing_edgepushed={})
DEBUG:numba.core.byteflow:block_infos State(pc_initial=102 nstack_initial=0):
AdaptBlockInfo(insts=((102, {'res': '$102load_global.0'}), (104, {'item': '$102load_global.0', 'res': '$104load_method.1'}), (106, {'res': '$x0106.2'}), (108, {'func': '$104load_method.1', 'args': ['$x0106.2'], 'res': '$108call_method.3'}), (110, {'res': '$110load_global.4'}), (112, {'item': '$110load_global.4', 'res': '$112load_method.5'}), (114, {'res': '$x1114.6'}), (116, {'func': '$112load_method.5', 'args': ['$x1114.6'], 'res': '$116call_method.7'}), (118, {'lhs': '$108call_method.3', 'rhs': '$116call_method.7', 'res': '$118compare_op.8'}), (120, {'retval': '$118compare_op.8', 'castval': '$120return_value.9'})), outgoing_phis={}, blockstack=(), active_try_block=None, outgoing_edgepushed={})
DEBUG:numba.core.interpreter:label 0:
    x = arg(0, name=x)                       ['x']
    threshold = arg(1, name=threshold)       ['threshold']
    zero_pos = arg(2, name=zero_pos)         ['zero_pos']
    $const4.1 = const(int, 0)                ['$const4.1']
    x0 = getitem(value=x, index=$const4.1, fn=<built-in function getitem>) ['$const4.1', 'x', 'x0']
    $12unary_negative.4 = unary(fn=<built-in function neg>, value=threshold) ['$12unary_negative.4', 'threshold']
    $20compare_op.7 = $12unary_negative.4 <= x0 ['$12unary_negative.4', '$20compare_op.7', 'x0']
    bool22 = global(bool: <class 'bool'>)    ['bool22']
    $22pred = call bool22($20compare_op.7, func=bool22, args=(Var($20compare_op.7, audio.py:1145),), kws=(), vararg=None, varkwarg=None, target=None) ['$20compare_op.7', '$22pred', 'bool22']
    $phi24.0 = x0                            ['$phi24.0', 'x0']
    branch $22pred, 24, 32                   ['$22pred']
label 24:
    $26compare_op.2 = $phi24.0 <= threshold  ['$26compare_op.2', '$phi24.0', 'threshold']
    bool28 = global(bool: <class 'bool'>)    ['bool28']
    $28pred = call bool28($26compare_op.2, func=bool28, args=(Var($26compare_op.2, audio.py:1145),), kws=(), vararg=None, varkwarg=None, target=None) ['$26compare_op.2', '$28pred', 'bool28']
    branch $28pred, 30, 40                   ['$28pred']
label 30:
    jump 36                                  []
label 32:
    jump 40                                  []
label 36:
    x0 = const(int, 0)                       ['x0']
    jump 40                                  []
label 40:
    $const42.1 = const(int, -1)              ['$const42.1']
    x1 = getitem(value=x, index=$const42.1, fn=<built-in function getitem>) ['$const42.1', 'x', 'x1']
    $50unary_negative.4 = unary(fn=<built-in function neg>, value=threshold) ['$50unary_negative.4', 'threshold']
    $58compare_op.7 = $50unary_negative.4 <= x1 ['$50unary_negative.4', '$58compare_op.7', 'x1']
    bool60 = global(bool: <class 'bool'>)    ['bool60']
    $60pred = call bool60($58compare_op.7, func=bool60, args=(Var($58compare_op.7, audio.py:1149),), kws=(), vararg=None, varkwarg=None, target=None) ['$58compare_op.7', '$60pred', 'bool60']
    $phi62.0 = x1                            ['$phi62.0', 'x1']
    branch $60pred, 62, 70                   ['$60pred']
label 62:
    $64compare_op.2 = $phi62.0 <= threshold  ['$64compare_op.2', '$phi62.0', 'threshold']
    bool66 = global(bool: <class 'bool'>)    ['bool66']
    $66pred = call bool66($64compare_op.2, func=bool66, args=(Var($64compare_op.2, audio.py:1149),), kws=(), vararg=None, varkwarg=None, target=None) ['$64compare_op.2', '$66pred', 'bool66']
    branch $66pred, 68, 78                   ['$66pred']
label 68:
    jump 74                                  []
label 70:
    jump 78                                  []
label 74:
    x1 = const(int, 0)                       ['x1']
    jump 78                                  []
label 78:
    bool80 = global(bool: <class 'bool'>)    ['bool80']
    $80pred = call bool80(zero_pos, func=bool80, args=(Var(zero_pos, audio.py:1141),), kws=(), vararg=None, varkwarg=None, target=None) ['$80pred', 'bool80', 'zero_pos']
    branch $80pred, 82, 102                  ['$80pred']
label 82:
    $82load_global.0 = global(np: <module 'numpy' from 'C:\\Users\\taskm\\anaconda3\\envs\\taskai\\lib\\site-packages\\numpy\\__init__.py'>) ['$82load_global.0']
    $84load_method.1 = getattr(value=$82load_global.0, attr=signbit) ['$82load_global.0', '$84load_method.1']
    $88call_method.3 = call $84load_method.1(x0, func=$84load_method.1, args=[Var(x0, audio.py:1144)], kws=(), vararg=None, varkwarg=None, target=None) ['$84load_method.1', '$88call_method.3', 'x0']
    $90load_global.4 = global(np: <module 'numpy' from 'C:\\Users\\taskm\\anaconda3\\envs\\taskai\\lib\\site-packages\\numpy\\__init__.py'>) ['$90load_global.4']
    $92load_method.5 = getattr(value=$90load_global.4, attr=signbit) ['$90load_global.4', '$92load_method.5']
    $96call_method.7 = call $92load_method.5(x1, func=$92load_method.5, args=[Var(x1, audio.py:1148)], kws=(), vararg=None, varkwarg=None, target=None) ['$92load_method.5', '$96call_method.7', 'x1']
    $98compare_op.8 = $88call_method.3 != $96call_method.7 ['$88call_method.3', '$96call_method.7', '$98compare_op.8']
    $100return_value.9 = cast(value=$98compare_op.8) ['$100return_value.9', '$98compare_op.8']
    return $100return_value.9                ['$100return_value.9']
label 102:
    $102load_global.0 = global(np: <module 'numpy' from 'C:\\Users\\taskm\\anaconda3\\envs\\taskai\\lib\\site-packages\\numpy\\__init__.py'>) ['$102load_global.0']
    $104load_method.1 = getattr(value=$102load_global.0, attr=sign) ['$102load_global.0', '$104load_method.1']
    $108call_method.3 = call $104load_method.1(x0, func=$104load_method.1, args=[Var(x0, audio.py:1144)], kws=(), vararg=None, varkwarg=None, target=None) ['$104load_method.1', '$108call_method.3', 'x0']
    $110load_global.4 = global(np: <module 'numpy' from 'C:\\Users\\taskm\\anaconda3\\envs\\taskai\\lib\\site-packages\\numpy\\__init__.py'>) ['$110load_global.4']
    $112load_method.5 = getattr(value=$110load_global.4, attr=sign) ['$110load_global.4', '$112load_method.5']
    $116call_method.7 = call $112load_method.5(x1, func=$112load_method.5, args=[Var(x1, audio.py:1148)], kws=(), vararg=None, varkwarg=None, target=None) ['$112load_method.5', '$116call_method.7', 'x1']
    $118compare_op.8 = $108call_method.3 != $116call_method.7 ['$108call_method.3', '$116call_method.7', '$118compare_op.8']
    $120return_value.9 = cast(value=$118compare_op.8) ['$118compare_op.8', '$120return_value.9']
    return $120return_value.9                ['$120return_value.9']

DEBUG:numba.core.byteflow:bytecode dump:
>          0	NOP(arg=None, lineno=1039)
           2	LOAD_FAST(arg=0, lineno=1042)
           4	LOAD_CONST(arg=1, lineno=1042)
           6	BINARY_SUBSCR(arg=None, lineno=1042)
           8	LOAD_FAST(arg=0, lineno=1042)
          10	LOAD_CONST(arg=2, lineno=1042)
          12	BINARY_SUBSCR(arg=None, lineno=1042)
          14	COMPARE_OP(arg=4, lineno=1042)
          16	LOAD_FAST(arg=0, lineno=1042)
          18	LOAD_CONST(arg=1, lineno=1042)
          20	BINARY_SUBSCR(arg=None, lineno=1042)
          22	LOAD_FAST(arg=0, lineno=1042)
          24	LOAD_CONST(arg=3, lineno=1042)
          26	BINARY_SUBSCR(arg=None, lineno=1042)
          28	COMPARE_OP(arg=5, lineno=1042)
          30	BINARY_AND(arg=None, lineno=1042)
          32	RETURN_VALUE(arg=None, lineno=1042)
DEBUG:numba.core.byteflow:pending: deque([State(pc_initial=0 nstack_initial=0)])
DEBUG:numba.core.byteflow:stack: []
DEBUG:numba.core.byteflow:state.pc_initial: State(pc_initial=0 nstack_initial=0)
DEBUG:numba.core.byteflow:dispatch pc=0, inst=NOP(arg=None, lineno=1039)
DEBUG:numba.core.byteflow:stack []
DEBUG:numba.core.byteflow:dispatch pc=2, inst=LOAD_FAST(arg=0, lineno=1042)
DEBUG:numba.core.byteflow:stack []
DEBUG:numba.core.byteflow:dispatch pc=4, inst=LOAD_CONST(arg=1, lineno=1042)
DEBUG:numba.core.byteflow:stack ['$x2.0']
DEBUG:numba.core.byteflow:dispatch pc=6, inst=BINARY_SUBSCR(arg=None, lineno=1042)
DEBUG:numba.core.byteflow:stack ['$x2.0', '$const4.1']
DEBUG:numba.core.byteflow:dispatch pc=8, inst=LOAD_FAST(arg=0, lineno=1042)
DEBUG:numba.core.byteflow:stack ['$6binary_subscr.2']
DEBUG:numba.core.byteflow:dispatch pc=10, inst=LOAD_CONST(arg=2, lineno=1042)
DEBUG:numba.core.byteflow:stack ['$6binary_subscr.2', '$x8.3']
DEBUG:numba.core.byteflow:dispatch pc=12, inst=BINARY_SUBSCR(arg=None, lineno=1042)
DEBUG:numba.core.byteflow:stack ['$6binary_subscr.2', '$x8.3', '$const10.4']
DEBUG:numba.core.byteflow:dispatch pc=14, inst=COMPARE_OP(arg=4, lineno=1042)
DEBUG:numba.core.byteflow:stack ['$6binary_subscr.2', '$12binary_subscr.5']
DEBUG:numba.core.byteflow:dispatch pc=16, inst=LOAD_FAST(arg=0, lineno=1042)
DEBUG:numba.core.byteflow:stack ['$14compare_op.6']
DEBUG:numba.core.byteflow:dispatch pc=18, inst=LOAD_CONST(arg=1, lineno=1042)
DEBUG:numba.core.byteflow:stack ['$14compare_op.6', '$x16.7']
DEBUG:numba.core.byteflow:dispatch pc=20, inst=BINARY_SUBSCR(arg=None, lineno=1042)
DEBUG:numba.core.byteflow:stack ['$14compare_op.6', '$x16.7', '$const18.8']
DEBUG:numba.core.byteflow:dispatch pc=22, inst=LOAD_FAST(arg=0, lineno=1042)
DEBUG:numba.core.byteflow:stack ['$14compare_op.6', '$20binary_subscr.9']
DEBUG:numba.core.byteflow:dispatch pc=24, inst=LOAD_CONST(arg=3, lineno=1042)
DEBUG:numba.core.byteflow:stack ['$14compare_op.6', '$20binary_subscr.9', '$x22.10']
DEBUG:numba.core.byteflow:dispatch pc=26, inst=BINARY_SUBSCR(arg=None, lineno=1042)
DEBUG:numba.core.byteflow:stack ['$14compare_op.6', '$20binary_subscr.9', '$x22.10', '$const24.11']
DEBUG:numba.core.byteflow:dispatch pc=28, inst=COMPARE_OP(arg=5, lineno=1042)
DEBUG:numba.core.byteflow:stack ['$14compare_op.6', '$20binary_subscr.9', '$26binary_subscr.12']
DEBUG:numba.core.byteflow:dispatch pc=30, inst=BINARY_AND(arg=None, lineno=1042)
DEBUG:numba.core.byteflow:stack ['$14compare_op.6', '$28compare_op.13']
DEBUG:numba.core.byteflow:dispatch pc=32, inst=RETURN_VALUE(arg=None, lineno=1042)
DEBUG:numba.core.byteflow:stack ['$30binary_and.14']
DEBUG:numba.core.byteflow:end state. edges=[]
DEBUG:numba.core.byteflow:-------------------------Prune PHIs-------------------------
DEBUG:numba.core.byteflow:Used_phis: defaultdict(<class 'set'>, {State(pc_initial=0 nstack_initial=0): set()})
DEBUG:numba.core.byteflow:defmap: {}
DEBUG:numba.core.byteflow:phismap: defaultdict(<class 'set'>, {})
DEBUG:numba.core.byteflow:changing phismap: defaultdict(<class 'set'>, {})
DEBUG:numba.core.byteflow:keep phismap: {}
DEBUG:numba.core.byteflow:new_out: defaultdict(<class 'dict'>, {})
DEBUG:numba.core.byteflow:----------------------DONE Prune PHIs-----------------------
DEBUG:numba.core.byteflow:block_infos State(pc_initial=0 nstack_initial=0):
AdaptBlockInfo(insts=((0, {}), (2, {'res': '$x2.0'}), (4, {'res': '$const4.1'}), (6, {'index': '$const4.1', 'target': '$x2.0', 'res': '$6binary_subscr.2'}), (8, {'res': '$x8.3'}), (10, {'res': '$const10.4'}), (12, {'index': '$const10.4', 'target': '$x8.3', 'res': '$12binary_subscr.5'}), (14, {'lhs': '$6binary_subscr.2', 'rhs': '$12binary_subscr.5', 'res': '$14compare_op.6'}), (16, {'res': '$x16.7'}), (18, {'res': '$const18.8'}), (20, {'index': '$const18.8', 'target': '$x16.7', 'res': '$20binary_subscr.9'}), (22, {'res': '$x22.10'}), (24, {'res': '$const24.11'}), (26, {'index': '$const24.11', 'target': '$x22.10', 'res': '$26binary_subscr.12'}), (28, {'lhs': '$20binary_subscr.9', 'rhs': '$26binary_subscr.12', 'res': '$28compare_op.13'}), (30, {'lhs': '$14compare_op.6', 'rhs': '$28compare_op.13', 'res': '$30binary_and.14'}), (32, {'retval': '$30binary_and.14', 'castval': '$32return_value.15'})), outgoing_phis={}, blockstack=(), active_try_block=None, outgoing_edgepushed={})
DEBUG:numba.core.interpreter:label 0:
    x = arg(0, name=x)                       ['x']
    $const4.1 = const(int, 0)                ['$const4.1']
    $6binary_subscr.2 = getitem(value=x, index=$const4.1, fn=<built-in function getitem>) ['$6binary_subscr.2', '$const4.1', 'x']
    $const10.4 = const(int, -1)              ['$const10.4']
    $12binary_subscr.5 = getitem(value=x, index=$const10.4, fn=<built-in function getitem>) ['$12binary_subscr.5', '$const10.4', 'x']
    $14compare_op.6 = $6binary_subscr.2 > $12binary_subscr.5 ['$12binary_subscr.5', '$14compare_op.6', '$6binary_subscr.2']
    $const18.8 = const(int, 0)               ['$const18.8']
    $20binary_subscr.9 = getitem(value=x, index=$const18.8, fn=<built-in function getitem>) ['$20binary_subscr.9', '$const18.8', 'x']
    $const24.11 = const(int, 1)              ['$const24.11']
    $26binary_subscr.12 = getitem(value=x, index=$const24.11, fn=<built-in function getitem>) ['$26binary_subscr.12', '$const24.11', 'x']
    $28compare_op.13 = $20binary_subscr.9 >= $26binary_subscr.12 ['$20binary_subscr.9', '$26binary_subscr.12', '$28compare_op.13']
    $30binary_and.14 = $14compare_op.6 & $28compare_op.13 ['$14compare_op.6', '$28compare_op.13', '$30binary_and.14']
    $32return_value.15 = cast(value=$30binary_and.14) ['$30binary_and.14', '$32return_value.15']
    return $32return_value.15                ['$32return_value.15']

DEBUG:numba.core.byteflow:bytecode dump:
>          0	NOP(arg=None, lineno=1045)
           2	LOAD_FAST(arg=0, lineno=1048)
           4	LOAD_CONST(arg=1, lineno=1048)
           6	BINARY_SUBSCR(arg=None, lineno=1048)
           8	LOAD_FAST(arg=0, lineno=1048)
          10	LOAD_CONST(arg=2, lineno=1048)
          12	BINARY_SUBSCR(arg=None, lineno=1048)
          14	COMPARE_OP(arg=0, lineno=1048)
          16	LOAD_FAST(arg=0, lineno=1048)
          18	LOAD_CONST(arg=1, lineno=1048)
          20	BINARY_SUBSCR(arg=None, lineno=1048)
          22	LOAD_FAST(arg=0, lineno=1048)
          24	LOAD_CONST(arg=3, lineno=1048)
          26	BINARY_SUBSCR(arg=None, lineno=1048)
          28	COMPARE_OP(arg=1, lineno=1048)
          30	BINARY_AND(arg=None, lineno=1048)
          32	RETURN_VALUE(arg=None, lineno=1048)
DEBUG:numba.core.byteflow:pending: deque([State(pc_initial=0 nstack_initial=0)])
DEBUG:numba.core.byteflow:stack: []
DEBUG:numba.core.byteflow:state.pc_initial: State(pc_initial=0 nstack_initial=0)
DEBUG:numba.core.byteflow:dispatch pc=0, inst=NOP(arg=None, lineno=1045)
DEBUG:numba.core.byteflow:stack []
DEBUG:numba.core.byteflow:dispatch pc=2, inst=LOAD_FAST(arg=0, lineno=1048)
DEBUG:numba.core.byteflow:stack []
DEBUG:numba.core.byteflow:dispatch pc=4, inst=LOAD_CONST(arg=1, lineno=1048)
DEBUG:numba.core.byteflow:stack ['$x2.0']
DEBUG:numba.core.byteflow:dispatch pc=6, inst=BINARY_SUBSCR(arg=None, lineno=1048)
DEBUG:numba.core.byteflow:stack ['$x2.0', '$const4.1']
DEBUG:numba.core.byteflow:dispatch pc=8, inst=LOAD_FAST(arg=0, lineno=1048)
DEBUG:numba.core.byteflow:stack ['$6binary_subscr.2']
DEBUG:numba.core.byteflow:dispatch pc=10, inst=LOAD_CONST(arg=2, lineno=1048)
DEBUG:numba.core.byteflow:stack ['$6binary_subscr.2', '$x8.3']
DEBUG:numba.core.byteflow:dispatch pc=12, inst=BINARY_SUBSCR(arg=None, lineno=1048)
DEBUG:numba.core.byteflow:stack ['$6binary_subscr.2', '$x8.3', '$const10.4']
DEBUG:numba.core.byteflow:dispatch pc=14, inst=COMPARE_OP(arg=0, lineno=1048)
DEBUG:numba.core.byteflow:stack ['$6binary_subscr.2', '$12binary_subscr.5']
DEBUG:numba.core.byteflow:dispatch pc=16, inst=LOAD_FAST(arg=0, lineno=1048)
DEBUG:numba.core.byteflow:stack ['$14compare_op.6']
DEBUG:numba.core.byteflow:dispatch pc=18, inst=LOAD_CONST(arg=1, lineno=1048)
DEBUG:numba.core.byteflow:stack ['$14compare_op.6', '$x16.7']
DEBUG:numba.core.byteflow:dispatch pc=20, inst=BINARY_SUBSCR(arg=None, lineno=1048)
DEBUG:numba.core.byteflow:stack ['$14compare_op.6', '$x16.7', '$const18.8']
DEBUG:numba.core.byteflow:dispatch pc=22, inst=LOAD_FAST(arg=0, lineno=1048)
DEBUG:numba.core.byteflow:stack ['$14compare_op.6', '$20binary_subscr.9']
DEBUG:numba.core.byteflow:dispatch pc=24, inst=LOAD_CONST(arg=3, lineno=1048)
DEBUG:numba.core.byteflow:stack ['$14compare_op.6', '$20binary_subscr.9', '$x22.10']
DEBUG:numba.core.byteflow:dispatch pc=26, inst=BINARY_SUBSCR(arg=None, lineno=1048)
DEBUG:numba.core.byteflow:stack ['$14compare_op.6', '$20binary_subscr.9', '$x22.10', '$const24.11']
DEBUG:numba.core.byteflow:dispatch pc=28, inst=COMPARE_OP(arg=1, lineno=1048)
DEBUG:numba.core.byteflow:stack ['$14compare_op.6', '$20binary_subscr.9', '$26binary_subscr.12']
DEBUG:numba.core.byteflow:dispatch pc=30, inst=BINARY_AND(arg=None, lineno=1048)
DEBUG:numba.core.byteflow:stack ['$14compare_op.6', '$28compare_op.13']
DEBUG:numba.core.byteflow:dispatch pc=32, inst=RETURN_VALUE(arg=None, lineno=1048)
DEBUG:numba.core.byteflow:stack ['$30binary_and.14']
DEBUG:numba.core.byteflow:end state. edges=[]
DEBUG:numba.core.byteflow:-------------------------Prune PHIs-------------------------
DEBUG:numba.core.byteflow:Used_phis: defaultdict(<class 'set'>, {State(pc_initial=0 nstack_initial=0): set()})
DEBUG:numba.core.byteflow:defmap: {}
DEBUG:numba.core.byteflow:phismap: defaultdict(<class 'set'>, {})
DEBUG:numba.core.byteflow:changing phismap: defaultdict(<class 'set'>, {})
DEBUG:numba.core.byteflow:keep phismap: {}
DEBUG:numba.core.byteflow:new_out: defaultdict(<class 'dict'>, {})
DEBUG:numba.core.byteflow:----------------------DONE Prune PHIs-----------------------
DEBUG:numba.core.byteflow:block_infos State(pc_initial=0 nstack_initial=0):
AdaptBlockInfo(insts=((0, {}), (2, {'res': '$x2.0'}), (4, {'res': '$const4.1'}), (6, {'index': '$const4.1', 'target': '$x2.0', 'res': '$6binary_subscr.2'}), (8, {'res': '$x8.3'}), (10, {'res': '$const10.4'}), (12, {'index': '$const10.4', 'target': '$x8.3', 'res': '$12binary_subscr.5'}), (14, {'lhs': '$6binary_subscr.2', 'rhs': '$12binary_subscr.5', 'res': '$14compare_op.6'}), (16, {'res': '$x16.7'}), (18, {'res': '$const18.8'}), (20, {'index': '$const18.8', 'target': '$x16.7', 'res': '$20binary_subscr.9'}), (22, {'res': '$x22.10'}), (24, {'res': '$const24.11'}), (26, {'index': '$const24.11', 'target': '$x22.10', 'res': '$26binary_subscr.12'}), (28, {'lhs': '$20binary_subscr.9', 'rhs': '$26binary_subscr.12', 'res': '$28compare_op.13'}), (30, {'lhs': '$14compare_op.6', 'rhs': '$28compare_op.13', 'res': '$30binary_and.14'}), (32, {'retval': '$30binary_and.14', 'castval': '$32return_value.15'})), outgoing_phis={}, blockstack=(), active_try_block=None, outgoing_edgepushed={})
DEBUG:numba.core.interpreter:label 0:
    x = arg(0, name=x)                       ['x']
    $const4.1 = const(int, 0)                ['$const4.1']
    $6binary_subscr.2 = getitem(value=x, index=$const4.1, fn=<built-in function getitem>) ['$6binary_subscr.2', '$const4.1', 'x']
    $const10.4 = const(int, -1)              ['$const10.4']
    $12binary_subscr.5 = getitem(value=x, index=$const10.4, fn=<built-in function getitem>) ['$12binary_subscr.5', '$const10.4', 'x']
    $14compare_op.6 = $6binary_subscr.2 < $12binary_subscr.5 ['$12binary_subscr.5', '$14compare_op.6', '$6binary_subscr.2']
    $const18.8 = const(int, 0)               ['$const18.8']
    $20binary_subscr.9 = getitem(value=x, index=$const18.8, fn=<built-in function getitem>) ['$20binary_subscr.9', '$const18.8', 'x']
    $const24.11 = const(int, 1)              ['$const24.11']
    $26binary_subscr.12 = getitem(value=x, index=$const24.11, fn=<built-in function getitem>) ['$26binary_subscr.12', '$const24.11', 'x']
    $28compare_op.13 = $20binary_subscr.9 <= $26binary_subscr.12 ['$20binary_subscr.9', '$26binary_subscr.12', '$28compare_op.13']
    $30binary_and.14 = $14compare_op.6 & $28compare_op.13 ['$14compare_op.6', '$28compare_op.13', '$30binary_and.14']
    $32return_value.15 = cast(value=$30binary_and.14) ['$30binary_and.14', '$32return_value.15']
    return $32return_value.15                ['$32return_value.15']

DEBUG:httpx:load_ssl_context verify=True cert=None trust_env=True http2=False
DEBUG:httpx:load_verify_locations cafile='C:\\Users\\taskm\\anaconda3\\Library\\ssl\\cacert.pem'
DEBUG:httpx:load_ssl_context verify=True cert=None trust_env=True http2=False
DEBUG:httpx:load_verify_locations cafile='C:\\Users\\taskm\\anaconda3\\Library\\ssl\\cacert.pem'
DEBUG:openai._base_client:Request options: {'method': 'post', 'url': '/completions', 'files': None, 'json_data': {'model': 'gpt-3.5-turbo-instruct', 'prompt': ["Summarize the following chapter in one sentence: put it out but so you said you the way you articulated though i really like that you said a cog in the machine they lost a cog in the machine yo did you see my game of thrones breakdown of the of the um the the the leagues in battle right right now how i framed it with you no i gotta seriously because i don't i try not to watch none of this yeah yeah i know you said you was gonna do that too no i was i was that was a little test along with a question because you say you were stepping away from the whole negativity and all of that and yo the fact that is that everybody's saying that is dying right now i think about you so much but anyway uh but what i pointed out was how like it seems like we're starting to feel the after effects of the lack of innovation from that side so you saying that the deal they made with caffeine was like them selling them a machine but a cog was missing in the machine it looked the same from the outside"], 'frequency_penalty': 0, 'logit_bias': {}, 'max_tokens': 256, 'n': 1, 'presence_penalty': 0, 'temperature': 0.7, 'top_p': 1}}
DEBUG:openai._base_client:Sending HTTP Request: POST https://api.openai.com/v1/completions
DEBUG:httpcore.connection:connect_tcp.started host='api.openai.com' port=443 local_address=None timeout=None socket_options=None
DEBUG:httpcore.connection:connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000002103FC2A820>
DEBUG:httpcore.connection:start_tls.started ssl_context=<ssl.SSLContext object at 0x000002103FCA9DC0> server_hostname='api.openai.com' timeout=None
DEBUG:httpcore.connection:start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000002103FC2A9D0>
DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>
DEBUG:httpcore.http11:send_request_headers.complete
DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>
DEBUG:httpcore.http11:send_request_body.complete
DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>
DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Mon, 08 Jul 2024 03:22:41 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-allow-origin', b'*'), (b'Cache-Control', b'no-cache, must-revalidate'), (b'openai-model', b'gpt-3.5-turbo-instruct'), (b'openai-organization', b'machinekingsmedia'), (b'openai-processing-ms', b'628'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains'), (b'x-ratelimit-limit-requests', b'3500'), (b'x-ratelimit-limit-tokens', b'90000'), (b'x-ratelimit-remaining-requests', b'3499'), (b'x-ratelimit-remaining-tokens', b'89497'), (b'x-ratelimit-reset-requests', b'17ms'), (b'x-ratelimit-reset-tokens', b'335ms'), (b'x-request-id', b'req_3071fc3a21ceea7002921d6b2e4de134'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Set-Cookie', b'__cf_bm=iX6tJw.PcpgZ7CMdHso_.H7B._IlBATkWq5_nSoHXIs-1720408961-1.0.1.1-LbOFTp9LOfsJJvs.Heb7rwjiBqfcbZQ1w8.Lw6bRgCgZyLRq3nOI9i4KyE9VLCJ0qmYYrvv3eLsDb3NwqdlLCQ; path=/; expires=Mon, 08-Jul-24 03:52:41 GMT; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), (b'Set-Cookie', b'_cfuvid=BcNTpYL0nZHwZ4INKxb7M9lw7Z_iaTLV4SmYfLsvp28-1720408961848-0.0.1.1-604800000; path=/; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), (b'Server', b'cloudflare'), (b'CF-RAY', b'89fcf386cd1282e1-IAD'), (b'Content-Encoding', b'br'), (b'alt-svc', b'h3=":443"; ma=86400')])
INFO:httpx:HTTP Request: POST https://api.openai.com/v1/completions "HTTP/1.1 200 OK"
DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>
DEBUG:httpcore.http11:receive_response_body.complete
DEBUG:httpcore.http11:response_closed.started
DEBUG:httpcore.http11:response_closed.complete
DEBUG:openai._base_client:HTTP Response: POST https://api.openai.com/v1/completions "200 OK" Headers([('date', 'Mon, 08 Jul 2024 03:22:41 GMT'), ('content-type', 'application/json'), ('transfer-encoding', 'chunked'), ('connection', 'keep-alive'), ('access-control-allow-origin', '*'), ('cache-control', 'no-cache, must-revalidate'), ('openai-model', 'gpt-3.5-turbo-instruct'), ('openai-organization', 'machinekingsmedia'), ('openai-processing-ms', '628'), ('openai-version', '2020-10-01'), ('strict-transport-security', 'max-age=31536000; includeSubDomains'), ('x-ratelimit-limit-requests', '3500'), ('x-ratelimit-limit-tokens', '90000'), ('x-ratelimit-remaining-requests', '3499'), ('x-ratelimit-remaining-tokens', '89497'), ('x-ratelimit-reset-requests', '17ms'), ('x-ratelimit-reset-tokens', '335ms'), ('x-request-id', 'req_3071fc3a21ceea7002921d6b2e4de134'), ('cf-cache-status', 'DYNAMIC'), ('set-cookie', '__cf_bm=iX6tJw.PcpgZ7CMdHso_.H7B._IlBATkWq5_nSoHXIs-1720408961-1.0.1.1-LbOFTp9LOfsJJvs.Heb7rwjiBqfcbZQ1w8.Lw6bRgCgZyLRq3nOI9i4KyE9VLCJ0qmYYrvv3eLsDb3NwqdlLCQ; path=/; expires=Mon, 08-Jul-24 03:52:41 GMT; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), ('set-cookie', '_cfuvid=BcNTpYL0nZHwZ4INKxb7M9lw7Z_iaTLV4SmYfLsvp28-1720408961848-0.0.1.1-604800000; path=/; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), ('server', 'cloudflare'), ('cf-ray', '89fcf386cd1282e1-IAD'), ('content-encoding', 'br'), ('alt-svc', 'h3=":443"; ma=86400')])
DEBUG:openai._base_client:request_id: req_3071fc3a21ceea7002921d6b2e4de134
DEBUG:openai._base_client:Request options: {'method': 'post', 'url': '/completions', 'files': None, 'json_data': {'model': 'gpt-3.5-turbo-instruct', 'prompt': ["Summarize the following chapter in one sentence: but on the inside an important piece was missing they made it basically look like that i had nothing to do with what went down yeah they framed it like that yeah because you were a talent scout yeah apparently yeah they knew they knew that that's not the case and caffeine knows it's not the case you understand and it's between me and you of course as i was going through this for three years they was sitting in on the meetings with me and my lawyers in this so why would they sit if they think it was important if they they're gonna stand behind these niggas and make it look like a notice just trying to scam these niggas yeah when they realize like hold on my nigga this nigga really did all of this shit and y'all think about it if you could if you acquiring a company and its routes and how it became successful why wouldn't you keep the same team wow yeah yeah i see what you're saying i see what you're saying and i can see that fueling"], 'frequency_penalty': 0, 'logit_bias': {}, 'max_tokens': 256, 'n': 1, 'presence_penalty': 0, 'temperature': 0.7, 'top_p': 1}}
DEBUG:openai._base_client:Sending HTTP Request: POST https://api.openai.com/v1/completions
DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>
DEBUG:httpcore.http11:send_request_headers.complete
DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>
DEBUG:httpcore.http11:send_request_body.complete
DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>
DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Mon, 08 Jul 2024 03:22:43 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-allow-origin', b'*'), (b'Cache-Control', b'no-cache, must-revalidate'), (b'openai-model', b'gpt-3.5-turbo-instruct'), (b'openai-organization', b'machinekingsmedia'), (b'openai-processing-ms', b'1093'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains'), (b'x-ratelimit-limit-requests', b'3500'), (b'x-ratelimit-limit-tokens', b'90000'), (b'x-ratelimit-remaining-requests', b'3499'), (b'x-ratelimit-remaining-tokens', b'89496'), (b'x-ratelimit-reset-requests', b'17ms'), (b'x-ratelimit-reset-tokens', b'336ms'), (b'x-request-id', b'req_c61c66e19194c07185d788b700db2718'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'89fcf38ba94f82e1-IAD'), (b'Content-Encoding', b'br'), (b'alt-svc', b'h3=":443"; ma=86400')])
INFO:httpx:HTTP Request: POST https://api.openai.com/v1/completions "HTTP/1.1 200 OK"
DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>
DEBUG:httpcore.http11:receive_response_body.complete
DEBUG:httpcore.http11:response_closed.started
DEBUG:httpcore.http11:response_closed.complete
DEBUG:openai._base_client:HTTP Response: POST https://api.openai.com/v1/completions "200 OK" Headers({'date': 'Mon, 08 Jul 2024 03:22:43 GMT', 'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'access-control-allow-origin': '*', 'cache-control': 'no-cache, must-revalidate', 'openai-model': 'gpt-3.5-turbo-instruct', 'openai-organization': 'machinekingsmedia', 'openai-processing-ms': '1093', 'openai-version': '2020-10-01', 'strict-transport-security': 'max-age=31536000; includeSubDomains', 'x-ratelimit-limit-requests': '3500', 'x-ratelimit-limit-tokens': '90000', 'x-ratelimit-remaining-requests': '3499', 'x-ratelimit-remaining-tokens': '89496', 'x-ratelimit-reset-requests': '17ms', 'x-ratelimit-reset-tokens': '336ms', 'x-request-id': 'req_c61c66e19194c07185d788b700db2718', 'cf-cache-status': 'DYNAMIC', 'server': 'cloudflare', 'cf-ray': '89fcf38ba94f82e1-IAD', 'content-encoding': 'br', 'alt-svc': 'h3=":443"; ma=86400'})
DEBUG:openai._base_client:request_id: req_c61c66e19194c07185d788b700db2718
DEBUG:openai._base_client:Request options: {'method': 'post', 'url': '/completions', 'files': None, 'json_data': {'model': 'gpt-3.5-turbo-instruct', 'prompt': ["Summarize the following chapter in one sentence: anger amongst their ranks yeah and you gotta think about it sound like it was all a money move with them bro they tried to cut me out they owe me money you know what i'm saying yeah they were wrong and bro when they went to that situation they were like yo you know three ways is better than four yeah yeah the way they looked at it i wanted to give me the credit that i deserved you know i'm saying period yeah yeah my mentality was i'm gonna make this stupid motherfucker look like he's the man sacrifice my own shit you feel me yeah and their mentality was like nah we caught this nigga out we didn't pay and like no like it was just it was just stupid on smack's part bro like you know you know what business you go before i got there and you was falling off the cliff why would you revert to what you was because when i got there this is the highest you've ever been yeah they was with me not with them niggas yeah but but"], 'frequency_penalty': 0, 'logit_bias': {}, 'max_tokens': 256, 'n': 1, 'presence_penalty': 0, 'temperature': 0.7, 'top_p': 1}}
DEBUG:openai._base_client:Sending HTTP Request: POST https://api.openai.com/v1/completions
DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>
DEBUG:httpcore.http11:send_request_headers.complete
DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>
DEBUG:httpcore.http11:send_request_body.complete
DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>
DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Mon, 08 Jul 2024 03:22:43 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-allow-origin', b'*'), (b'Cache-Control', b'no-cache, must-revalidate'), (b'openai-model', b'gpt-3.5-turbo-instruct'), (b'openai-organization', b'machinekingsmedia'), (b'openai-processing-ms', b'612'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains'), (b'x-ratelimit-limit-requests', b'3500'), (b'x-ratelimit-limit-tokens', b'90000'), (b'x-ratelimit-remaining-requests', b'3499'), (b'x-ratelimit-remaining-tokens', b'89500'), (b'x-ratelimit-reset-requests', b'17ms'), (b'x-ratelimit-reset-tokens', b'333ms'), (b'x-request-id', b'req_73356da6221cb0813b829da8e51563bd'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'89fcf39358e182e1-IAD'), (b'Content-Encoding', b'br'), (b'alt-svc', b'h3=":443"; ma=86400')])
INFO:httpx:HTTP Request: POST https://api.openai.com/v1/completions "HTTP/1.1 200 OK"
DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>
DEBUG:httpcore.http11:receive_response_body.complete
DEBUG:httpcore.http11:response_closed.started
DEBUG:httpcore.http11:response_closed.complete
DEBUG:openai._base_client:HTTP Response: POST https://api.openai.com/v1/completions "200 OK" Headers({'date': 'Mon, 08 Jul 2024 03:22:43 GMT', 'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'access-control-allow-origin': '*', 'cache-control': 'no-cache, must-revalidate', 'openai-model': 'gpt-3.5-turbo-instruct', 'openai-organization': 'machinekingsmedia', 'openai-processing-ms': '612', 'openai-version': '2020-10-01', 'strict-transport-security': 'max-age=31536000; includeSubDomains', 'x-ratelimit-limit-requests': '3500', 'x-ratelimit-limit-tokens': '90000', 'x-ratelimit-remaining-requests': '3499', 'x-ratelimit-remaining-tokens': '89500', 'x-ratelimit-reset-requests': '17ms', 'x-ratelimit-reset-tokens': '333ms', 'x-request-id': 'req_73356da6221cb0813b829da8e51563bd', 'cf-cache-status': 'DYNAMIC', 'server': 'cloudflare', 'cf-ray': '89fcf39358e182e1-IAD', 'content-encoding': 'br', 'alt-svc': 'h3=":443"; ma=86400'})
DEBUG:openai._base_client:request_id: req_73356da6221cb0813b829da8e51563bd
DEBUG:openai._base_client:Request options: {'method': 'post', 'url': '/completions', 'files': None, 'json_data': {'model': 'gpt-3.5-turbo-instruct', 'prompt': ["Summarize the following chapter in one sentence: just like just like with uh just like with the dallas cowboys um yeah jerry jerry jerry jones thought that you know he could he could run it was the it was the logo the star whatever he could do it without jimmy johnson and um and they haven't been there since i was so happy they ain't advancing the playoff this year too so we can still run with that it is bro it's like when ufc got bored they didn't give it a thing of white in a way it was the cog he was the yeah but he was the face though he was he was the face i remember too when you were wrong i was one of the faces it was me and smack they asked me to fall back because i was out shining him yeah i remember that but smack wasn't really out there talking to the bloggers you used to go on angry fan and big up smack used to be in here i did so much shit that people don't even know that's what i'm saying like it's just it's just if you see them they don't even know what to do"], 'frequency_penalty': 0, 'logit_bias': {}, 'max_tokens': 256, 'n': 1, 'presence_penalty': 0, 'temperature': 0.7, 'top_p': 1}}
DEBUG:openai._base_client:Sending HTTP Request: POST https://api.openai.com/v1/completions
DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>
DEBUG:httpcore.http11:send_request_headers.complete
DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>
DEBUG:httpcore.http11:send_request_body.complete
DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>
DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Mon, 08 Jul 2024 03:22:45 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-allow-origin', b'*'), (b'Cache-Control', b'no-cache, must-revalidate'), (b'openai-model', b'gpt-3.5-turbo-instruct'), (b'openai-organization', b'machinekingsmedia'), (b'openai-processing-ms', b'1022'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains'), (b'x-ratelimit-limit-requests', b'3500'), (b'x-ratelimit-limit-tokens', b'90000'), (b'x-ratelimit-remaining-requests', b'3499'), (b'x-ratelimit-remaining-tokens', b'89497'), (b'x-ratelimit-reset-requests', b'17ms'), (b'x-ratelimit-reset-tokens', b'335ms'), (b'x-request-id', b'req_cbb9b9c30592f42e30e1aee4e360d5bd'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'89fcf397ce1a82e1-IAD'), (b'Content-Encoding', b'br'), (b'alt-svc', b'h3=":443"; ma=86400')])
INFO:httpx:HTTP Request: POST https://api.openai.com/v1/completions "HTTP/1.1 200 OK"
DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>
DEBUG:httpcore.http11:receive_response_body.complete
DEBUG:httpcore.http11:response_closed.started
DEBUG:httpcore.http11:response_closed.complete
DEBUG:openai._base_client:HTTP Response: POST https://api.openai.com/v1/completions "200 OK" Headers({'date': 'Mon, 08 Jul 2024 03:22:45 GMT', 'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'access-control-allow-origin': '*', 'cache-control': 'no-cache, must-revalidate', 'openai-model': 'gpt-3.5-turbo-instruct', 'openai-organization': 'machinekingsmedia', 'openai-processing-ms': '1022', 'openai-version': '2020-10-01', 'strict-transport-security': 'max-age=31536000; includeSubDomains', 'x-ratelimit-limit-requests': '3500', 'x-ratelimit-limit-tokens': '90000', 'x-ratelimit-remaining-requests': '3499', 'x-ratelimit-remaining-tokens': '89497', 'x-ratelimit-reset-requests': '17ms', 'x-ratelimit-reset-tokens': '335ms', 'x-request-id': 'req_cbb9b9c30592f42e30e1aee4e360d5bd', 'cf-cache-status': 'DYNAMIC', 'server': 'cloudflare', 'cf-ray': '89fcf397ce1a82e1-IAD', 'content-encoding': 'br', 'alt-svc': 'h3=":443"; ma=86400'})
DEBUG:openai._base_client:request_id: req_cbb9b9c30592f42e30e1aee4e360d5bd
DEBUG:openai._base_client:Request options: {'method': 'post', 'url': '/completions', 'files': None, 'json_data': {'model': 'gpt-3.5-turbo-instruct', 'prompt': ["Summarize the following chapter in one sentence: they don't know what to say they don't even like yo bro like even in business meetings like bro like i have ideas right now that'll that'll make the shit go crazy but i'd never fuck with you know the amount of money you would have to pay me just to resurrect your company yeah but and i've been hearing the adam rumors getting louder this week too about him retiring like as if he made he's done i told you that shit yeah you did tell me that but i didn't see an official announcement but everybody's talking about it no he was never gonna make an announcement he was just like he's gonna do the last event which was uh that's the full circle oh max out back on his last event okay he's not even there oh and he's just gonna do it he was like he's just tired the same shit i said tired of the culture you're putting up money he said he made enough money in the last event where he's just like yo nigga it's not worth the risk why would i put it"], 'frequency_penalty': 0, 'logit_bias': {}, 'max_tokens': 256, 'n': 1, 'presence_penalty': 0, 'temperature': 0.7, 'top_p': 1}}
DEBUG:openai._base_client:Sending HTTP Request: POST https://api.openai.com/v1/completions
DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>
DEBUG:httpcore.http11:send_request_headers.complete
DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>
DEBUG:httpcore.http11:send_request_body.complete
DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>
DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Mon, 08 Jul 2024 03:22:46 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-allow-origin', b'*'), (b'Cache-Control', b'no-cache, must-revalidate'), (b'openai-model', b'gpt-3.5-turbo-instruct'), (b'openai-organization', b'machinekingsmedia'), (b'openai-processing-ms', b'601'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains'), (b'x-ratelimit-limit-requests', b'3500'), (b'x-ratelimit-limit-tokens', b'90000'), (b'x-ratelimit-remaining-requests', b'3499'), (b'x-ratelimit-remaining-tokens', b'89496'), (b'x-ratelimit-reset-requests', b'17ms'), (b'x-ratelimit-reset-tokens', b'336ms'), (b'x-request-id', b'req_3c8a8c5d28f26767d7a530790529e3a5'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'89fcf3a1c84a82e1-IAD'), (b'Content-Encoding', b'br'), (b'alt-svc', b'h3=":443"; ma=86400')])
INFO:httpx:HTTP Request: POST https://api.openai.com/v1/completions "HTTP/1.1 200 OK"
DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>
DEBUG:httpcore.http11:receive_response_body.complete
DEBUG:httpcore.http11:response_closed.started
DEBUG:httpcore.http11:response_closed.complete
DEBUG:openai._base_client:HTTP Response: POST https://api.openai.com/v1/completions "200 OK" Headers({'date': 'Mon, 08 Jul 2024 03:22:46 GMT', 'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'access-control-allow-origin': '*', 'cache-control': 'no-cache, must-revalidate', 'openai-model': 'gpt-3.5-turbo-instruct', 'openai-organization': 'machinekingsmedia', 'openai-processing-ms': '601', 'openai-version': '2020-10-01', 'strict-transport-security': 'max-age=31536000; includeSubDomains', 'x-ratelimit-limit-requests': '3500', 'x-ratelimit-limit-tokens': '90000', 'x-ratelimit-remaining-requests': '3499', 'x-ratelimit-remaining-tokens': '89496', 'x-ratelimit-reset-requests': '17ms', 'x-ratelimit-reset-tokens': '336ms', 'x-request-id': 'req_3c8a8c5d28f26767d7a530790529e3a5', 'cf-cache-status': 'DYNAMIC', 'server': 'cloudflare', 'cf-ray': '89fcf3a1c84a82e1-IAD', 'content-encoding': 'br', 'alt-svc': 'h3=":443"; ma=86400'})
DEBUG:openai._base_client:request_id: req_3c8a8c5d28f26767d7a530790529e3a5
DEBUG:openai._base_client:Request options: {'method': 'post', 'url': '/completions', 'files': None, 'json_data': {'model': 'gpt-3.5-turbo-instruct', 'prompt': ["Summarize the following chapter in one sentence: back in bro these i think the usd shop in sugar selling chicken out of his house wait a minute wait up i thought that let's get one wing straight was a store nigga it's in his crib he's selling chicken plates out of his house my nigga no no i saw somebody say congratulations and then i was okay he got a little store or whatever chicken wings out of his crib no i no all right look for the location yeah i'm going to check the location i'm gonna look that up i'm gonna look that up bro yo dog i'm not even bullshitting him he's selling when he don't have a crib he's selling him out of where he's staying out of the crib just selling wings the nigga got the shotgun sauce like it's crazy let's get one wing straight i i was i was like i was i i imagine like like since nunu got the teeth store i thought he was next up hold on my son is an animal dog it's okay it's okay it's all right"], 'frequency_penalty': 0, 'logit_bias': {}, 'max_tokens': 256, 'n': 1, 'presence_penalty': 0, 'temperature': 0.7, 'top_p': 1}}
DEBUG:openai._base_client:Sending HTTP Request: POST https://api.openai.com/v1/completions
DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>
DEBUG:httpcore.http11:send_request_headers.complete
DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>
DEBUG:httpcore.http11:send_request_body.complete
DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>
DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Mon, 08 Jul 2024 03:22:47 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-allow-origin', b'*'), (b'Cache-Control', b'no-cache, must-revalidate'), (b'openai-model', b'gpt-3.5-turbo-instruct'), (b'openai-organization', b'machinekingsmedia'), (b'openai-processing-ms', b'675'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains'), (b'x-ratelimit-limit-requests', b'3500'), (b'x-ratelimit-limit-tokens', b'90000'), (b'x-ratelimit-remaining-requests', b'3499'), (b'x-ratelimit-remaining-tokens', b'89511'), (b'x-ratelimit-reset-requests', b'17ms'), (b'x-ratelimit-reset-tokens', b'326ms'), (b'x-request-id', b'req_a779e3b66355bc861ccd02de6a0eaa19'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'89fcf3a92fc282e1-IAD'), (b'Content-Encoding', b'br'), (b'alt-svc', b'h3=":443"; ma=86400')])
INFO:httpx:HTTP Request: POST https://api.openai.com/v1/completions "HTTP/1.1 200 OK"
DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>
DEBUG:httpcore.http11:receive_response_body.complete
DEBUG:httpcore.http11:response_closed.started
DEBUG:httpcore.http11:response_closed.complete
DEBUG:openai._base_client:HTTP Response: POST https://api.openai.com/v1/completions "200 OK" Headers({'date': 'Mon, 08 Jul 2024 03:22:47 GMT', 'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'access-control-allow-origin': '*', 'cache-control': 'no-cache, must-revalidate', 'openai-model': 'gpt-3.5-turbo-instruct', 'openai-organization': 'machinekingsmedia', 'openai-processing-ms': '675', 'openai-version': '2020-10-01', 'strict-transport-security': 'max-age=31536000; includeSubDomains', 'x-ratelimit-limit-requests': '3500', 'x-ratelimit-limit-tokens': '90000', 'x-ratelimit-remaining-requests': '3499', 'x-ratelimit-remaining-tokens': '89511', 'x-ratelimit-reset-requests': '17ms', 'x-ratelimit-reset-tokens': '326ms', 'x-request-id': 'req_a779e3b66355bc861ccd02de6a0eaa19', 'cf-cache-status': 'DYNAMIC', 'server': 'cloudflare', 'cf-ray': '89fcf3a92fc282e1-IAD', 'content-encoding': 'br', 'alt-svc': 'h3=":443"; ma=86400'})
DEBUG:openai._base_client:request_id: req_a779e3b66355bc861ccd02de6a0eaa19
DEBUG:openai._base_client:Request options: {'method': 'post', 'url': '/completions', 'files': None, 'json_data': {'model': 'gpt-3.5-turbo-instruct', 'prompt': ["Summarize the following chapter in one sentence: i'm gonna check up on that noise i might have to do a little something on that all you gotta do is be like hey shotgun where's the location at i'm trying to pull up to buy some wings no no i ain't gonna ask him now i'm gonna just do my little research that's very yo bro i know uh i'm a thousand percent sure niggas call me and i started laughing i said wait what are you talking about he's selling wings he's supposed to be delivering them but he he can't so he'll be having he'll have to come to the square to pick it up wow yeah and he turning down uh the eternal he turned down the easy battle because of loyalty to url as well like he he priced himself out rather than saying no to look bad in the culture bro he didn't price myself out he was told not to do it and he was like oh i'm scared of smack and now look at him bro them niggas are broke right now them niggas they're broke i don't know"], 'frequency_penalty': 0, 'logit_bias': {}, 'max_tokens': 256, 'n': 1, 'presence_penalty': 0, 'temperature': 0.7, 'top_p': 1}}
DEBUG:openai._base_client:Sending HTTP Request: POST https://api.openai.com/v1/completions
DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>
DEBUG:httpcore.http11:send_request_headers.complete
DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>
DEBUG:httpcore.http11:send_request_body.complete
DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>
DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Mon, 08 Jul 2024 03:22:48 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-allow-origin', b'*'), (b'Cache-Control', b'no-cache, must-revalidate'), (b'openai-model', b'gpt-3.5-turbo-instruct'), (b'openai-organization', b'machinekingsmedia'), (b'openai-processing-ms', b'581'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains'), (b'x-ratelimit-limit-requests', b'3500'), (b'x-ratelimit-limit-tokens', b'90000'), (b'x-ratelimit-remaining-requests', b'3499'), (b'x-ratelimit-remaining-tokens', b'89506'), (b'x-ratelimit-reset-requests', b'17ms'), (b'x-ratelimit-reset-tokens', b'328ms'), (b'x-request-id', b'req_5e36ee7a0f8354cdc8529072d81dc0eb'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'89fcf3b0e87b82e1-IAD'), (b'Content-Encoding', b'br'), (b'alt-svc', b'h3=":443"; ma=86400')])
INFO:httpx:HTTP Request: POST https://api.openai.com/v1/completions "HTTP/1.1 200 OK"
DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>
DEBUG:httpcore.http11:receive_response_body.complete
DEBUG:httpcore.http11:response_closed.started
DEBUG:httpcore.http11:response_closed.complete
DEBUG:openai._base_client:HTTP Response: POST https://api.openai.com/v1/completions "200 OK" Headers({'date': 'Mon, 08 Jul 2024 03:22:48 GMT', 'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'access-control-allow-origin': '*', 'cache-control': 'no-cache, must-revalidate', 'openai-model': 'gpt-3.5-turbo-instruct', 'openai-organization': 'machinekingsmedia', 'openai-processing-ms': '581', 'openai-version': '2020-10-01', 'strict-transport-security': 'max-age=31536000; includeSubDomains', 'x-ratelimit-limit-requests': '3500', 'x-ratelimit-limit-tokens': '90000', 'x-ratelimit-remaining-requests': '3499', 'x-ratelimit-remaining-tokens': '89506', 'x-ratelimit-reset-requests': '17ms', 'x-ratelimit-reset-tokens': '328ms', 'x-request-id': 'req_5e36ee7a0f8354cdc8529072d81dc0eb', 'cf-cache-status': 'DYNAMIC', 'server': 'cloudflare', 'cf-ray': '89fcf3b0e87b82e1-IAD', 'content-encoding': 'br', 'alt-svc': 'h3=":443"; ma=86400'})
DEBUG:openai._base_client:request_id: req_5e36ee7a0f8354cdc8529072d81dc0eb
DEBUG:openai._base_client:Request options: {'method': 'post', 'url': '/completions', 'files': None, 'json_data': {'model': 'gpt-3.5-turbo-instruct', 'prompt': ["Summarize the following chapter in one sentence: how bad i was gonna eat i heard i heard that nigga tate rockets back at his well he's been back at his mama crib but he's in the basement just chilling like this they got me a cake though yeah and sent sending them to the bahamas or something right come on that's how you satisfy that's how you satisfy a certain section of society yeah you know just a bit of your style you could say that's how you that's how you satisfy them man and and also like like we discussed many many times before um the the blind loyalty to the brand is it's crazy and and i think it has a little bit to do with nostalgia um and the work that you put in and the glory days of old not what's going on right now but the glory days of old but but this app situation just seems like the worst possible thing for the culture as a whole and their mentality it's not even the app"], 'frequency_penalty': 0, 'logit_bias': {}, 'max_tokens': 256, 'n': 1, 'presence_penalty': 0, 'temperature': 0.7, 'top_p': 1}}
DEBUG:openai._base_client:Sending HTTP Request: POST https://api.openai.com/v1/completions
DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>
DEBUG:httpcore.http11:send_request_headers.complete
DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>
DEBUG:httpcore.http11:send_request_body.complete
DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>
DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Mon, 08 Jul 2024 03:22:49 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-allow-origin', b'*'), (b'Cache-Control', b'no-cache, must-revalidate'), (b'openai-model', b'gpt-3.5-turbo-instruct'), (b'openai-organization', b'machinekingsmedia'), (b'openai-processing-ms', b'701'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains'), (b'x-ratelimit-limit-requests', b'3500'), (b'x-ratelimit-limit-tokens', b'90000'), (b'x-ratelimit-remaining-requests', b'3499'), (b'x-ratelimit-remaining-tokens', b'89520'), (b'x-ratelimit-reset-requests', b'17ms'), (b'x-ratelimit-reset-tokens', b'320ms'), (b'x-request-id', b'req_46fdd54324eb5cc389c31545036f29e4'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'89fcf3b52c7482e1-IAD'), (b'Content-Encoding', b'br'), (b'alt-svc', b'h3=":443"; ma=86400')])
INFO:httpx:HTTP Request: POST https://api.openai.com/v1/completions "HTTP/1.1 200 OK"
DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>
DEBUG:httpcore.http11:receive_response_body.complete
DEBUG:httpcore.http11:response_closed.started
DEBUG:httpcore.http11:response_closed.complete
DEBUG:openai._base_client:HTTP Response: POST https://api.openai.com/v1/completions "200 OK" Headers({'date': 'Mon, 08 Jul 2024 03:22:49 GMT', 'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'access-control-allow-origin': '*', 'cache-control': 'no-cache, must-revalidate', 'openai-model': 'gpt-3.5-turbo-instruct', 'openai-organization': 'machinekingsmedia', 'openai-processing-ms': '701', 'openai-version': '2020-10-01', 'strict-transport-security': 'max-age=31536000; includeSubDomains', 'x-ratelimit-limit-requests': '3500', 'x-ratelimit-limit-tokens': '90000', 'x-ratelimit-remaining-requests': '3499', 'x-ratelimit-remaining-tokens': '89520', 'x-ratelimit-reset-requests': '17ms', 'x-ratelimit-reset-tokens': '320ms', 'x-request-id': 'req_46fdd54324eb5cc389c31545036f29e4', 'cf-cache-status': 'DYNAMIC', 'server': 'cloudflare', 'cf-ray': '89fcf3b52c7482e1-IAD', 'content-encoding': 'br', 'alt-svc': 'h3=":443"; ma=86400'})
DEBUG:openai._base_client:request_id: req_46fdd54324eb5cc389c31545036f29e4
DEBUG:openai._base_client:Request options: {'method': 'post', 'url': '/completions', 'files': None, 'json_data': {'model': 'gpt-3.5-turbo-instruct', 'prompt': ["Summarize the following chapter in one sentence: situation no more bro it's the fact that they don't have no backing they're not gonna pay for shit ain't nobody gonna put up no money for the niggas but they they're making their own money but they're spending it on jewelry and cars and things of that nature that um if they don't have an office that might be a reflection of them not reinvested into the business itself um you don't even need all that son you don't even need to over half of this shit these niggas are so fucking dumb like yo bro it's the ad situation they they they probably gonna try to sell off what they can if they can at this point so but what can they sell off the the the the battles somebody used the library worth it you know they already have but i just don't see them cashing out in that situation but what's hold on let me let me get my let me get these niggas no problem all right"], 'frequency_penalty': 0, 'logit_bias': {}, 'max_tokens': 256, 'n': 1, 'presence_penalty': 0, 'temperature': 0.7, 'top_p': 1}}
DEBUG:openai._base_client:Sending HTTP Request: POST https://api.openai.com/v1/completions
DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>
DEBUG:httpcore.http11:send_request_headers.complete
DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>
DEBUG:httpcore.http11:send_request_body.complete
DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>
DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Mon, 08 Jul 2024 03:22:50 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-allow-origin', b'*'), (b'Cache-Control', b'no-cache, must-revalidate'), (b'openai-model', b'gpt-3.5-turbo-instruct'), (b'openai-organization', b'machinekingsmedia'), (b'openai-processing-ms', b'715'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains'), (b'x-ratelimit-limit-requests', b'3500'), (b'x-ratelimit-limit-tokens', b'90000'), (b'x-ratelimit-remaining-requests', b'3499'), (b'x-ratelimit-remaining-tokens', b'89517'), (b'x-ratelimit-reset-requests', b'17ms'), (b'x-ratelimit-reset-tokens', b'322ms'), (b'x-request-id', b'req_6bd27cc5e1ea8322a73bfc77504c79c6'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'89fcf3bd6cad82e1-IAD'), (b'Content-Encoding', b'br'), (b'alt-svc', b'h3=":443"; ma=86400')])
INFO:httpx:HTTP Request: POST https://api.openai.com/v1/completions "HTTP/1.1 200 OK"
DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>
DEBUG:httpcore.http11:receive_response_body.complete
DEBUG:httpcore.http11:response_closed.started
DEBUG:httpcore.http11:response_closed.complete
DEBUG:openai._base_client:HTTP Response: POST https://api.openai.com/v1/completions "200 OK" Headers({'date': 'Mon, 08 Jul 2024 03:22:50 GMT', 'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'access-control-allow-origin': '*', 'cache-control': 'no-cache, must-revalidate', 'openai-model': 'gpt-3.5-turbo-instruct', 'openai-organization': 'machinekingsmedia', 'openai-processing-ms': '715', 'openai-version': '2020-10-01', 'strict-transport-security': 'max-age=31536000; includeSubDomains', 'x-ratelimit-limit-requests': '3500', 'x-ratelimit-limit-tokens': '90000', 'x-ratelimit-remaining-requests': '3499', 'x-ratelimit-remaining-tokens': '89517', 'x-ratelimit-reset-requests': '17ms', 'x-ratelimit-reset-tokens': '322ms', 'x-request-id': 'req_6bd27cc5e1ea8322a73bfc77504c79c6', 'cf-cache-status': 'DYNAMIC', 'server': 'cloudflare', 'cf-ray': '89fcf3bd6cad82e1-IAD', 'content-encoding': 'br', 'alt-svc': 'h3=":443"; ma=86400'})
DEBUG:openai._base_client:request_id: req_6bd27cc5e1ea8322a73bfc77504c79c6
DEBUG:openai._base_client:Request options: {'method': 'post', 'url': '/completions', 'files': None, 'json_data': {'model': 'gpt-3.5-turbo-instruct', 'prompt': ["Summarize the following chapter in one sentence: and like if i'm if i'm a business dude and i purchased a company and you sold me everything but what made the engine run i'm gonna feel a way yeah when you're telling them like yo this shit got no legs he bullshitting you don't think they did their own research they was just like yo we don't want to get sued and miss this shit because then what happens is you look like y'all was in it with them to kick me out yeah so they was just trying to protect their own ass but they did their own research nigga they knew that i wasn't just a scout too many people saying the same shit like come on bro let me just minimize it bro i've seen their faces i i've seen with smack down when we did the depositions i've seen all that shit on those niggas is a dummy but you know you know so you can just tell like like there's one part like i can't really talk about the deposition shit because it's not whatever but i just recall"], 'frequency_penalty': 0, 'logit_bias': {}, 'max_tokens': 256, 'n': 1, 'presence_penalty': 0, 'temperature': 0.7, 'top_p': 1}}
DEBUG:openai._base_client:Sending HTTP Request: POST https://api.openai.com/v1/completions
DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>
DEBUG:httpcore.http11:send_request_headers.complete
DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>
DEBUG:httpcore.http11:send_request_body.complete
DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>
DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Mon, 08 Jul 2024 03:22:51 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-allow-origin', b'*'), (b'Cache-Control', b'no-cache, must-revalidate'), (b'openai-model', b'gpt-3.5-turbo-instruct'), (b'openai-organization', b'machinekingsmedia'), (b'openai-processing-ms', b'714'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains'), (b'x-ratelimit-limit-requests', b'3500'), (b'x-ratelimit-limit-tokens', b'90000'), (b'x-ratelimit-remaining-requests', b'3499'), (b'x-ratelimit-remaining-tokens', b'89503'), (b'x-ratelimit-reset-requests', b'17ms'), (b'x-ratelimit-reset-tokens', b'331ms'), (b'x-request-id', b'req_0ee528cc07f96850c5524e53c5b342bf'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'89fcf3c289ca82e1-IAD'), (b'Content-Encoding', b'br'), (b'alt-svc', b'h3=":443"; ma=86400')])
INFO:httpx:HTTP Request: POST https://api.openai.com/v1/completions "HTTP/1.1 200 OK"
DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>
DEBUG:httpcore.http11:receive_response_body.complete
DEBUG:httpcore.http11:response_closed.started
DEBUG:httpcore.http11:response_closed.complete
DEBUG:openai._base_client:HTTP Response: POST https://api.openai.com/v1/completions "200 OK" Headers({'date': 'Mon, 08 Jul 2024 03:22:51 GMT', 'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'access-control-allow-origin': '*', 'cache-control': 'no-cache, must-revalidate', 'openai-model': 'gpt-3.5-turbo-instruct', 'openai-organization': 'machinekingsmedia', 'openai-processing-ms': '714', 'openai-version': '2020-10-01', 'strict-transport-security': 'max-age=31536000; includeSubDomains', 'x-ratelimit-limit-requests': '3500', 'x-ratelimit-limit-tokens': '90000', 'x-ratelimit-remaining-requests': '3499', 'x-ratelimit-remaining-tokens': '89503', 'x-ratelimit-reset-requests': '17ms', 'x-ratelimit-reset-tokens': '331ms', 'x-request-id': 'req_0ee528cc07f96850c5524e53c5b342bf', 'cf-cache-status': 'DYNAMIC', 'server': 'cloudflare', 'cf-ray': '89fcf3c289ca82e1-IAD', 'content-encoding': 'br', 'alt-svc': 'h3=":443"; ma=86400'})
DEBUG:openai._base_client:request_id: req_0ee528cc07f96850c5524e53c5b342bf
DEBUG:openai._base_client:Request options: {'method': 'post', 'url': '/completions', 'files': None, 'json_data': {'model': 'gpt-3.5-turbo-instruct', 'prompt': ["Summarize the following chapter in one sentence: yeah don't don't speak on it if you can't speak on it now because i am recording yeah i'm recording now i was gonna say like i could just see it bro i could see it in his face specifically like i know troy and i know what moves troy i know what motivates him and it's it's all stupid shit like you probably say nigga shit you're in your mid-40s my nigga you worried about a chain and a fucking 300 shirt for 500 whatever the fuck you pay for those shirts yeah yeah you can see the way that they dress like the way they dress as of late like they are the accessorized chains yeah you can kind of see that yeah you don't see fucking jay-z doing that like and then there's something about money that's different like when you got money you don't act like you got money you know what i'm saying yeah like it's one of those situations where you kind of like"], 'frequency_penalty': 0, 'logit_bias': {}, 'max_tokens': 256, 'n': 1, 'presence_penalty': 0, 'temperature': 0.7, 'top_p': 1}}
DEBUG:openai._base_client:Sending HTTP Request: POST https://api.openai.com/v1/completions
DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>
DEBUG:httpcore.http11:send_request_headers.complete
DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>
DEBUG:httpcore.http11:send_request_body.complete
DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>
DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Mon, 08 Jul 2024 03:22:52 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-allow-origin', b'*'), (b'Cache-Control', b'no-cache, must-revalidate'), (b'openai-model', b'gpt-3.5-turbo-instruct'), (b'openai-organization', b'machinekingsmedia'), (b'openai-processing-ms', b'521'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains'), (b'x-ratelimit-limit-requests', b'3500'), (b'x-ratelimit-limit-tokens', b'90000'), (b'x-ratelimit-remaining-requests', b'3499'), (b'x-ratelimit-remaining-tokens', b'89518'), (b'x-ratelimit-reset-requests', b'17ms'), (b'x-ratelimit-reset-tokens', b'320ms'), (b'x-request-id', b'req_8c07206a925b1274b2b8e2c67e71526a'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'89fcf3c7af2782e1-IAD'), (b'Content-Encoding', b'br'), (b'alt-svc', b'h3=":443"; ma=86400')])
INFO:httpx:HTTP Request: POST https://api.openai.com/v1/completions "HTTP/1.1 200 OK"
DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>
DEBUG:httpcore.http11:receive_response_body.complete
DEBUG:httpcore.http11:response_closed.started
DEBUG:httpcore.http11:response_closed.complete
DEBUG:openai._base_client:HTTP Response: POST https://api.openai.com/v1/completions "200 OK" Headers({'date': 'Mon, 08 Jul 2024 03:22:52 GMT', 'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'access-control-allow-origin': '*', 'cache-control': 'no-cache, must-revalidate', 'openai-model': 'gpt-3.5-turbo-instruct', 'openai-organization': 'machinekingsmedia', 'openai-processing-ms': '521', 'openai-version': '2020-10-01', 'strict-transport-security': 'max-age=31536000; includeSubDomains', 'x-ratelimit-limit-requests': '3500', 'x-ratelimit-limit-tokens': '90000', 'x-ratelimit-remaining-requests': '3499', 'x-ratelimit-remaining-tokens': '89518', 'x-ratelimit-reset-requests': '17ms', 'x-ratelimit-reset-tokens': '320ms', 'x-request-id': 'req_8c07206a925b1274b2b8e2c67e71526a', 'cf-cache-status': 'DYNAMIC', 'server': 'cloudflare', 'cf-ray': '89fcf3c7af2782e1-IAD', 'content-encoding': 'br', 'alt-svc': 'h3=":443"; ma=86400'})
DEBUG:openai._base_client:request_id: req_8c07206a925b1274b2b8e2c67e71526a
DEBUG:openai._base_client:Request options: {'method': 'post', 'url': '/completions', 'files': None, 'json_data': {'model': 'gpt-3.5-turbo-instruct', 'prompt': ["Summarize the following chapter in one sentence: they were niggas with real money and then you see a nigga like smack Logan they laugh at them because they don't even know how to like you got to show off you got but you don't show this shit off you don't how you live but but they they are a hip-hop culture though so there is that but their business as business owners that that work with these independent contractors it just seems as though from my observation of that the subscription money has got they've gotten so much reliable subscription money over the years that they kind of got comfortable and they just haven't they it seemed like they've even given up trying to innovate no i mean bro you understand bro like it's like who said who's not the best when you win a championship you gotta understand and that you gotta continue what you're doing in fact you gotta do more because not as expectation their only expectation was to to fuck bitches and wear"], 'frequency_penalty': 0, 'logit_bias': {}, 'max_tokens': 256, 'n': 1, 'presence_penalty': 0, 'temperature': 0.7, 'top_p': 1}}
DEBUG:openai._base_client:Sending HTTP Request: POST https://api.openai.com/v1/completions
DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>
DEBUG:httpcore.http11:send_request_headers.complete
DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>
DEBUG:httpcore.http11:send_request_body.complete
DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>
DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Mon, 08 Jul 2024 03:22:52 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-allow-origin', b'*'), (b'Cache-Control', b'no-cache, must-revalidate'), (b'openai-model', b'gpt-3.5-turbo-instruct'), (b'openai-organization', b'machinekingsmedia'), (b'openai-processing-ms', b'684'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains'), (b'x-ratelimit-limit-requests', b'3500'), (b'x-ratelimit-limit-tokens', b'90000'), (b'x-ratelimit-remaining-requests', b'3499'), (b'x-ratelimit-remaining-tokens', b'89503'), (b'x-ratelimit-reset-requests', b'17ms'), (b'x-ratelimit-reset-tokens', b'331ms'), (b'x-request-id', b'req_0b52df9f1b7958c3f206a2e140323b1f'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'89fcf3cbab3282e1-IAD'), (b'Content-Encoding', b'br'), (b'alt-svc', b'h3=":443"; ma=86400')])
INFO:httpx:HTTP Request: POST https://api.openai.com/v1/completions "HTTP/1.1 200 OK"
DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>
DEBUG:httpcore.http11:receive_response_body.complete
DEBUG:httpcore.http11:response_closed.started
DEBUG:httpcore.http11:response_closed.complete
DEBUG:openai._base_client:HTTP Response: POST https://api.openai.com/v1/completions "200 OK" Headers({'date': 'Mon, 08 Jul 2024 03:22:52 GMT', 'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'access-control-allow-origin': '*', 'cache-control': 'no-cache, must-revalidate', 'openai-model': 'gpt-3.5-turbo-instruct', 'openai-organization': 'machinekingsmedia', 'openai-processing-ms': '684', 'openai-version': '2020-10-01', 'strict-transport-security': 'max-age=31536000; includeSubDomains', 'x-ratelimit-limit-requests': '3500', 'x-ratelimit-limit-tokens': '90000', 'x-ratelimit-remaining-requests': '3499', 'x-ratelimit-remaining-tokens': '89503', 'x-ratelimit-reset-requests': '17ms', 'x-ratelimit-reset-tokens': '331ms', 'x-request-id': 'req_0b52df9f1b7958c3f206a2e140323b1f', 'cf-cache-status': 'DYNAMIC', 'server': 'cloudflare', 'cf-ray': '89fcf3cbab3282e1-IAD', 'content-encoding': 'br', 'alt-svc': 'h3=":443"; ma=86400'})
DEBUG:openai._base_client:request_id: req_0b52df9f1b7958c3f206a2e140323b1f
DEBUG:openai._base_client:Request options: {'method': 'post', 'url': '/completions', 'files': None, 'json_data': {'model': 'gpt-3.5-turbo-instruct', 'prompt': ["Summarize the following chapter in one sentence: jewelry that's they should yo i'm gonna use that that championship shit for that that um that cowboys that cowboys video that i'm gonna make yeah that's a fire analogy i'm gonna use that analogy for them winning a championship with you pretty much and etching themselves in history with you norbs etching themselves in history but then the expectations are there now and now that without you because yo yo i paid i called you tyrian lannister like you you fuck hold on you watch game of thrones i started through i never finished oh okay okay but you still should watch that shit though norbs because that shit that that's that that's a good ass blog that's a good ass blog and i'll watch i'll hit you back i'm gonna get this nigga started i'll watch it right now all right i'm gonna send it uh maybe like nothing i make is over 12 minutes so it's probably like eight or nine minutes maybe i'll send it to you okay i gotta put this nigga he follow me"], 'frequency_penalty': 0, 'logit_bias': {}, 'max_tokens': 256, 'n': 1, 'presence_penalty': 0, 'temperature': 0.7, 'top_p': 1}}
DEBUG:openai._base_client:Sending HTTP Request: POST https://api.openai.com/v1/completions
DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>
DEBUG:httpcore.http11:send_request_headers.complete
DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>
DEBUG:httpcore.http11:send_request_body.complete
DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>
DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Mon, 08 Jul 2024 03:22:54 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-allow-origin', b'*'), (b'Cache-Control', b'no-cache, must-revalidate'), (b'openai-model', b'gpt-3.5-turbo-instruct'), (b'openai-organization', b'machinekingsmedia'), (b'openai-processing-ms', b'1217'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains'), (b'x-ratelimit-limit-requests', b'3500'), (b'x-ratelimit-limit-tokens', b'90000'), (b'x-ratelimit-remaining-requests', b'3499'), (b'x-ratelimit-remaining-tokens', b'89494'), (b'x-ratelimit-reset-requests', b'17ms'), (b'x-ratelimit-reset-tokens', b'336ms'), (b'x-request-id', b'req_2e3dae431518aef421f132c825828f6a'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'89fcf3d09f7082e1-IAD'), (b'Content-Encoding', b'br'), (b'alt-svc', b'h3=":443"; ma=86400')])
INFO:httpx:HTTP Request: POST https://api.openai.com/v1/completions "HTTP/1.1 200 OK"
DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>
DEBUG:httpcore.http11:receive_response_body.complete
DEBUG:httpcore.http11:response_closed.started
DEBUG:httpcore.http11:response_closed.complete
DEBUG:openai._base_client:HTTP Response: POST https://api.openai.com/v1/completions "200 OK" Headers({'date': 'Mon, 08 Jul 2024 03:22:54 GMT', 'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'access-control-allow-origin': '*', 'cache-control': 'no-cache, must-revalidate', 'openai-model': 'gpt-3.5-turbo-instruct', 'openai-organization': 'machinekingsmedia', 'openai-processing-ms': '1217', 'openai-version': '2020-10-01', 'strict-transport-security': 'max-age=31536000; includeSubDomains', 'x-ratelimit-limit-requests': '3500', 'x-ratelimit-limit-tokens': '90000', 'x-ratelimit-remaining-requests': '3499', 'x-ratelimit-remaining-tokens': '89494', 'x-ratelimit-reset-requests': '17ms', 'x-ratelimit-reset-tokens': '336ms', 'x-request-id': 'req_2e3dae431518aef421f132c825828f6a', 'cf-cache-status': 'DYNAMIC', 'server': 'cloudflare', 'cf-ray': '89fcf3d09f7082e1-IAD', 'content-encoding': 'br', 'alt-svc': 'h3=":443"; ma=86400'})
DEBUG:openai._base_client:request_id: req_2e3dae431518aef421f132c825828f6a
DEBUG:openai._base_client:Request options: {'method': 'post', 'url': '/completions', 'files': None, 'json_data': {'model': 'gpt-3.5-turbo-instruct', 'prompt': ["Summarize the following chapter in one sentence: around all right my my baby girl i'm gonna be putting her to bed at any moment so um if not then i'll hit you later on tonight all right"], 'frequency_penalty': 0, 'logit_bias': {}, 'max_tokens': 256, 'n': 1, 'presence_penalty': 0, 'temperature': 0.7, 'top_p': 1}}
DEBUG:openai._base_client:Sending HTTP Request: POST https://api.openai.com/v1/completions
DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>
DEBUG:httpcore.http11:send_request_headers.complete
DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>
DEBUG:httpcore.http11:send_request_body.complete
DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>
DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Mon, 08 Jul 2024 03:22:55 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-allow-origin', b'*'), (b'Cache-Control', b'no-cache, must-revalidate'), (b'openai-model', b'gpt-3.5-turbo-instruct'), (b'openai-organization', b'machinekingsmedia'), (b'openai-processing-ms', b'358'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains'), (b'x-ratelimit-limit-requests', b'3500'), (b'x-ratelimit-limit-tokens', b'90000'), (b'x-ratelimit-remaining-requests', b'3499'), (b'x-ratelimit-remaining-tokens', b'89697'), (b'x-ratelimit-reset-requests', b'17ms'), (b'x-ratelimit-reset-tokens', b'201ms'), (b'x-request-id', b'req_de5afda83c7a87253d7e657b0eda4b63'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'89fcf3dbd9d082e1-IAD'), (b'Content-Encoding', b'br'), (b'alt-svc', b'h3=":443"; ma=86400')])
INFO:httpx:HTTP Request: POST https://api.openai.com/v1/completions "HTTP/1.1 200 OK"
DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>
DEBUG:httpcore.http11:receive_response_body.complete
DEBUG:httpcore.http11:response_closed.started
DEBUG:httpcore.http11:response_closed.complete
DEBUG:openai._base_client:HTTP Response: POST https://api.openai.com/v1/completions "200 OK" Headers({'date': 'Mon, 08 Jul 2024 03:22:55 GMT', 'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'access-control-allow-origin': '*', 'cache-control': 'no-cache, must-revalidate', 'openai-model': 'gpt-3.5-turbo-instruct', 'openai-organization': 'machinekingsmedia', 'openai-processing-ms': '358', 'openai-version': '2020-10-01', 'strict-transport-security': 'max-age=31536000; includeSubDomains', 'x-ratelimit-limit-requests': '3500', 'x-ratelimit-limit-tokens': '90000', 'x-ratelimit-remaining-requests': '3499', 'x-ratelimit-remaining-tokens': '89697', 'x-ratelimit-reset-requests': '17ms', 'x-ratelimit-reset-tokens': '201ms', 'x-request-id': 'req_de5afda83c7a87253d7e657b0eda4b63', 'cf-cache-status': 'DYNAMIC', 'server': 'cloudflare', 'cf-ray': '89fcf3dbd9d082e1-IAD', 'content-encoding': 'br', 'alt-svc': 'h3=":443"; ma=86400'})
DEBUG:openai._base_client:request_id: req_de5afda83c7a87253d7e657b0eda4b63
DEBUG:openai._base_client:Request options: {'method': 'post', 'url': '/completions', 'files': None, 'json_data': {'model': 'gpt-3.5-turbo-instruct', 'prompt': ["Provide a brief summary of the following transcript: File: 240204_1940.mp3\nRecording Date: 2024-02-04\nDuration: 00:13:13.60\nTranscription Date: 2024-07-07 06:17:21\n\n1\n00:00:00,000 --> 00:00:04,800\nput it out but so you said you the way you articulated though i really like that you said\n\n2\n00:00:04,800 --> 00:00:09,120\na cog in the machine they lost a cog in the machine yo did you see my game of thrones\n\n3\n00:00:09,120 --> 00:00:15,600\nbreakdown of the of the um the the the leagues in battle right right now how i framed it with you\n\n4\n00:00:16,719 --> 00:00:20,639\nno i gotta seriously because i don't i try not to watch none of this yeah yeah i know you said\n\n5\n00:00:20,639 --> 00:00:24,879\nyou was gonna do that too no i was i was that was a little test along with a question because\n\n6\n00:00:24,879 --> 00:00:30,959\nyou say you were stepping away from the whole negativity and all of that and yo the fact that\n\n7\n00:00:30,959 --> 00:00:37,840\nis that everybody's saying that is dying right now i think about you so much but anyway uh but\n\n8\n00:00:37,840 --> 00:00:44,880\nwhat i pointed out was how like it seems like we're starting to feel the after effects of the\n\n9\n00:00:44,880 --> 00:00:51,919\nlack of innovation from that side so you saying that the deal they made with caffeine was like\n\n10\n00:00:51,919 --> 00:00:56,959\nthem selling them a machine but a cog was missing in the machine it looked the same from the outside\n\n11\n00:00:56,959 --> 00:01:02,000\nbut on the inside an important piece was missing they made it basically look like that i had\n\n12\n00:01:02,000 --> 00:01:07,599\nnothing to do with what went down yeah they framed it like that yeah because you were a talent scout\n\n13\n00:01:08,320 --> 00:01:16,800\nyeah apparently yeah they knew they knew that that's not the case and caffeine knows it's not\n\n14\n00:01:16,800 --> 00:01:22,959\nthe case you understand and it's between me and you of course as i was going through this for three\n\n15\n00:01:22,959 --> 00:01:28,720\nyears they was sitting in on the meetings with me and my lawyers in this so why would they sit\n\n16\n00:01:28,720 --> 00:01:32,320\nif they think it was important if they they're gonna stand behind these niggas and make it look\n\n17\n00:01:32,320 --> 00:01:37,040\nlike a notice just trying to scam these niggas yeah when they realize like hold on my nigga\n\n18\n00:01:38,320 --> 00:01:43,680\nthis nigga really did all of this shit and y'all think about it if you could if you acquiring a\n\n19\n00:01:43,680 --> 00:01:50,000\ncompany and its routes and how it became successful why wouldn't you keep the same team\n\n20\n00:01:51,919 --> 00:01:57,599\nwow yeah yeah i see what you're saying i see what you're saying and i can see that fueling\n\n21\n00:01:57,599 --> 00:02:03,839\nanger amongst their ranks yeah and you gotta think about it sound like it was all a money\n\n22\n00:02:03,839 --> 00:02:07,680\nmove with them bro they tried to cut me out they owe me money you know what i'm saying yeah\n\n23\n00:02:07,680 --> 00:02:14,559\nthey were wrong and bro when they went to that situation they were like yo you know three ways\n\n24\n00:02:14,559 --> 00:02:20,559\nis better than four yeah yeah the way they looked at it i wanted to give me the credit that i\n\n25\n00:02:20,559 --> 00:02:26,720\ndeserved you know i'm saying period yeah yeah my mentality was i'm gonna make this stupid\n\n26\n00:02:26,720 --> 00:02:33,839\nmotherfucker look like he's the man sacrifice my own shit you feel me yeah and their mentality\n\n27\n00:02:33,839 --> 00:02:38,880\nwas like nah we caught this nigga out we didn't pay and like no like it was just it was just\n\n28\n00:02:38,880 --> 00:02:44,399\nstupid on smack's part bro like you know you know what business you go before i got there\n\n29\n00:02:44,399 --> 00:02:50,639\nand you was falling off the cliff why would you revert to what you was because when i got there\n\n30\n00:02:50,639 --> 00:02:58,559\nthis is the highest you've ever been yeah they was with me not with them niggas yeah but but\n\n31\n00:02:58,559 --> 00:03:07,039\njust like just like with uh just like with the dallas cowboys um yeah jerry jerry jerry jones\n\n32\n00:03:07,039 --> 00:03:11,520\nthought that you know he could he could run it was the it was the logo the star whatever he could\n\n33\n00:03:11,520 --> 00:03:16,399\ndo it without jimmy johnson and um and they haven't been there since i was so happy they\n\n34\n00:03:16,399 --> 00:03:23,199\nain't advancing the playoff this year too so we can still run with that it is bro it's like\n\n35\n00:03:23,839 --> 00:03:28,639\nwhen ufc got bored they didn't give it a thing of white in a way it was the cog he was the\n\n36\n00:03:28,639 --> 00:03:34,800\nyeah but he was the face though he was he was the face i remember too when you were wrong i was one\n\n37\n00:03:34,800 --> 00:03:39,600\nof the faces it was me and smack they asked me to fall back because i was out shining him yeah i\n\n38\n00:03:39,600 --> 00:03:45,199\nremember that but smack wasn't really out there talking to the bloggers you used to go on angry\n\n39\n00:03:45,199 --> 00:03:52,080\nfan and big up smack used to be in here i did so much shit that people don't even know that's\n\n40\n00:03:52,080 --> 00:03:57,279\nwhat i'm saying like it's just it's just if you see them they don't even know what to do\n\n41\n00:03:58,160 --> 00:04:02,399\nthey don't know what to say they don't even like yo bro like even in business meetings\n\n42\n00:04:03,039 --> 00:04:08,479\nlike bro like i have ideas right now that'll that'll make the shit go crazy but i'd never\n\n43\n00:04:08,479 --> 00:04:12,559\nfuck with you know the amount of money you would have to pay me just to resurrect your company\n\n44\n00:04:13,759 --> 00:04:20,239\nyeah but and i've been hearing the adam rumors getting louder this week too about him retiring\n\n45\n00:04:20,239 --> 00:04:26,079\nlike as if he made he's done i told you that shit yeah you did tell me that but i didn't see an\n\n46\n00:04:26,079 --> 00:04:30,799\nofficial announcement but everybody's talking about it no he was never gonna make an announcement\n\n47\n00:04:30,799 --> 00:04:35,839\nhe was just like he's gonna do the last event which was uh that's the full circle oh max out\n\n48\n00:04:35,839 --> 00:04:41,839\nback on his last event okay he's not even there oh and he's just gonna do it he was like he's just\n\n49\n00:04:41,839 --> 00:04:46,239\ntired the same shit i said tired of the culture you're putting up money he said he made enough\n\n50\n00:04:46,239 --> 00:04:50,239\nmoney in the last event where he's just like yo nigga it's not worth the risk why would i put it\n\n51\n00:04:50,239 --> 00:04:57,359\nback in bro these i think the usd shop in sugar selling chicken out of his house wait a minute\n\n52\n00:04:57,359 --> 00:05:04,480\nwait up i thought that let's get one wing straight was a store nigga it's in his crib he's selling\n\n53\n00:05:04,480 --> 00:05:12,640\nchicken plates out of his house my nigga no no i saw somebody say congratulations and then i was\n\n54\n00:05:12,640 --> 00:05:21,200\nokay he got a little store or whatever chicken wings out of his crib no i no all right look for\n\n55\n00:05:21,200 --> 00:05:27,519\nthe location yeah i'm going to check the location i'm gonna look that up i'm gonna look that up bro\n\n56\n00:05:28,079 --> 00:05:32,640\nyo dog i'm not even bullshitting him he's selling when he don't have a crib he's selling him out of\n\n57\n00:05:32,640 --> 00:05:36,320\nwhere he's staying out of the crib just selling wings the nigga got the shotgun sauce\n\n58\n00:05:36,480 --> 00:05:44,079\nlike it's crazy let's get one wing straight i i was i was like i was i i imagine like\n\n59\n00:05:44,079 --> 00:05:50,239\nlike since nunu got the teeth store i thought he was next up hold on my son is an animal dog\n\n60\n00:05:52,799 --> 00:05:55,920\nit's okay it's okay it's all right\n\n61\n00:05:57,760 --> 00:06:01,440\ni'm gonna check up on that noise i might have to do a little something on that\n\n62\n00:06:01,519 --> 00:06:06,640\nall you gotta do is be like hey shotgun where's the location at i'm trying to pull up to buy some\n\n63\n00:06:06,640 --> 00:06:11,200\nwings no no i ain't gonna ask him now i'm gonna just do my little research that's very\n\n64\n00:06:11,839 --> 00:06:16,160\nyo bro i know uh i'm a thousand percent sure niggas call me and i started laughing i said\n\n65\n00:06:16,160 --> 00:06:21,839\nwait what are you talking about he's selling wings he's supposed to be delivering them\n\n66\n00:06:21,839 --> 00:06:25,359\nbut he he can't so he'll be having he'll have to come to the square to pick it up\n\n67\n00:06:26,320 --> 00:06:34,320\nwow yeah and he turning down uh the eternal he turned down the easy battle because of loyalty\n\n68\n00:06:34,320 --> 00:06:40,079\nto url as well like he he priced himself out rather than saying no to look bad in the culture\n\n69\n00:06:40,959 --> 00:06:46,239\nbro he didn't price myself out he was told not to do it and he was like oh i'm scared of smack\n\n70\n00:06:47,200 --> 00:06:52,720\nand now look at him bro them niggas are broke right now them niggas they're broke i don't know\n\n71\n00:06:52,720 --> 00:06:57,279\nhow bad i was gonna eat i heard i heard that nigga tate rockets back at his well he's been\n\n72\n00:06:57,279 --> 00:07:02,959\nback at his mama crib but he's in the basement just chilling like this they got me a cake though\n\n73\n00:07:03,839 --> 00:07:06,239\nyeah and sent sending them to the bahamas or something right\n\n74\n00:07:07,679 --> 00:07:15,679\ncome on that's how you satisfy that's how you satisfy a certain section of society\n\n75\n00:07:16,640 --> 00:07:18,480\nyeah you know just a bit of your style you could say\n\n76\n00:07:21,760 --> 00:07:27,519\nthat's how you that's how you satisfy them man and and also like like we discussed many many\n\n77\n00:07:27,519 --> 00:07:36,079\ntimes before um the the blind loyalty to the brand is it's crazy and and i think it has a\n\n78\n00:07:36,079 --> 00:07:43,359\nlittle bit to do with nostalgia um and the work that you put in and the glory days of old not\n\n79\n00:07:43,359 --> 00:07:50,640\nwhat's going on right now but the glory days of old but but this app situation just seems like\n\n80\n00:07:50,640 --> 00:07:58,799\nthe worst possible thing for the culture as a whole and their mentality it's not even the app\n\n81\n00:07:58,799 --> 00:08:03,279\nsituation no more bro it's the fact that they don't have no backing they're not gonna pay for\n\n82\n00:08:03,279 --> 00:08:09,679\nshit ain't nobody gonna put up no money for the niggas but they they're making their own money\n\n83\n00:08:09,679 --> 00:08:15,040\nbut they're spending it on jewelry and cars and things of that nature that um if they don't have\n\n84\n00:08:15,040 --> 00:08:22,160\nan office that might be a reflection of them not reinvested into the business itself um you don't\n\n85\n00:08:22,160 --> 00:08:26,959\neven need all that son you don't even need to over half of this shit these niggas are so fucking dumb\n\n86\n00:08:27,920 --> 00:08:34,479\nlike yo bro it's the ad situation they they they probably gonna try to sell off what they can\n\n87\n00:08:34,479 --> 00:08:43,440\nif they can at this point so but what can they sell off the the the the battles\n\n88\n00:08:44,320 --> 00:08:49,119\nsomebody used the library worth it you know they already have but i just don't see them\n\n89\n00:08:49,119 --> 00:08:57,119\ncashing out in that situation but what's hold on let me let me get my let me get these niggas\n\n90\n00:08:57,919 --> 00:08:58,960\nno problem all right\n\n91\n00:09:06,239 --> 00:09:09,760\nand like if i'm if i'm a business dude and i purchased a company\n\n92\n00:09:10,559 --> 00:09:17,440\nand you sold me everything but what made the engine run i'm gonna feel a way yeah\n\n93\n00:09:18,239 --> 00:09:22,159\nwhen you're telling them like yo this shit got no legs he bullshitting you don't think they did\n\n94\n00:09:22,159 --> 00:09:29,520\ntheir own research they was just like yo we don't want to get sued and miss this shit because then\n\n95\n00:09:29,520 --> 00:09:37,440\nwhat happens is you look like y'all was in it with them to kick me out yeah so they was just\n\n96\n00:09:37,440 --> 00:09:42,159\ntrying to protect their own ass but they did their own research nigga they knew that i wasn't just\n\n97\n00:09:43,280 --> 00:09:51,280\na scout too many people saying the same shit like come on bro let me just minimize it bro\n\n98\n00:09:51,280 --> 00:09:55,760\ni've seen their faces i i've seen with smack down when we did the depositions i've seen all that\n\n99\n00:09:55,760 --> 00:10:04,320\nshit on those niggas is a dummy but you know you know so you can just tell like like there's one\n\n100\n00:10:04,320 --> 00:10:09,760\npart like i can't really talk about the deposition shit because it's not whatever but i just recall\n\n101\n00:10:09,760 --> 00:10:16,640\nyeah don't don't speak on it if you can't speak on it now because i am recording yeah i'm recording\n\n102\n00:10:17,280 --> 00:10:22,400\nnow i was gonna say like i could just see it bro i could see it in his face specifically\n\n103\n00:10:22,400 --> 00:10:28,400\nlike i know troy and i know what moves troy i know what motivates him and it's it's all\n\n104\n00:10:29,200 --> 00:10:31,440\nstupid shit like you probably say nigga shit\n\n105\n00:10:33,440 --> 00:10:36,320\nyou're in your mid-40s my nigga you worried about a chain and a fucking\n\n106\n00:10:36,880 --> 00:10:41,760\n300 shirt for 500 whatever the fuck you pay for those shirts yeah yeah you can see the way that\n\n107\n00:10:42,479 --> 00:10:48,320\nthey dress like the way they dress as of late like they are the accessorized chains yeah you\n\n108\n00:10:48,320 --> 00:10:54,000\ncan kind of see that yeah you don't see fucking jay-z doing that like and then there's something\n\n109\n00:10:54,000 --> 00:11:00,080\nabout money that's different like when you got money you don't act like you got money\n\n110\n00:11:00,640 --> 00:11:04,880\nyou know what i'm saying yeah like it's one of those situations where you kind of like\n\n111\n00:11:06,000 --> 00:11:10,640\nthey were niggas with real money and then you see a nigga like smack Logan they laugh at them because\n\n112\n00:11:11,280 --> 00:11:14,880\nthey don't even know how to like you got to show off you got but you don't show this shit off\n\n113\n00:11:16,080 --> 00:11:22,559\nyou don't how you live but but they they are a hip-hop culture though so there is that but\n\n114\n00:11:22,559 --> 00:11:27,440\ntheir business as business owners that that work with these independent contractors\n\n115\n00:11:28,400 --> 00:11:34,559\nit just seems as though from my observation of that the subscription money has got they've gotten\n\n116\n00:11:34,559 --> 00:11:40,719\nso much reliable subscription money over the years that they kind of got comfortable\n\n117\n00:11:41,599 --> 00:11:46,080\nand they just haven't they it seemed like they've even given up trying to innovate\n\n118\n00:11:47,039 --> 00:11:52,400\nno i mean bro you understand bro like it's like who said who's not the best when you win a\n\n119\n00:11:52,400 --> 00:11:59,679\nchampionship you gotta understand and that you gotta continue what you're doing in fact you\n\n120\n00:11:59,679 --> 00:12:05,520\ngotta do more because not as expectation their only expectation was to to fuck bitches and wear\n\n121\n00:12:05,520 --> 00:12:10,719\njewelry that's they should yo i'm gonna use that that championship shit for that that um that\n\n122\n00:12:10,719 --> 00:12:18,880\ncowboys that cowboys video that i'm gonna make yeah that's a fire analogy i'm gonna use that\n\n123\n00:12:18,880 --> 00:12:24,640\nanalogy for them winning a championship with you pretty much and etching themselves in history\n\n124\n00:12:25,359 --> 00:12:32,000\nwith you norbs etching themselves in history but then the expectations are there now and now that\n\n125\n00:12:32,000 --> 00:12:38,559\nwithout you because yo yo i paid i called you tyrian lannister like you you fuck hold on you\n\n126\n00:12:38,559 --> 00:12:43,919\nwatch game of thrones i started through i never finished oh okay okay but you still should watch\n\n127\n00:12:43,919 --> 00:12:49,119\nthat shit though norbs because that shit that that's that that's a good ass blog that's a good\n\n128\n00:12:49,119 --> 00:12:53,840\nass blog and i'll watch i'll hit you back i'm gonna get this nigga started i'll watch it right\n\n129\n00:12:53,840 --> 00:12:58,799\nnow all right i'm gonna send it uh maybe like nothing i make is over 12 minutes so it's probably\n\n130\n00:12:58,799 --> 00:13:03,679\nlike eight or nine minutes maybe i'll send it to you okay i gotta put this nigga he follow me\n\n131\n00:13:03,679 --> 00:13:09,280\naround all right my my baby girl i'm gonna be putting her to bed at any moment so um if not\n\n132\n00:13:09,280 --> 00:13:13,599\nthen i'll hit you later on tonight all right\n\n\n"], 'frequency_penalty': 0, 'logit_bias': {}, 'max_tokens': 256, 'n': 1, 'presence_penalty': 0, 'temperature': 0.7, 'top_p': 1}}
DEBUG:openai._base_client:Sending HTTP Request: POST https://api.openai.com/v1/completions
DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>
DEBUG:httpcore.http11:send_request_headers.complete
DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>
DEBUG:httpcore.http11:send_request_body.complete
DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>
DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 400, b'Bad Request', [(b'Date', b'Mon, 08 Jul 2024 03:22:55 GMT'), (b'Content-Type', b'application/json'), (b'Content-Length', b'294'), (b'Connection', b'keep-alive'), (b'access-control-allow-origin', b'*'), (b'openai-model', b'gpt-3.5-turbo-instruct'), (b'openai-organization', b'machinekingsmedia'), (b'openai-processing-ms', b'211'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains'), (b'x-ratelimit-limit-requests', b'3500'), (b'x-ratelimit-limit-tokens', b'90000'), (b'x-ratelimit-remaining-requests', b'3499'), (b'x-ratelimit-remaining-tokens', b'85579'), (b'x-ratelimit-reset-requests', b'17ms'), (b'x-ratelimit-reset-tokens', b'2.946s'), (b'x-request-id', b'req_d26487decd5ec2fae0095b9958ad2e3b'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'89fcf3debc6882e1-IAD'), (b'alt-svc', b'h3=":443"; ma=86400')])
INFO:httpx:HTTP Request: POST https://api.openai.com/v1/completions "HTTP/1.1 400 Bad Request"
DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>
DEBUG:httpcore.http11:receive_response_body.complete
DEBUG:httpcore.http11:response_closed.started
DEBUG:httpcore.http11:response_closed.complete
DEBUG:openai._base_client:HTTP Response: POST https://api.openai.com/v1/completions "400 Bad Request" Headers({'date': 'Mon, 08 Jul 2024 03:22:55 GMT', 'content-type': 'application/json', 'content-length': '294', 'connection': 'keep-alive', 'access-control-allow-origin': '*', 'openai-model': 'gpt-3.5-turbo-instruct', 'openai-organization': 'machinekingsmedia', 'openai-processing-ms': '211', 'openai-version': '2020-10-01', 'strict-transport-security': 'max-age=31536000; includeSubDomains', 'x-ratelimit-limit-requests': '3500', 'x-ratelimit-limit-tokens': '90000', 'x-ratelimit-remaining-requests': '3499', 'x-ratelimit-remaining-tokens': '85579', 'x-ratelimit-reset-requests': '17ms', 'x-ratelimit-reset-tokens': '2.946s', 'x-request-id': 'req_d26487decd5ec2fae0095b9958ad2e3b', 'cf-cache-status': 'DYNAMIC', 'server': 'cloudflare', 'cf-ray': '89fcf3debc6882e1-IAD', 'alt-svc': 'h3=":443"; ma=86400'})
DEBUG:openai._base_client:request_id: req_d26487decd5ec2fae0095b9958ad2e3b
DEBUG:openai._base_client:Encountered httpx.HTTPStatusError
Traceback (most recent call last):
  File "C:\Users\taskm\anaconda3\envs\taskai\lib\site-packages\openai\_base_client.py", line 1020, in _request
    response.raise_for_status()
  File "C:\Users\taskm\anaconda3\envs\taskai\lib\site-packages\httpx\_models.py", line 761, in raise_for_status
    raise HTTPStatusError(message, request=request, response=self)
httpx.HTTPStatusError: Client error '400 Bad Request' for url 'https://api.openai.com/v1/completions'
For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/400
DEBUG:openai._base_client:Not retrying
DEBUG:openai._base_client:Re-raising status error
ERROR:root:Error processing chapters: Error code: 400 - {'error': {'message': "This model's maximum context length is 4097 tokens, however you requested 5625 tokens (5369 in your prompt; 256 for the completion). Please reduce your prompt; or completion length.", 'type': 'invalid_request_error', 'param': None, 'code': None}}
Traceback (most recent call last):
  File "D:\github\chappymedium\chappie.py", line 262, in process_chapters
    result = self.chappie_processor.process_srt(srt_content)
  File "D:\github\chappymedium\chappie_processor.py", line 17, in process_srt
    overall_summary = self._generate_overall_summary(srt_content)
  File "D:\github\chappymedium\chappie_processor.py", line 72, in _generate_overall_summary
    return chain.run(transcript=srt_content)
  File "C:\Users\taskm\anaconda3\envs\taskai\lib\site-packages\langchain_core\_api\deprecation.py", line 168, in warning_emitting_wrapper
    return wrapped(*args, **kwargs)
  File "C:\Users\taskm\anaconda3\envs\taskai\lib\site-packages\langchain\chains\base.py", line 605, in run
    return self(kwargs, callbacks=callbacks, tags=tags, metadata=metadata)[
  File "C:\Users\taskm\anaconda3\envs\taskai\lib\site-packages\langchain_core\_api\deprecation.py", line 168, in warning_emitting_wrapper
    return wrapped(*args, **kwargs)
  File "C:\Users\taskm\anaconda3\envs\taskai\lib\site-packages\langchain\chains\base.py", line 383, in __call__
    return self.invoke(
  File "C:\Users\taskm\anaconda3\envs\taskai\lib\site-packages\langchain\chains\base.py", line 166, in invoke
    raise e
  File "C:\Users\taskm\anaconda3\envs\taskai\lib\site-packages\langchain\chains\base.py", line 156, in invoke
    self._call(inputs, run_manager=run_manager)
  File "C:\Users\taskm\anaconda3\envs\taskai\lib\site-packages\langchain\chains\llm.py", line 127, in _call
    response = self.generate([inputs], run_manager=run_manager)
  File "C:\Users\taskm\anaconda3\envs\taskai\lib\site-packages\langchain\chains\llm.py", line 139, in generate
    return self.llm.generate_prompt(
  File "C:\Users\taskm\anaconda3\envs\taskai\lib\site-packages\langchain_core\language_models\llms.py", line 633, in generate_prompt
    return self.generate(prompt_strings, stop=stop, callbacks=callbacks, **kwargs)
  File "C:\Users\taskm\anaconda3\envs\taskai\lib\site-packages\langchain_core\language_models\llms.py", line 803, in generate
    output = self._generate_helper(
  File "C:\Users\taskm\anaconda3\envs\taskai\lib\site-packages\langchain_core\language_models\llms.py", line 670, in _generate_helper
    raise e
  File "C:\Users\taskm\anaconda3\envs\taskai\lib\site-packages\langchain_core\language_models\llms.py", line 657, in _generate_helper
    self._generate(
  File "C:\Users\taskm\anaconda3\envs\taskai\lib\site-packages\langchain_openai\llms\base.py", line 353, in _generate
    response = self.client.create(prompt=_prompts, **params)
  File "C:\Users\taskm\anaconda3\envs\taskai\lib\site-packages\openai\_utils\_utils.py", line 277, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\taskm\anaconda3\envs\taskai\lib\site-packages\openai\resources\completions.py", line 528, in create
    return self._post(
  File "C:\Users\taskm\anaconda3\envs\taskai\lib\site-packages\openai\_base_client.py", line 1261, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "C:\Users\taskm\anaconda3\envs\taskai\lib\site-packages\openai\_base_client.py", line 942, in request
    return self._request(
  File "C:\Users\taskm\anaconda3\envs\taskai\lib\site-packages\openai\_base_client.py", line 1041, in _request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "This model's maximum context length is 4097 tokens, however you requested 5625 tokens (5369 in your prompt; 256 for the completion). Please reduce your prompt; or completion length.", 'type': 'invalid_request_error', 'param': None, 'code': None}}
DEBUG:numba.core.byteflow:bytecode dump:
>          0	NOP(arg=None, lineno=1141)
           2	LOAD_FAST(arg=0, lineno=1144)
           4	LOAD_CONST(arg=1, lineno=1144)
           6	BINARY_SUBSCR(arg=None, lineno=1144)
           8	STORE_FAST(arg=3, lineno=1144)
          10	LOAD_FAST(arg=1, lineno=1145)
          12	UNARY_NEGATIVE(arg=None, lineno=1145)
          14	LOAD_FAST(arg=3, lineno=1145)
          16	DUP_TOP(arg=None, lineno=1145)
          18	ROT_THREE(arg=None, lineno=1145)
          20	COMPARE_OP(arg=1, lineno=1145)
          22	POP_JUMP_IF_FALSE(arg=32, lineno=1145)
          24	LOAD_FAST(arg=1, lineno=1145)
          26	COMPARE_OP(arg=1, lineno=1145)
          28	POP_JUMP_IF_FALSE(arg=40, lineno=1145)
          30	JUMP_FORWARD(arg=4, lineno=1145)
>         32	POP_TOP(arg=None, lineno=1145)
          34	JUMP_FORWARD(arg=4, lineno=1145)
>         36	LOAD_CONST(arg=1, lineno=1146)
          38	STORE_FAST(arg=3, lineno=1146)
>         40	LOAD_FAST(arg=0, lineno=1148)
          42	LOAD_CONST(arg=2, lineno=1148)
          44	BINARY_SUBSCR(arg=None, lineno=1148)
          46	STORE_FAST(arg=4, lineno=1148)
          48	LOAD_FAST(arg=1, lineno=1149)
          50	UNARY_NEGATIVE(arg=None, lineno=1149)
          52	LOAD_FAST(arg=4, lineno=1149)
          54	DUP_TOP(arg=None, lineno=1149)
          56	ROT_THREE(arg=None, lineno=1149)
          58	COMPARE_OP(arg=1, lineno=1149)
          60	POP_JUMP_IF_FALSE(arg=70, lineno=1149)
          62	LOAD_FAST(arg=1, lineno=1149)
          64	COMPARE_OP(arg=1, lineno=1149)
          66	POP_JUMP_IF_FALSE(arg=78, lineno=1149)
          68	JUMP_FORWARD(arg=4, lineno=1149)
>         70	POP_TOP(arg=None, lineno=1149)
          72	JUMP_FORWARD(arg=4, lineno=1149)
>         74	LOAD_CONST(arg=1, lineno=1150)
          76	STORE_FAST(arg=4, lineno=1150)
>         78	LOAD_FAST(arg=2, lineno=1152)
          80	POP_JUMP_IF_FALSE(arg=102, lineno=1152)
          82	LOAD_GLOBAL(arg=0, lineno=1153)
          84	LOAD_METHOD(arg=1, lineno=1153)
          86	LOAD_FAST(arg=3, lineno=1153)
          88	CALL_METHOD(arg=1, lineno=1153)
          90	LOAD_GLOBAL(arg=0, lineno=1153)
          92	LOAD_METHOD(arg=1, lineno=1153)
          94	LOAD_FAST(arg=4, lineno=1153)
          96	CALL_METHOD(arg=1, lineno=1153)
          98	COMPARE_OP(arg=3, lineno=1153)
         100	RETURN_VALUE(arg=None, lineno=1153)
>        102	LOAD_GLOBAL(arg=0, lineno=1155)
         104	LOAD_METHOD(arg=2, lineno=1155)
         106	LOAD_FAST(arg=3, lineno=1155)
         108	CALL_METHOD(arg=1, lineno=1155)
         110	LOAD_GLOBAL(arg=0, lineno=1155)
         112	LOAD_METHOD(arg=2, lineno=1155)
         114	LOAD_FAST(arg=4, lineno=1155)
         116	CALL_METHOD(arg=1, lineno=1155)
         118	COMPARE_OP(arg=3, lineno=1155)
         120	RETURN_VALUE(arg=None, lineno=1155)
         122	LOAD_CONST(arg=3, lineno=1155)
         124	RETURN_VALUE(arg=None, lineno=1155)
DEBUG:numba.core.byteflow:pending: deque([State(pc_initial=0 nstack_initial=0)])
DEBUG:numba.core.byteflow:stack: []
DEBUG:numba.core.byteflow:state.pc_initial: State(pc_initial=0 nstack_initial=0)
DEBUG:numba.core.byteflow:dispatch pc=0, inst=NOP(arg=None, lineno=1141)
DEBUG:numba.core.byteflow:stack []
DEBUG:numba.core.byteflow:dispatch pc=2, inst=LOAD_FAST(arg=0, lineno=1144)
DEBUG:numba.core.byteflow:stack []
DEBUG:numba.core.byteflow:dispatch pc=4, inst=LOAD_CONST(arg=1, lineno=1144)
DEBUG:numba.core.byteflow:stack ['$x2.0']
DEBUG:numba.core.byteflow:dispatch pc=6, inst=BINARY_SUBSCR(arg=None, lineno=1144)
DEBUG:numba.core.byteflow:stack ['$x2.0', '$const4.1']
DEBUG:numba.core.byteflow:dispatch pc=8, inst=STORE_FAST(arg=3, lineno=1144)
DEBUG:numba.core.byteflow:stack ['$6binary_subscr.2']
DEBUG:numba.core.byteflow:dispatch pc=10, inst=LOAD_FAST(arg=1, lineno=1145)
DEBUG:numba.core.byteflow:stack []
DEBUG:numba.core.byteflow:dispatch pc=12, inst=UNARY_NEGATIVE(arg=None, lineno=1145)
DEBUG:numba.core.byteflow:stack ['$threshold10.3']
DEBUG:numba.core.byteflow:dispatch pc=14, inst=LOAD_FAST(arg=3, lineno=1145)
DEBUG:numba.core.byteflow:stack ['$12unary_negative.4']
DEBUG:numba.core.byteflow:dispatch pc=16, inst=DUP_TOP(arg=None, lineno=1145)
DEBUG:numba.core.byteflow:stack ['$12unary_negative.4', '$x014.5']
DEBUG:numba.core.byteflow:dispatch pc=18, inst=ROT_THREE(arg=None, lineno=1145)
DEBUG:numba.core.byteflow:stack ['$12unary_negative.4', '$x014.5', '$16dup_top.6']
DEBUG:numba.core.byteflow:dispatch pc=20, inst=COMPARE_OP(arg=1, lineno=1145)
DEBUG:numba.core.byteflow:stack ['$16dup_top.6', '$12unary_negative.4', '$x014.5']
DEBUG:numba.core.byteflow:dispatch pc=22, inst=POP_JUMP_IF_FALSE(arg=32, lineno=1145)
DEBUG:numba.core.byteflow:stack ['$16dup_top.6', '$20compare_op.7']
DEBUG:numba.core.byteflow:end state. edges=[Edge(pc=24, stack=('$16dup_top.6',), blockstack=(), npush=0), Edge(pc=32, stack=('$16dup_top.6',), blockstack=(), npush=0)]
DEBUG:numba.core.byteflow:pending: deque([State(pc_initial=24 nstack_initial=1), State(pc_initial=32 nstack_initial=1)])
DEBUG:numba.core.byteflow:stack: ['$phi24.0']
DEBUG:numba.core.byteflow:state.pc_initial: State(pc_initial=24 nstack_initial=1)
DEBUG:numba.core.byteflow:dispatch pc=24, inst=LOAD_FAST(arg=1, lineno=1145)
DEBUG:numba.core.byteflow:stack ['$phi24.0']
DEBUG:numba.core.byteflow:dispatch pc=26, inst=COMPARE_OP(arg=1, lineno=1145)
DEBUG:numba.core.byteflow:stack ['$phi24.0', '$threshold24.1']
DEBUG:numba.core.byteflow:dispatch pc=28, inst=POP_JUMP_IF_FALSE(arg=40, lineno=1145)
DEBUG:numba.core.byteflow:stack ['$26compare_op.2']
DEBUG:numba.core.byteflow:end state. edges=[Edge(pc=30, stack=(), blockstack=(), npush=0), Edge(pc=40, stack=(), blockstack=(), npush=0)]
DEBUG:numba.core.byteflow:pending: deque([State(pc_initial=32 nstack_initial=1), State(pc_initial=30 nstack_initial=0), State(pc_initial=40 nstack_initial=0)])
DEBUG:numba.core.byteflow:stack: ['$phi32.0']
DEBUG:numba.core.byteflow:state.pc_initial: State(pc_initial=32 nstack_initial=1)
DEBUG:numba.core.byteflow:dispatch pc=32, inst=POP_TOP(arg=None, lineno=1145)
DEBUG:numba.core.byteflow:stack ['$phi32.0']
DEBUG:numba.core.byteflow:dispatch pc=34, inst=JUMP_FORWARD(arg=4, lineno=1145)
DEBUG:numba.core.byteflow:stack []
DEBUG:numba.core.byteflow:end state. edges=[Edge(pc=40, stack=(), blockstack=(), npush=0)]
DEBUG:numba.core.byteflow:pending: deque([State(pc_initial=30 nstack_initial=0), State(pc_initial=40 nstack_initial=0), State(pc_initial=40 nstack_initial=0)])
DEBUG:numba.core.byteflow:stack: []
DEBUG:numba.core.byteflow:state.pc_initial: State(pc_initial=30 nstack_initial=0)
DEBUG:numba.core.byteflow:dispatch pc=30, inst=JUMP_FORWARD(arg=4, lineno=1145)
DEBUG:numba.core.byteflow:stack []
DEBUG:numba.core.byteflow:end state. edges=[Edge(pc=36, stack=(), blockstack=(), npush=0)]
DEBUG:numba.core.byteflow:pending: deque([State(pc_initial=40 nstack_initial=0), State(pc_initial=40 nstack_initial=0), State(pc_initial=36 nstack_initial=0)])
DEBUG:numba.core.byteflow:stack: []
DEBUG:numba.core.byteflow:state.pc_initial: State(pc_initial=40 nstack_initial=0)
DEBUG:numba.core.byteflow:dispatch pc=40, inst=LOAD_FAST(arg=0, lineno=1148)
DEBUG:numba.core.byteflow:stack []
DEBUG:numba.core.byteflow:dispatch pc=42, inst=LOAD_CONST(arg=2, lineno=1148)
DEBUG:numba.core.byteflow:stack ['$x40.0']
DEBUG:numba.core.byteflow:dispatch pc=44, inst=BINARY_SUBSCR(arg=None, lineno=1148)
DEBUG:numba.core.byteflow:stack ['$x40.0', '$const42.1']
DEBUG:numba.core.byteflow:dispatch pc=46, inst=STORE_FAST(arg=4, lineno=1148)
DEBUG:numba.core.byteflow:stack ['$44binary_subscr.2']
DEBUG:numba.core.byteflow:dispatch pc=48, inst=LOAD_FAST(arg=1, lineno=1149)
DEBUG:numba.core.byteflow:stack []
DEBUG:numba.core.byteflow:dispatch pc=50, inst=UNARY_NEGATIVE(arg=None, lineno=1149)
DEBUG:numba.core.byteflow:stack ['$threshold48.3']
DEBUG:numba.core.byteflow:dispatch pc=52, inst=LOAD_FAST(arg=4, lineno=1149)
DEBUG:numba.core.byteflow:stack ['$50unary_negative.4']
DEBUG:numba.core.byteflow:dispatch pc=54, inst=DUP_TOP(arg=None, lineno=1149)
DEBUG:numba.core.byteflow:stack ['$50unary_negative.4', '$x152.5']
DEBUG:numba.core.byteflow:dispatch pc=56, inst=ROT_THREE(arg=None, lineno=1149)
DEBUG:numba.core.byteflow:stack ['$50unary_negative.4', '$x152.5', '$54dup_top.6']
DEBUG:numba.core.byteflow:dispatch pc=58, inst=COMPARE_OP(arg=1, lineno=1149)
DEBUG:numba.core.byteflow:stack ['$54dup_top.6', '$50unary_negative.4', '$x152.5']
DEBUG:numba.core.byteflow:dispatch pc=60, inst=POP_JUMP_IF_FALSE(arg=70, lineno=1149)
DEBUG:numba.core.byteflow:stack ['$54dup_top.6', '$58compare_op.7']
DEBUG:numba.core.byteflow:end state. edges=[Edge(pc=62, stack=('$54dup_top.6',), blockstack=(), npush=0), Edge(pc=70, stack=('$54dup_top.6',), blockstack=(), npush=0)]
DEBUG:numba.core.byteflow:pending: deque([State(pc_initial=40 nstack_initial=0), State(pc_initial=36 nstack_initial=0), State(pc_initial=62 nstack_initial=1), State(pc_initial=70 nstack_initial=1)])
DEBUG:numba.core.byteflow:pending: deque([State(pc_initial=36 nstack_initial=0), State(pc_initial=62 nstack_initial=1), State(pc_initial=70 nstack_initial=1)])
DEBUG:numba.core.byteflow:stack: []
DEBUG:numba.core.byteflow:state.pc_initial: State(pc_initial=36 nstack_initial=0)
DEBUG:numba.core.byteflow:dispatch pc=36, inst=LOAD_CONST(arg=1, lineno=1146)
DEBUG:numba.core.byteflow:stack []
DEBUG:numba.core.byteflow:dispatch pc=38, inst=STORE_FAST(arg=3, lineno=1146)
DEBUG:numba.core.byteflow:stack ['$const36.0']
DEBUG:numba.core.byteflow:end state. edges=[Edge(pc=40, stack=(), blockstack=(), npush=0)]
DEBUG:numba.core.byteflow:pending: deque([State(pc_initial=62 nstack_initial=1), State(pc_initial=70 nstack_initial=1), State(pc_initial=40 nstack_initial=0)])
DEBUG:numba.core.byteflow:stack: ['$phi62.0']
DEBUG:numba.core.byteflow:state.pc_initial: State(pc_initial=62 nstack_initial=1)
DEBUG:numba.core.byteflow:dispatch pc=62, inst=LOAD_FAST(arg=1, lineno=1149)
DEBUG:numba.core.byteflow:stack ['$phi62.0']
DEBUG:numba.core.byteflow:dispatch pc=64, inst=COMPARE_OP(arg=1, lineno=1149)
DEBUG:numba.core.byteflow:stack ['$phi62.0', '$threshold62.1']
DEBUG:numba.core.byteflow:dispatch pc=66, inst=POP_JUMP_IF_FALSE(arg=78, lineno=1149)
DEBUG:numba.core.byteflow:stack ['$64compare_op.2']
DEBUG:numba.core.byteflow:end state. edges=[Edge(pc=68, stack=(), blockstack=(), npush=0), Edge(pc=78, stack=(), blockstack=(), npush=0)]
DEBUG:numba.core.byteflow:pending: deque([State(pc_initial=70 nstack_initial=1), State(pc_initial=40 nstack_initial=0), State(pc_initial=68 nstack_initial=0), State(pc_initial=78 nstack_initial=0)])
DEBUG:numba.core.byteflow:stack: ['$phi70.0']
DEBUG:numba.core.byteflow:state.pc_initial: State(pc_initial=70 nstack_initial=1)
DEBUG:numba.core.byteflow:dispatch pc=70, inst=POP_TOP(arg=None, lineno=1149)
DEBUG:numba.core.byteflow:stack ['$phi70.0']
DEBUG:numba.core.byteflow:dispatch pc=72, inst=JUMP_FORWARD(arg=4, lineno=1149)
DEBUG:numba.core.byteflow:stack []
DEBUG:numba.core.byteflow:end state. edges=[Edge(pc=78, stack=(), blockstack=(), npush=0)]
DEBUG:numba.core.byteflow:pending: deque([State(pc_initial=40 nstack_initial=0), State(pc_initial=68 nstack_initial=0), State(pc_initial=78 nstack_initial=0), State(pc_initial=78 nstack_initial=0)])
DEBUG:numba.core.byteflow:pending: deque([State(pc_initial=68 nstack_initial=0), State(pc_initial=78 nstack_initial=0), State(pc_initial=78 nstack_initial=0)])
DEBUG:numba.core.byteflow:stack: []
DEBUG:numba.core.byteflow:state.pc_initial: State(pc_initial=68 nstack_initial=0)
DEBUG:numba.core.byteflow:dispatch pc=68, inst=JUMP_FORWARD(arg=4, lineno=1149)
DEBUG:numba.core.byteflow:stack []
DEBUG:numba.core.byteflow:end state. edges=[Edge(pc=74, stack=(), blockstack=(), npush=0)]
DEBUG:numba.core.byteflow:pending: deque([State(pc_initial=78 nstack_initial=0), State(pc_initial=78 nstack_initial=0), State(pc_initial=74 nstack_initial=0)])
DEBUG:numba.core.byteflow:stack: []
DEBUG:numba.core.byteflow:state.pc_initial: State(pc_initial=78 nstack_initial=0)
DEBUG:numba.core.byteflow:dispatch pc=78, inst=LOAD_FAST(arg=2, lineno=1152)
DEBUG:numba.core.byteflow:stack []
DEBUG:numba.core.byteflow:dispatch pc=80, inst=POP_JUMP_IF_FALSE(arg=102, lineno=1152)
DEBUG:numba.core.byteflow:stack ['$zero_pos78.0']
DEBUG:numba.core.byteflow:end state. edges=[Edge(pc=82, stack=(), blockstack=(), npush=0), Edge(pc=102, stack=(), blockstack=(), npush=0)]
DEBUG:numba.core.byteflow:pending: deque([State(pc_initial=78 nstack_initial=0), State(pc_initial=74 nstack_initial=0), State(pc_initial=82 nstack_initial=0), State(pc_initial=102 nstack_initial=0)])
DEBUG:numba.core.byteflow:pending: deque([State(pc_initial=74 nstack_initial=0), State(pc_initial=82 nstack_initial=0), State(pc_initial=102 nstack_initial=0)])
DEBUG:numba.core.byteflow:stack: []
DEBUG:numba.core.byteflow:state.pc_initial: State(pc_initial=74 nstack_initial=0)
DEBUG:numba.core.byteflow:dispatch pc=74, inst=LOAD_CONST(arg=1, lineno=1150)
DEBUG:numba.core.byteflow:stack []
DEBUG:numba.core.byteflow:dispatch pc=76, inst=STORE_FAST(arg=4, lineno=1150)
DEBUG:numba.core.byteflow:stack ['$const74.0']
DEBUG:numba.core.byteflow:end state. edges=[Edge(pc=78, stack=(), blockstack=(), npush=0)]
DEBUG:numba.core.byteflow:pending: deque([State(pc_initial=82 nstack_initial=0), State(pc_initial=102 nstack_initial=0), State(pc_initial=78 nstack_initial=0)])
DEBUG:numba.core.byteflow:stack: []
DEBUG:numba.core.byteflow:state.pc_initial: State(pc_initial=82 nstack_initial=0)
DEBUG:numba.core.byteflow:dispatch pc=82, inst=LOAD_GLOBAL(arg=0, lineno=1153)
DEBUG:numba.core.byteflow:stack []
DEBUG:numba.core.byteflow:dispatch pc=84, inst=LOAD_METHOD(arg=1, lineno=1153)
DEBUG:numba.core.byteflow:stack ['$82load_global.0']
DEBUG:numba.core.byteflow:dispatch pc=86, inst=LOAD_FAST(arg=3, lineno=1153)
DEBUG:numba.core.byteflow:stack ['$84load_method.1']
DEBUG:numba.core.byteflow:dispatch pc=88, inst=CALL_METHOD(arg=1, lineno=1153)
DEBUG:numba.core.byteflow:stack ['$84load_method.1', '$x086.2']
DEBUG:numba.core.byteflow:dispatch pc=90, inst=LOAD_GLOBAL(arg=0, lineno=1153)
DEBUG:numba.core.byteflow:stack ['$88call_method.3']
DEBUG:numba.core.byteflow:dispatch pc=92, inst=LOAD_METHOD(arg=1, lineno=1153)
DEBUG:numba.core.byteflow:stack ['$88call_method.3', '$90load_global.4']
DEBUG:numba.core.byteflow:dispatch pc=94, inst=LOAD_FAST(arg=4, lineno=1153)
DEBUG:numba.core.byteflow:stack ['$88call_method.3', '$92load_method.5']
DEBUG:numba.core.byteflow:dispatch pc=96, inst=CALL_METHOD(arg=1, lineno=1153)
DEBUG:numba.core.byteflow:stack ['$88call_method.3', '$92load_method.5', '$x194.6']
DEBUG:numba.core.byteflow:dispatch pc=98, inst=COMPARE_OP(arg=3, lineno=1153)
DEBUG:numba.core.byteflow:stack ['$88call_method.3', '$96call_method.7']
DEBUG:numba.core.byteflow:dispatch pc=100, inst=RETURN_VALUE(arg=None, lineno=1153)
DEBUG:numba.core.byteflow:stack ['$98compare_op.8']
DEBUG:numba.core.byteflow:end state. edges=[]
DEBUG:numba.core.byteflow:pending: deque([State(pc_initial=102 nstack_initial=0), State(pc_initial=78 nstack_initial=0)])
DEBUG:numba.core.byteflow:stack: []
DEBUG:numba.core.byteflow:state.pc_initial: State(pc_initial=102 nstack_initial=0)
DEBUG:numba.core.byteflow:dispatch pc=102, inst=LOAD_GLOBAL(arg=0, lineno=1155)
DEBUG:numba.core.byteflow:stack []
DEBUG:numba.core.byteflow:dispatch pc=104, inst=LOAD_METHOD(arg=2, lineno=1155)
DEBUG:numba.core.byteflow:stack ['$102load_global.0']
DEBUG:numba.core.byteflow:dispatch pc=106, inst=LOAD_FAST(arg=3, lineno=1155)
DEBUG:numba.core.byteflow:stack ['$104load_method.1']
DEBUG:numba.core.byteflow:dispatch pc=108, inst=CALL_METHOD(arg=1, lineno=1155)
DEBUG:numba.core.byteflow:stack ['$104load_method.1', '$x0106.2']
DEBUG:numba.core.byteflow:dispatch pc=110, inst=LOAD_GLOBAL(arg=0, lineno=1155)
DEBUG:numba.core.byteflow:stack ['$108call_method.3']
DEBUG:numba.core.byteflow:dispatch pc=112, inst=LOAD_METHOD(arg=2, lineno=1155)
DEBUG:numba.core.byteflow:stack ['$108call_method.3', '$110load_global.4']
DEBUG:numba.core.byteflow:dispatch pc=114, inst=LOAD_FAST(arg=4, lineno=1155)
DEBUG:numba.core.byteflow:stack ['$108call_method.3', '$112load_method.5']
DEBUG:numba.core.byteflow:dispatch pc=116, inst=CALL_METHOD(arg=1, lineno=1155)
DEBUG:numba.core.byteflow:stack ['$108call_method.3', '$112load_method.5', '$x1114.6']
DEBUG:numba.core.byteflow:dispatch pc=118, inst=COMPARE_OP(arg=3, lineno=1155)
DEBUG:numba.core.byteflow:stack ['$108call_method.3', '$116call_method.7']
DEBUG:numba.core.byteflow:dispatch pc=120, inst=RETURN_VALUE(arg=None, lineno=1155)
DEBUG:numba.core.byteflow:stack ['$118compare_op.8']
DEBUG:numba.core.byteflow:end state. edges=[]
DEBUG:numba.core.byteflow:pending: deque([State(pc_initial=78 nstack_initial=0)])
DEBUG:numba.core.byteflow:-------------------------Prune PHIs-------------------------
DEBUG:numba.core.byteflow:Used_phis: defaultdict(<class 'set'>,
            {State(pc_initial=0 nstack_initial=0): set(),
             State(pc_initial=24 nstack_initial=1): {'$phi24.0'},
             State(pc_initial=30 nstack_initial=0): set(),
             State(pc_initial=32 nstack_initial=1): set(),
             State(pc_initial=36 nstack_initial=0): set(),
             State(pc_initial=40 nstack_initial=0): set(),
             State(pc_initial=62 nstack_initial=1): {'$phi62.0'},
             State(pc_initial=68 nstack_initial=0): set(),
             State(pc_initial=70 nstack_initial=1): set(),
             State(pc_initial=74 nstack_initial=0): set(),
             State(pc_initial=78 nstack_initial=0): set(),
             State(pc_initial=82 nstack_initial=0): set(),
             State(pc_initial=102 nstack_initial=0): set()})
DEBUG:numba.core.byteflow:defmap: {'$phi24.0': State(pc_initial=0 nstack_initial=0),
 '$phi32.0': State(pc_initial=0 nstack_initial=0),
 '$phi62.0': State(pc_initial=40 nstack_initial=0),
 '$phi70.0': State(pc_initial=40 nstack_initial=0)}
DEBUG:numba.core.byteflow:phismap: defaultdict(<class 'set'>,
            {'$phi24.0': {('$16dup_top.6',
                           State(pc_initial=0 nstack_initial=0))},
             '$phi32.0': {('$16dup_top.6',
                           State(pc_initial=0 nstack_initial=0))},
             '$phi62.0': {('$54dup_top.6',
                           State(pc_initial=40 nstack_initial=0))},
             '$phi70.0': {('$54dup_top.6',
                           State(pc_initial=40 nstack_initial=0))}})
DEBUG:numba.core.byteflow:changing phismap: defaultdict(<class 'set'>,
            {'$phi24.0': {('$16dup_top.6',
                           State(pc_initial=0 nstack_initial=0))},
             '$phi32.0': {('$16dup_top.6',
                           State(pc_initial=0 nstack_initial=0))},
             '$phi62.0': {('$54dup_top.6',
                           State(pc_initial=40 nstack_initial=0))},
             '$phi70.0': {('$54dup_top.6',
                           State(pc_initial=40 nstack_initial=0))}})
DEBUG:numba.core.byteflow:keep phismap: {'$phi24.0': {('$16dup_top.6', State(pc_initial=0 nstack_initial=0))},
 '$phi62.0': {('$54dup_top.6', State(pc_initial=40 nstack_initial=0))}}
DEBUG:numba.core.byteflow:new_out: defaultdict(<class 'dict'>,
            {State(pc_initial=0 nstack_initial=0): {'$phi24.0': '$16dup_top.6'},
             State(pc_initial=40 nstack_initial=0): {'$phi62.0': '$54dup_top.6'}})
DEBUG:numba.core.byteflow:----------------------DONE Prune PHIs-----------------------
DEBUG:numba.core.byteflow:block_infos State(pc_initial=0 nstack_initial=0):
AdaptBlockInfo(insts=((0, {}), (2, {'res': '$x2.0'}), (4, {'res': '$const4.1'}), (6, {'index': '$const4.1', 'target': '$x2.0', 'res': '$6binary_subscr.2'}), (8, {'value': '$6binary_subscr.2'}), (10, {'res': '$threshold10.3'}), (12, {'value': '$threshold10.3', 'res': '$12unary_negative.4'}), (14, {'res': '$x014.5'}), (16, {'orig': ['$x014.5'], 'duped': ['$16dup_top.6']}), (20, {'lhs': '$12unary_negative.4', 'rhs': '$x014.5', 'res': '$20compare_op.7'}), (22, {'pred': '$20compare_op.7'})), outgoing_phis={'$phi24.0': '$16dup_top.6'}, blockstack=(), active_try_block=None, outgoing_edgepushed={24: ('$16dup_top.6',), 32: ('$16dup_top.6',)})
DEBUG:numba.core.byteflow:block_infos State(pc_initial=24 nstack_initial=1):
AdaptBlockInfo(insts=((24, {'res': '$threshold24.1'}), (26, {'lhs': '$phi24.0', 'rhs': '$threshold24.1', 'res': '$26compare_op.2'}), (28, {'pred': '$26compare_op.2'})), outgoing_phis={}, blockstack=(), active_try_block=None, outgoing_edgepushed={30: (), 40: ()})
DEBUG:numba.core.byteflow:block_infos State(pc_initial=30 nstack_initial=0):
AdaptBlockInfo(insts=((30, {}),), outgoing_phis={}, blockstack=(), active_try_block=None, outgoing_edgepushed={36: ()})
DEBUG:numba.core.byteflow:block_infos State(pc_initial=32 nstack_initial=1):
AdaptBlockInfo(insts=((34, {}),), outgoing_phis={}, blockstack=(), active_try_block=None, outgoing_edgepushed={40: ()})
DEBUG:numba.core.byteflow:block_infos State(pc_initial=36 nstack_initial=0):
AdaptBlockInfo(insts=((36, {'res': '$const36.0'}), (38, {'value': '$const36.0'})), outgoing_phis={}, blockstack=(), active_try_block=None, outgoing_edgepushed={40: ()})
DEBUG:numba.core.byteflow:block_infos State(pc_initial=40 nstack_initial=0):
AdaptBlockInfo(insts=((40, {'res': '$x40.0'}), (42, {'res': '$const42.1'}), (44, {'index': '$const42.1', 'target': '$x40.0', 'res': '$44binary_subscr.2'}), (46, {'value': '$44binary_subscr.2'}), (48, {'res': '$threshold48.3'}), (50, {'value': '$threshold48.3', 'res': '$50unary_negative.4'}), (52, {'res': '$x152.5'}), (54, {'orig': ['$x152.5'], 'duped': ['$54dup_top.6']}), (58, {'lhs': '$50unary_negative.4', 'rhs': '$x152.5', 'res': '$58compare_op.7'}), (60, {'pred': '$58compare_op.7'})), outgoing_phis={'$phi62.0': '$54dup_top.6'}, blockstack=(), active_try_block=None, outgoing_edgepushed={62: ('$54dup_top.6',), 70: ('$54dup_top.6',)})
DEBUG:numba.core.byteflow:block_infos State(pc_initial=62 nstack_initial=1):
AdaptBlockInfo(insts=((62, {'res': '$threshold62.1'}), (64, {'lhs': '$phi62.0', 'rhs': '$threshold62.1', 'res': '$64compare_op.2'}), (66, {'pred': '$64compare_op.2'})), outgoing_phis={}, blockstack=(), active_try_block=None, outgoing_edgepushed={68: (), 78: ()})
DEBUG:numba.core.byteflow:block_infos State(pc_initial=68 nstack_initial=0):
AdaptBlockInfo(insts=((68, {}),), outgoing_phis={}, blockstack=(), active_try_block=None, outgoing_edgepushed={74: ()})
DEBUG:numba.core.byteflow:block_infos State(pc_initial=70 nstack_initial=1):
AdaptBlockInfo(insts=((72, {}),), outgoing_phis={}, blockstack=(), active_try_block=None, outgoing_edgepushed={78: ()})
DEBUG:numba.core.byteflow:block_infos State(pc_initial=74 nstack_initial=0):
AdaptBlockInfo(insts=((74, {'res': '$const74.0'}), (76, {'value': '$const74.0'})), outgoing_phis={}, blockstack=(), active_try_block=None, outgoing_edgepushed={78: ()})
DEBUG:numba.core.byteflow:block_infos State(pc_initial=78 nstack_initial=0):
AdaptBlockInfo(insts=((78, {'res': '$zero_pos78.0'}), (80, {'pred': '$zero_pos78.0'})), outgoing_phis={}, blockstack=(), active_try_block=None, outgoing_edgepushed={82: (), 102: ()})
DEBUG:numba.core.byteflow:block_infos State(pc_initial=82 nstack_initial=0):
AdaptBlockInfo(insts=((82, {'res': '$82load_global.0'}), (84, {'item': '$82load_global.0', 'res': '$84load_method.1'}), (86, {'res': '$x086.2'}), (88, {'func': '$84load_method.1', 'args': ['$x086.2'], 'res': '$88call_method.3'}), (90, {'res': '$90load_global.4'}), (92, {'item': '$90load_global.4', 'res': '$92load_method.5'}), (94, {'res': '$x194.6'}), (96, {'func': '$92load_method.5', 'args': ['$x194.6'], 'res': '$96call_method.7'}), (98, {'lhs': '$88call_method.3', 'rhs': '$96call_method.7', 'res': '$98compare_op.8'}), (100, {'retval': '$98compare_op.8', 'castval': '$100return_value.9'})), outgoing_phis={}, blockstack=(), active_try_block=None, outgoing_edgepushed={})
DEBUG:numba.core.byteflow:block_infos State(pc_initial=102 nstack_initial=0):
AdaptBlockInfo(insts=((102, {'res': '$102load_global.0'}), (104, {'item': '$102load_global.0', 'res': '$104load_method.1'}), (106, {'res': '$x0106.2'}), (108, {'func': '$104load_method.1', 'args': ['$x0106.2'], 'res': '$108call_method.3'}), (110, {'res': '$110load_global.4'}), (112, {'item': '$110load_global.4', 'res': '$112load_method.5'}), (114, {'res': '$x1114.6'}), (116, {'func': '$112load_method.5', 'args': ['$x1114.6'], 'res': '$116call_method.7'}), (118, {'lhs': '$108call_method.3', 'rhs': '$116call_method.7', 'res': '$118compare_op.8'}), (120, {'retval': '$118compare_op.8', 'castval': '$120return_value.9'})), outgoing_phis={}, blockstack=(), active_try_block=None, outgoing_edgepushed={})
DEBUG:numba.core.interpreter:label 0:
    x = arg(0, name=x)                       ['x']
    threshold = arg(1, name=threshold)       ['threshold']
    zero_pos = arg(2, name=zero_pos)         ['zero_pos']
    $const4.1 = const(int, 0)                ['$const4.1']
    x0 = getitem(value=x, index=$const4.1, fn=<built-in function getitem>) ['$const4.1', 'x', 'x0']
    $12unary_negative.4 = unary(fn=<built-in function neg>, value=threshold) ['$12unary_negative.4', 'threshold']
    $20compare_op.7 = $12unary_negative.4 <= x0 ['$12unary_negative.4', '$20compare_op.7', 'x0']
    bool22 = global(bool: <class 'bool'>)    ['bool22']
    $22pred = call bool22($20compare_op.7, func=bool22, args=(Var($20compare_op.7, audio.py:1145),), kws=(), vararg=None, varkwarg=None, target=None) ['$20compare_op.7', '$22pred', 'bool22']
    $phi24.0 = x0                            ['$phi24.0', 'x0']
    branch $22pred, 24, 32                   ['$22pred']
label 24:
    $26compare_op.2 = $phi24.0 <= threshold  ['$26compare_op.2', '$phi24.0', 'threshold']
    bool28 = global(bool: <class 'bool'>)    ['bool28']
    $28pred = call bool28($26compare_op.2, func=bool28, args=(Var($26compare_op.2, audio.py:1145),), kws=(), vararg=None, varkwarg=None, target=None) ['$26compare_op.2', '$28pred', 'bool28']
    branch $28pred, 30, 40                   ['$28pred']
label 30:
    jump 36                                  []
label 32:
    jump 40                                  []
label 36:
    x0 = const(int, 0)                       ['x0']
    jump 40                                  []
label 40:
    $const42.1 = const(int, -1)              ['$const42.1']
    x1 = getitem(value=x, index=$const42.1, fn=<built-in function getitem>) ['$const42.1', 'x', 'x1']
    $50unary_negative.4 = unary(fn=<built-in function neg>, value=threshold) ['$50unary_negative.4', 'threshold']
    $58compare_op.7 = $50unary_negative.4 <= x1 ['$50unary_negative.4', '$58compare_op.7', 'x1']
    bool60 = global(bool: <class 'bool'>)    ['bool60']
    $60pred = call bool60($58compare_op.7, func=bool60, args=(Var($58compare_op.7, audio.py:1149),), kws=(), vararg=None, varkwarg=None, target=None) ['$58compare_op.7', '$60pred', 'bool60']
    $phi62.0 = x1                            ['$phi62.0', 'x1']
    branch $60pred, 62, 70                   ['$60pred']
label 62:
    $64compare_op.2 = $phi62.0 <= threshold  ['$64compare_op.2', '$phi62.0', 'threshold']
    bool66 = global(bool: <class 'bool'>)    ['bool66']
    $66pred = call bool66($64compare_op.2, func=bool66, args=(Var($64compare_op.2, audio.py:1149),), kws=(), vararg=None, varkwarg=None, target=None) ['$64compare_op.2', '$66pred', 'bool66']
    branch $66pred, 68, 78                   ['$66pred']
label 68:
    jump 74                                  []
label 70:
    jump 78                                  []
label 74:
    x1 = const(int, 0)                       ['x1']
    jump 78                                  []
label 78:
    bool80 = global(bool: <class 'bool'>)    ['bool80']
    $80pred = call bool80(zero_pos, func=bool80, args=(Var(zero_pos, audio.py:1141),), kws=(), vararg=None, varkwarg=None, target=None) ['$80pred', 'bool80', 'zero_pos']
    branch $80pred, 82, 102                  ['$80pred']
label 82:
    $82load_global.0 = global(np: <module 'numpy' from 'C:\\Users\\taskm\\anaconda3\\envs\\taskai\\lib\\site-packages\\numpy\\__init__.py'>) ['$82load_global.0']
    $84load_method.1 = getattr(value=$82load_global.0, attr=signbit) ['$82load_global.0', '$84load_method.1']
    $88call_method.3 = call $84load_method.1(x0, func=$84load_method.1, args=[Var(x0, audio.py:1144)], kws=(), vararg=None, varkwarg=None, target=None) ['$84load_method.1', '$88call_method.3', 'x0']
    $90load_global.4 = global(np: <module 'numpy' from 'C:\\Users\\taskm\\anaconda3\\envs\\taskai\\lib\\site-packages\\numpy\\__init__.py'>) ['$90load_global.4']
    $92load_method.5 = getattr(value=$90load_global.4, attr=signbit) ['$90load_global.4', '$92load_method.5']
    $96call_method.7 = call $92load_method.5(x1, func=$92load_method.5, args=[Var(x1, audio.py:1148)], kws=(), vararg=None, varkwarg=None, target=None) ['$92load_method.5', '$96call_method.7', 'x1']
    $98compare_op.8 = $88call_method.3 != $96call_method.7 ['$88call_method.3', '$96call_method.7', '$98compare_op.8']
    $100return_value.9 = cast(value=$98compare_op.8) ['$100return_value.9', '$98compare_op.8']
    return $100return_value.9                ['$100return_value.9']
label 102:
    $102load_global.0 = global(np: <module 'numpy' from 'C:\\Users\\taskm\\anaconda3\\envs\\taskai\\lib\\site-packages\\numpy\\__init__.py'>) ['$102load_global.0']
    $104load_method.1 = getattr(value=$102load_global.0, attr=sign) ['$102load_global.0', '$104load_method.1']
    $108call_method.3 = call $104load_method.1(x0, func=$104load_method.1, args=[Var(x0, audio.py:1144)], kws=(), vararg=None, varkwarg=None, target=None) ['$104load_method.1', '$108call_method.3', 'x0']
    $110load_global.4 = global(np: <module 'numpy' from 'C:\\Users\\taskm\\anaconda3\\envs\\taskai\\lib\\site-packages\\numpy\\__init__.py'>) ['$110load_global.4']
    $112load_method.5 = getattr(value=$110load_global.4, attr=sign) ['$110load_global.4', '$112load_method.5']
    $116call_method.7 = call $112load_method.5(x1, func=$112load_method.5, args=[Var(x1, audio.py:1148)], kws=(), vararg=None, varkwarg=None, target=None) ['$112load_method.5', '$116call_method.7', 'x1']
    $118compare_op.8 = $108call_method.3 != $116call_method.7 ['$108call_method.3', '$116call_method.7', '$118compare_op.8']
    $120return_value.9 = cast(value=$118compare_op.8) ['$118compare_op.8', '$120return_value.9']
    return $120return_value.9                ['$120return_value.9']

DEBUG:numba.core.byteflow:bytecode dump:
>          0	NOP(arg=None, lineno=1039)
           2	LOAD_FAST(arg=0, lineno=1042)
           4	LOAD_CONST(arg=1, lineno=1042)
           6	BINARY_SUBSCR(arg=None, lineno=1042)
           8	LOAD_FAST(arg=0, lineno=1042)
          10	LOAD_CONST(arg=2, lineno=1042)
          12	BINARY_SUBSCR(arg=None, lineno=1042)
          14	COMPARE_OP(arg=4, lineno=1042)
          16	LOAD_FAST(arg=0, lineno=1042)
          18	LOAD_CONST(arg=1, lineno=1042)
          20	BINARY_SUBSCR(arg=None, lineno=1042)
          22	LOAD_FAST(arg=0, lineno=1042)
          24	LOAD_CONST(arg=3, lineno=1042)
          26	BINARY_SUBSCR(arg=None, lineno=1042)
          28	COMPARE_OP(arg=5, lineno=1042)
          30	BINARY_AND(arg=None, lineno=1042)
          32	RETURN_VALUE(arg=None, lineno=1042)
DEBUG:numba.core.byteflow:pending: deque([State(pc_initial=0 nstack_initial=0)])
DEBUG:numba.core.byteflow:stack: []
DEBUG:numba.core.byteflow:state.pc_initial: State(pc_initial=0 nstack_initial=0)
DEBUG:numba.core.byteflow:dispatch pc=0, inst=NOP(arg=None, lineno=1039)
DEBUG:numba.core.byteflow:stack []
DEBUG:numba.core.byteflow:dispatch pc=2, inst=LOAD_FAST(arg=0, lineno=1042)
DEBUG:numba.core.byteflow:stack []
DEBUG:numba.core.byteflow:dispatch pc=4, inst=LOAD_CONST(arg=1, lineno=1042)
DEBUG:numba.core.byteflow:stack ['$x2.0']
DEBUG:numba.core.byteflow:dispatch pc=6, inst=BINARY_SUBSCR(arg=None, lineno=1042)
DEBUG:numba.core.byteflow:stack ['$x2.0', '$const4.1']
DEBUG:numba.core.byteflow:dispatch pc=8, inst=LOAD_FAST(arg=0, lineno=1042)
DEBUG:numba.core.byteflow:stack ['$6binary_subscr.2']
DEBUG:numba.core.byteflow:dispatch pc=10, inst=LOAD_CONST(arg=2, lineno=1042)
DEBUG:numba.core.byteflow:stack ['$6binary_subscr.2', '$x8.3']
DEBUG:numba.core.byteflow:dispatch pc=12, inst=BINARY_SUBSCR(arg=None, lineno=1042)
DEBUG:numba.core.byteflow:stack ['$6binary_subscr.2', '$x8.3', '$const10.4']
DEBUG:numba.core.byteflow:dispatch pc=14, inst=COMPARE_OP(arg=4, lineno=1042)
DEBUG:numba.core.byteflow:stack ['$6binary_subscr.2', '$12binary_subscr.5']
DEBUG:numba.core.byteflow:dispatch pc=16, inst=LOAD_FAST(arg=0, lineno=1042)
DEBUG:numba.core.byteflow:stack ['$14compare_op.6']
DEBUG:numba.core.byteflow:dispatch pc=18, inst=LOAD_CONST(arg=1, lineno=1042)
DEBUG:numba.core.byteflow:stack ['$14compare_op.6', '$x16.7']
DEBUG:numba.core.byteflow:dispatch pc=20, inst=BINARY_SUBSCR(arg=None, lineno=1042)
DEBUG:numba.core.byteflow:stack ['$14compare_op.6', '$x16.7', '$const18.8']
DEBUG:numba.core.byteflow:dispatch pc=22, inst=LOAD_FAST(arg=0, lineno=1042)
DEBUG:numba.core.byteflow:stack ['$14compare_op.6', '$20binary_subscr.9']
DEBUG:numba.core.byteflow:dispatch pc=24, inst=LOAD_CONST(arg=3, lineno=1042)
DEBUG:numba.core.byteflow:stack ['$14compare_op.6', '$20binary_subscr.9', '$x22.10']
DEBUG:numba.core.byteflow:dispatch pc=26, inst=BINARY_SUBSCR(arg=None, lineno=1042)
DEBUG:numba.core.byteflow:stack ['$14compare_op.6', '$20binary_subscr.9', '$x22.10', '$const24.11']
DEBUG:numba.core.byteflow:dispatch pc=28, inst=COMPARE_OP(arg=5, lineno=1042)
DEBUG:numba.core.byteflow:stack ['$14compare_op.6', '$20binary_subscr.9', '$26binary_subscr.12']
DEBUG:numba.core.byteflow:dispatch pc=30, inst=BINARY_AND(arg=None, lineno=1042)
DEBUG:numba.core.byteflow:stack ['$14compare_op.6', '$28compare_op.13']
DEBUG:numba.core.byteflow:dispatch pc=32, inst=RETURN_VALUE(arg=None, lineno=1042)
DEBUG:numba.core.byteflow:stack ['$30binary_and.14']
DEBUG:numba.core.byteflow:end state. edges=[]
DEBUG:numba.core.byteflow:-------------------------Prune PHIs-------------------------
DEBUG:numba.core.byteflow:Used_phis: defaultdict(<class 'set'>, {State(pc_initial=0 nstack_initial=0): set()})
DEBUG:numba.core.byteflow:defmap: {}
DEBUG:numba.core.byteflow:phismap: defaultdict(<class 'set'>, {})
DEBUG:numba.core.byteflow:changing phismap: defaultdict(<class 'set'>, {})
DEBUG:numba.core.byteflow:keep phismap: {}
DEBUG:numba.core.byteflow:new_out: defaultdict(<class 'dict'>, {})
DEBUG:numba.core.byteflow:----------------------DONE Prune PHIs-----------------------
DEBUG:numba.core.byteflow:block_infos State(pc_initial=0 nstack_initial=0):
AdaptBlockInfo(insts=((0, {}), (2, {'res': '$x2.0'}), (4, {'res': '$const4.1'}), (6, {'index': '$const4.1', 'target': '$x2.0', 'res': '$6binary_subscr.2'}), (8, {'res': '$x8.3'}), (10, {'res': '$const10.4'}), (12, {'index': '$const10.4', 'target': '$x8.3', 'res': '$12binary_subscr.5'}), (14, {'lhs': '$6binary_subscr.2', 'rhs': '$12binary_subscr.5', 'res': '$14compare_op.6'}), (16, {'res': '$x16.7'}), (18, {'res': '$const18.8'}), (20, {'index': '$const18.8', 'target': '$x16.7', 'res': '$20binary_subscr.9'}), (22, {'res': '$x22.10'}), (24, {'res': '$const24.11'}), (26, {'index': '$const24.11', 'target': '$x22.10', 'res': '$26binary_subscr.12'}), (28, {'lhs': '$20binary_subscr.9', 'rhs': '$26binary_subscr.12', 'res': '$28compare_op.13'}), (30, {'lhs': '$14compare_op.6', 'rhs': '$28compare_op.13', 'res': '$30binary_and.14'}), (32, {'retval': '$30binary_and.14', 'castval': '$32return_value.15'})), outgoing_phis={}, blockstack=(), active_try_block=None, outgoing_edgepushed={})
DEBUG:numba.core.interpreter:label 0:
    x = arg(0, name=x)                       ['x']
    $const4.1 = const(int, 0)                ['$const4.1']
    $6binary_subscr.2 = getitem(value=x, index=$const4.1, fn=<built-in function getitem>) ['$6binary_subscr.2', '$const4.1', 'x']
    $const10.4 = const(int, -1)              ['$const10.4']
    $12binary_subscr.5 = getitem(value=x, index=$const10.4, fn=<built-in function getitem>) ['$12binary_subscr.5', '$const10.4', 'x']
    $14compare_op.6 = $6binary_subscr.2 > $12binary_subscr.5 ['$12binary_subscr.5', '$14compare_op.6', '$6binary_subscr.2']
    $const18.8 = const(int, 0)               ['$const18.8']
    $20binary_subscr.9 = getitem(value=x, index=$const18.8, fn=<built-in function getitem>) ['$20binary_subscr.9', '$const18.8', 'x']
    $const24.11 = const(int, 1)              ['$const24.11']
    $26binary_subscr.12 = getitem(value=x, index=$const24.11, fn=<built-in function getitem>) ['$26binary_subscr.12', '$const24.11', 'x']
    $28compare_op.13 = $20binary_subscr.9 >= $26binary_subscr.12 ['$20binary_subscr.9', '$26binary_subscr.12', '$28compare_op.13']
    $30binary_and.14 = $14compare_op.6 & $28compare_op.13 ['$14compare_op.6', '$28compare_op.13', '$30binary_and.14']
    $32return_value.15 = cast(value=$30binary_and.14) ['$30binary_and.14', '$32return_value.15']
    return $32return_value.15                ['$32return_value.15']

DEBUG:numba.core.byteflow:bytecode dump:
>          0	NOP(arg=None, lineno=1045)
           2	LOAD_FAST(arg=0, lineno=1048)
           4	LOAD_CONST(arg=1, lineno=1048)
           6	BINARY_SUBSCR(arg=None, lineno=1048)
           8	LOAD_FAST(arg=0, lineno=1048)
          10	LOAD_CONST(arg=2, lineno=1048)
          12	BINARY_SUBSCR(arg=None, lineno=1048)
          14	COMPARE_OP(arg=0, lineno=1048)
          16	LOAD_FAST(arg=0, lineno=1048)
          18	LOAD_CONST(arg=1, lineno=1048)
          20	BINARY_SUBSCR(arg=None, lineno=1048)
          22	LOAD_FAST(arg=0, lineno=1048)
          24	LOAD_CONST(arg=3, lineno=1048)
          26	BINARY_SUBSCR(arg=None, lineno=1048)
          28	COMPARE_OP(arg=1, lineno=1048)
          30	BINARY_AND(arg=None, lineno=1048)
          32	RETURN_VALUE(arg=None, lineno=1048)
DEBUG:numba.core.byteflow:pending: deque([State(pc_initial=0 nstack_initial=0)])
DEBUG:numba.core.byteflow:stack: []
DEBUG:numba.core.byteflow:state.pc_initial: State(pc_initial=0 nstack_initial=0)
DEBUG:numba.core.byteflow:dispatch pc=0, inst=NOP(arg=None, lineno=1045)
DEBUG:numba.core.byteflow:stack []
DEBUG:numba.core.byteflow:dispatch pc=2, inst=LOAD_FAST(arg=0, lineno=1048)
DEBUG:numba.core.byteflow:stack []
DEBUG:numba.core.byteflow:dispatch pc=4, inst=LOAD_CONST(arg=1, lineno=1048)
DEBUG:numba.core.byteflow:stack ['$x2.0']
DEBUG:numba.core.byteflow:dispatch pc=6, inst=BINARY_SUBSCR(arg=None, lineno=1048)
DEBUG:numba.core.byteflow:stack ['$x2.0', '$const4.1']
DEBUG:numba.core.byteflow:dispatch pc=8, inst=LOAD_FAST(arg=0, lineno=1048)
DEBUG:numba.core.byteflow:stack ['$6binary_subscr.2']
DEBUG:numba.core.byteflow:dispatch pc=10, inst=LOAD_CONST(arg=2, lineno=1048)
DEBUG:numba.core.byteflow:stack ['$6binary_subscr.2', '$x8.3']
DEBUG:numba.core.byteflow:dispatch pc=12, inst=BINARY_SUBSCR(arg=None, lineno=1048)
DEBUG:numba.core.byteflow:stack ['$6binary_subscr.2', '$x8.3', '$const10.4']
DEBUG:numba.core.byteflow:dispatch pc=14, inst=COMPARE_OP(arg=0, lineno=1048)
DEBUG:numba.core.byteflow:stack ['$6binary_subscr.2', '$12binary_subscr.5']
DEBUG:numba.core.byteflow:dispatch pc=16, inst=LOAD_FAST(arg=0, lineno=1048)
DEBUG:numba.core.byteflow:stack ['$14compare_op.6']
DEBUG:numba.core.byteflow:dispatch pc=18, inst=LOAD_CONST(arg=1, lineno=1048)
DEBUG:numba.core.byteflow:stack ['$14compare_op.6', '$x16.7']
DEBUG:numba.core.byteflow:dispatch pc=20, inst=BINARY_SUBSCR(arg=None, lineno=1048)
DEBUG:numba.core.byteflow:stack ['$14compare_op.6', '$x16.7', '$const18.8']
DEBUG:numba.core.byteflow:dispatch pc=22, inst=LOAD_FAST(arg=0, lineno=1048)
DEBUG:numba.core.byteflow:stack ['$14compare_op.6', '$20binary_subscr.9']
DEBUG:numba.core.byteflow:dispatch pc=24, inst=LOAD_CONST(arg=3, lineno=1048)
DEBUG:numba.core.byteflow:stack ['$14compare_op.6', '$20binary_subscr.9', '$x22.10']
DEBUG:numba.core.byteflow:dispatch pc=26, inst=BINARY_SUBSCR(arg=None, lineno=1048)
DEBUG:numba.core.byteflow:stack ['$14compare_op.6', '$20binary_subscr.9', '$x22.10', '$const24.11']
DEBUG:numba.core.byteflow:dispatch pc=28, inst=COMPARE_OP(arg=1, lineno=1048)
DEBUG:numba.core.byteflow:stack ['$14compare_op.6', '$20binary_subscr.9', '$26binary_subscr.12']
DEBUG:numba.core.byteflow:dispatch pc=30, inst=BINARY_AND(arg=None, lineno=1048)
DEBUG:numba.core.byteflow:stack ['$14compare_op.6', '$28compare_op.13']
DEBUG:numba.core.byteflow:dispatch pc=32, inst=RETURN_VALUE(arg=None, lineno=1048)
DEBUG:numba.core.byteflow:stack ['$30binary_and.14']
DEBUG:numba.core.byteflow:end state. edges=[]
DEBUG:numba.core.byteflow:-------------------------Prune PHIs-------------------------
DEBUG:numba.core.byteflow:Used_phis: defaultdict(<class 'set'>, {State(pc_initial=0 nstack_initial=0): set()})
DEBUG:numba.core.byteflow:defmap: {}
DEBUG:numba.core.byteflow:phismap: defaultdict(<class 'set'>, {})
DEBUG:numba.core.byteflow:changing phismap: defaultdict(<class 'set'>, {})
DEBUG:numba.core.byteflow:keep phismap: {}
DEBUG:numba.core.byteflow:new_out: defaultdict(<class 'dict'>, {})
DEBUG:numba.core.byteflow:----------------------DONE Prune PHIs-----------------------
DEBUG:numba.core.byteflow:block_infos State(pc_initial=0 nstack_initial=0):
AdaptBlockInfo(insts=((0, {}), (2, {'res': '$x2.0'}), (4, {'res': '$const4.1'}), (6, {'index': '$const4.1', 'target': '$x2.0', 'res': '$6binary_subscr.2'}), (8, {'res': '$x8.3'}), (10, {'res': '$const10.4'}), (12, {'index': '$const10.4', 'target': '$x8.3', 'res': '$12binary_subscr.5'}), (14, {'lhs': '$6binary_subscr.2', 'rhs': '$12binary_subscr.5', 'res': '$14compare_op.6'}), (16, {'res': '$x16.7'}), (18, {'res': '$const18.8'}), (20, {'index': '$const18.8', 'target': '$x16.7', 'res': '$20binary_subscr.9'}), (22, {'res': '$x22.10'}), (24, {'res': '$const24.11'}), (26, {'index': '$const24.11', 'target': '$x22.10', 'res': '$26binary_subscr.12'}), (28, {'lhs': '$20binary_subscr.9', 'rhs': '$26binary_subscr.12', 'res': '$28compare_op.13'}), (30, {'lhs': '$14compare_op.6', 'rhs': '$28compare_op.13', 'res': '$30binary_and.14'}), (32, {'retval': '$30binary_and.14', 'castval': '$32return_value.15'})), outgoing_phis={}, blockstack=(), active_try_block=None, outgoing_edgepushed={})
DEBUG:numba.core.interpreter:label 0:
    x = arg(0, name=x)                       ['x']
    $const4.1 = const(int, 0)                ['$const4.1']
    $6binary_subscr.2 = getitem(value=x, index=$const4.1, fn=<built-in function getitem>) ['$6binary_subscr.2', '$const4.1', 'x']
    $const10.4 = const(int, -1)              ['$const10.4']
    $12binary_subscr.5 = getitem(value=x, index=$const10.4, fn=<built-in function getitem>) ['$12binary_subscr.5', '$const10.4', 'x']
    $14compare_op.6 = $6binary_subscr.2 < $12binary_subscr.5 ['$12binary_subscr.5', '$14compare_op.6', '$6binary_subscr.2']
    $const18.8 = const(int, 0)               ['$const18.8']
    $20binary_subscr.9 = getitem(value=x, index=$const18.8, fn=<built-in function getitem>) ['$20binary_subscr.9', '$const18.8', 'x']
    $const24.11 = const(int, 1)              ['$const24.11']
    $26binary_subscr.12 = getitem(value=x, index=$const24.11, fn=<built-in function getitem>) ['$26binary_subscr.12', '$const24.11', 'x']
    $28compare_op.13 = $20binary_subscr.9 <= $26binary_subscr.12 ['$20binary_subscr.9', '$26binary_subscr.12', '$28compare_op.13']
    $30binary_and.14 = $14compare_op.6 & $28compare_op.13 ['$14compare_op.6', '$28compare_op.13', '$30binary_and.14']
    $32return_value.15 = cast(value=$30binary_and.14) ['$30binary_and.14', '$32return_value.15']
    return $32return_value.15                ['$32return_value.15']

ERROR:root:Error processing chapters: name 'ChatOpenAI' is not defined
Traceback (most recent call last):
  File "D:\github\chappymedium\chappie.py", line 260, in process_chapters
    self.chappie_processor = ChappieProcessor(api_key)
  File "D:\github\chappymedium\chappie_processor.py", line 11, in __init__
    self.llm = ChatOpenAI(
NameError: name 'ChatOpenAI' is not defined
ERROR:root:Error processing chapters: name 'ChatOpenAI' is not defined
Traceback (most recent call last):
  File "D:\github\chappymedium\chappie.py", line 260, in process_chapters
    self.chappie_processor = ChappieProcessor(api_key)
  File "D:\github\chappymedium\chappie_processor.py", line 11, in __init__
    self.llm = ChatOpenAI(
NameError: name 'ChatOpenAI' is not defined
ERROR:root:Error processing chapters: name 'ChatOpenAI' is not defined
Traceback (most recent call last):
  File "D:\github\chappymedium\chappie.py", line 260, in process_chapters
    self.chappie_processor = ChappieProcessor(api_key)
  File "D:\github\chappymedium\chappie_processor.py", line 11, in __init__
    self.llm = ChatOpenAI(
NameError: name 'ChatOpenAI' is not defined
DEBUG:numba.core.byteflow:bytecode dump:
>          0	NOP(arg=None, lineno=1141)
           2	LOAD_FAST(arg=0, lineno=1144)
           4	LOAD_CONST(arg=1, lineno=1144)
           6	BINARY_SUBSCR(arg=None, lineno=1144)
           8	STORE_FAST(arg=3, lineno=1144)
          10	LOAD_FAST(arg=1, lineno=1145)
          12	UNARY_NEGATIVE(arg=None, lineno=1145)
          14	LOAD_FAST(arg=3, lineno=1145)
          16	DUP_TOP(arg=None, lineno=1145)
          18	ROT_THREE(arg=None, lineno=1145)
          20	COMPARE_OP(arg=1, lineno=1145)
          22	POP_JUMP_IF_FALSE(arg=32, lineno=1145)
          24	LOAD_FAST(arg=1, lineno=1145)
          26	COMPARE_OP(arg=1, lineno=1145)
          28	POP_JUMP_IF_FALSE(arg=40, lineno=1145)
          30	JUMP_FORWARD(arg=4, lineno=1145)
>         32	POP_TOP(arg=None, lineno=1145)
          34	JUMP_FORWARD(arg=4, lineno=1145)
>         36	LOAD_CONST(arg=1, lineno=1146)
          38	STORE_FAST(arg=3, lineno=1146)
>         40	LOAD_FAST(arg=0, lineno=1148)
          42	LOAD_CONST(arg=2, lineno=1148)
          44	BINARY_SUBSCR(arg=None, lineno=1148)
          46	STORE_FAST(arg=4, lineno=1148)
          48	LOAD_FAST(arg=1, lineno=1149)
          50	UNARY_NEGATIVE(arg=None, lineno=1149)
          52	LOAD_FAST(arg=4, lineno=1149)
          54	DUP_TOP(arg=None, lineno=1149)
          56	ROT_THREE(arg=None, lineno=1149)
          58	COMPARE_OP(arg=1, lineno=1149)
          60	POP_JUMP_IF_FALSE(arg=70, lineno=1149)
          62	LOAD_FAST(arg=1, lineno=1149)
          64	COMPARE_OP(arg=1, lineno=1149)
          66	POP_JUMP_IF_FALSE(arg=78, lineno=1149)
          68	JUMP_FORWARD(arg=4, lineno=1149)
>         70	POP_TOP(arg=None, lineno=1149)
          72	JUMP_FORWARD(arg=4, lineno=1149)
>         74	LOAD_CONST(arg=1, lineno=1150)
          76	STORE_FAST(arg=4, lineno=1150)
>         78	LOAD_FAST(arg=2, lineno=1152)
          80	POP_JUMP_IF_FALSE(arg=102, lineno=1152)
          82	LOAD_GLOBAL(arg=0, lineno=1153)
          84	LOAD_METHOD(arg=1, lineno=1153)
          86	LOAD_FAST(arg=3, lineno=1153)
          88	CALL_METHOD(arg=1, lineno=1153)
          90	LOAD_GLOBAL(arg=0, lineno=1153)
          92	LOAD_METHOD(arg=1, lineno=1153)
          94	LOAD_FAST(arg=4, lineno=1153)
          96	CALL_METHOD(arg=1, lineno=1153)
          98	COMPARE_OP(arg=3, lineno=1153)
         100	RETURN_VALUE(arg=None, lineno=1153)
>        102	LOAD_GLOBAL(arg=0, lineno=1155)
         104	LOAD_METHOD(arg=2, lineno=1155)
         106	LOAD_FAST(arg=3, lineno=1155)
         108	CALL_METHOD(arg=1, lineno=1155)
         110	LOAD_GLOBAL(arg=0, lineno=1155)
         112	LOAD_METHOD(arg=2, lineno=1155)
         114	LOAD_FAST(arg=4, lineno=1155)
         116	CALL_METHOD(arg=1, lineno=1155)
         118	COMPARE_OP(arg=3, lineno=1155)
         120	RETURN_VALUE(arg=None, lineno=1155)
         122	LOAD_CONST(arg=3, lineno=1155)
         124	RETURN_VALUE(arg=None, lineno=1155)
DEBUG:numba.core.byteflow:pending: deque([State(pc_initial=0 nstack_initial=0)])
DEBUG:numba.core.byteflow:stack: []
DEBUG:numba.core.byteflow:state.pc_initial: State(pc_initial=0 nstack_initial=0)
DEBUG:numba.core.byteflow:dispatch pc=0, inst=NOP(arg=None, lineno=1141)
DEBUG:numba.core.byteflow:stack []
DEBUG:numba.core.byteflow:dispatch pc=2, inst=LOAD_FAST(arg=0, lineno=1144)
DEBUG:numba.core.byteflow:stack []
DEBUG:numba.core.byteflow:dispatch pc=4, inst=LOAD_CONST(arg=1, lineno=1144)
DEBUG:numba.core.byteflow:stack ['$x2.0']
DEBUG:numba.core.byteflow:dispatch pc=6, inst=BINARY_SUBSCR(arg=None, lineno=1144)
DEBUG:numba.core.byteflow:stack ['$x2.0', '$const4.1']
DEBUG:numba.core.byteflow:dispatch pc=8, inst=STORE_FAST(arg=3, lineno=1144)
DEBUG:numba.core.byteflow:stack ['$6binary_subscr.2']
DEBUG:numba.core.byteflow:dispatch pc=10, inst=LOAD_FAST(arg=1, lineno=1145)
DEBUG:numba.core.byteflow:stack []
DEBUG:numba.core.byteflow:dispatch pc=12, inst=UNARY_NEGATIVE(arg=None, lineno=1145)
DEBUG:numba.core.byteflow:stack ['$threshold10.3']
DEBUG:numba.core.byteflow:dispatch pc=14, inst=LOAD_FAST(arg=3, lineno=1145)
DEBUG:numba.core.byteflow:stack ['$12unary_negative.4']
DEBUG:numba.core.byteflow:dispatch pc=16, inst=DUP_TOP(arg=None, lineno=1145)
DEBUG:numba.core.byteflow:stack ['$12unary_negative.4', '$x014.5']
DEBUG:numba.core.byteflow:dispatch pc=18, inst=ROT_THREE(arg=None, lineno=1145)
DEBUG:numba.core.byteflow:stack ['$12unary_negative.4', '$x014.5', '$16dup_top.6']
DEBUG:numba.core.byteflow:dispatch pc=20, inst=COMPARE_OP(arg=1, lineno=1145)
DEBUG:numba.core.byteflow:stack ['$16dup_top.6', '$12unary_negative.4', '$x014.5']
DEBUG:numba.core.byteflow:dispatch pc=22, inst=POP_JUMP_IF_FALSE(arg=32, lineno=1145)
DEBUG:numba.core.byteflow:stack ['$16dup_top.6', '$20compare_op.7']
DEBUG:numba.core.byteflow:end state. edges=[Edge(pc=24, stack=('$16dup_top.6',), blockstack=(), npush=0), Edge(pc=32, stack=('$16dup_top.6',), blockstack=(), npush=0)]
DEBUG:numba.core.byteflow:pending: deque([State(pc_initial=24 nstack_initial=1), State(pc_initial=32 nstack_initial=1)])
DEBUG:numba.core.byteflow:stack: ['$phi24.0']
DEBUG:numba.core.byteflow:state.pc_initial: State(pc_initial=24 nstack_initial=1)
DEBUG:numba.core.byteflow:dispatch pc=24, inst=LOAD_FAST(arg=1, lineno=1145)
DEBUG:numba.core.byteflow:stack ['$phi24.0']
DEBUG:numba.core.byteflow:dispatch pc=26, inst=COMPARE_OP(arg=1, lineno=1145)
DEBUG:numba.core.byteflow:stack ['$phi24.0', '$threshold24.1']
DEBUG:numba.core.byteflow:dispatch pc=28, inst=POP_JUMP_IF_FALSE(arg=40, lineno=1145)
DEBUG:numba.core.byteflow:stack ['$26compare_op.2']
DEBUG:numba.core.byteflow:end state. edges=[Edge(pc=30, stack=(), blockstack=(), npush=0), Edge(pc=40, stack=(), blockstack=(), npush=0)]
DEBUG:numba.core.byteflow:pending: deque([State(pc_initial=32 nstack_initial=1), State(pc_initial=30 nstack_initial=0), State(pc_initial=40 nstack_initial=0)])
DEBUG:numba.core.byteflow:stack: ['$phi32.0']
DEBUG:numba.core.byteflow:state.pc_initial: State(pc_initial=32 nstack_initial=1)
DEBUG:numba.core.byteflow:dispatch pc=32, inst=POP_TOP(arg=None, lineno=1145)
DEBUG:numba.core.byteflow:stack ['$phi32.0']
DEBUG:numba.core.byteflow:dispatch pc=34, inst=JUMP_FORWARD(arg=4, lineno=1145)
DEBUG:numba.core.byteflow:stack []
DEBUG:numba.core.byteflow:end state. edges=[Edge(pc=40, stack=(), blockstack=(), npush=0)]
DEBUG:numba.core.byteflow:pending: deque([State(pc_initial=30 nstack_initial=0), State(pc_initial=40 nstack_initial=0), State(pc_initial=40 nstack_initial=0)])
DEBUG:numba.core.byteflow:stack: []
DEBUG:numba.core.byteflow:state.pc_initial: State(pc_initial=30 nstack_initial=0)
DEBUG:numba.core.byteflow:dispatch pc=30, inst=JUMP_FORWARD(arg=4, lineno=1145)
DEBUG:numba.core.byteflow:stack []
DEBUG:numba.core.byteflow:end state. edges=[Edge(pc=36, stack=(), blockstack=(), npush=0)]
DEBUG:numba.core.byteflow:pending: deque([State(pc_initial=40 nstack_initial=0), State(pc_initial=40 nstack_initial=0), State(pc_initial=36 nstack_initial=0)])
DEBUG:numba.core.byteflow:stack: []
DEBUG:numba.core.byteflow:state.pc_initial: State(pc_initial=40 nstack_initial=0)
DEBUG:numba.core.byteflow:dispatch pc=40, inst=LOAD_FAST(arg=0, lineno=1148)
DEBUG:numba.core.byteflow:stack []
DEBUG:numba.core.byteflow:dispatch pc=42, inst=LOAD_CONST(arg=2, lineno=1148)
DEBUG:numba.core.byteflow:stack ['$x40.0']
DEBUG:numba.core.byteflow:dispatch pc=44, inst=BINARY_SUBSCR(arg=None, lineno=1148)
DEBUG:numba.core.byteflow:stack ['$x40.0', '$const42.1']
DEBUG:numba.core.byteflow:dispatch pc=46, inst=STORE_FAST(arg=4, lineno=1148)
DEBUG:numba.core.byteflow:stack ['$44binary_subscr.2']
DEBUG:numba.core.byteflow:dispatch pc=48, inst=LOAD_FAST(arg=1, lineno=1149)
DEBUG:numba.core.byteflow:stack []
DEBUG:numba.core.byteflow:dispatch pc=50, inst=UNARY_NEGATIVE(arg=None, lineno=1149)
DEBUG:numba.core.byteflow:stack ['$threshold48.3']
DEBUG:numba.core.byteflow:dispatch pc=52, inst=LOAD_FAST(arg=4, lineno=1149)
DEBUG:numba.core.byteflow:stack ['$50unary_negative.4']
DEBUG:numba.core.byteflow:dispatch pc=54, inst=DUP_TOP(arg=None, lineno=1149)
DEBUG:numba.core.byteflow:stack ['$50unary_negative.4', '$x152.5']
DEBUG:numba.core.byteflow:dispatch pc=56, inst=ROT_THREE(arg=None, lineno=1149)
DEBUG:numba.core.byteflow:stack ['$50unary_negative.4', '$x152.5', '$54dup_top.6']
DEBUG:numba.core.byteflow:dispatch pc=58, inst=COMPARE_OP(arg=1, lineno=1149)
DEBUG:numba.core.byteflow:stack ['$54dup_top.6', '$50unary_negative.4', '$x152.5']
DEBUG:numba.core.byteflow:dispatch pc=60, inst=POP_JUMP_IF_FALSE(arg=70, lineno=1149)
DEBUG:numba.core.byteflow:stack ['$54dup_top.6', '$58compare_op.7']
DEBUG:numba.core.byteflow:end state. edges=[Edge(pc=62, stack=('$54dup_top.6',), blockstack=(), npush=0), Edge(pc=70, stack=('$54dup_top.6',), blockstack=(), npush=0)]
DEBUG:numba.core.byteflow:pending: deque([State(pc_initial=40 nstack_initial=0), State(pc_initial=36 nstack_initial=0), State(pc_initial=62 nstack_initial=1), State(pc_initial=70 nstack_initial=1)])
DEBUG:numba.core.byteflow:pending: deque([State(pc_initial=36 nstack_initial=0), State(pc_initial=62 nstack_initial=1), State(pc_initial=70 nstack_initial=1)])
DEBUG:numba.core.byteflow:stack: []
DEBUG:numba.core.byteflow:state.pc_initial: State(pc_initial=36 nstack_initial=0)
DEBUG:numba.core.byteflow:dispatch pc=36, inst=LOAD_CONST(arg=1, lineno=1146)
DEBUG:numba.core.byteflow:stack []
DEBUG:numba.core.byteflow:dispatch pc=38, inst=STORE_FAST(arg=3, lineno=1146)
DEBUG:numba.core.byteflow:stack ['$const36.0']
DEBUG:numba.core.byteflow:end state. edges=[Edge(pc=40, stack=(), blockstack=(), npush=0)]
DEBUG:numba.core.byteflow:pending: deque([State(pc_initial=62 nstack_initial=1), State(pc_initial=70 nstack_initial=1), State(pc_initial=40 nstack_initial=0)])
DEBUG:numba.core.byteflow:stack: ['$phi62.0']
DEBUG:numba.core.byteflow:state.pc_initial: State(pc_initial=62 nstack_initial=1)
DEBUG:numba.core.byteflow:dispatch pc=62, inst=LOAD_FAST(arg=1, lineno=1149)
DEBUG:numba.core.byteflow:stack ['$phi62.0']
DEBUG:numba.core.byteflow:dispatch pc=64, inst=COMPARE_OP(arg=1, lineno=1149)
DEBUG:numba.core.byteflow:stack ['$phi62.0', '$threshold62.1']
DEBUG:numba.core.byteflow:dispatch pc=66, inst=POP_JUMP_IF_FALSE(arg=78, lineno=1149)
DEBUG:numba.core.byteflow:stack ['$64compare_op.2']
DEBUG:numba.core.byteflow:end state. edges=[Edge(pc=68, stack=(), blockstack=(), npush=0), Edge(pc=78, stack=(), blockstack=(), npush=0)]
DEBUG:numba.core.byteflow:pending: deque([State(pc_initial=70 nstack_initial=1), State(pc_initial=40 nstack_initial=0), State(pc_initial=68 nstack_initial=0), State(pc_initial=78 nstack_initial=0)])
DEBUG:numba.core.byteflow:stack: ['$phi70.0']
DEBUG:numba.core.byteflow:state.pc_initial: State(pc_initial=70 nstack_initial=1)
DEBUG:numba.core.byteflow:dispatch pc=70, inst=POP_TOP(arg=None, lineno=1149)
DEBUG:numba.core.byteflow:stack ['$phi70.0']
DEBUG:numba.core.byteflow:dispatch pc=72, inst=JUMP_FORWARD(arg=4, lineno=1149)
DEBUG:numba.core.byteflow:stack []
DEBUG:numba.core.byteflow:end state. edges=[Edge(pc=78, stack=(), blockstack=(), npush=0)]
DEBUG:numba.core.byteflow:pending: deque([State(pc_initial=40 nstack_initial=0), State(pc_initial=68 nstack_initial=0), State(pc_initial=78 nstack_initial=0), State(pc_initial=78 nstack_initial=0)])
DEBUG:numba.core.byteflow:pending: deque([State(pc_initial=68 nstack_initial=0), State(pc_initial=78 nstack_initial=0), State(pc_initial=78 nstack_initial=0)])
DEBUG:numba.core.byteflow:stack: []
DEBUG:numba.core.byteflow:state.pc_initial: State(pc_initial=68 nstack_initial=0)
DEBUG:numba.core.byteflow:dispatch pc=68, inst=JUMP_FORWARD(arg=4, lineno=1149)
DEBUG:numba.core.byteflow:stack []
DEBUG:numba.core.byteflow:end state. edges=[Edge(pc=74, stack=(), blockstack=(), npush=0)]
DEBUG:numba.core.byteflow:pending: deque([State(pc_initial=78 nstack_initial=0), State(pc_initial=78 nstack_initial=0), State(pc_initial=74 nstack_initial=0)])
DEBUG:numba.core.byteflow:stack: []
DEBUG:numba.core.byteflow:state.pc_initial: State(pc_initial=78 nstack_initial=0)
DEBUG:numba.core.byteflow:dispatch pc=78, inst=LOAD_FAST(arg=2, lineno=1152)
DEBUG:numba.core.byteflow:stack []
DEBUG:numba.core.byteflow:dispatch pc=80, inst=POP_JUMP_IF_FALSE(arg=102, lineno=1152)
DEBUG:numba.core.byteflow:stack ['$zero_pos78.0']
DEBUG:numba.core.byteflow:end state. edges=[Edge(pc=82, stack=(), blockstack=(), npush=0), Edge(pc=102, stack=(), blockstack=(), npush=0)]
DEBUG:numba.core.byteflow:pending: deque([State(pc_initial=78 nstack_initial=0), State(pc_initial=74 nstack_initial=0), State(pc_initial=82 nstack_initial=0), State(pc_initial=102 nstack_initial=0)])
DEBUG:numba.core.byteflow:pending: deque([State(pc_initial=74 nstack_initial=0), State(pc_initial=82 nstack_initial=0), State(pc_initial=102 nstack_initial=0)])
DEBUG:numba.core.byteflow:stack: []
DEBUG:numba.core.byteflow:state.pc_initial: State(pc_initial=74 nstack_initial=0)
DEBUG:numba.core.byteflow:dispatch pc=74, inst=LOAD_CONST(arg=1, lineno=1150)
DEBUG:numba.core.byteflow:stack []
DEBUG:numba.core.byteflow:dispatch pc=76, inst=STORE_FAST(arg=4, lineno=1150)
DEBUG:numba.core.byteflow:stack ['$const74.0']
DEBUG:numba.core.byteflow:end state. edges=[Edge(pc=78, stack=(), blockstack=(), npush=0)]
DEBUG:numba.core.byteflow:pending: deque([State(pc_initial=82 nstack_initial=0), State(pc_initial=102 nstack_initial=0), State(pc_initial=78 nstack_initial=0)])
DEBUG:numba.core.byteflow:stack: []
DEBUG:numba.core.byteflow:state.pc_initial: State(pc_initial=82 nstack_initial=0)
DEBUG:numba.core.byteflow:dispatch pc=82, inst=LOAD_GLOBAL(arg=0, lineno=1153)
DEBUG:numba.core.byteflow:stack []
DEBUG:numba.core.byteflow:dispatch pc=84, inst=LOAD_METHOD(arg=1, lineno=1153)
DEBUG:numba.core.byteflow:stack ['$82load_global.0']
DEBUG:numba.core.byteflow:dispatch pc=86, inst=LOAD_FAST(arg=3, lineno=1153)
DEBUG:numba.core.byteflow:stack ['$84load_method.1']
DEBUG:numba.core.byteflow:dispatch pc=88, inst=CALL_METHOD(arg=1, lineno=1153)
DEBUG:numba.core.byteflow:stack ['$84load_method.1', '$x086.2']
DEBUG:numba.core.byteflow:dispatch pc=90, inst=LOAD_GLOBAL(arg=0, lineno=1153)
DEBUG:numba.core.byteflow:stack ['$88call_method.3']
DEBUG:numba.core.byteflow:dispatch pc=92, inst=LOAD_METHOD(arg=1, lineno=1153)
DEBUG:numba.core.byteflow:stack ['$88call_method.3', '$90load_global.4']
DEBUG:numba.core.byteflow:dispatch pc=94, inst=LOAD_FAST(arg=4, lineno=1153)
DEBUG:numba.core.byteflow:stack ['$88call_method.3', '$92load_method.5']
DEBUG:numba.core.byteflow:dispatch pc=96, inst=CALL_METHOD(arg=1, lineno=1153)
DEBUG:numba.core.byteflow:stack ['$88call_method.3', '$92load_method.5', '$x194.6']
DEBUG:numba.core.byteflow:dispatch pc=98, inst=COMPARE_OP(arg=3, lineno=1153)
DEBUG:numba.core.byteflow:stack ['$88call_method.3', '$96call_method.7']
DEBUG:numba.core.byteflow:dispatch pc=100, inst=RETURN_VALUE(arg=None, lineno=1153)
DEBUG:numba.core.byteflow:stack ['$98compare_op.8']
DEBUG:numba.core.byteflow:end state. edges=[]
DEBUG:numba.core.byteflow:pending: deque([State(pc_initial=102 nstack_initial=0), State(pc_initial=78 nstack_initial=0)])
DEBUG:numba.core.byteflow:stack: []
DEBUG:numba.core.byteflow:state.pc_initial: State(pc_initial=102 nstack_initial=0)
DEBUG:numba.core.byteflow:dispatch pc=102, inst=LOAD_GLOBAL(arg=0, lineno=1155)
DEBUG:numba.core.byteflow:stack []
DEBUG:numba.core.byteflow:dispatch pc=104, inst=LOAD_METHOD(arg=2, lineno=1155)
DEBUG:numba.core.byteflow:stack ['$102load_global.0']
DEBUG:numba.core.byteflow:dispatch pc=106, inst=LOAD_FAST(arg=3, lineno=1155)
DEBUG:numba.core.byteflow:stack ['$104load_method.1']
DEBUG:numba.core.byteflow:dispatch pc=108, inst=CALL_METHOD(arg=1, lineno=1155)
DEBUG:numba.core.byteflow:stack ['$104load_method.1', '$x0106.2']
DEBUG:numba.core.byteflow:dispatch pc=110, inst=LOAD_GLOBAL(arg=0, lineno=1155)
DEBUG:numba.core.byteflow:stack ['$108call_method.3']
DEBUG:numba.core.byteflow:dispatch pc=112, inst=LOAD_METHOD(arg=2, lineno=1155)
DEBUG:numba.core.byteflow:stack ['$108call_method.3', '$110load_global.4']
DEBUG:numba.core.byteflow:dispatch pc=114, inst=LOAD_FAST(arg=4, lineno=1155)
DEBUG:numba.core.byteflow:stack ['$108call_method.3', '$112load_method.5']
DEBUG:numba.core.byteflow:dispatch pc=116, inst=CALL_METHOD(arg=1, lineno=1155)
DEBUG:numba.core.byteflow:stack ['$108call_method.3', '$112load_method.5', '$x1114.6']
DEBUG:numba.core.byteflow:dispatch pc=118, inst=COMPARE_OP(arg=3, lineno=1155)
DEBUG:numba.core.byteflow:stack ['$108call_method.3', '$116call_method.7']
DEBUG:numba.core.byteflow:dispatch pc=120, inst=RETURN_VALUE(arg=None, lineno=1155)
DEBUG:numba.core.byteflow:stack ['$118compare_op.8']
DEBUG:numba.core.byteflow:end state. edges=[]
DEBUG:numba.core.byteflow:pending: deque([State(pc_initial=78 nstack_initial=0)])
DEBUG:numba.core.byteflow:-------------------------Prune PHIs-------------------------
DEBUG:numba.core.byteflow:Used_phis: defaultdict(<class 'set'>,
            {State(pc_initial=0 nstack_initial=0): set(),
             State(pc_initial=24 nstack_initial=1): {'$phi24.0'},
             State(pc_initial=30 nstack_initial=0): set(),
             State(pc_initial=32 nstack_initial=1): set(),
             State(pc_initial=36 nstack_initial=0): set(),
             State(pc_initial=40 nstack_initial=0): set(),
             State(pc_initial=62 nstack_initial=1): {'$phi62.0'},
             State(pc_initial=68 nstack_initial=0): set(),
             State(pc_initial=70 nstack_initial=1): set(),
             State(pc_initial=74 nstack_initial=0): set(),
             State(pc_initial=78 nstack_initial=0): set(),
             State(pc_initial=82 nstack_initial=0): set(),
             State(pc_initial=102 nstack_initial=0): set()})
DEBUG:numba.core.byteflow:defmap: {'$phi24.0': State(pc_initial=0 nstack_initial=0),
 '$phi32.0': State(pc_initial=0 nstack_initial=0),
 '$phi62.0': State(pc_initial=40 nstack_initial=0),
 '$phi70.0': State(pc_initial=40 nstack_initial=0)}
DEBUG:numba.core.byteflow:phismap: defaultdict(<class 'set'>,
            {'$phi24.0': {('$16dup_top.6',
                           State(pc_initial=0 nstack_initial=0))},
             '$phi32.0': {('$16dup_top.6',
                           State(pc_initial=0 nstack_initial=0))},
             '$phi62.0': {('$54dup_top.6',
                           State(pc_initial=40 nstack_initial=0))},
             '$phi70.0': {('$54dup_top.6',
                           State(pc_initial=40 nstack_initial=0))}})
DEBUG:numba.core.byteflow:changing phismap: defaultdict(<class 'set'>,
            {'$phi24.0': {('$16dup_top.6',
                           State(pc_initial=0 nstack_initial=0))},
             '$phi32.0': {('$16dup_top.6',
                           State(pc_initial=0 nstack_initial=0))},
             '$phi62.0': {('$54dup_top.6',
                           State(pc_initial=40 nstack_initial=0))},
             '$phi70.0': {('$54dup_top.6',
                           State(pc_initial=40 nstack_initial=0))}})
DEBUG:numba.core.byteflow:keep phismap: {'$phi24.0': {('$16dup_top.6', State(pc_initial=0 nstack_initial=0))},
 '$phi62.0': {('$54dup_top.6', State(pc_initial=40 nstack_initial=0))}}
DEBUG:numba.core.byteflow:new_out: defaultdict(<class 'dict'>,
            {State(pc_initial=0 nstack_initial=0): {'$phi24.0': '$16dup_top.6'},
             State(pc_initial=40 nstack_initial=0): {'$phi62.0': '$54dup_top.6'}})
DEBUG:numba.core.byteflow:----------------------DONE Prune PHIs-----------------------
DEBUG:numba.core.byteflow:block_infos State(pc_initial=0 nstack_initial=0):
AdaptBlockInfo(insts=((0, {}), (2, {'res': '$x2.0'}), (4, {'res': '$const4.1'}), (6, {'index': '$const4.1', 'target': '$x2.0', 'res': '$6binary_subscr.2'}), (8, {'value': '$6binary_subscr.2'}), (10, {'res': '$threshold10.3'}), (12, {'value': '$threshold10.3', 'res': '$12unary_negative.4'}), (14, {'res': '$x014.5'}), (16, {'orig': ['$x014.5'], 'duped': ['$16dup_top.6']}), (20, {'lhs': '$12unary_negative.4', 'rhs': '$x014.5', 'res': '$20compare_op.7'}), (22, {'pred': '$20compare_op.7'})), outgoing_phis={'$phi24.0': '$16dup_top.6'}, blockstack=(), active_try_block=None, outgoing_edgepushed={24: ('$16dup_top.6',), 32: ('$16dup_top.6',)})
DEBUG:numba.core.byteflow:block_infos State(pc_initial=24 nstack_initial=1):
AdaptBlockInfo(insts=((24, {'res': '$threshold24.1'}), (26, {'lhs': '$phi24.0', 'rhs': '$threshold24.1', 'res': '$26compare_op.2'}), (28, {'pred': '$26compare_op.2'})), outgoing_phis={}, blockstack=(), active_try_block=None, outgoing_edgepushed={30: (), 40: ()})
DEBUG:numba.core.byteflow:block_infos State(pc_initial=30 nstack_initial=0):
AdaptBlockInfo(insts=((30, {}),), outgoing_phis={}, blockstack=(), active_try_block=None, outgoing_edgepushed={36: ()})
DEBUG:numba.core.byteflow:block_infos State(pc_initial=32 nstack_initial=1):
AdaptBlockInfo(insts=((34, {}),), outgoing_phis={}, blockstack=(), active_try_block=None, outgoing_edgepushed={40: ()})
DEBUG:numba.core.byteflow:block_infos State(pc_initial=36 nstack_initial=0):
AdaptBlockInfo(insts=((36, {'res': '$const36.0'}), (38, {'value': '$const36.0'})), outgoing_phis={}, blockstack=(), active_try_block=None, outgoing_edgepushed={40: ()})
DEBUG:numba.core.byteflow:block_infos State(pc_initial=40 nstack_initial=0):
AdaptBlockInfo(insts=((40, {'res': '$x40.0'}), (42, {'res': '$const42.1'}), (44, {'index': '$const42.1', 'target': '$x40.0', 'res': '$44binary_subscr.2'}), (46, {'value': '$44binary_subscr.2'}), (48, {'res': '$threshold48.3'}), (50, {'value': '$threshold48.3', 'res': '$50unary_negative.4'}), (52, {'res': '$x152.5'}), (54, {'orig': ['$x152.5'], 'duped': ['$54dup_top.6']}), (58, {'lhs': '$50unary_negative.4', 'rhs': '$x152.5', 'res': '$58compare_op.7'}), (60, {'pred': '$58compare_op.7'})), outgoing_phis={'$phi62.0': '$54dup_top.6'}, blockstack=(), active_try_block=None, outgoing_edgepushed={62: ('$54dup_top.6',), 70: ('$54dup_top.6',)})
DEBUG:numba.core.byteflow:block_infos State(pc_initial=62 nstack_initial=1):
AdaptBlockInfo(insts=((62, {'res': '$threshold62.1'}), (64, {'lhs': '$phi62.0', 'rhs': '$threshold62.1', 'res': '$64compare_op.2'}), (66, {'pred': '$64compare_op.2'})), outgoing_phis={}, blockstack=(), active_try_block=None, outgoing_edgepushed={68: (), 78: ()})
DEBUG:numba.core.byteflow:block_infos State(pc_initial=68 nstack_initial=0):
AdaptBlockInfo(insts=((68, {}),), outgoing_phis={}, blockstack=(), active_try_block=None, outgoing_edgepushed={74: ()})
DEBUG:numba.core.byteflow:block_infos State(pc_initial=70 nstack_initial=1):
AdaptBlockInfo(insts=((72, {}),), outgoing_phis={}, blockstack=(), active_try_block=None, outgoing_edgepushed={78: ()})
DEBUG:numba.core.byteflow:block_infos State(pc_initial=74 nstack_initial=0):
AdaptBlockInfo(insts=((74, {'res': '$const74.0'}), (76, {'value': '$const74.0'})), outgoing_phis={}, blockstack=(), active_try_block=None, outgoing_edgepushed={78: ()})
DEBUG:numba.core.byteflow:block_infos State(pc_initial=78 nstack_initial=0):
AdaptBlockInfo(insts=((78, {'res': '$zero_pos78.0'}), (80, {'pred': '$zero_pos78.0'})), outgoing_phis={}, blockstack=(), active_try_block=None, outgoing_edgepushed={82: (), 102: ()})
DEBUG:numba.core.byteflow:block_infos State(pc_initial=82 nstack_initial=0):
AdaptBlockInfo(insts=((82, {'res': '$82load_global.0'}), (84, {'item': '$82load_global.0', 'res': '$84load_method.1'}), (86, {'res': '$x086.2'}), (88, {'func': '$84load_method.1', 'args': ['$x086.2'], 'res': '$88call_method.3'}), (90, {'res': '$90load_global.4'}), (92, {'item': '$90load_global.4', 'res': '$92load_method.5'}), (94, {'res': '$x194.6'}), (96, {'func': '$92load_method.5', 'args': ['$x194.6'], 'res': '$96call_method.7'}), (98, {'lhs': '$88call_method.3', 'rhs': '$96call_method.7', 'res': '$98compare_op.8'}), (100, {'retval': '$98compare_op.8', 'castval': '$100return_value.9'})), outgoing_phis={}, blockstack=(), active_try_block=None, outgoing_edgepushed={})
DEBUG:numba.core.byteflow:block_infos State(pc_initial=102 nstack_initial=0):
AdaptBlockInfo(insts=((102, {'res': '$102load_global.0'}), (104, {'item': '$102load_global.0', 'res': '$104load_method.1'}), (106, {'res': '$x0106.2'}), (108, {'func': '$104load_method.1', 'args': ['$x0106.2'], 'res': '$108call_method.3'}), (110, {'res': '$110load_global.4'}), (112, {'item': '$110load_global.4', 'res': '$112load_method.5'}), (114, {'res': '$x1114.6'}), (116, {'func': '$112load_method.5', 'args': ['$x1114.6'], 'res': '$116call_method.7'}), (118, {'lhs': '$108call_method.3', 'rhs': '$116call_method.7', 'res': '$118compare_op.8'}), (120, {'retval': '$118compare_op.8', 'castval': '$120return_value.9'})), outgoing_phis={}, blockstack=(), active_try_block=None, outgoing_edgepushed={})
DEBUG:numba.core.interpreter:label 0:
    x = arg(0, name=x)                       ['x']
    threshold = arg(1, name=threshold)       ['threshold']
    zero_pos = arg(2, name=zero_pos)         ['zero_pos']
    $const4.1 = const(int, 0)                ['$const4.1']
    x0 = getitem(value=x, index=$const4.1, fn=<built-in function getitem>) ['$const4.1', 'x', 'x0']
    $12unary_negative.4 = unary(fn=<built-in function neg>, value=threshold) ['$12unary_negative.4', 'threshold']
    $20compare_op.7 = $12unary_negative.4 <= x0 ['$12unary_negative.4', '$20compare_op.7', 'x0']
    bool22 = global(bool: <class 'bool'>)    ['bool22']
    $22pred = call bool22($20compare_op.7, func=bool22, args=(Var($20compare_op.7, audio.py:1145),), kws=(), vararg=None, varkwarg=None, target=None) ['$20compare_op.7', '$22pred', 'bool22']
    $phi24.0 = x0                            ['$phi24.0', 'x0']
    branch $22pred, 24, 32                   ['$22pred']
label 24:
    $26compare_op.2 = $phi24.0 <= threshold  ['$26compare_op.2', '$phi24.0', 'threshold']
    bool28 = global(bool: <class 'bool'>)    ['bool28']
    $28pred = call bool28($26compare_op.2, func=bool28, args=(Var($26compare_op.2, audio.py:1145),), kws=(), vararg=None, varkwarg=None, target=None) ['$26compare_op.2', '$28pred', 'bool28']
    branch $28pred, 30, 40                   ['$28pred']
label 30:
    jump 36                                  []
label 32:
    jump 40                                  []
label 36:
    x0 = const(int, 0)                       ['x0']
    jump 40                                  []
label 40:
    $const42.1 = const(int, -1)              ['$const42.1']
    x1 = getitem(value=x, index=$const42.1, fn=<built-in function getitem>) ['$const42.1', 'x', 'x1']
    $50unary_negative.4 = unary(fn=<built-in function neg>, value=threshold) ['$50unary_negative.4', 'threshold']
    $58compare_op.7 = $50unary_negative.4 <= x1 ['$50unary_negative.4', '$58compare_op.7', 'x1']
    bool60 = global(bool: <class 'bool'>)    ['bool60']
    $60pred = call bool60($58compare_op.7, func=bool60, args=(Var($58compare_op.7, audio.py:1149),), kws=(), vararg=None, varkwarg=None, target=None) ['$58compare_op.7', '$60pred', 'bool60']
    $phi62.0 = x1                            ['$phi62.0', 'x1']
    branch $60pred, 62, 70                   ['$60pred']
label 62:
    $64compare_op.2 = $phi62.0 <= threshold  ['$64compare_op.2', '$phi62.0', 'threshold']
    bool66 = global(bool: <class 'bool'>)    ['bool66']
    $66pred = call bool66($64compare_op.2, func=bool66, args=(Var($64compare_op.2, audio.py:1149),), kws=(), vararg=None, varkwarg=None, target=None) ['$64compare_op.2', '$66pred', 'bool66']
    branch $66pred, 68, 78                   ['$66pred']
label 68:
    jump 74                                  []
label 70:
    jump 78                                  []
label 74:
    x1 = const(int, 0)                       ['x1']
    jump 78                                  []
label 78:
    bool80 = global(bool: <class 'bool'>)    ['bool80']
    $80pred = call bool80(zero_pos, func=bool80, args=(Var(zero_pos, audio.py:1141),), kws=(), vararg=None, varkwarg=None, target=None) ['$80pred', 'bool80', 'zero_pos']
    branch $80pred, 82, 102                  ['$80pred']
label 82:
    $82load_global.0 = global(np: <module 'numpy' from 'C:\\Users\\taskm\\anaconda3\\envs\\taskai\\lib\\site-packages\\numpy\\__init__.py'>) ['$82load_global.0']
    $84load_method.1 = getattr(value=$82load_global.0, attr=signbit) ['$82load_global.0', '$84load_method.1']
    $88call_method.3 = call $84load_method.1(x0, func=$84load_method.1, args=[Var(x0, audio.py:1144)], kws=(), vararg=None, varkwarg=None, target=None) ['$84load_method.1', '$88call_method.3', 'x0']
    $90load_global.4 = global(np: <module 'numpy' from 'C:\\Users\\taskm\\anaconda3\\envs\\taskai\\lib\\site-packages\\numpy\\__init__.py'>) ['$90load_global.4']
    $92load_method.5 = getattr(value=$90load_global.4, attr=signbit) ['$90load_global.4', '$92load_method.5']
    $96call_method.7 = call $92load_method.5(x1, func=$92load_method.5, args=[Var(x1, audio.py:1148)], kws=(), vararg=None, varkwarg=None, target=None) ['$92load_method.5', '$96call_method.7', 'x1']
    $98compare_op.8 = $88call_method.3 != $96call_method.7 ['$88call_method.3', '$96call_method.7', '$98compare_op.8']
    $100return_value.9 = cast(value=$98compare_op.8) ['$100return_value.9', '$98compare_op.8']
    return $100return_value.9                ['$100return_value.9']
label 102:
    $102load_global.0 = global(np: <module 'numpy' from 'C:\\Users\\taskm\\anaconda3\\envs\\taskai\\lib\\site-packages\\numpy\\__init__.py'>) ['$102load_global.0']
    $104load_method.1 = getattr(value=$102load_global.0, attr=sign) ['$102load_global.0', '$104load_method.1']
    $108call_method.3 = call $104load_method.1(x0, func=$104load_method.1, args=[Var(x0, audio.py:1144)], kws=(), vararg=None, varkwarg=None, target=None) ['$104load_method.1', '$108call_method.3', 'x0']
    $110load_global.4 = global(np: <module 'numpy' from 'C:\\Users\\taskm\\anaconda3\\envs\\taskai\\lib\\site-packages\\numpy\\__init__.py'>) ['$110load_global.4']
    $112load_method.5 = getattr(value=$110load_global.4, attr=sign) ['$110load_global.4', '$112load_method.5']
    $116call_method.7 = call $112load_method.5(x1, func=$112load_method.5, args=[Var(x1, audio.py:1148)], kws=(), vararg=None, varkwarg=None, target=None) ['$112load_method.5', '$116call_method.7', 'x1']
    $118compare_op.8 = $108call_method.3 != $116call_method.7 ['$108call_method.3', '$116call_method.7', '$118compare_op.8']
    $120return_value.9 = cast(value=$118compare_op.8) ['$118compare_op.8', '$120return_value.9']
    return $120return_value.9                ['$120return_value.9']

DEBUG:numba.core.byteflow:bytecode dump:
>          0	NOP(arg=None, lineno=1039)
           2	LOAD_FAST(arg=0, lineno=1042)
           4	LOAD_CONST(arg=1, lineno=1042)
           6	BINARY_SUBSCR(arg=None, lineno=1042)
           8	LOAD_FAST(arg=0, lineno=1042)
          10	LOAD_CONST(arg=2, lineno=1042)
          12	BINARY_SUBSCR(arg=None, lineno=1042)
          14	COMPARE_OP(arg=4, lineno=1042)
          16	LOAD_FAST(arg=0, lineno=1042)
          18	LOAD_CONST(arg=1, lineno=1042)
          20	BINARY_SUBSCR(arg=None, lineno=1042)
          22	LOAD_FAST(arg=0, lineno=1042)
          24	LOAD_CONST(arg=3, lineno=1042)
          26	BINARY_SUBSCR(arg=None, lineno=1042)
          28	COMPARE_OP(arg=5, lineno=1042)
          30	BINARY_AND(arg=None, lineno=1042)
          32	RETURN_VALUE(arg=None, lineno=1042)
DEBUG:numba.core.byteflow:pending: deque([State(pc_initial=0 nstack_initial=0)])
DEBUG:numba.core.byteflow:stack: []
DEBUG:numba.core.byteflow:state.pc_initial: State(pc_initial=0 nstack_initial=0)
DEBUG:numba.core.byteflow:dispatch pc=0, inst=NOP(arg=None, lineno=1039)
DEBUG:numba.core.byteflow:stack []
DEBUG:numba.core.byteflow:dispatch pc=2, inst=LOAD_FAST(arg=0, lineno=1042)
DEBUG:numba.core.byteflow:stack []
DEBUG:numba.core.byteflow:dispatch pc=4, inst=LOAD_CONST(arg=1, lineno=1042)
DEBUG:numba.core.byteflow:stack ['$x2.0']
DEBUG:numba.core.byteflow:dispatch pc=6, inst=BINARY_SUBSCR(arg=None, lineno=1042)
DEBUG:numba.core.byteflow:stack ['$x2.0', '$const4.1']
DEBUG:numba.core.byteflow:dispatch pc=8, inst=LOAD_FAST(arg=0, lineno=1042)
DEBUG:numba.core.byteflow:stack ['$6binary_subscr.2']
DEBUG:numba.core.byteflow:dispatch pc=10, inst=LOAD_CONST(arg=2, lineno=1042)
DEBUG:numba.core.byteflow:stack ['$6binary_subscr.2', '$x8.3']
DEBUG:numba.core.byteflow:dispatch pc=12, inst=BINARY_SUBSCR(arg=None, lineno=1042)
DEBUG:numba.core.byteflow:stack ['$6binary_subscr.2', '$x8.3', '$const10.4']
DEBUG:numba.core.byteflow:dispatch pc=14, inst=COMPARE_OP(arg=4, lineno=1042)
DEBUG:numba.core.byteflow:stack ['$6binary_subscr.2', '$12binary_subscr.5']
DEBUG:numba.core.byteflow:dispatch pc=16, inst=LOAD_FAST(arg=0, lineno=1042)
DEBUG:numba.core.byteflow:stack ['$14compare_op.6']
DEBUG:numba.core.byteflow:dispatch pc=18, inst=LOAD_CONST(arg=1, lineno=1042)
DEBUG:numba.core.byteflow:stack ['$14compare_op.6', '$x16.7']
DEBUG:numba.core.byteflow:dispatch pc=20, inst=BINARY_SUBSCR(arg=None, lineno=1042)
DEBUG:numba.core.byteflow:stack ['$14compare_op.6', '$x16.7', '$const18.8']
DEBUG:numba.core.byteflow:dispatch pc=22, inst=LOAD_FAST(arg=0, lineno=1042)
DEBUG:numba.core.byteflow:stack ['$14compare_op.6', '$20binary_subscr.9']
DEBUG:numba.core.byteflow:dispatch pc=24, inst=LOAD_CONST(arg=3, lineno=1042)
DEBUG:numba.core.byteflow:stack ['$14compare_op.6', '$20binary_subscr.9', '$x22.10']
DEBUG:numba.core.byteflow:dispatch pc=26, inst=BINARY_SUBSCR(arg=None, lineno=1042)
DEBUG:numba.core.byteflow:stack ['$14compare_op.6', '$20binary_subscr.9', '$x22.10', '$const24.11']
DEBUG:numba.core.byteflow:dispatch pc=28, inst=COMPARE_OP(arg=5, lineno=1042)
DEBUG:numba.core.byteflow:stack ['$14compare_op.6', '$20binary_subscr.9', '$26binary_subscr.12']
DEBUG:numba.core.byteflow:dispatch pc=30, inst=BINARY_AND(arg=None, lineno=1042)
DEBUG:numba.core.byteflow:stack ['$14compare_op.6', '$28compare_op.13']
DEBUG:numba.core.byteflow:dispatch pc=32, inst=RETURN_VALUE(arg=None, lineno=1042)
DEBUG:numba.core.byteflow:stack ['$30binary_and.14']
DEBUG:numba.core.byteflow:end state. edges=[]
DEBUG:numba.core.byteflow:-------------------------Prune PHIs-------------------------
DEBUG:numba.core.byteflow:Used_phis: defaultdict(<class 'set'>, {State(pc_initial=0 nstack_initial=0): set()})
DEBUG:numba.core.byteflow:defmap: {}
DEBUG:numba.core.byteflow:phismap: defaultdict(<class 'set'>, {})
DEBUG:numba.core.byteflow:changing phismap: defaultdict(<class 'set'>, {})
DEBUG:numba.core.byteflow:keep phismap: {}
DEBUG:numba.core.byteflow:new_out: defaultdict(<class 'dict'>, {})
DEBUG:numba.core.byteflow:----------------------DONE Prune PHIs-----------------------
DEBUG:numba.core.byteflow:block_infos State(pc_initial=0 nstack_initial=0):
AdaptBlockInfo(insts=((0, {}), (2, {'res': '$x2.0'}), (4, {'res': '$const4.1'}), (6, {'index': '$const4.1', 'target': '$x2.0', 'res': '$6binary_subscr.2'}), (8, {'res': '$x8.3'}), (10, {'res': '$const10.4'}), (12, {'index': '$const10.4', 'target': '$x8.3', 'res': '$12binary_subscr.5'}), (14, {'lhs': '$6binary_subscr.2', 'rhs': '$12binary_subscr.5', 'res': '$14compare_op.6'}), (16, {'res': '$x16.7'}), (18, {'res': '$const18.8'}), (20, {'index': '$const18.8', 'target': '$x16.7', 'res': '$20binary_subscr.9'}), (22, {'res': '$x22.10'}), (24, {'res': '$const24.11'}), (26, {'index': '$const24.11', 'target': '$x22.10', 'res': '$26binary_subscr.12'}), (28, {'lhs': '$20binary_subscr.9', 'rhs': '$26binary_subscr.12', 'res': '$28compare_op.13'}), (30, {'lhs': '$14compare_op.6', 'rhs': '$28compare_op.13', 'res': '$30binary_and.14'}), (32, {'retval': '$30binary_and.14', 'castval': '$32return_value.15'})), outgoing_phis={}, blockstack=(), active_try_block=None, outgoing_edgepushed={})
DEBUG:numba.core.interpreter:label 0:
    x = arg(0, name=x)                       ['x']
    $const4.1 = const(int, 0)                ['$const4.1']
    $6binary_subscr.2 = getitem(value=x, index=$const4.1, fn=<built-in function getitem>) ['$6binary_subscr.2', '$const4.1', 'x']
    $const10.4 = const(int, -1)              ['$const10.4']
    $12binary_subscr.5 = getitem(value=x, index=$const10.4, fn=<built-in function getitem>) ['$12binary_subscr.5', '$const10.4', 'x']
    $14compare_op.6 = $6binary_subscr.2 > $12binary_subscr.5 ['$12binary_subscr.5', '$14compare_op.6', '$6binary_subscr.2']
    $const18.8 = const(int, 0)               ['$const18.8']
    $20binary_subscr.9 = getitem(value=x, index=$const18.8, fn=<built-in function getitem>) ['$20binary_subscr.9', '$const18.8', 'x']
    $const24.11 = const(int, 1)              ['$const24.11']
    $26binary_subscr.12 = getitem(value=x, index=$const24.11, fn=<built-in function getitem>) ['$26binary_subscr.12', '$const24.11', 'x']
    $28compare_op.13 = $20binary_subscr.9 >= $26binary_subscr.12 ['$20binary_subscr.9', '$26binary_subscr.12', '$28compare_op.13']
    $30binary_and.14 = $14compare_op.6 & $28compare_op.13 ['$14compare_op.6', '$28compare_op.13', '$30binary_and.14']
    $32return_value.15 = cast(value=$30binary_and.14) ['$30binary_and.14', '$32return_value.15']
    return $32return_value.15                ['$32return_value.15']

DEBUG:numba.core.byteflow:bytecode dump:
>          0	NOP(arg=None, lineno=1045)
           2	LOAD_FAST(arg=0, lineno=1048)
           4	LOAD_CONST(arg=1, lineno=1048)
           6	BINARY_SUBSCR(arg=None, lineno=1048)
           8	LOAD_FAST(arg=0, lineno=1048)
          10	LOAD_CONST(arg=2, lineno=1048)
          12	BINARY_SUBSCR(arg=None, lineno=1048)
          14	COMPARE_OP(arg=0, lineno=1048)
          16	LOAD_FAST(arg=0, lineno=1048)
          18	LOAD_CONST(arg=1, lineno=1048)
          20	BINARY_SUBSCR(arg=None, lineno=1048)
          22	LOAD_FAST(arg=0, lineno=1048)
          24	LOAD_CONST(arg=3, lineno=1048)
          26	BINARY_SUBSCR(arg=None, lineno=1048)
          28	COMPARE_OP(arg=1, lineno=1048)
          30	BINARY_AND(arg=None, lineno=1048)
          32	RETURN_VALUE(arg=None, lineno=1048)
DEBUG:numba.core.byteflow:pending: deque([State(pc_initial=0 nstack_initial=0)])
DEBUG:numba.core.byteflow:stack: []
DEBUG:numba.core.byteflow:state.pc_initial: State(pc_initial=0 nstack_initial=0)
DEBUG:numba.core.byteflow:dispatch pc=0, inst=NOP(arg=None, lineno=1045)
DEBUG:numba.core.byteflow:stack []
DEBUG:numba.core.byteflow:dispatch pc=2, inst=LOAD_FAST(arg=0, lineno=1048)
DEBUG:numba.core.byteflow:stack []
DEBUG:numba.core.byteflow:dispatch pc=4, inst=LOAD_CONST(arg=1, lineno=1048)
DEBUG:numba.core.byteflow:stack ['$x2.0']
DEBUG:numba.core.byteflow:dispatch pc=6, inst=BINARY_SUBSCR(arg=None, lineno=1048)
DEBUG:numba.core.byteflow:stack ['$x2.0', '$const4.1']
DEBUG:numba.core.byteflow:dispatch pc=8, inst=LOAD_FAST(arg=0, lineno=1048)
DEBUG:numba.core.byteflow:stack ['$6binary_subscr.2']
DEBUG:numba.core.byteflow:dispatch pc=10, inst=LOAD_CONST(arg=2, lineno=1048)
DEBUG:numba.core.byteflow:stack ['$6binary_subscr.2', '$x8.3']
DEBUG:numba.core.byteflow:dispatch pc=12, inst=BINARY_SUBSCR(arg=None, lineno=1048)
DEBUG:numba.core.byteflow:stack ['$6binary_subscr.2', '$x8.3', '$const10.4']
DEBUG:numba.core.byteflow:dispatch pc=14, inst=COMPARE_OP(arg=0, lineno=1048)
DEBUG:numba.core.byteflow:stack ['$6binary_subscr.2', '$12binary_subscr.5']
DEBUG:numba.core.byteflow:dispatch pc=16, inst=LOAD_FAST(arg=0, lineno=1048)
DEBUG:numba.core.byteflow:stack ['$14compare_op.6']
DEBUG:numba.core.byteflow:dispatch pc=18, inst=LOAD_CONST(arg=1, lineno=1048)
DEBUG:numba.core.byteflow:stack ['$14compare_op.6', '$x16.7']
DEBUG:numba.core.byteflow:dispatch pc=20, inst=BINARY_SUBSCR(arg=None, lineno=1048)
DEBUG:numba.core.byteflow:stack ['$14compare_op.6', '$x16.7', '$const18.8']
DEBUG:numba.core.byteflow:dispatch pc=22, inst=LOAD_FAST(arg=0, lineno=1048)
DEBUG:numba.core.byteflow:stack ['$14compare_op.6', '$20binary_subscr.9']
DEBUG:numba.core.byteflow:dispatch pc=24, inst=LOAD_CONST(arg=3, lineno=1048)
DEBUG:numba.core.byteflow:stack ['$14compare_op.6', '$20binary_subscr.9', '$x22.10']
DEBUG:numba.core.byteflow:dispatch pc=26, inst=BINARY_SUBSCR(arg=None, lineno=1048)
DEBUG:numba.core.byteflow:stack ['$14compare_op.6', '$20binary_subscr.9', '$x22.10', '$const24.11']
DEBUG:numba.core.byteflow:dispatch pc=28, inst=COMPARE_OP(arg=1, lineno=1048)
DEBUG:numba.core.byteflow:stack ['$14compare_op.6', '$20binary_subscr.9', '$26binary_subscr.12']
DEBUG:numba.core.byteflow:dispatch pc=30, inst=BINARY_AND(arg=None, lineno=1048)
DEBUG:numba.core.byteflow:stack ['$14compare_op.6', '$28compare_op.13']
DEBUG:numba.core.byteflow:dispatch pc=32, inst=RETURN_VALUE(arg=None, lineno=1048)
DEBUG:numba.core.byteflow:stack ['$30binary_and.14']
DEBUG:numba.core.byteflow:end state. edges=[]
DEBUG:numba.core.byteflow:-------------------------Prune PHIs-------------------------
DEBUG:numba.core.byteflow:Used_phis: defaultdict(<class 'set'>, {State(pc_initial=0 nstack_initial=0): set()})
DEBUG:numba.core.byteflow:defmap: {}
DEBUG:numba.core.byteflow:phismap: defaultdict(<class 'set'>, {})
DEBUG:numba.core.byteflow:changing phismap: defaultdict(<class 'set'>, {})
DEBUG:numba.core.byteflow:keep phismap: {}
DEBUG:numba.core.byteflow:new_out: defaultdict(<class 'dict'>, {})
DEBUG:numba.core.byteflow:----------------------DONE Prune PHIs-----------------------
DEBUG:numba.core.byteflow:block_infos State(pc_initial=0 nstack_initial=0):
AdaptBlockInfo(insts=((0, {}), (2, {'res': '$x2.0'}), (4, {'res': '$const4.1'}), (6, {'index': '$const4.1', 'target': '$x2.0', 'res': '$6binary_subscr.2'}), (8, {'res': '$x8.3'}), (10, {'res': '$const10.4'}), (12, {'index': '$const10.4', 'target': '$x8.3', 'res': '$12binary_subscr.5'}), (14, {'lhs': '$6binary_subscr.2', 'rhs': '$12binary_subscr.5', 'res': '$14compare_op.6'}), (16, {'res': '$x16.7'}), (18, {'res': '$const18.8'}), (20, {'index': '$const18.8', 'target': '$x16.7', 'res': '$20binary_subscr.9'}), (22, {'res': '$x22.10'}), (24, {'res': '$const24.11'}), (26, {'index': '$const24.11', 'target': '$x22.10', 'res': '$26binary_subscr.12'}), (28, {'lhs': '$20binary_subscr.9', 'rhs': '$26binary_subscr.12', 'res': '$28compare_op.13'}), (30, {'lhs': '$14compare_op.6', 'rhs': '$28compare_op.13', 'res': '$30binary_and.14'}), (32, {'retval': '$30binary_and.14', 'castval': '$32return_value.15'})), outgoing_phis={}, blockstack=(), active_try_block=None, outgoing_edgepushed={})
DEBUG:numba.core.interpreter:label 0:
    x = arg(0, name=x)                       ['x']
    $const4.1 = const(int, 0)                ['$const4.1']
    $6binary_subscr.2 = getitem(value=x, index=$const4.1, fn=<built-in function getitem>) ['$6binary_subscr.2', '$const4.1', 'x']
    $const10.4 = const(int, -1)              ['$const10.4']
    $12binary_subscr.5 = getitem(value=x, index=$const10.4, fn=<built-in function getitem>) ['$12binary_subscr.5', '$const10.4', 'x']
    $14compare_op.6 = $6binary_subscr.2 < $12binary_subscr.5 ['$12binary_subscr.5', '$14compare_op.6', '$6binary_subscr.2']
    $const18.8 = const(int, 0)               ['$const18.8']
    $20binary_subscr.9 = getitem(value=x, index=$const18.8, fn=<built-in function getitem>) ['$20binary_subscr.9', '$const18.8', 'x']
    $const24.11 = const(int, 1)              ['$const24.11']
    $26binary_subscr.12 = getitem(value=x, index=$const24.11, fn=<built-in function getitem>) ['$26binary_subscr.12', '$const24.11', 'x']
    $28compare_op.13 = $20binary_subscr.9 <= $26binary_subscr.12 ['$20binary_subscr.9', '$26binary_subscr.12', '$28compare_op.13']
    $30binary_and.14 = $14compare_op.6 & $28compare_op.13 ['$14compare_op.6', '$28compare_op.13', '$30binary_and.14']
    $32return_value.15 = cast(value=$30binary_and.14) ['$30binary_and.14', '$32return_value.15']
    return $32return_value.15                ['$32return_value.15']

ERROR:root:Error processing chapters: name 'ChatOpenAI' is not defined
Traceback (most recent call last):
  File "D:\github\chappymedium\chappie.py", line 260, in process_chapters
    self.chappie_processor = ChappieProcessor(api_key)
  File "D:\github\chappymedium\chappie_processor.py", line 11, in __init__
    self.llm = ChatOpenAI(
NameError: name 'ChatOpenAI' is not defined
DEBUG:numba.core.byteflow:bytecode dump:
>          0	NOP(arg=None, lineno=1141)
           2	LOAD_FAST(arg=0, lineno=1144)
           4	LOAD_CONST(arg=1, lineno=1144)
           6	BINARY_SUBSCR(arg=None, lineno=1144)
           8	STORE_FAST(arg=3, lineno=1144)
          10	LOAD_FAST(arg=1, lineno=1145)
          12	UNARY_NEGATIVE(arg=None, lineno=1145)
          14	LOAD_FAST(arg=3, lineno=1145)
          16	DUP_TOP(arg=None, lineno=1145)
          18	ROT_THREE(arg=None, lineno=1145)
          20	COMPARE_OP(arg=1, lineno=1145)
          22	POP_JUMP_IF_FALSE(arg=32, lineno=1145)
          24	LOAD_FAST(arg=1, lineno=1145)
          26	COMPARE_OP(arg=1, lineno=1145)
          28	POP_JUMP_IF_FALSE(arg=40, lineno=1145)
          30	JUMP_FORWARD(arg=4, lineno=1145)
>         32	POP_TOP(arg=None, lineno=1145)
          34	JUMP_FORWARD(arg=4, lineno=1145)
>         36	LOAD_CONST(arg=1, lineno=1146)
          38	STORE_FAST(arg=3, lineno=1146)
>         40	LOAD_FAST(arg=0, lineno=1148)
          42	LOAD_CONST(arg=2, lineno=1148)
          44	BINARY_SUBSCR(arg=None, lineno=1148)
          46	STORE_FAST(arg=4, lineno=1148)
          48	LOAD_FAST(arg=1, lineno=1149)
          50	UNARY_NEGATIVE(arg=None, lineno=1149)
          52	LOAD_FAST(arg=4, lineno=1149)
          54	DUP_TOP(arg=None, lineno=1149)
          56	ROT_THREE(arg=None, lineno=1149)
          58	COMPARE_OP(arg=1, lineno=1149)
          60	POP_JUMP_IF_FALSE(arg=70, lineno=1149)
          62	LOAD_FAST(arg=1, lineno=1149)
          64	COMPARE_OP(arg=1, lineno=1149)
          66	POP_JUMP_IF_FALSE(arg=78, lineno=1149)
          68	JUMP_FORWARD(arg=4, lineno=1149)
>         70	POP_TOP(arg=None, lineno=1149)
          72	JUMP_FORWARD(arg=4, lineno=1149)
>         74	LOAD_CONST(arg=1, lineno=1150)
          76	STORE_FAST(arg=4, lineno=1150)
>         78	LOAD_FAST(arg=2, lineno=1152)
          80	POP_JUMP_IF_FALSE(arg=102, lineno=1152)
          82	LOAD_GLOBAL(arg=0, lineno=1153)
          84	LOAD_METHOD(arg=1, lineno=1153)
          86	LOAD_FAST(arg=3, lineno=1153)
          88	CALL_METHOD(arg=1, lineno=1153)
          90	LOAD_GLOBAL(arg=0, lineno=1153)
          92	LOAD_METHOD(arg=1, lineno=1153)
          94	LOAD_FAST(arg=4, lineno=1153)
          96	CALL_METHOD(arg=1, lineno=1153)
          98	COMPARE_OP(arg=3, lineno=1153)
         100	RETURN_VALUE(arg=None, lineno=1153)
>        102	LOAD_GLOBAL(arg=0, lineno=1155)
         104	LOAD_METHOD(arg=2, lineno=1155)
         106	LOAD_FAST(arg=3, lineno=1155)
         108	CALL_METHOD(arg=1, lineno=1155)
         110	LOAD_GLOBAL(arg=0, lineno=1155)
         112	LOAD_METHOD(arg=2, lineno=1155)
         114	LOAD_FAST(arg=4, lineno=1155)
         116	CALL_METHOD(arg=1, lineno=1155)
         118	COMPARE_OP(arg=3, lineno=1155)
         120	RETURN_VALUE(arg=None, lineno=1155)
         122	LOAD_CONST(arg=3, lineno=1155)
         124	RETURN_VALUE(arg=None, lineno=1155)
DEBUG:numba.core.byteflow:pending: deque([State(pc_initial=0 nstack_initial=0)])
DEBUG:numba.core.byteflow:stack: []
DEBUG:numba.core.byteflow:state.pc_initial: State(pc_initial=0 nstack_initial=0)
DEBUG:numba.core.byteflow:dispatch pc=0, inst=NOP(arg=None, lineno=1141)
DEBUG:numba.core.byteflow:stack []
DEBUG:numba.core.byteflow:dispatch pc=2, inst=LOAD_FAST(arg=0, lineno=1144)
DEBUG:numba.core.byteflow:stack []
DEBUG:numba.core.byteflow:dispatch pc=4, inst=LOAD_CONST(arg=1, lineno=1144)
DEBUG:numba.core.byteflow:stack ['$x2.0']
DEBUG:numba.core.byteflow:dispatch pc=6, inst=BINARY_SUBSCR(arg=None, lineno=1144)
DEBUG:numba.core.byteflow:stack ['$x2.0', '$const4.1']
DEBUG:numba.core.byteflow:dispatch pc=8, inst=STORE_FAST(arg=3, lineno=1144)
DEBUG:numba.core.byteflow:stack ['$6binary_subscr.2']
DEBUG:numba.core.byteflow:dispatch pc=10, inst=LOAD_FAST(arg=1, lineno=1145)
DEBUG:numba.core.byteflow:stack []
DEBUG:numba.core.byteflow:dispatch pc=12, inst=UNARY_NEGATIVE(arg=None, lineno=1145)
DEBUG:numba.core.byteflow:stack ['$threshold10.3']
DEBUG:numba.core.byteflow:dispatch pc=14, inst=LOAD_FAST(arg=3, lineno=1145)
DEBUG:numba.core.byteflow:stack ['$12unary_negative.4']
DEBUG:numba.core.byteflow:dispatch pc=16, inst=DUP_TOP(arg=None, lineno=1145)
DEBUG:numba.core.byteflow:stack ['$12unary_negative.4', '$x014.5']
DEBUG:numba.core.byteflow:dispatch pc=18, inst=ROT_THREE(arg=None, lineno=1145)
DEBUG:numba.core.byteflow:stack ['$12unary_negative.4', '$x014.5', '$16dup_top.6']
DEBUG:numba.core.byteflow:dispatch pc=20, inst=COMPARE_OP(arg=1, lineno=1145)
DEBUG:numba.core.byteflow:stack ['$16dup_top.6', '$12unary_negative.4', '$x014.5']
DEBUG:numba.core.byteflow:dispatch pc=22, inst=POP_JUMP_IF_FALSE(arg=32, lineno=1145)
DEBUG:numba.core.byteflow:stack ['$16dup_top.6', '$20compare_op.7']
DEBUG:numba.core.byteflow:end state. edges=[Edge(pc=24, stack=('$16dup_top.6',), blockstack=(), npush=0), Edge(pc=32, stack=('$16dup_top.6',), blockstack=(), npush=0)]
DEBUG:numba.core.byteflow:pending: deque([State(pc_initial=24 nstack_initial=1), State(pc_initial=32 nstack_initial=1)])
DEBUG:numba.core.byteflow:stack: ['$phi24.0']
DEBUG:numba.core.byteflow:state.pc_initial: State(pc_initial=24 nstack_initial=1)
DEBUG:numba.core.byteflow:dispatch pc=24, inst=LOAD_FAST(arg=1, lineno=1145)
DEBUG:numba.core.byteflow:stack ['$phi24.0']
DEBUG:numba.core.byteflow:dispatch pc=26, inst=COMPARE_OP(arg=1, lineno=1145)
DEBUG:numba.core.byteflow:stack ['$phi24.0', '$threshold24.1']
DEBUG:numba.core.byteflow:dispatch pc=28, inst=POP_JUMP_IF_FALSE(arg=40, lineno=1145)
DEBUG:numba.core.byteflow:stack ['$26compare_op.2']
DEBUG:numba.core.byteflow:end state. edges=[Edge(pc=30, stack=(), blockstack=(), npush=0), Edge(pc=40, stack=(), blockstack=(), npush=0)]
DEBUG:numba.core.byteflow:pending: deque([State(pc_initial=32 nstack_initial=1), State(pc_initial=30 nstack_initial=0), State(pc_initial=40 nstack_initial=0)])
DEBUG:numba.core.byteflow:stack: ['$phi32.0']
DEBUG:numba.core.byteflow:state.pc_initial: State(pc_initial=32 nstack_initial=1)
DEBUG:numba.core.byteflow:dispatch pc=32, inst=POP_TOP(arg=None, lineno=1145)
DEBUG:numba.core.byteflow:stack ['$phi32.0']
DEBUG:numba.core.byteflow:dispatch pc=34, inst=JUMP_FORWARD(arg=4, lineno=1145)
DEBUG:numba.core.byteflow:stack []
DEBUG:numba.core.byteflow:end state. edges=[Edge(pc=40, stack=(), blockstack=(), npush=0)]
DEBUG:numba.core.byteflow:pending: deque([State(pc_initial=30 nstack_initial=0), State(pc_initial=40 nstack_initial=0), State(pc_initial=40 nstack_initial=0)])
DEBUG:numba.core.byteflow:stack: []
DEBUG:numba.core.byteflow:state.pc_initial: State(pc_initial=30 nstack_initial=0)
DEBUG:numba.core.byteflow:dispatch pc=30, inst=JUMP_FORWARD(arg=4, lineno=1145)
DEBUG:numba.core.byteflow:stack []
DEBUG:numba.core.byteflow:end state. edges=[Edge(pc=36, stack=(), blockstack=(), npush=0)]
DEBUG:numba.core.byteflow:pending: deque([State(pc_initial=40 nstack_initial=0), State(pc_initial=40 nstack_initial=0), State(pc_initial=36 nstack_initial=0)])
DEBUG:numba.core.byteflow:stack: []
DEBUG:numba.core.byteflow:state.pc_initial: State(pc_initial=40 nstack_initial=0)
DEBUG:numba.core.byteflow:dispatch pc=40, inst=LOAD_FAST(arg=0, lineno=1148)
DEBUG:numba.core.byteflow:stack []
DEBUG:numba.core.byteflow:dispatch pc=42, inst=LOAD_CONST(arg=2, lineno=1148)
DEBUG:numba.core.byteflow:stack ['$x40.0']
DEBUG:numba.core.byteflow:dispatch pc=44, inst=BINARY_SUBSCR(arg=None, lineno=1148)
DEBUG:numba.core.byteflow:stack ['$x40.0', '$const42.1']
DEBUG:numba.core.byteflow:dispatch pc=46, inst=STORE_FAST(arg=4, lineno=1148)
DEBUG:numba.core.byteflow:stack ['$44binary_subscr.2']
DEBUG:numba.core.byteflow:dispatch pc=48, inst=LOAD_FAST(arg=1, lineno=1149)
DEBUG:numba.core.byteflow:stack []
DEBUG:numba.core.byteflow:dispatch pc=50, inst=UNARY_NEGATIVE(arg=None, lineno=1149)
DEBUG:numba.core.byteflow:stack ['$threshold48.3']
DEBUG:numba.core.byteflow:dispatch pc=52, inst=LOAD_FAST(arg=4, lineno=1149)
DEBUG:numba.core.byteflow:stack ['$50unary_negative.4']
DEBUG:numba.core.byteflow:dispatch pc=54, inst=DUP_TOP(arg=None, lineno=1149)
DEBUG:numba.core.byteflow:stack ['$50unary_negative.4', '$x152.5']
DEBUG:numba.core.byteflow:dispatch pc=56, inst=ROT_THREE(arg=None, lineno=1149)
DEBUG:numba.core.byteflow:stack ['$50unary_negative.4', '$x152.5', '$54dup_top.6']
DEBUG:numba.core.byteflow:dispatch pc=58, inst=COMPARE_OP(arg=1, lineno=1149)
DEBUG:numba.core.byteflow:stack ['$54dup_top.6', '$50unary_negative.4', '$x152.5']
DEBUG:numba.core.byteflow:dispatch pc=60, inst=POP_JUMP_IF_FALSE(arg=70, lineno=1149)
DEBUG:numba.core.byteflow:stack ['$54dup_top.6', '$58compare_op.7']
DEBUG:numba.core.byteflow:end state. edges=[Edge(pc=62, stack=('$54dup_top.6',), blockstack=(), npush=0), Edge(pc=70, stack=('$54dup_top.6',), blockstack=(), npush=0)]
DEBUG:numba.core.byteflow:pending: deque([State(pc_initial=40 nstack_initial=0), State(pc_initial=36 nstack_initial=0), State(pc_initial=62 nstack_initial=1), State(pc_initial=70 nstack_initial=1)])
DEBUG:numba.core.byteflow:pending: deque([State(pc_initial=36 nstack_initial=0), State(pc_initial=62 nstack_initial=1), State(pc_initial=70 nstack_initial=1)])
DEBUG:numba.core.byteflow:stack: []
DEBUG:numba.core.byteflow:state.pc_initial: State(pc_initial=36 nstack_initial=0)
DEBUG:numba.core.byteflow:dispatch pc=36, inst=LOAD_CONST(arg=1, lineno=1146)
DEBUG:numba.core.byteflow:stack []
DEBUG:numba.core.byteflow:dispatch pc=38, inst=STORE_FAST(arg=3, lineno=1146)
DEBUG:numba.core.byteflow:stack ['$const36.0']
DEBUG:numba.core.byteflow:end state. edges=[Edge(pc=40, stack=(), blockstack=(), npush=0)]
DEBUG:numba.core.byteflow:pending: deque([State(pc_initial=62 nstack_initial=1), State(pc_initial=70 nstack_initial=1), State(pc_initial=40 nstack_initial=0)])
DEBUG:numba.core.byteflow:stack: ['$phi62.0']
DEBUG:numba.core.byteflow:state.pc_initial: State(pc_initial=62 nstack_initial=1)
DEBUG:numba.core.byteflow:dispatch pc=62, inst=LOAD_FAST(arg=1, lineno=1149)
DEBUG:numba.core.byteflow:stack ['$phi62.0']
DEBUG:numba.core.byteflow:dispatch pc=64, inst=COMPARE_OP(arg=1, lineno=1149)
DEBUG:numba.core.byteflow:stack ['$phi62.0', '$threshold62.1']
DEBUG:numba.core.byteflow:dispatch pc=66, inst=POP_JUMP_IF_FALSE(arg=78, lineno=1149)
DEBUG:numba.core.byteflow:stack ['$64compare_op.2']
DEBUG:numba.core.byteflow:end state. edges=[Edge(pc=68, stack=(), blockstack=(), npush=0), Edge(pc=78, stack=(), blockstack=(), npush=0)]
DEBUG:numba.core.byteflow:pending: deque([State(pc_initial=70 nstack_initial=1), State(pc_initial=40 nstack_initial=0), State(pc_initial=68 nstack_initial=0), State(pc_initial=78 nstack_initial=0)])
DEBUG:numba.core.byteflow:stack: ['$phi70.0']
DEBUG:numba.core.byteflow:state.pc_initial: State(pc_initial=70 nstack_initial=1)
DEBUG:numba.core.byteflow:dispatch pc=70, inst=POP_TOP(arg=None, lineno=1149)
DEBUG:numba.core.byteflow:stack ['$phi70.0']
DEBUG:numba.core.byteflow:dispatch pc=72, inst=JUMP_FORWARD(arg=4, lineno=1149)
DEBUG:numba.core.byteflow:stack []
DEBUG:numba.core.byteflow:end state. edges=[Edge(pc=78, stack=(), blockstack=(), npush=0)]
DEBUG:numba.core.byteflow:pending: deque([State(pc_initial=40 nstack_initial=0), State(pc_initial=68 nstack_initial=0), State(pc_initial=78 nstack_initial=0), State(pc_initial=78 nstack_initial=0)])
DEBUG:numba.core.byteflow:pending: deque([State(pc_initial=68 nstack_initial=0), State(pc_initial=78 nstack_initial=0), State(pc_initial=78 nstack_initial=0)])
DEBUG:numba.core.byteflow:stack: []
DEBUG:numba.core.byteflow:state.pc_initial: State(pc_initial=68 nstack_initial=0)
DEBUG:numba.core.byteflow:dispatch pc=68, inst=JUMP_FORWARD(arg=4, lineno=1149)
DEBUG:numba.core.byteflow:stack []
DEBUG:numba.core.byteflow:end state. edges=[Edge(pc=74, stack=(), blockstack=(), npush=0)]
DEBUG:numba.core.byteflow:pending: deque([State(pc_initial=78 nstack_initial=0), State(pc_initial=78 nstack_initial=0), State(pc_initial=74 nstack_initial=0)])
DEBUG:numba.core.byteflow:stack: []
DEBUG:numba.core.byteflow:state.pc_initial: State(pc_initial=78 nstack_initial=0)
DEBUG:numba.core.byteflow:dispatch pc=78, inst=LOAD_FAST(arg=2, lineno=1152)
DEBUG:numba.core.byteflow:stack []
DEBUG:numba.core.byteflow:dispatch pc=80, inst=POP_JUMP_IF_FALSE(arg=102, lineno=1152)
DEBUG:numba.core.byteflow:stack ['$zero_pos78.0']
DEBUG:numba.core.byteflow:end state. edges=[Edge(pc=82, stack=(), blockstack=(), npush=0), Edge(pc=102, stack=(), blockstack=(), npush=0)]
DEBUG:numba.core.byteflow:pending: deque([State(pc_initial=78 nstack_initial=0), State(pc_initial=74 nstack_initial=0), State(pc_initial=82 nstack_initial=0), State(pc_initial=102 nstack_initial=0)])
DEBUG:numba.core.byteflow:pending: deque([State(pc_initial=74 nstack_initial=0), State(pc_initial=82 nstack_initial=0), State(pc_initial=102 nstack_initial=0)])
DEBUG:numba.core.byteflow:stack: []
DEBUG:numba.core.byteflow:state.pc_initial: State(pc_initial=74 nstack_initial=0)
DEBUG:numba.core.byteflow:dispatch pc=74, inst=LOAD_CONST(arg=1, lineno=1150)
DEBUG:numba.core.byteflow:stack []
DEBUG:numba.core.byteflow:dispatch pc=76, inst=STORE_FAST(arg=4, lineno=1150)
DEBUG:numba.core.byteflow:stack ['$const74.0']
DEBUG:numba.core.byteflow:end state. edges=[Edge(pc=78, stack=(), blockstack=(), npush=0)]
DEBUG:numba.core.byteflow:pending: deque([State(pc_initial=82 nstack_initial=0), State(pc_initial=102 nstack_initial=0), State(pc_initial=78 nstack_initial=0)])
DEBUG:numba.core.byteflow:stack: []
DEBUG:numba.core.byteflow:state.pc_initial: State(pc_initial=82 nstack_initial=0)
DEBUG:numba.core.byteflow:dispatch pc=82, inst=LOAD_GLOBAL(arg=0, lineno=1153)
DEBUG:numba.core.byteflow:stack []
DEBUG:numba.core.byteflow:dispatch pc=84, inst=LOAD_METHOD(arg=1, lineno=1153)
DEBUG:numba.core.byteflow:stack ['$82load_global.0']
DEBUG:numba.core.byteflow:dispatch pc=86, inst=LOAD_FAST(arg=3, lineno=1153)
DEBUG:numba.core.byteflow:stack ['$84load_method.1']
DEBUG:numba.core.byteflow:dispatch pc=88, inst=CALL_METHOD(arg=1, lineno=1153)
DEBUG:numba.core.byteflow:stack ['$84load_method.1', '$x086.2']
DEBUG:numba.core.byteflow:dispatch pc=90, inst=LOAD_GLOBAL(arg=0, lineno=1153)
DEBUG:numba.core.byteflow:stack ['$88call_method.3']
DEBUG:numba.core.byteflow:dispatch pc=92, inst=LOAD_METHOD(arg=1, lineno=1153)
DEBUG:numba.core.byteflow:stack ['$88call_method.3', '$90load_global.4']
DEBUG:numba.core.byteflow:dispatch pc=94, inst=LOAD_FAST(arg=4, lineno=1153)
DEBUG:numba.core.byteflow:stack ['$88call_method.3', '$92load_method.5']
DEBUG:numba.core.byteflow:dispatch pc=96, inst=CALL_METHOD(arg=1, lineno=1153)
DEBUG:numba.core.byteflow:stack ['$88call_method.3', '$92load_method.5', '$x194.6']
DEBUG:numba.core.byteflow:dispatch pc=98, inst=COMPARE_OP(arg=3, lineno=1153)
DEBUG:numba.core.byteflow:stack ['$88call_method.3', '$96call_method.7']
DEBUG:numba.core.byteflow:dispatch pc=100, inst=RETURN_VALUE(arg=None, lineno=1153)
DEBUG:numba.core.byteflow:stack ['$98compare_op.8']
DEBUG:numba.core.byteflow:end state. edges=[]
DEBUG:numba.core.byteflow:pending: deque([State(pc_initial=102 nstack_initial=0), State(pc_initial=78 nstack_initial=0)])
DEBUG:numba.core.byteflow:stack: []
DEBUG:numba.core.byteflow:state.pc_initial: State(pc_initial=102 nstack_initial=0)
DEBUG:numba.core.byteflow:dispatch pc=102, inst=LOAD_GLOBAL(arg=0, lineno=1155)
DEBUG:numba.core.byteflow:stack []
DEBUG:numba.core.byteflow:dispatch pc=104, inst=LOAD_METHOD(arg=2, lineno=1155)
DEBUG:numba.core.byteflow:stack ['$102load_global.0']
DEBUG:numba.core.byteflow:dispatch pc=106, inst=LOAD_FAST(arg=3, lineno=1155)
DEBUG:numba.core.byteflow:stack ['$104load_method.1']
DEBUG:numba.core.byteflow:dispatch pc=108, inst=CALL_METHOD(arg=1, lineno=1155)
DEBUG:numba.core.byteflow:stack ['$104load_method.1', '$x0106.2']
DEBUG:numba.core.byteflow:dispatch pc=110, inst=LOAD_GLOBAL(arg=0, lineno=1155)
DEBUG:numba.core.byteflow:stack ['$108call_method.3']
DEBUG:numba.core.byteflow:dispatch pc=112, inst=LOAD_METHOD(arg=2, lineno=1155)
DEBUG:numba.core.byteflow:stack ['$108call_method.3', '$110load_global.4']
DEBUG:numba.core.byteflow:dispatch pc=114, inst=LOAD_FAST(arg=4, lineno=1155)
DEBUG:numba.core.byteflow:stack ['$108call_method.3', '$112load_method.5']
DEBUG:numba.core.byteflow:dispatch pc=116, inst=CALL_METHOD(arg=1, lineno=1155)
DEBUG:numba.core.byteflow:stack ['$108call_method.3', '$112load_method.5', '$x1114.6']
DEBUG:numba.core.byteflow:dispatch pc=118, inst=COMPARE_OP(arg=3, lineno=1155)
DEBUG:numba.core.byteflow:stack ['$108call_method.3', '$116call_method.7']
DEBUG:numba.core.byteflow:dispatch pc=120, inst=RETURN_VALUE(arg=None, lineno=1155)
DEBUG:numba.core.byteflow:stack ['$118compare_op.8']
DEBUG:numba.core.byteflow:end state. edges=[]
DEBUG:numba.core.byteflow:pending: deque([State(pc_initial=78 nstack_initial=0)])
DEBUG:numba.core.byteflow:-------------------------Prune PHIs-------------------------
DEBUG:numba.core.byteflow:Used_phis: defaultdict(<class 'set'>,
            {State(pc_initial=0 nstack_initial=0): set(),
             State(pc_initial=24 nstack_initial=1): {'$phi24.0'},
             State(pc_initial=30 nstack_initial=0): set(),
             State(pc_initial=32 nstack_initial=1): set(),
             State(pc_initial=36 nstack_initial=0): set(),
             State(pc_initial=40 nstack_initial=0): set(),
             State(pc_initial=62 nstack_initial=1): {'$phi62.0'},
             State(pc_initial=68 nstack_initial=0): set(),
             State(pc_initial=70 nstack_initial=1): set(),
             State(pc_initial=74 nstack_initial=0): set(),
             State(pc_initial=78 nstack_initial=0): set(),
             State(pc_initial=82 nstack_initial=0): set(),
             State(pc_initial=102 nstack_initial=0): set()})
DEBUG:numba.core.byteflow:defmap: {'$phi24.0': State(pc_initial=0 nstack_initial=0),
 '$phi32.0': State(pc_initial=0 nstack_initial=0),
 '$phi62.0': State(pc_initial=40 nstack_initial=0),
 '$phi70.0': State(pc_initial=40 nstack_initial=0)}
DEBUG:numba.core.byteflow:phismap: defaultdict(<class 'set'>,
            {'$phi24.0': {('$16dup_top.6',
                           State(pc_initial=0 nstack_initial=0))},
             '$phi32.0': {('$16dup_top.6',
                           State(pc_initial=0 nstack_initial=0))},
             '$phi62.0': {('$54dup_top.6',
                           State(pc_initial=40 nstack_initial=0))},
             '$phi70.0': {('$54dup_top.6',
                           State(pc_initial=40 nstack_initial=0))}})
DEBUG:numba.core.byteflow:changing phismap: defaultdict(<class 'set'>,
            {'$phi24.0': {('$16dup_top.6',
                           State(pc_initial=0 nstack_initial=0))},
             '$phi32.0': {('$16dup_top.6',
                           State(pc_initial=0 nstack_initial=0))},
             '$phi62.0': {('$54dup_top.6',
                           State(pc_initial=40 nstack_initial=0))},
             '$phi70.0': {('$54dup_top.6',
                           State(pc_initial=40 nstack_initial=0))}})
DEBUG:numba.core.byteflow:keep phismap: {'$phi24.0': {('$16dup_top.6', State(pc_initial=0 nstack_initial=0))},
 '$phi62.0': {('$54dup_top.6', State(pc_initial=40 nstack_initial=0))}}
DEBUG:numba.core.byteflow:new_out: defaultdict(<class 'dict'>,
            {State(pc_initial=0 nstack_initial=0): {'$phi24.0': '$16dup_top.6'},
             State(pc_initial=40 nstack_initial=0): {'$phi62.0': '$54dup_top.6'}})
DEBUG:numba.core.byteflow:----------------------DONE Prune PHIs-----------------------
DEBUG:numba.core.byteflow:block_infos State(pc_initial=0 nstack_initial=0):
AdaptBlockInfo(insts=((0, {}), (2, {'res': '$x2.0'}), (4, {'res': '$const4.1'}), (6, {'index': '$const4.1', 'target': '$x2.0', 'res': '$6binary_subscr.2'}), (8, {'value': '$6binary_subscr.2'}), (10, {'res': '$threshold10.3'}), (12, {'value': '$threshold10.3', 'res': '$12unary_negative.4'}), (14, {'res': '$x014.5'}), (16, {'orig': ['$x014.5'], 'duped': ['$16dup_top.6']}), (20, {'lhs': '$12unary_negative.4', 'rhs': '$x014.5', 'res': '$20compare_op.7'}), (22, {'pred': '$20compare_op.7'})), outgoing_phis={'$phi24.0': '$16dup_top.6'}, blockstack=(), active_try_block=None, outgoing_edgepushed={24: ('$16dup_top.6',), 32: ('$16dup_top.6',)})
DEBUG:numba.core.byteflow:block_infos State(pc_initial=24 nstack_initial=1):
AdaptBlockInfo(insts=((24, {'res': '$threshold24.1'}), (26, {'lhs': '$phi24.0', 'rhs': '$threshold24.1', 'res': '$26compare_op.2'}), (28, {'pred': '$26compare_op.2'})), outgoing_phis={}, blockstack=(), active_try_block=None, outgoing_edgepushed={30: (), 40: ()})
DEBUG:numba.core.byteflow:block_infos State(pc_initial=30 nstack_initial=0):
AdaptBlockInfo(insts=((30, {}),), outgoing_phis={}, blockstack=(), active_try_block=None, outgoing_edgepushed={36: ()})
DEBUG:numba.core.byteflow:block_infos State(pc_initial=32 nstack_initial=1):
AdaptBlockInfo(insts=((34, {}),), outgoing_phis={}, blockstack=(), active_try_block=None, outgoing_edgepushed={40: ()})
DEBUG:numba.core.byteflow:block_infos State(pc_initial=36 nstack_initial=0):
AdaptBlockInfo(insts=((36, {'res': '$const36.0'}), (38, {'value': '$const36.0'})), outgoing_phis={}, blockstack=(), active_try_block=None, outgoing_edgepushed={40: ()})
DEBUG:numba.core.byteflow:block_infos State(pc_initial=40 nstack_initial=0):
AdaptBlockInfo(insts=((40, {'res': '$x40.0'}), (42, {'res': '$const42.1'}), (44, {'index': '$const42.1', 'target': '$x40.0', 'res': '$44binary_subscr.2'}), (46, {'value': '$44binary_subscr.2'}), (48, {'res': '$threshold48.3'}), (50, {'value': '$threshold48.3', 'res': '$50unary_negative.4'}), (52, {'res': '$x152.5'}), (54, {'orig': ['$x152.5'], 'duped': ['$54dup_top.6']}), (58, {'lhs': '$50unary_negative.4', 'rhs': '$x152.5', 'res': '$58compare_op.7'}), (60, {'pred': '$58compare_op.7'})), outgoing_phis={'$phi62.0': '$54dup_top.6'}, blockstack=(), active_try_block=None, outgoing_edgepushed={62: ('$54dup_top.6',), 70: ('$54dup_top.6',)})
DEBUG:numba.core.byteflow:block_infos State(pc_initial=62 nstack_initial=1):
AdaptBlockInfo(insts=((62, {'res': '$threshold62.1'}), (64, {'lhs': '$phi62.0', 'rhs': '$threshold62.1', 'res': '$64compare_op.2'}), (66, {'pred': '$64compare_op.2'})), outgoing_phis={}, blockstack=(), active_try_block=None, outgoing_edgepushed={68: (), 78: ()})
DEBUG:numba.core.byteflow:block_infos State(pc_initial=68 nstack_initial=0):
AdaptBlockInfo(insts=((68, {}),), outgoing_phis={}, blockstack=(), active_try_block=None, outgoing_edgepushed={74: ()})
DEBUG:numba.core.byteflow:block_infos State(pc_initial=70 nstack_initial=1):
AdaptBlockInfo(insts=((72, {}),), outgoing_phis={}, blockstack=(), active_try_block=None, outgoing_edgepushed={78: ()})
DEBUG:numba.core.byteflow:block_infos State(pc_initial=74 nstack_initial=0):
AdaptBlockInfo(insts=((74, {'res': '$const74.0'}), (76, {'value': '$const74.0'})), outgoing_phis={}, blockstack=(), active_try_block=None, outgoing_edgepushed={78: ()})
DEBUG:numba.core.byteflow:block_infos State(pc_initial=78 nstack_initial=0):
AdaptBlockInfo(insts=((78, {'res': '$zero_pos78.0'}), (80, {'pred': '$zero_pos78.0'})), outgoing_phis={}, blockstack=(), active_try_block=None, outgoing_edgepushed={82: (), 102: ()})
DEBUG:numba.core.byteflow:block_infos State(pc_initial=82 nstack_initial=0):
AdaptBlockInfo(insts=((82, {'res': '$82load_global.0'}), (84, {'item': '$82load_global.0', 'res': '$84load_method.1'}), (86, {'res': '$x086.2'}), (88, {'func': '$84load_method.1', 'args': ['$x086.2'], 'res': '$88call_method.3'}), (90, {'res': '$90load_global.4'}), (92, {'item': '$90load_global.4', 'res': '$92load_method.5'}), (94, {'res': '$x194.6'}), (96, {'func': '$92load_method.5', 'args': ['$x194.6'], 'res': '$96call_method.7'}), (98, {'lhs': '$88call_method.3', 'rhs': '$96call_method.7', 'res': '$98compare_op.8'}), (100, {'retval': '$98compare_op.8', 'castval': '$100return_value.9'})), outgoing_phis={}, blockstack=(), active_try_block=None, outgoing_edgepushed={})
DEBUG:numba.core.byteflow:block_infos State(pc_initial=102 nstack_initial=0):
AdaptBlockInfo(insts=((102, {'res': '$102load_global.0'}), (104, {'item': '$102load_global.0', 'res': '$104load_method.1'}), (106, {'res': '$x0106.2'}), (108, {'func': '$104load_method.1', 'args': ['$x0106.2'], 'res': '$108call_method.3'}), (110, {'res': '$110load_global.4'}), (112, {'item': '$110load_global.4', 'res': '$112load_method.5'}), (114, {'res': '$x1114.6'}), (116, {'func': '$112load_method.5', 'args': ['$x1114.6'], 'res': '$116call_method.7'}), (118, {'lhs': '$108call_method.3', 'rhs': '$116call_method.7', 'res': '$118compare_op.8'}), (120, {'retval': '$118compare_op.8', 'castval': '$120return_value.9'})), outgoing_phis={}, blockstack=(), active_try_block=None, outgoing_edgepushed={})
DEBUG:numba.core.interpreter:label 0:
    x = arg(0, name=x)                       ['x']
    threshold = arg(1, name=threshold)       ['threshold']
    zero_pos = arg(2, name=zero_pos)         ['zero_pos']
    $const4.1 = const(int, 0)                ['$const4.1']
    x0 = getitem(value=x, index=$const4.1, fn=<built-in function getitem>) ['$const4.1', 'x', 'x0']
    $12unary_negative.4 = unary(fn=<built-in function neg>, value=threshold) ['$12unary_negative.4', 'threshold']
    $20compare_op.7 = $12unary_negative.4 <= x0 ['$12unary_negative.4', '$20compare_op.7', 'x0']
    bool22 = global(bool: <class 'bool'>)    ['bool22']
    $22pred = call bool22($20compare_op.7, func=bool22, args=(Var($20compare_op.7, audio.py:1145),), kws=(), vararg=None, varkwarg=None, target=None) ['$20compare_op.7', '$22pred', 'bool22']
    $phi24.0 = x0                            ['$phi24.0', 'x0']
    branch $22pred, 24, 32                   ['$22pred']
label 24:
    $26compare_op.2 = $phi24.0 <= threshold  ['$26compare_op.2', '$phi24.0', 'threshold']
    bool28 = global(bool: <class 'bool'>)    ['bool28']
    $28pred = call bool28($26compare_op.2, func=bool28, args=(Var($26compare_op.2, audio.py:1145),), kws=(), vararg=None, varkwarg=None, target=None) ['$26compare_op.2', '$28pred', 'bool28']
    branch $28pred, 30, 40                   ['$28pred']
label 30:
    jump 36                                  []
label 32:
    jump 40                                  []
label 36:
    x0 = const(int, 0)                       ['x0']
    jump 40                                  []
label 40:
    $const42.1 = const(int, -1)              ['$const42.1']
    x1 = getitem(value=x, index=$const42.1, fn=<built-in function getitem>) ['$const42.1', 'x', 'x1']
    $50unary_negative.4 = unary(fn=<built-in function neg>, value=threshold) ['$50unary_negative.4', 'threshold']
    $58compare_op.7 = $50unary_negative.4 <= x1 ['$50unary_negative.4', '$58compare_op.7', 'x1']
    bool60 = global(bool: <class 'bool'>)    ['bool60']
    $60pred = call bool60($58compare_op.7, func=bool60, args=(Var($58compare_op.7, audio.py:1149),), kws=(), vararg=None, varkwarg=None, target=None) ['$58compare_op.7', '$60pred', 'bool60']
    $phi62.0 = x1                            ['$phi62.0', 'x1']
    branch $60pred, 62, 70                   ['$60pred']
label 62:
    $64compare_op.2 = $phi62.0 <= threshold  ['$64compare_op.2', '$phi62.0', 'threshold']
    bool66 = global(bool: <class 'bool'>)    ['bool66']
    $66pred = call bool66($64compare_op.2, func=bool66, args=(Var($64compare_op.2, audio.py:1149),), kws=(), vararg=None, varkwarg=None, target=None) ['$64compare_op.2', '$66pred', 'bool66']
    branch $66pred, 68, 78                   ['$66pred']
label 68:
    jump 74                                  []
label 70:
    jump 78                                  []
label 74:
    x1 = const(int, 0)                       ['x1']
    jump 78                                  []
label 78:
    bool80 = global(bool: <class 'bool'>)    ['bool80']
    $80pred = call bool80(zero_pos, func=bool80, args=(Var(zero_pos, audio.py:1141),), kws=(), vararg=None, varkwarg=None, target=None) ['$80pred', 'bool80', 'zero_pos']
    branch $80pred, 82, 102                  ['$80pred']
label 82:
    $82load_global.0 = global(np: <module 'numpy' from 'C:\\Users\\taskm\\anaconda3\\envs\\taskai\\lib\\site-packages\\numpy\\__init__.py'>) ['$82load_global.0']
    $84load_method.1 = getattr(value=$82load_global.0, attr=signbit) ['$82load_global.0', '$84load_method.1']
    $88call_method.3 = call $84load_method.1(x0, func=$84load_method.1, args=[Var(x0, audio.py:1144)], kws=(), vararg=None, varkwarg=None, target=None) ['$84load_method.1', '$88call_method.3', 'x0']
    $90load_global.4 = global(np: <module 'numpy' from 'C:\\Users\\taskm\\anaconda3\\envs\\taskai\\lib\\site-packages\\numpy\\__init__.py'>) ['$90load_global.4']
    $92load_method.5 = getattr(value=$90load_global.4, attr=signbit) ['$90load_global.4', '$92load_method.5']
    $96call_method.7 = call $92load_method.5(x1, func=$92load_method.5, args=[Var(x1, audio.py:1148)], kws=(), vararg=None, varkwarg=None, target=None) ['$92load_method.5', '$96call_method.7', 'x1']
    $98compare_op.8 = $88call_method.3 != $96call_method.7 ['$88call_method.3', '$96call_method.7', '$98compare_op.8']
    $100return_value.9 = cast(value=$98compare_op.8) ['$100return_value.9', '$98compare_op.8']
    return $100return_value.9                ['$100return_value.9']
label 102:
    $102load_global.0 = global(np: <module 'numpy' from 'C:\\Users\\taskm\\anaconda3\\envs\\taskai\\lib\\site-packages\\numpy\\__init__.py'>) ['$102load_global.0']
    $104load_method.1 = getattr(value=$102load_global.0, attr=sign) ['$102load_global.0', '$104load_method.1']
    $108call_method.3 = call $104load_method.1(x0, func=$104load_method.1, args=[Var(x0, audio.py:1144)], kws=(), vararg=None, varkwarg=None, target=None) ['$104load_method.1', '$108call_method.3', 'x0']
    $110load_global.4 = global(np: <module 'numpy' from 'C:\\Users\\taskm\\anaconda3\\envs\\taskai\\lib\\site-packages\\numpy\\__init__.py'>) ['$110load_global.4']
    $112load_method.5 = getattr(value=$110load_global.4, attr=sign) ['$110load_global.4', '$112load_method.5']
    $116call_method.7 = call $112load_method.5(x1, func=$112load_method.5, args=[Var(x1, audio.py:1148)], kws=(), vararg=None, varkwarg=None, target=None) ['$112load_method.5', '$116call_method.7', 'x1']
    $118compare_op.8 = $108call_method.3 != $116call_method.7 ['$108call_method.3', '$116call_method.7', '$118compare_op.8']
    $120return_value.9 = cast(value=$118compare_op.8) ['$118compare_op.8', '$120return_value.9']
    return $120return_value.9                ['$120return_value.9']

DEBUG:numba.core.byteflow:bytecode dump:
>          0	NOP(arg=None, lineno=1039)
           2	LOAD_FAST(arg=0, lineno=1042)
           4	LOAD_CONST(arg=1, lineno=1042)
           6	BINARY_SUBSCR(arg=None, lineno=1042)
           8	LOAD_FAST(arg=0, lineno=1042)
          10	LOAD_CONST(arg=2, lineno=1042)
          12	BINARY_SUBSCR(arg=None, lineno=1042)
          14	COMPARE_OP(arg=4, lineno=1042)
          16	LOAD_FAST(arg=0, lineno=1042)
          18	LOAD_CONST(arg=1, lineno=1042)
          20	BINARY_SUBSCR(arg=None, lineno=1042)
          22	LOAD_FAST(arg=0, lineno=1042)
          24	LOAD_CONST(arg=3, lineno=1042)
          26	BINARY_SUBSCR(arg=None, lineno=1042)
          28	COMPARE_OP(arg=5, lineno=1042)
          30	BINARY_AND(arg=None, lineno=1042)
          32	RETURN_VALUE(arg=None, lineno=1042)
DEBUG:numba.core.byteflow:pending: deque([State(pc_initial=0 nstack_initial=0)])
DEBUG:numba.core.byteflow:stack: []
DEBUG:numba.core.byteflow:state.pc_initial: State(pc_initial=0 nstack_initial=0)
DEBUG:numba.core.byteflow:dispatch pc=0, inst=NOP(arg=None, lineno=1039)
DEBUG:numba.core.byteflow:stack []
DEBUG:numba.core.byteflow:dispatch pc=2, inst=LOAD_FAST(arg=0, lineno=1042)
DEBUG:numba.core.byteflow:stack []
DEBUG:numba.core.byteflow:dispatch pc=4, inst=LOAD_CONST(arg=1, lineno=1042)
DEBUG:numba.core.byteflow:stack ['$x2.0']
DEBUG:numba.core.byteflow:dispatch pc=6, inst=BINARY_SUBSCR(arg=None, lineno=1042)
DEBUG:numba.core.byteflow:stack ['$x2.0', '$const4.1']
DEBUG:numba.core.byteflow:dispatch pc=8, inst=LOAD_FAST(arg=0, lineno=1042)
DEBUG:numba.core.byteflow:stack ['$6binary_subscr.2']
DEBUG:numba.core.byteflow:dispatch pc=10, inst=LOAD_CONST(arg=2, lineno=1042)
DEBUG:numba.core.byteflow:stack ['$6binary_subscr.2', '$x8.3']
DEBUG:numba.core.byteflow:dispatch pc=12, inst=BINARY_SUBSCR(arg=None, lineno=1042)
DEBUG:numba.core.byteflow:stack ['$6binary_subscr.2', '$x8.3', '$const10.4']
DEBUG:numba.core.byteflow:dispatch pc=14, inst=COMPARE_OP(arg=4, lineno=1042)
DEBUG:numba.core.byteflow:stack ['$6binary_subscr.2', '$12binary_subscr.5']
DEBUG:numba.core.byteflow:dispatch pc=16, inst=LOAD_FAST(arg=0, lineno=1042)
DEBUG:numba.core.byteflow:stack ['$14compare_op.6']
DEBUG:numba.core.byteflow:dispatch pc=18, inst=LOAD_CONST(arg=1, lineno=1042)
DEBUG:numba.core.byteflow:stack ['$14compare_op.6', '$x16.7']
DEBUG:numba.core.byteflow:dispatch pc=20, inst=BINARY_SUBSCR(arg=None, lineno=1042)
DEBUG:numba.core.byteflow:stack ['$14compare_op.6', '$x16.7', '$const18.8']
DEBUG:numba.core.byteflow:dispatch pc=22, inst=LOAD_FAST(arg=0, lineno=1042)
DEBUG:numba.core.byteflow:stack ['$14compare_op.6', '$20binary_subscr.9']
DEBUG:numba.core.byteflow:dispatch pc=24, inst=LOAD_CONST(arg=3, lineno=1042)
DEBUG:numba.core.byteflow:stack ['$14compare_op.6', '$20binary_subscr.9', '$x22.10']
DEBUG:numba.core.byteflow:dispatch pc=26, inst=BINARY_SUBSCR(arg=None, lineno=1042)
DEBUG:numba.core.byteflow:stack ['$14compare_op.6', '$20binary_subscr.9', '$x22.10', '$const24.11']
DEBUG:numba.core.byteflow:dispatch pc=28, inst=COMPARE_OP(arg=5, lineno=1042)
DEBUG:numba.core.byteflow:stack ['$14compare_op.6', '$20binary_subscr.9', '$26binary_subscr.12']
DEBUG:numba.core.byteflow:dispatch pc=30, inst=BINARY_AND(arg=None, lineno=1042)
DEBUG:numba.core.byteflow:stack ['$14compare_op.6', '$28compare_op.13']
DEBUG:numba.core.byteflow:dispatch pc=32, inst=RETURN_VALUE(arg=None, lineno=1042)
DEBUG:numba.core.byteflow:stack ['$30binary_and.14']
DEBUG:numba.core.byteflow:end state. edges=[]
DEBUG:numba.core.byteflow:-------------------------Prune PHIs-------------------------
DEBUG:numba.core.byteflow:Used_phis: defaultdict(<class 'set'>, {State(pc_initial=0 nstack_initial=0): set()})
DEBUG:numba.core.byteflow:defmap: {}
DEBUG:numba.core.byteflow:phismap: defaultdict(<class 'set'>, {})
DEBUG:numba.core.byteflow:changing phismap: defaultdict(<class 'set'>, {})
DEBUG:numba.core.byteflow:keep phismap: {}
DEBUG:numba.core.byteflow:new_out: defaultdict(<class 'dict'>, {})
DEBUG:numba.core.byteflow:----------------------DONE Prune PHIs-----------------------
DEBUG:numba.core.byteflow:block_infos State(pc_initial=0 nstack_initial=0):
AdaptBlockInfo(insts=((0, {}), (2, {'res': '$x2.0'}), (4, {'res': '$const4.1'}), (6, {'index': '$const4.1', 'target': '$x2.0', 'res': '$6binary_subscr.2'}), (8, {'res': '$x8.3'}), (10, {'res': '$const10.4'}), (12, {'index': '$const10.4', 'target': '$x8.3', 'res': '$12binary_subscr.5'}), (14, {'lhs': '$6binary_subscr.2', 'rhs': '$12binary_subscr.5', 'res': '$14compare_op.6'}), (16, {'res': '$x16.7'}), (18, {'res': '$const18.8'}), (20, {'index': '$const18.8', 'target': '$x16.7', 'res': '$20binary_subscr.9'}), (22, {'res': '$x22.10'}), (24, {'res': '$const24.11'}), (26, {'index': '$const24.11', 'target': '$x22.10', 'res': '$26binary_subscr.12'}), (28, {'lhs': '$20binary_subscr.9', 'rhs': '$26binary_subscr.12', 'res': '$28compare_op.13'}), (30, {'lhs': '$14compare_op.6', 'rhs': '$28compare_op.13', 'res': '$30binary_and.14'}), (32, {'retval': '$30binary_and.14', 'castval': '$32return_value.15'})), outgoing_phis={}, blockstack=(), active_try_block=None, outgoing_edgepushed={})
DEBUG:numba.core.interpreter:label 0:
    x = arg(0, name=x)                       ['x']
    $const4.1 = const(int, 0)                ['$const4.1']
    $6binary_subscr.2 = getitem(value=x, index=$const4.1, fn=<built-in function getitem>) ['$6binary_subscr.2', '$const4.1', 'x']
    $const10.4 = const(int, -1)              ['$const10.4']
    $12binary_subscr.5 = getitem(value=x, index=$const10.4, fn=<built-in function getitem>) ['$12binary_subscr.5', '$const10.4', 'x']
    $14compare_op.6 = $6binary_subscr.2 > $12binary_subscr.5 ['$12binary_subscr.5', '$14compare_op.6', '$6binary_subscr.2']
    $const18.8 = const(int, 0)               ['$const18.8']
    $20binary_subscr.9 = getitem(value=x, index=$const18.8, fn=<built-in function getitem>) ['$20binary_subscr.9', '$const18.8', 'x']
    $const24.11 = const(int, 1)              ['$const24.11']
    $26binary_subscr.12 = getitem(value=x, index=$const24.11, fn=<built-in function getitem>) ['$26binary_subscr.12', '$const24.11', 'x']
    $28compare_op.13 = $20binary_subscr.9 >= $26binary_subscr.12 ['$20binary_subscr.9', '$26binary_subscr.12', '$28compare_op.13']
    $30binary_and.14 = $14compare_op.6 & $28compare_op.13 ['$14compare_op.6', '$28compare_op.13', '$30binary_and.14']
    $32return_value.15 = cast(value=$30binary_and.14) ['$30binary_and.14', '$32return_value.15']
    return $32return_value.15                ['$32return_value.15']

DEBUG:numba.core.byteflow:bytecode dump:
>          0	NOP(arg=None, lineno=1045)
           2	LOAD_FAST(arg=0, lineno=1048)
           4	LOAD_CONST(arg=1, lineno=1048)
           6	BINARY_SUBSCR(arg=None, lineno=1048)
           8	LOAD_FAST(arg=0, lineno=1048)
          10	LOAD_CONST(arg=2, lineno=1048)
          12	BINARY_SUBSCR(arg=None, lineno=1048)
          14	COMPARE_OP(arg=0, lineno=1048)
          16	LOAD_FAST(arg=0, lineno=1048)
          18	LOAD_CONST(arg=1, lineno=1048)
          20	BINARY_SUBSCR(arg=None, lineno=1048)
          22	LOAD_FAST(arg=0, lineno=1048)
          24	LOAD_CONST(arg=3, lineno=1048)
          26	BINARY_SUBSCR(arg=None, lineno=1048)
          28	COMPARE_OP(arg=1, lineno=1048)
          30	BINARY_AND(arg=None, lineno=1048)
          32	RETURN_VALUE(arg=None, lineno=1048)
DEBUG:numba.core.byteflow:pending: deque([State(pc_initial=0 nstack_initial=0)])
DEBUG:numba.core.byteflow:stack: []
DEBUG:numba.core.byteflow:state.pc_initial: State(pc_initial=0 nstack_initial=0)
DEBUG:numba.core.byteflow:dispatch pc=0, inst=NOP(arg=None, lineno=1045)
DEBUG:numba.core.byteflow:stack []
DEBUG:numba.core.byteflow:dispatch pc=2, inst=LOAD_FAST(arg=0, lineno=1048)
DEBUG:numba.core.byteflow:stack []
DEBUG:numba.core.byteflow:dispatch pc=4, inst=LOAD_CONST(arg=1, lineno=1048)
DEBUG:numba.core.byteflow:stack ['$x2.0']
DEBUG:numba.core.byteflow:dispatch pc=6, inst=BINARY_SUBSCR(arg=None, lineno=1048)
DEBUG:numba.core.byteflow:stack ['$x2.0', '$const4.1']
DEBUG:numba.core.byteflow:dispatch pc=8, inst=LOAD_FAST(arg=0, lineno=1048)
DEBUG:numba.core.byteflow:stack ['$6binary_subscr.2']
DEBUG:numba.core.byteflow:dispatch pc=10, inst=LOAD_CONST(arg=2, lineno=1048)
DEBUG:numba.core.byteflow:stack ['$6binary_subscr.2', '$x8.3']
DEBUG:numba.core.byteflow:dispatch pc=12, inst=BINARY_SUBSCR(arg=None, lineno=1048)
DEBUG:numba.core.byteflow:stack ['$6binary_subscr.2', '$x8.3', '$const10.4']
DEBUG:numba.core.byteflow:dispatch pc=14, inst=COMPARE_OP(arg=0, lineno=1048)
DEBUG:numba.core.byteflow:stack ['$6binary_subscr.2', '$12binary_subscr.5']
DEBUG:numba.core.byteflow:dispatch pc=16, inst=LOAD_FAST(arg=0, lineno=1048)
DEBUG:numba.core.byteflow:stack ['$14compare_op.6']
DEBUG:numba.core.byteflow:dispatch pc=18, inst=LOAD_CONST(arg=1, lineno=1048)
DEBUG:numba.core.byteflow:stack ['$14compare_op.6', '$x16.7']
DEBUG:numba.core.byteflow:dispatch pc=20, inst=BINARY_SUBSCR(arg=None, lineno=1048)
DEBUG:numba.core.byteflow:stack ['$14compare_op.6', '$x16.7', '$const18.8']
DEBUG:numba.core.byteflow:dispatch pc=22, inst=LOAD_FAST(arg=0, lineno=1048)
DEBUG:numba.core.byteflow:stack ['$14compare_op.6', '$20binary_subscr.9']
DEBUG:numba.core.byteflow:dispatch pc=24, inst=LOAD_CONST(arg=3, lineno=1048)
DEBUG:numba.core.byteflow:stack ['$14compare_op.6', '$20binary_subscr.9', '$x22.10']
DEBUG:numba.core.byteflow:dispatch pc=26, inst=BINARY_SUBSCR(arg=None, lineno=1048)
DEBUG:numba.core.byteflow:stack ['$14compare_op.6', '$20binary_subscr.9', '$x22.10', '$const24.11']
DEBUG:numba.core.byteflow:dispatch pc=28, inst=COMPARE_OP(arg=1, lineno=1048)
DEBUG:numba.core.byteflow:stack ['$14compare_op.6', '$20binary_subscr.9', '$26binary_subscr.12']
DEBUG:numba.core.byteflow:dispatch pc=30, inst=BINARY_AND(arg=None, lineno=1048)
DEBUG:numba.core.byteflow:stack ['$14compare_op.6', '$28compare_op.13']
DEBUG:numba.core.byteflow:dispatch pc=32, inst=RETURN_VALUE(arg=None, lineno=1048)
DEBUG:numba.core.byteflow:stack ['$30binary_and.14']
DEBUG:numba.core.byteflow:end state. edges=[]
DEBUG:numba.core.byteflow:-------------------------Prune PHIs-------------------------
DEBUG:numba.core.byteflow:Used_phis: defaultdict(<class 'set'>, {State(pc_initial=0 nstack_initial=0): set()})
DEBUG:numba.core.byteflow:defmap: {}
DEBUG:numba.core.byteflow:phismap: defaultdict(<class 'set'>, {})
DEBUG:numba.core.byteflow:changing phismap: defaultdict(<class 'set'>, {})
DEBUG:numba.core.byteflow:keep phismap: {}
DEBUG:numba.core.byteflow:new_out: defaultdict(<class 'dict'>, {})
DEBUG:numba.core.byteflow:----------------------DONE Prune PHIs-----------------------
DEBUG:numba.core.byteflow:block_infos State(pc_initial=0 nstack_initial=0):
AdaptBlockInfo(insts=((0, {}), (2, {'res': '$x2.0'}), (4, {'res': '$const4.1'}), (6, {'index': '$const4.1', 'target': '$x2.0', 'res': '$6binary_subscr.2'}), (8, {'res': '$x8.3'}), (10, {'res': '$const10.4'}), (12, {'index': '$const10.4', 'target': '$x8.3', 'res': '$12binary_subscr.5'}), (14, {'lhs': '$6binary_subscr.2', 'rhs': '$12binary_subscr.5', 'res': '$14compare_op.6'}), (16, {'res': '$x16.7'}), (18, {'res': '$const18.8'}), (20, {'index': '$const18.8', 'target': '$x16.7', 'res': '$20binary_subscr.9'}), (22, {'res': '$x22.10'}), (24, {'res': '$const24.11'}), (26, {'index': '$const24.11', 'target': '$x22.10', 'res': '$26binary_subscr.12'}), (28, {'lhs': '$20binary_subscr.9', 'rhs': '$26binary_subscr.12', 'res': '$28compare_op.13'}), (30, {'lhs': '$14compare_op.6', 'rhs': '$28compare_op.13', 'res': '$30binary_and.14'}), (32, {'retval': '$30binary_and.14', 'castval': '$32return_value.15'})), outgoing_phis={}, blockstack=(), active_try_block=None, outgoing_edgepushed={})
DEBUG:numba.core.interpreter:label 0:
    x = arg(0, name=x)                       ['x']
    $const4.1 = const(int, 0)                ['$const4.1']
    $6binary_subscr.2 = getitem(value=x, index=$const4.1, fn=<built-in function getitem>) ['$6binary_subscr.2', '$const4.1', 'x']
    $const10.4 = const(int, -1)              ['$const10.4']
    $12binary_subscr.5 = getitem(value=x, index=$const10.4, fn=<built-in function getitem>) ['$12binary_subscr.5', '$const10.4', 'x']
    $14compare_op.6 = $6binary_subscr.2 < $12binary_subscr.5 ['$12binary_subscr.5', '$14compare_op.6', '$6binary_subscr.2']
    $const18.8 = const(int, 0)               ['$const18.8']
    $20binary_subscr.9 = getitem(value=x, index=$const18.8, fn=<built-in function getitem>) ['$20binary_subscr.9', '$const18.8', 'x']
    $const24.11 = const(int, 1)              ['$const24.11']
    $26binary_subscr.12 = getitem(value=x, index=$const24.11, fn=<built-in function getitem>) ['$26binary_subscr.12', '$const24.11', 'x']
    $28compare_op.13 = $20binary_subscr.9 <= $26binary_subscr.12 ['$20binary_subscr.9', '$26binary_subscr.12', '$28compare_op.13']
    $30binary_and.14 = $14compare_op.6 & $28compare_op.13 ['$14compare_op.6', '$28compare_op.13', '$30binary_and.14']
    $32return_value.15 = cast(value=$30binary_and.14) ['$30binary_and.14', '$32return_value.15']
    return $32return_value.15                ['$32return_value.15']

ERROR:root:Error processing chapters: name 'ChatOpenAI' is not defined
Traceback (most recent call last):
  File "D:\github\chappymedium\chappie.py", line 260, in process_chapters
    self.chappie_processor = ChappieProcessor(api_key)
  File "D:\github\chappymedium\chappie_processor.py", line 11, in __init__
    self.llm = ChatOpenAI(
NameError: name 'ChatOpenAI' is not defined
DEBUG:httpx:load_ssl_context verify=True cert=None trust_env=True http2=False
DEBUG:httpx:load_verify_locations cafile='C:\\Users\\taskm\\anaconda3\\Library\\ssl\\cacert.pem'
DEBUG:httpx:load_ssl_context verify=True cert=None trust_env=True http2=False
DEBUG:httpx:load_verify_locations cafile='C:\\Users\\taskm\\anaconda3\\Library\\ssl\\cacert.pem'
DEBUG:numba.core.byteflow:bytecode dump:
>          0	NOP(arg=None, lineno=1141)
           2	LOAD_FAST(arg=0, lineno=1144)
           4	LOAD_CONST(arg=1, lineno=1144)
           6	BINARY_SUBSCR(arg=None, lineno=1144)
           8	STORE_FAST(arg=3, lineno=1144)
          10	LOAD_FAST(arg=1, lineno=1145)
          12	UNARY_NEGATIVE(arg=None, lineno=1145)
          14	LOAD_FAST(arg=3, lineno=1145)
          16	DUP_TOP(arg=None, lineno=1145)
          18	ROT_THREE(arg=None, lineno=1145)
          20	COMPARE_OP(arg=1, lineno=1145)
          22	POP_JUMP_IF_FALSE(arg=32, lineno=1145)
          24	LOAD_FAST(arg=1, lineno=1145)
          26	COMPARE_OP(arg=1, lineno=1145)
          28	POP_JUMP_IF_FALSE(arg=40, lineno=1145)
          30	JUMP_FORWARD(arg=4, lineno=1145)
>         32	POP_TOP(arg=None, lineno=1145)
          34	JUMP_FORWARD(arg=4, lineno=1145)
>         36	LOAD_CONST(arg=1, lineno=1146)
          38	STORE_FAST(arg=3, lineno=1146)
>         40	LOAD_FAST(arg=0, lineno=1148)
          42	LOAD_CONST(arg=2, lineno=1148)
          44	BINARY_SUBSCR(arg=None, lineno=1148)
          46	STORE_FAST(arg=4, lineno=1148)
          48	LOAD_FAST(arg=1, lineno=1149)
          50	UNARY_NEGATIVE(arg=None, lineno=1149)
          52	LOAD_FAST(arg=4, lineno=1149)
          54	DUP_TOP(arg=None, lineno=1149)
          56	ROT_THREE(arg=None, lineno=1149)
          58	COMPARE_OP(arg=1, lineno=1149)
          60	POP_JUMP_IF_FALSE(arg=70, lineno=1149)
          62	LOAD_FAST(arg=1, lineno=1149)
          64	COMPARE_OP(arg=1, lineno=1149)
          66	POP_JUMP_IF_FALSE(arg=78, lineno=1149)
          68	JUMP_FORWARD(arg=4, lineno=1149)
>         70	POP_TOP(arg=None, lineno=1149)
          72	JUMP_FORWARD(arg=4, lineno=1149)
>         74	LOAD_CONST(arg=1, lineno=1150)
          76	STORE_FAST(arg=4, lineno=1150)
>         78	LOAD_FAST(arg=2, lineno=1152)
          80	POP_JUMP_IF_FALSE(arg=102, lineno=1152)
          82	LOAD_GLOBAL(arg=0, lineno=1153)
          84	LOAD_METHOD(arg=1, lineno=1153)
          86	LOAD_FAST(arg=3, lineno=1153)
          88	CALL_METHOD(arg=1, lineno=1153)
          90	LOAD_GLOBAL(arg=0, lineno=1153)
          92	LOAD_METHOD(arg=1, lineno=1153)
          94	LOAD_FAST(arg=4, lineno=1153)
          96	CALL_METHOD(arg=1, lineno=1153)
          98	COMPARE_OP(arg=3, lineno=1153)
         100	RETURN_VALUE(arg=None, lineno=1153)
>        102	LOAD_GLOBAL(arg=0, lineno=1155)
         104	LOAD_METHOD(arg=2, lineno=1155)
         106	LOAD_FAST(arg=3, lineno=1155)
         108	CALL_METHOD(arg=1, lineno=1155)
         110	LOAD_GLOBAL(arg=0, lineno=1155)
         112	LOAD_METHOD(arg=2, lineno=1155)
         114	LOAD_FAST(arg=4, lineno=1155)
         116	CALL_METHOD(arg=1, lineno=1155)
         118	COMPARE_OP(arg=3, lineno=1155)
         120	RETURN_VALUE(arg=None, lineno=1155)
         122	LOAD_CONST(arg=3, lineno=1155)
         124	RETURN_VALUE(arg=None, lineno=1155)
DEBUG:numba.core.byteflow:pending: deque([State(pc_initial=0 nstack_initial=0)])
DEBUG:numba.core.byteflow:stack: []
DEBUG:numba.core.byteflow:state.pc_initial: State(pc_initial=0 nstack_initial=0)
DEBUG:numba.core.byteflow:dispatch pc=0, inst=NOP(arg=None, lineno=1141)
DEBUG:numba.core.byteflow:stack []
DEBUG:numba.core.byteflow:dispatch pc=2, inst=LOAD_FAST(arg=0, lineno=1144)
DEBUG:numba.core.byteflow:stack []
DEBUG:numba.core.byteflow:dispatch pc=4, inst=LOAD_CONST(arg=1, lineno=1144)
DEBUG:numba.core.byteflow:stack ['$x2.0']
DEBUG:numba.core.byteflow:dispatch pc=6, inst=BINARY_SUBSCR(arg=None, lineno=1144)
DEBUG:numba.core.byteflow:stack ['$x2.0', '$const4.1']
DEBUG:numba.core.byteflow:dispatch pc=8, inst=STORE_FAST(arg=3, lineno=1144)
DEBUG:numba.core.byteflow:stack ['$6binary_subscr.2']
DEBUG:numba.core.byteflow:dispatch pc=10, inst=LOAD_FAST(arg=1, lineno=1145)
DEBUG:numba.core.byteflow:stack []
DEBUG:numba.core.byteflow:dispatch pc=12, inst=UNARY_NEGATIVE(arg=None, lineno=1145)
DEBUG:numba.core.byteflow:stack ['$threshold10.3']
DEBUG:numba.core.byteflow:dispatch pc=14, inst=LOAD_FAST(arg=3, lineno=1145)
DEBUG:numba.core.byteflow:stack ['$12unary_negative.4']
DEBUG:numba.core.byteflow:dispatch pc=16, inst=DUP_TOP(arg=None, lineno=1145)
DEBUG:numba.core.byteflow:stack ['$12unary_negative.4', '$x014.5']
DEBUG:numba.core.byteflow:dispatch pc=18, inst=ROT_THREE(arg=None, lineno=1145)
DEBUG:numba.core.byteflow:stack ['$12unary_negative.4', '$x014.5', '$16dup_top.6']
DEBUG:numba.core.byteflow:dispatch pc=20, inst=COMPARE_OP(arg=1, lineno=1145)
DEBUG:numba.core.byteflow:stack ['$16dup_top.6', '$12unary_negative.4', '$x014.5']
DEBUG:numba.core.byteflow:dispatch pc=22, inst=POP_JUMP_IF_FALSE(arg=32, lineno=1145)
DEBUG:numba.core.byteflow:stack ['$16dup_top.6', '$20compare_op.7']
DEBUG:numba.core.byteflow:end state. edges=[Edge(pc=24, stack=('$16dup_top.6',), blockstack=(), npush=0), Edge(pc=32, stack=('$16dup_top.6',), blockstack=(), npush=0)]
DEBUG:numba.core.byteflow:pending: deque([State(pc_initial=24 nstack_initial=1), State(pc_initial=32 nstack_initial=1)])
DEBUG:numba.core.byteflow:stack: ['$phi24.0']
DEBUG:numba.core.byteflow:state.pc_initial: State(pc_initial=24 nstack_initial=1)
DEBUG:numba.core.byteflow:dispatch pc=24, inst=LOAD_FAST(arg=1, lineno=1145)
DEBUG:numba.core.byteflow:stack ['$phi24.0']
DEBUG:numba.core.byteflow:dispatch pc=26, inst=COMPARE_OP(arg=1, lineno=1145)
DEBUG:numba.core.byteflow:stack ['$phi24.0', '$threshold24.1']
DEBUG:numba.core.byteflow:dispatch pc=28, inst=POP_JUMP_IF_FALSE(arg=40, lineno=1145)
DEBUG:numba.core.byteflow:stack ['$26compare_op.2']
DEBUG:numba.core.byteflow:end state. edges=[Edge(pc=30, stack=(), blockstack=(), npush=0), Edge(pc=40, stack=(), blockstack=(), npush=0)]
DEBUG:numba.core.byteflow:pending: deque([State(pc_initial=32 nstack_initial=1), State(pc_initial=30 nstack_initial=0), State(pc_initial=40 nstack_initial=0)])
DEBUG:numba.core.byteflow:stack: ['$phi32.0']
DEBUG:numba.core.byteflow:state.pc_initial: State(pc_initial=32 nstack_initial=1)
DEBUG:numba.core.byteflow:dispatch pc=32, inst=POP_TOP(arg=None, lineno=1145)
DEBUG:numba.core.byteflow:stack ['$phi32.0']
DEBUG:numba.core.byteflow:dispatch pc=34, inst=JUMP_FORWARD(arg=4, lineno=1145)
DEBUG:numba.core.byteflow:stack []
DEBUG:numba.core.byteflow:end state. edges=[Edge(pc=40, stack=(), blockstack=(), npush=0)]
DEBUG:numba.core.byteflow:pending: deque([State(pc_initial=30 nstack_initial=0), State(pc_initial=40 nstack_initial=0), State(pc_initial=40 nstack_initial=0)])
DEBUG:numba.core.byteflow:stack: []
DEBUG:numba.core.byteflow:state.pc_initial: State(pc_initial=30 nstack_initial=0)
DEBUG:numba.core.byteflow:dispatch pc=30, inst=JUMP_FORWARD(arg=4, lineno=1145)
DEBUG:numba.core.byteflow:stack []
DEBUG:numba.core.byteflow:end state. edges=[Edge(pc=36, stack=(), blockstack=(), npush=0)]
DEBUG:numba.core.byteflow:pending: deque([State(pc_initial=40 nstack_initial=0), State(pc_initial=40 nstack_initial=0), State(pc_initial=36 nstack_initial=0)])
DEBUG:numba.core.byteflow:stack: []
DEBUG:numba.core.byteflow:state.pc_initial: State(pc_initial=40 nstack_initial=0)
DEBUG:numba.core.byteflow:dispatch pc=40, inst=LOAD_FAST(arg=0, lineno=1148)
DEBUG:numba.core.byteflow:stack []
DEBUG:numba.core.byteflow:dispatch pc=42, inst=LOAD_CONST(arg=2, lineno=1148)
DEBUG:numba.core.byteflow:stack ['$x40.0']
DEBUG:numba.core.byteflow:dispatch pc=44, inst=BINARY_SUBSCR(arg=None, lineno=1148)
DEBUG:numba.core.byteflow:stack ['$x40.0', '$const42.1']
DEBUG:numba.core.byteflow:dispatch pc=46, inst=STORE_FAST(arg=4, lineno=1148)
DEBUG:numba.core.byteflow:stack ['$44binary_subscr.2']
DEBUG:numba.core.byteflow:dispatch pc=48, inst=LOAD_FAST(arg=1, lineno=1149)
DEBUG:numba.core.byteflow:stack []
DEBUG:numba.core.byteflow:dispatch pc=50, inst=UNARY_NEGATIVE(arg=None, lineno=1149)
DEBUG:numba.core.byteflow:stack ['$threshold48.3']
DEBUG:numba.core.byteflow:dispatch pc=52, inst=LOAD_FAST(arg=4, lineno=1149)
DEBUG:numba.core.byteflow:stack ['$50unary_negative.4']
DEBUG:numba.core.byteflow:dispatch pc=54, inst=DUP_TOP(arg=None, lineno=1149)
DEBUG:numba.core.byteflow:stack ['$50unary_negative.4', '$x152.5']
DEBUG:numba.core.byteflow:dispatch pc=56, inst=ROT_THREE(arg=None, lineno=1149)
DEBUG:numba.core.byteflow:stack ['$50unary_negative.4', '$x152.5', '$54dup_top.6']
DEBUG:numba.core.byteflow:dispatch pc=58, inst=COMPARE_OP(arg=1, lineno=1149)
DEBUG:numba.core.byteflow:stack ['$54dup_top.6', '$50unary_negative.4', '$x152.5']
DEBUG:numba.core.byteflow:dispatch pc=60, inst=POP_JUMP_IF_FALSE(arg=70, lineno=1149)
DEBUG:numba.core.byteflow:stack ['$54dup_top.6', '$58compare_op.7']
DEBUG:numba.core.byteflow:end state. edges=[Edge(pc=62, stack=('$54dup_top.6',), blockstack=(), npush=0), Edge(pc=70, stack=('$54dup_top.6',), blockstack=(), npush=0)]
DEBUG:numba.core.byteflow:pending: deque([State(pc_initial=40 nstack_initial=0), State(pc_initial=36 nstack_initial=0), State(pc_initial=62 nstack_initial=1), State(pc_initial=70 nstack_initial=1)])
DEBUG:numba.core.byteflow:pending: deque([State(pc_initial=36 nstack_initial=0), State(pc_initial=62 nstack_initial=1), State(pc_initial=70 nstack_initial=1)])
DEBUG:numba.core.byteflow:stack: []
DEBUG:numba.core.byteflow:state.pc_initial: State(pc_initial=36 nstack_initial=0)
DEBUG:numba.core.byteflow:dispatch pc=36, inst=LOAD_CONST(arg=1, lineno=1146)
DEBUG:numba.core.byteflow:stack []
DEBUG:numba.core.byteflow:dispatch pc=38, inst=STORE_FAST(arg=3, lineno=1146)
DEBUG:numba.core.byteflow:stack ['$const36.0']
DEBUG:numba.core.byteflow:end state. edges=[Edge(pc=40, stack=(), blockstack=(), npush=0)]
DEBUG:numba.core.byteflow:pending: deque([State(pc_initial=62 nstack_initial=1), State(pc_initial=70 nstack_initial=1), State(pc_initial=40 nstack_initial=0)])
DEBUG:numba.core.byteflow:stack: ['$phi62.0']
DEBUG:numba.core.byteflow:state.pc_initial: State(pc_initial=62 nstack_initial=1)
DEBUG:numba.core.byteflow:dispatch pc=62, inst=LOAD_FAST(arg=1, lineno=1149)
DEBUG:numba.core.byteflow:stack ['$phi62.0']
DEBUG:numba.core.byteflow:dispatch pc=64, inst=COMPARE_OP(arg=1, lineno=1149)
DEBUG:numba.core.byteflow:stack ['$phi62.0', '$threshold62.1']
DEBUG:numba.core.byteflow:dispatch pc=66, inst=POP_JUMP_IF_FALSE(arg=78, lineno=1149)
DEBUG:numba.core.byteflow:stack ['$64compare_op.2']
DEBUG:numba.core.byteflow:end state. edges=[Edge(pc=68, stack=(), blockstack=(), npush=0), Edge(pc=78, stack=(), blockstack=(), npush=0)]
DEBUG:numba.core.byteflow:pending: deque([State(pc_initial=70 nstack_initial=1), State(pc_initial=40 nstack_initial=0), State(pc_initial=68 nstack_initial=0), State(pc_initial=78 nstack_initial=0)])
DEBUG:numba.core.byteflow:stack: ['$phi70.0']
DEBUG:numba.core.byteflow:state.pc_initial: State(pc_initial=70 nstack_initial=1)
DEBUG:numba.core.byteflow:dispatch pc=70, inst=POP_TOP(arg=None, lineno=1149)
DEBUG:numba.core.byteflow:stack ['$phi70.0']
DEBUG:numba.core.byteflow:dispatch pc=72, inst=JUMP_FORWARD(arg=4, lineno=1149)
DEBUG:numba.core.byteflow:stack []
DEBUG:numba.core.byteflow:end state. edges=[Edge(pc=78, stack=(), blockstack=(), npush=0)]
DEBUG:numba.core.byteflow:pending: deque([State(pc_initial=40 nstack_initial=0), State(pc_initial=68 nstack_initial=0), State(pc_initial=78 nstack_initial=0), State(pc_initial=78 nstack_initial=0)])
DEBUG:numba.core.byteflow:pending: deque([State(pc_initial=68 nstack_initial=0), State(pc_initial=78 nstack_initial=0), State(pc_initial=78 nstack_initial=0)])
DEBUG:numba.core.byteflow:stack: []
DEBUG:numba.core.byteflow:state.pc_initial: State(pc_initial=68 nstack_initial=0)
DEBUG:numba.core.byteflow:dispatch pc=68, inst=JUMP_FORWARD(arg=4, lineno=1149)
DEBUG:numba.core.byteflow:stack []
DEBUG:numba.core.byteflow:end state. edges=[Edge(pc=74, stack=(), blockstack=(), npush=0)]
DEBUG:numba.core.byteflow:pending: deque([State(pc_initial=78 nstack_initial=0), State(pc_initial=78 nstack_initial=0), State(pc_initial=74 nstack_initial=0)])
DEBUG:numba.core.byteflow:stack: []
DEBUG:numba.core.byteflow:state.pc_initial: State(pc_initial=78 nstack_initial=0)
DEBUG:numba.core.byteflow:dispatch pc=78, inst=LOAD_FAST(arg=2, lineno=1152)
DEBUG:numba.core.byteflow:stack []
DEBUG:numba.core.byteflow:dispatch pc=80, inst=POP_JUMP_IF_FALSE(arg=102, lineno=1152)
DEBUG:numba.core.byteflow:stack ['$zero_pos78.0']
DEBUG:numba.core.byteflow:end state. edges=[Edge(pc=82, stack=(), blockstack=(), npush=0), Edge(pc=102, stack=(), blockstack=(), npush=0)]
DEBUG:numba.core.byteflow:pending: deque([State(pc_initial=78 nstack_initial=0), State(pc_initial=74 nstack_initial=0), State(pc_initial=82 nstack_initial=0), State(pc_initial=102 nstack_initial=0)])
DEBUG:numba.core.byteflow:pending: deque([State(pc_initial=74 nstack_initial=0), State(pc_initial=82 nstack_initial=0), State(pc_initial=102 nstack_initial=0)])
DEBUG:numba.core.byteflow:stack: []
DEBUG:numba.core.byteflow:state.pc_initial: State(pc_initial=74 nstack_initial=0)
DEBUG:numba.core.byteflow:dispatch pc=74, inst=LOAD_CONST(arg=1, lineno=1150)
DEBUG:numba.core.byteflow:stack []
DEBUG:numba.core.byteflow:dispatch pc=76, inst=STORE_FAST(arg=4, lineno=1150)
DEBUG:numba.core.byteflow:stack ['$const74.0']
DEBUG:numba.core.byteflow:end state. edges=[Edge(pc=78, stack=(), blockstack=(), npush=0)]
DEBUG:numba.core.byteflow:pending: deque([State(pc_initial=82 nstack_initial=0), State(pc_initial=102 nstack_initial=0), State(pc_initial=78 nstack_initial=0)])
DEBUG:numba.core.byteflow:stack: []
DEBUG:numba.core.byteflow:state.pc_initial: State(pc_initial=82 nstack_initial=0)
DEBUG:numba.core.byteflow:dispatch pc=82, inst=LOAD_GLOBAL(arg=0, lineno=1153)
DEBUG:numba.core.byteflow:stack []
DEBUG:numba.core.byteflow:dispatch pc=84, inst=LOAD_METHOD(arg=1, lineno=1153)
DEBUG:numba.core.byteflow:stack ['$82load_global.0']
DEBUG:numba.core.byteflow:dispatch pc=86, inst=LOAD_FAST(arg=3, lineno=1153)
DEBUG:numba.core.byteflow:stack ['$84load_method.1']
DEBUG:numba.core.byteflow:dispatch pc=88, inst=CALL_METHOD(arg=1, lineno=1153)
DEBUG:numba.core.byteflow:stack ['$84load_method.1', '$x086.2']
DEBUG:numba.core.byteflow:dispatch pc=90, inst=LOAD_GLOBAL(arg=0, lineno=1153)
DEBUG:numba.core.byteflow:stack ['$88call_method.3']
DEBUG:numba.core.byteflow:dispatch pc=92, inst=LOAD_METHOD(arg=1, lineno=1153)
DEBUG:numba.core.byteflow:stack ['$88call_method.3', '$90load_global.4']
DEBUG:numba.core.byteflow:dispatch pc=94, inst=LOAD_FAST(arg=4, lineno=1153)
DEBUG:numba.core.byteflow:stack ['$88call_method.3', '$92load_method.5']
DEBUG:numba.core.byteflow:dispatch pc=96, inst=CALL_METHOD(arg=1, lineno=1153)
DEBUG:numba.core.byteflow:stack ['$88call_method.3', '$92load_method.5', '$x194.6']
DEBUG:numba.core.byteflow:dispatch pc=98, inst=COMPARE_OP(arg=3, lineno=1153)
DEBUG:numba.core.byteflow:stack ['$88call_method.3', '$96call_method.7']
DEBUG:numba.core.byteflow:dispatch pc=100, inst=RETURN_VALUE(arg=None, lineno=1153)
DEBUG:numba.core.byteflow:stack ['$98compare_op.8']
DEBUG:numba.core.byteflow:end state. edges=[]
DEBUG:numba.core.byteflow:pending: deque([State(pc_initial=102 nstack_initial=0), State(pc_initial=78 nstack_initial=0)])
DEBUG:numba.core.byteflow:stack: []
DEBUG:numba.core.byteflow:state.pc_initial: State(pc_initial=102 nstack_initial=0)
DEBUG:numba.core.byteflow:dispatch pc=102, inst=LOAD_GLOBAL(arg=0, lineno=1155)
DEBUG:numba.core.byteflow:stack []
DEBUG:numba.core.byteflow:dispatch pc=104, inst=LOAD_METHOD(arg=2, lineno=1155)
DEBUG:numba.core.byteflow:stack ['$102load_global.0']
DEBUG:numba.core.byteflow:dispatch pc=106, inst=LOAD_FAST(arg=3, lineno=1155)
DEBUG:numba.core.byteflow:stack ['$104load_method.1']
DEBUG:numba.core.byteflow:dispatch pc=108, inst=CALL_METHOD(arg=1, lineno=1155)
DEBUG:numba.core.byteflow:stack ['$104load_method.1', '$x0106.2']
DEBUG:numba.core.byteflow:dispatch pc=110, inst=LOAD_GLOBAL(arg=0, lineno=1155)
DEBUG:numba.core.byteflow:stack ['$108call_method.3']
DEBUG:numba.core.byteflow:dispatch pc=112, inst=LOAD_METHOD(arg=2, lineno=1155)
DEBUG:numba.core.byteflow:stack ['$108call_method.3', '$110load_global.4']
DEBUG:numba.core.byteflow:dispatch pc=114, inst=LOAD_FAST(arg=4, lineno=1155)
DEBUG:numba.core.byteflow:stack ['$108call_method.3', '$112load_method.5']
DEBUG:numba.core.byteflow:dispatch pc=116, inst=CALL_METHOD(arg=1, lineno=1155)
DEBUG:numba.core.byteflow:stack ['$108call_method.3', '$112load_method.5', '$x1114.6']
DEBUG:numba.core.byteflow:dispatch pc=118, inst=COMPARE_OP(arg=3, lineno=1155)
DEBUG:numba.core.byteflow:stack ['$108call_method.3', '$116call_method.7']
DEBUG:numba.core.byteflow:dispatch pc=120, inst=RETURN_VALUE(arg=None, lineno=1155)
DEBUG:numba.core.byteflow:stack ['$118compare_op.8']
DEBUG:numba.core.byteflow:end state. edges=[]
DEBUG:numba.core.byteflow:pending: deque([State(pc_initial=78 nstack_initial=0)])
DEBUG:numba.core.byteflow:-------------------------Prune PHIs-------------------------
DEBUG:numba.core.byteflow:Used_phis: defaultdict(<class 'set'>,
            {State(pc_initial=0 nstack_initial=0): set(),
             State(pc_initial=24 nstack_initial=1): {'$phi24.0'},
             State(pc_initial=30 nstack_initial=0): set(),
             State(pc_initial=32 nstack_initial=1): set(),
             State(pc_initial=36 nstack_initial=0): set(),
             State(pc_initial=40 nstack_initial=0): set(),
             State(pc_initial=62 nstack_initial=1): {'$phi62.0'},
             State(pc_initial=68 nstack_initial=0): set(),
             State(pc_initial=70 nstack_initial=1): set(),
             State(pc_initial=74 nstack_initial=0): set(),
             State(pc_initial=78 nstack_initial=0): set(),
             State(pc_initial=82 nstack_initial=0): set(),
             State(pc_initial=102 nstack_initial=0): set()})
DEBUG:numba.core.byteflow:defmap: {'$phi24.0': State(pc_initial=0 nstack_initial=0),
 '$phi32.0': State(pc_initial=0 nstack_initial=0),
 '$phi62.0': State(pc_initial=40 nstack_initial=0),
 '$phi70.0': State(pc_initial=40 nstack_initial=0)}
DEBUG:numba.core.byteflow:phismap: defaultdict(<class 'set'>,
            {'$phi24.0': {('$16dup_top.6',
                           State(pc_initial=0 nstack_initial=0))},
             '$phi32.0': {('$16dup_top.6',
                           State(pc_initial=0 nstack_initial=0))},
             '$phi62.0': {('$54dup_top.6',
                           State(pc_initial=40 nstack_initial=0))},
             '$phi70.0': {('$54dup_top.6',
                           State(pc_initial=40 nstack_initial=0))}})
DEBUG:numba.core.byteflow:changing phismap: defaultdict(<class 'set'>,
            {'$phi24.0': {('$16dup_top.6',
                           State(pc_initial=0 nstack_initial=0))},
             '$phi32.0': {('$16dup_top.6',
                           State(pc_initial=0 nstack_initial=0))},
             '$phi62.0': {('$54dup_top.6',
                           State(pc_initial=40 nstack_initial=0))},
             '$phi70.0': {('$54dup_top.6',
                           State(pc_initial=40 nstack_initial=0))}})
DEBUG:numba.core.byteflow:keep phismap: {'$phi24.0': {('$16dup_top.6', State(pc_initial=0 nstack_initial=0))},
 '$phi62.0': {('$54dup_top.6', State(pc_initial=40 nstack_initial=0))}}
DEBUG:numba.core.byteflow:new_out: defaultdict(<class 'dict'>,
            {State(pc_initial=0 nstack_initial=0): {'$phi24.0': '$16dup_top.6'},
             State(pc_initial=40 nstack_initial=0): {'$phi62.0': '$54dup_top.6'}})
DEBUG:numba.core.byteflow:----------------------DONE Prune PHIs-----------------------
DEBUG:numba.core.byteflow:block_infos State(pc_initial=0 nstack_initial=0):
AdaptBlockInfo(insts=((0, {}), (2, {'res': '$x2.0'}), (4, {'res': '$const4.1'}), (6, {'index': '$const4.1', 'target': '$x2.0', 'res': '$6binary_subscr.2'}), (8, {'value': '$6binary_subscr.2'}), (10, {'res': '$threshold10.3'}), (12, {'value': '$threshold10.3', 'res': '$12unary_negative.4'}), (14, {'res': '$x014.5'}), (16, {'orig': ['$x014.5'], 'duped': ['$16dup_top.6']}), (20, {'lhs': '$12unary_negative.4', 'rhs': '$x014.5', 'res': '$20compare_op.7'}), (22, {'pred': '$20compare_op.7'})), outgoing_phis={'$phi24.0': '$16dup_top.6'}, blockstack=(), active_try_block=None, outgoing_edgepushed={24: ('$16dup_top.6',), 32: ('$16dup_top.6',)})
DEBUG:numba.core.byteflow:block_infos State(pc_initial=24 nstack_initial=1):
AdaptBlockInfo(insts=((24, {'res': '$threshold24.1'}), (26, {'lhs': '$phi24.0', 'rhs': '$threshold24.1', 'res': '$26compare_op.2'}), (28, {'pred': '$26compare_op.2'})), outgoing_phis={}, blockstack=(), active_try_block=None, outgoing_edgepushed={30: (), 40: ()})
DEBUG:numba.core.byteflow:block_infos State(pc_initial=30 nstack_initial=0):
AdaptBlockInfo(insts=((30, {}),), outgoing_phis={}, blockstack=(), active_try_block=None, outgoing_edgepushed={36: ()})
DEBUG:numba.core.byteflow:block_infos State(pc_initial=32 nstack_initial=1):
AdaptBlockInfo(insts=((34, {}),), outgoing_phis={}, blockstack=(), active_try_block=None, outgoing_edgepushed={40: ()})
DEBUG:numba.core.byteflow:block_infos State(pc_initial=36 nstack_initial=0):
AdaptBlockInfo(insts=((36, {'res': '$const36.0'}), (38, {'value': '$const36.0'})), outgoing_phis={}, blockstack=(), active_try_block=None, outgoing_edgepushed={40: ()})
DEBUG:numba.core.byteflow:block_infos State(pc_initial=40 nstack_initial=0):
AdaptBlockInfo(insts=((40, {'res': '$x40.0'}), (42, {'res': '$const42.1'}), (44, {'index': '$const42.1', 'target': '$x40.0', 'res': '$44binary_subscr.2'}), (46, {'value': '$44binary_subscr.2'}), (48, {'res': '$threshold48.3'}), (50, {'value': '$threshold48.3', 'res': '$50unary_negative.4'}), (52, {'res': '$x152.5'}), (54, {'orig': ['$x152.5'], 'duped': ['$54dup_top.6']}), (58, {'lhs': '$50unary_negative.4', 'rhs': '$x152.5', 'res': '$58compare_op.7'}), (60, {'pred': '$58compare_op.7'})), outgoing_phis={'$phi62.0': '$54dup_top.6'}, blockstack=(), active_try_block=None, outgoing_edgepushed={62: ('$54dup_top.6',), 70: ('$54dup_top.6',)})
DEBUG:numba.core.byteflow:block_infos State(pc_initial=62 nstack_initial=1):
AdaptBlockInfo(insts=((62, {'res': '$threshold62.1'}), (64, {'lhs': '$phi62.0', 'rhs': '$threshold62.1', 'res': '$64compare_op.2'}), (66, {'pred': '$64compare_op.2'})), outgoing_phis={}, blockstack=(), active_try_block=None, outgoing_edgepushed={68: (), 78: ()})
DEBUG:numba.core.byteflow:block_infos State(pc_initial=68 nstack_initial=0):
AdaptBlockInfo(insts=((68, {}),), outgoing_phis={}, blockstack=(), active_try_block=None, outgoing_edgepushed={74: ()})
DEBUG:numba.core.byteflow:block_infos State(pc_initial=70 nstack_initial=1):
AdaptBlockInfo(insts=((72, {}),), outgoing_phis={}, blockstack=(), active_try_block=None, outgoing_edgepushed={78: ()})
DEBUG:numba.core.byteflow:block_infos State(pc_initial=74 nstack_initial=0):
AdaptBlockInfo(insts=((74, {'res': '$const74.0'}), (76, {'value': '$const74.0'})), outgoing_phis={}, blockstack=(), active_try_block=None, outgoing_edgepushed={78: ()})
DEBUG:numba.core.byteflow:block_infos State(pc_initial=78 nstack_initial=0):
AdaptBlockInfo(insts=((78, {'res': '$zero_pos78.0'}), (80, {'pred': '$zero_pos78.0'})), outgoing_phis={}, blockstack=(), active_try_block=None, outgoing_edgepushed={82: (), 102: ()})
DEBUG:numba.core.byteflow:block_infos State(pc_initial=82 nstack_initial=0):
AdaptBlockInfo(insts=((82, {'res': '$82load_global.0'}), (84, {'item': '$82load_global.0', 'res': '$84load_method.1'}), (86, {'res': '$x086.2'}), (88, {'func': '$84load_method.1', 'args': ['$x086.2'], 'res': '$88call_method.3'}), (90, {'res': '$90load_global.4'}), (92, {'item': '$90load_global.4', 'res': '$92load_method.5'}), (94, {'res': '$x194.6'}), (96, {'func': '$92load_method.5', 'args': ['$x194.6'], 'res': '$96call_method.7'}), (98, {'lhs': '$88call_method.3', 'rhs': '$96call_method.7', 'res': '$98compare_op.8'}), (100, {'retval': '$98compare_op.8', 'castval': '$100return_value.9'})), outgoing_phis={}, blockstack=(), active_try_block=None, outgoing_edgepushed={})
DEBUG:numba.core.byteflow:block_infos State(pc_initial=102 nstack_initial=0):
AdaptBlockInfo(insts=((102, {'res': '$102load_global.0'}), (104, {'item': '$102load_global.0', 'res': '$104load_method.1'}), (106, {'res': '$x0106.2'}), (108, {'func': '$104load_method.1', 'args': ['$x0106.2'], 'res': '$108call_method.3'}), (110, {'res': '$110load_global.4'}), (112, {'item': '$110load_global.4', 'res': '$112load_method.5'}), (114, {'res': '$x1114.6'}), (116, {'func': '$112load_method.5', 'args': ['$x1114.6'], 'res': '$116call_method.7'}), (118, {'lhs': '$108call_method.3', 'rhs': '$116call_method.7', 'res': '$118compare_op.8'}), (120, {'retval': '$118compare_op.8', 'castval': '$120return_value.9'})), outgoing_phis={}, blockstack=(), active_try_block=None, outgoing_edgepushed={})
DEBUG:numba.core.interpreter:label 0:
    x = arg(0, name=x)                       ['x']
    threshold = arg(1, name=threshold)       ['threshold']
    zero_pos = arg(2, name=zero_pos)         ['zero_pos']
    $const4.1 = const(int, 0)                ['$const4.1']
    x0 = getitem(value=x, index=$const4.1, fn=<built-in function getitem>) ['$const4.1', 'x', 'x0']
    $12unary_negative.4 = unary(fn=<built-in function neg>, value=threshold) ['$12unary_negative.4', 'threshold']
    $20compare_op.7 = $12unary_negative.4 <= x0 ['$12unary_negative.4', '$20compare_op.7', 'x0']
    bool22 = global(bool: <class 'bool'>)    ['bool22']
    $22pred = call bool22($20compare_op.7, func=bool22, args=(Var($20compare_op.7, audio.py:1145),), kws=(), vararg=None, varkwarg=None, target=None) ['$20compare_op.7', '$22pred', 'bool22']
    $phi24.0 = x0                            ['$phi24.0', 'x0']
    branch $22pred, 24, 32                   ['$22pred']
label 24:
    $26compare_op.2 = $phi24.0 <= threshold  ['$26compare_op.2', '$phi24.0', 'threshold']
    bool28 = global(bool: <class 'bool'>)    ['bool28']
    $28pred = call bool28($26compare_op.2, func=bool28, args=(Var($26compare_op.2, audio.py:1145),), kws=(), vararg=None, varkwarg=None, target=None) ['$26compare_op.2', '$28pred', 'bool28']
    branch $28pred, 30, 40                   ['$28pred']
label 30:
    jump 36                                  []
label 32:
    jump 40                                  []
label 36:
    x0 = const(int, 0)                       ['x0']
    jump 40                                  []
label 40:
    $const42.1 = const(int, -1)              ['$const42.1']
    x1 = getitem(value=x, index=$const42.1, fn=<built-in function getitem>) ['$const42.1', 'x', 'x1']
    $50unary_negative.4 = unary(fn=<built-in function neg>, value=threshold) ['$50unary_negative.4', 'threshold']
    $58compare_op.7 = $50unary_negative.4 <= x1 ['$50unary_negative.4', '$58compare_op.7', 'x1']
    bool60 = global(bool: <class 'bool'>)    ['bool60']
    $60pred = call bool60($58compare_op.7, func=bool60, args=(Var($58compare_op.7, audio.py:1149),), kws=(), vararg=None, varkwarg=None, target=None) ['$58compare_op.7', '$60pred', 'bool60']
    $phi62.0 = x1                            ['$phi62.0', 'x1']
    branch $60pred, 62, 70                   ['$60pred']
label 62:
    $64compare_op.2 = $phi62.0 <= threshold  ['$64compare_op.2', '$phi62.0', 'threshold']
    bool66 = global(bool: <class 'bool'>)    ['bool66']
    $66pred = call bool66($64compare_op.2, func=bool66, args=(Var($64compare_op.2, audio.py:1149),), kws=(), vararg=None, varkwarg=None, target=None) ['$64compare_op.2', '$66pred', 'bool66']
    branch $66pred, 68, 78                   ['$66pred']
label 68:
    jump 74                                  []
label 70:
    jump 78                                  []
label 74:
    x1 = const(int, 0)                       ['x1']
    jump 78                                  []
label 78:
    bool80 = global(bool: <class 'bool'>)    ['bool80']
    $80pred = call bool80(zero_pos, func=bool80, args=(Var(zero_pos, audio.py:1141),), kws=(), vararg=None, varkwarg=None, target=None) ['$80pred', 'bool80', 'zero_pos']
    branch $80pred, 82, 102                  ['$80pred']
label 82:
    $82load_global.0 = global(np: <module 'numpy' from 'C:\\Users\\taskm\\anaconda3\\envs\\taskai\\lib\\site-packages\\numpy\\__init__.py'>) ['$82load_global.0']
    $84load_method.1 = getattr(value=$82load_global.0, attr=signbit) ['$82load_global.0', '$84load_method.1']
    $88call_method.3 = call $84load_method.1(x0, func=$84load_method.1, args=[Var(x0, audio.py:1144)], kws=(), vararg=None, varkwarg=None, target=None) ['$84load_method.1', '$88call_method.3', 'x0']
    $90load_global.4 = global(np: <module 'numpy' from 'C:\\Users\\taskm\\anaconda3\\envs\\taskai\\lib\\site-packages\\numpy\\__init__.py'>) ['$90load_global.4']
    $92load_method.5 = getattr(value=$90load_global.4, attr=signbit) ['$90load_global.4', '$92load_method.5']
    $96call_method.7 = call $92load_method.5(x1, func=$92load_method.5, args=[Var(x1, audio.py:1148)], kws=(), vararg=None, varkwarg=None, target=None) ['$92load_method.5', '$96call_method.7', 'x1']
    $98compare_op.8 = $88call_method.3 != $96call_method.7 ['$88call_method.3', '$96call_method.7', '$98compare_op.8']
    $100return_value.9 = cast(value=$98compare_op.8) ['$100return_value.9', '$98compare_op.8']
    return $100return_value.9                ['$100return_value.9']
label 102:
    $102load_global.0 = global(np: <module 'numpy' from 'C:\\Users\\taskm\\anaconda3\\envs\\taskai\\lib\\site-packages\\numpy\\__init__.py'>) ['$102load_global.0']
    $104load_method.1 = getattr(value=$102load_global.0, attr=sign) ['$102load_global.0', '$104load_method.1']
    $108call_method.3 = call $104load_method.1(x0, func=$104load_method.1, args=[Var(x0, audio.py:1144)], kws=(), vararg=None, varkwarg=None, target=None) ['$104load_method.1', '$108call_method.3', 'x0']
    $110load_global.4 = global(np: <module 'numpy' from 'C:\\Users\\taskm\\anaconda3\\envs\\taskai\\lib\\site-packages\\numpy\\__init__.py'>) ['$110load_global.4']
    $112load_method.5 = getattr(value=$110load_global.4, attr=sign) ['$110load_global.4', '$112load_method.5']
    $116call_method.7 = call $112load_method.5(x1, func=$112load_method.5, args=[Var(x1, audio.py:1148)], kws=(), vararg=None, varkwarg=None, target=None) ['$112load_method.5', '$116call_method.7', 'x1']
    $118compare_op.8 = $108call_method.3 != $116call_method.7 ['$108call_method.3', '$116call_method.7', '$118compare_op.8']
    $120return_value.9 = cast(value=$118compare_op.8) ['$118compare_op.8', '$120return_value.9']
    return $120return_value.9                ['$120return_value.9']

DEBUG:numba.core.byteflow:bytecode dump:
>          0	NOP(arg=None, lineno=1039)
           2	LOAD_FAST(arg=0, lineno=1042)
           4	LOAD_CONST(arg=1, lineno=1042)
           6	BINARY_SUBSCR(arg=None, lineno=1042)
           8	LOAD_FAST(arg=0, lineno=1042)
          10	LOAD_CONST(arg=2, lineno=1042)
          12	BINARY_SUBSCR(arg=None, lineno=1042)
          14	COMPARE_OP(arg=4, lineno=1042)
          16	LOAD_FAST(arg=0, lineno=1042)
          18	LOAD_CONST(arg=1, lineno=1042)
          20	BINARY_SUBSCR(arg=None, lineno=1042)
          22	LOAD_FAST(arg=0, lineno=1042)
          24	LOAD_CONST(arg=3, lineno=1042)
          26	BINARY_SUBSCR(arg=None, lineno=1042)
          28	COMPARE_OP(arg=5, lineno=1042)
          30	BINARY_AND(arg=None, lineno=1042)
          32	RETURN_VALUE(arg=None, lineno=1042)
DEBUG:numba.core.byteflow:pending: deque([State(pc_initial=0 nstack_initial=0)])
DEBUG:numba.core.byteflow:stack: []
DEBUG:numba.core.byteflow:state.pc_initial: State(pc_initial=0 nstack_initial=0)
DEBUG:numba.core.byteflow:dispatch pc=0, inst=NOP(arg=None, lineno=1039)
DEBUG:numba.core.byteflow:stack []
DEBUG:numba.core.byteflow:dispatch pc=2, inst=LOAD_FAST(arg=0, lineno=1042)
DEBUG:numba.core.byteflow:stack []
DEBUG:numba.core.byteflow:dispatch pc=4, inst=LOAD_CONST(arg=1, lineno=1042)
DEBUG:numba.core.byteflow:stack ['$x2.0']
DEBUG:numba.core.byteflow:dispatch pc=6, inst=BINARY_SUBSCR(arg=None, lineno=1042)
DEBUG:numba.core.byteflow:stack ['$x2.0', '$const4.1']
DEBUG:numba.core.byteflow:dispatch pc=8, inst=LOAD_FAST(arg=0, lineno=1042)
DEBUG:numba.core.byteflow:stack ['$6binary_subscr.2']
DEBUG:numba.core.byteflow:dispatch pc=10, inst=LOAD_CONST(arg=2, lineno=1042)
DEBUG:numba.core.byteflow:stack ['$6binary_subscr.2', '$x8.3']
DEBUG:numba.core.byteflow:dispatch pc=12, inst=BINARY_SUBSCR(arg=None, lineno=1042)
DEBUG:numba.core.byteflow:stack ['$6binary_subscr.2', '$x8.3', '$const10.4']
DEBUG:numba.core.byteflow:dispatch pc=14, inst=COMPARE_OP(arg=4, lineno=1042)
DEBUG:numba.core.byteflow:stack ['$6binary_subscr.2', '$12binary_subscr.5']
DEBUG:numba.core.byteflow:dispatch pc=16, inst=LOAD_FAST(arg=0, lineno=1042)
DEBUG:numba.core.byteflow:stack ['$14compare_op.6']
DEBUG:numba.core.byteflow:dispatch pc=18, inst=LOAD_CONST(arg=1, lineno=1042)
DEBUG:numba.core.byteflow:stack ['$14compare_op.6', '$x16.7']
DEBUG:numba.core.byteflow:dispatch pc=20, inst=BINARY_SUBSCR(arg=None, lineno=1042)
DEBUG:numba.core.byteflow:stack ['$14compare_op.6', '$x16.7', '$const18.8']
DEBUG:numba.core.byteflow:dispatch pc=22, inst=LOAD_FAST(arg=0, lineno=1042)
DEBUG:numba.core.byteflow:stack ['$14compare_op.6', '$20binary_subscr.9']
DEBUG:numba.core.byteflow:dispatch pc=24, inst=LOAD_CONST(arg=3, lineno=1042)
DEBUG:numba.core.byteflow:stack ['$14compare_op.6', '$20binary_subscr.9', '$x22.10']
DEBUG:numba.core.byteflow:dispatch pc=26, inst=BINARY_SUBSCR(arg=None, lineno=1042)
DEBUG:numba.core.byteflow:stack ['$14compare_op.6', '$20binary_subscr.9', '$x22.10', '$const24.11']
DEBUG:numba.core.byteflow:dispatch pc=28, inst=COMPARE_OP(arg=5, lineno=1042)
DEBUG:numba.core.byteflow:stack ['$14compare_op.6', '$20binary_subscr.9', '$26binary_subscr.12']
DEBUG:numba.core.byteflow:dispatch pc=30, inst=BINARY_AND(arg=None, lineno=1042)
DEBUG:numba.core.byteflow:stack ['$14compare_op.6', '$28compare_op.13']
DEBUG:numba.core.byteflow:dispatch pc=32, inst=RETURN_VALUE(arg=None, lineno=1042)
DEBUG:numba.core.byteflow:stack ['$30binary_and.14']
DEBUG:numba.core.byteflow:end state. edges=[]
DEBUG:numba.core.byteflow:-------------------------Prune PHIs-------------------------
DEBUG:numba.core.byteflow:Used_phis: defaultdict(<class 'set'>, {State(pc_initial=0 nstack_initial=0): set()})
DEBUG:numba.core.byteflow:defmap: {}
DEBUG:numba.core.byteflow:phismap: defaultdict(<class 'set'>, {})
DEBUG:numba.core.byteflow:changing phismap: defaultdict(<class 'set'>, {})
DEBUG:numba.core.byteflow:keep phismap: {}
DEBUG:numba.core.byteflow:new_out: defaultdict(<class 'dict'>, {})
DEBUG:numba.core.byteflow:----------------------DONE Prune PHIs-----------------------
DEBUG:numba.core.byteflow:block_infos State(pc_initial=0 nstack_initial=0):
AdaptBlockInfo(insts=((0, {}), (2, {'res': '$x2.0'}), (4, {'res': '$const4.1'}), (6, {'index': '$const4.1', 'target': '$x2.0', 'res': '$6binary_subscr.2'}), (8, {'res': '$x8.3'}), (10, {'res': '$const10.4'}), (12, {'index': '$const10.4', 'target': '$x8.3', 'res': '$12binary_subscr.5'}), (14, {'lhs': '$6binary_subscr.2', 'rhs': '$12binary_subscr.5', 'res': '$14compare_op.6'}), (16, {'res': '$x16.7'}), (18, {'res': '$const18.8'}), (20, {'index': '$const18.8', 'target': '$x16.7', 'res': '$20binary_subscr.9'}), (22, {'res': '$x22.10'}), (24, {'res': '$const24.11'}), (26, {'index': '$const24.11', 'target': '$x22.10', 'res': '$26binary_subscr.12'}), (28, {'lhs': '$20binary_subscr.9', 'rhs': '$26binary_subscr.12', 'res': '$28compare_op.13'}), (30, {'lhs': '$14compare_op.6', 'rhs': '$28compare_op.13', 'res': '$30binary_and.14'}), (32, {'retval': '$30binary_and.14', 'castval': '$32return_value.15'})), outgoing_phis={}, blockstack=(), active_try_block=None, outgoing_edgepushed={})
DEBUG:numba.core.interpreter:label 0:
    x = arg(0, name=x)                       ['x']
    $const4.1 = const(int, 0)                ['$const4.1']
    $6binary_subscr.2 = getitem(value=x, index=$const4.1, fn=<built-in function getitem>) ['$6binary_subscr.2', '$const4.1', 'x']
    $const10.4 = const(int, -1)              ['$const10.4']
    $12binary_subscr.5 = getitem(value=x, index=$const10.4, fn=<built-in function getitem>) ['$12binary_subscr.5', '$const10.4', 'x']
    $14compare_op.6 = $6binary_subscr.2 > $12binary_subscr.5 ['$12binary_subscr.5', '$14compare_op.6', '$6binary_subscr.2']
    $const18.8 = const(int, 0)               ['$const18.8']
    $20binary_subscr.9 = getitem(value=x, index=$const18.8, fn=<built-in function getitem>) ['$20binary_subscr.9', '$const18.8', 'x']
    $const24.11 = const(int, 1)              ['$const24.11']
    $26binary_subscr.12 = getitem(value=x, index=$const24.11, fn=<built-in function getitem>) ['$26binary_subscr.12', '$const24.11', 'x']
    $28compare_op.13 = $20binary_subscr.9 >= $26binary_subscr.12 ['$20binary_subscr.9', '$26binary_subscr.12', '$28compare_op.13']
    $30binary_and.14 = $14compare_op.6 & $28compare_op.13 ['$14compare_op.6', '$28compare_op.13', '$30binary_and.14']
    $32return_value.15 = cast(value=$30binary_and.14) ['$30binary_and.14', '$32return_value.15']
    return $32return_value.15                ['$32return_value.15']

DEBUG:numba.core.byteflow:bytecode dump:
>          0	NOP(arg=None, lineno=1045)
           2	LOAD_FAST(arg=0, lineno=1048)
           4	LOAD_CONST(arg=1, lineno=1048)
           6	BINARY_SUBSCR(arg=None, lineno=1048)
           8	LOAD_FAST(arg=0, lineno=1048)
          10	LOAD_CONST(arg=2, lineno=1048)
          12	BINARY_SUBSCR(arg=None, lineno=1048)
          14	COMPARE_OP(arg=0, lineno=1048)
          16	LOAD_FAST(arg=0, lineno=1048)
          18	LOAD_CONST(arg=1, lineno=1048)
          20	BINARY_SUBSCR(arg=None, lineno=1048)
          22	LOAD_FAST(arg=0, lineno=1048)
          24	LOAD_CONST(arg=3, lineno=1048)
          26	BINARY_SUBSCR(arg=None, lineno=1048)
          28	COMPARE_OP(arg=1, lineno=1048)
          30	BINARY_AND(arg=None, lineno=1048)
          32	RETURN_VALUE(arg=None, lineno=1048)
DEBUG:numba.core.byteflow:pending: deque([State(pc_initial=0 nstack_initial=0)])
DEBUG:numba.core.byteflow:stack: []
DEBUG:numba.core.byteflow:state.pc_initial: State(pc_initial=0 nstack_initial=0)
DEBUG:numba.core.byteflow:dispatch pc=0, inst=NOP(arg=None, lineno=1045)
DEBUG:numba.core.byteflow:stack []
DEBUG:numba.core.byteflow:dispatch pc=2, inst=LOAD_FAST(arg=0, lineno=1048)
DEBUG:numba.core.byteflow:stack []
DEBUG:numba.core.byteflow:dispatch pc=4, inst=LOAD_CONST(arg=1, lineno=1048)
DEBUG:numba.core.byteflow:stack ['$x2.0']
DEBUG:numba.core.byteflow:dispatch pc=6, inst=BINARY_SUBSCR(arg=None, lineno=1048)
DEBUG:numba.core.byteflow:stack ['$x2.0', '$const4.1']
DEBUG:numba.core.byteflow:dispatch pc=8, inst=LOAD_FAST(arg=0, lineno=1048)
DEBUG:numba.core.byteflow:stack ['$6binary_subscr.2']
DEBUG:numba.core.byteflow:dispatch pc=10, inst=LOAD_CONST(arg=2, lineno=1048)
DEBUG:numba.core.byteflow:stack ['$6binary_subscr.2', '$x8.3']
DEBUG:numba.core.byteflow:dispatch pc=12, inst=BINARY_SUBSCR(arg=None, lineno=1048)
DEBUG:numba.core.byteflow:stack ['$6binary_subscr.2', '$x8.3', '$const10.4']
DEBUG:numba.core.byteflow:dispatch pc=14, inst=COMPARE_OP(arg=0, lineno=1048)
DEBUG:numba.core.byteflow:stack ['$6binary_subscr.2', '$12binary_subscr.5']
DEBUG:numba.core.byteflow:dispatch pc=16, inst=LOAD_FAST(arg=0, lineno=1048)
DEBUG:numba.core.byteflow:stack ['$14compare_op.6']
DEBUG:numba.core.byteflow:dispatch pc=18, inst=LOAD_CONST(arg=1, lineno=1048)
DEBUG:numba.core.byteflow:stack ['$14compare_op.6', '$x16.7']
DEBUG:numba.core.byteflow:dispatch pc=20, inst=BINARY_SUBSCR(arg=None, lineno=1048)
DEBUG:numba.core.byteflow:stack ['$14compare_op.6', '$x16.7', '$const18.8']
DEBUG:numba.core.byteflow:dispatch pc=22, inst=LOAD_FAST(arg=0, lineno=1048)
DEBUG:numba.core.byteflow:stack ['$14compare_op.6', '$20binary_subscr.9']
DEBUG:numba.core.byteflow:dispatch pc=24, inst=LOAD_CONST(arg=3, lineno=1048)
DEBUG:numba.core.byteflow:stack ['$14compare_op.6', '$20binary_subscr.9', '$x22.10']
DEBUG:numba.core.byteflow:dispatch pc=26, inst=BINARY_SUBSCR(arg=None, lineno=1048)
DEBUG:numba.core.byteflow:stack ['$14compare_op.6', '$20binary_subscr.9', '$x22.10', '$const24.11']
DEBUG:numba.core.byteflow:dispatch pc=28, inst=COMPARE_OP(arg=1, lineno=1048)
DEBUG:numba.core.byteflow:stack ['$14compare_op.6', '$20binary_subscr.9', '$26binary_subscr.12']
DEBUG:numba.core.byteflow:dispatch pc=30, inst=BINARY_AND(arg=None, lineno=1048)
DEBUG:numba.core.byteflow:stack ['$14compare_op.6', '$28compare_op.13']
DEBUG:numba.core.byteflow:dispatch pc=32, inst=RETURN_VALUE(arg=None, lineno=1048)
DEBUG:numba.core.byteflow:stack ['$30binary_and.14']
DEBUG:numba.core.byteflow:end state. edges=[]
DEBUG:numba.core.byteflow:-------------------------Prune PHIs-------------------------
DEBUG:numba.core.byteflow:Used_phis: defaultdict(<class 'set'>, {State(pc_initial=0 nstack_initial=0): set()})
DEBUG:numba.core.byteflow:defmap: {}
DEBUG:numba.core.byteflow:phismap: defaultdict(<class 'set'>, {})
DEBUG:numba.core.byteflow:changing phismap: defaultdict(<class 'set'>, {})
DEBUG:numba.core.byteflow:keep phismap: {}
DEBUG:numba.core.byteflow:new_out: defaultdict(<class 'dict'>, {})
DEBUG:numba.core.byteflow:----------------------DONE Prune PHIs-----------------------
DEBUG:numba.core.byteflow:block_infos State(pc_initial=0 nstack_initial=0):
AdaptBlockInfo(insts=((0, {}), (2, {'res': '$x2.0'}), (4, {'res': '$const4.1'}), (6, {'index': '$const4.1', 'target': '$x2.0', 'res': '$6binary_subscr.2'}), (8, {'res': '$x8.3'}), (10, {'res': '$const10.4'}), (12, {'index': '$const10.4', 'target': '$x8.3', 'res': '$12binary_subscr.5'}), (14, {'lhs': '$6binary_subscr.2', 'rhs': '$12binary_subscr.5', 'res': '$14compare_op.6'}), (16, {'res': '$x16.7'}), (18, {'res': '$const18.8'}), (20, {'index': '$const18.8', 'target': '$x16.7', 'res': '$20binary_subscr.9'}), (22, {'res': '$x22.10'}), (24, {'res': '$const24.11'}), (26, {'index': '$const24.11', 'target': '$x22.10', 'res': '$26binary_subscr.12'}), (28, {'lhs': '$20binary_subscr.9', 'rhs': '$26binary_subscr.12', 'res': '$28compare_op.13'}), (30, {'lhs': '$14compare_op.6', 'rhs': '$28compare_op.13', 'res': '$30binary_and.14'}), (32, {'retval': '$30binary_and.14', 'castval': '$32return_value.15'})), outgoing_phis={}, blockstack=(), active_try_block=None, outgoing_edgepushed={})
DEBUG:numba.core.interpreter:label 0:
    x = arg(0, name=x)                       ['x']
    $const4.1 = const(int, 0)                ['$const4.1']
    $6binary_subscr.2 = getitem(value=x, index=$const4.1, fn=<built-in function getitem>) ['$6binary_subscr.2', '$const4.1', 'x']
    $const10.4 = const(int, -1)              ['$const10.4']
    $12binary_subscr.5 = getitem(value=x, index=$const10.4, fn=<built-in function getitem>) ['$12binary_subscr.5', '$const10.4', 'x']
    $14compare_op.6 = $6binary_subscr.2 < $12binary_subscr.5 ['$12binary_subscr.5', '$14compare_op.6', '$6binary_subscr.2']
    $const18.8 = const(int, 0)               ['$const18.8']
    $20binary_subscr.9 = getitem(value=x, index=$const18.8, fn=<built-in function getitem>) ['$20binary_subscr.9', '$const18.8', 'x']
    $const24.11 = const(int, 1)              ['$const24.11']
    $26binary_subscr.12 = getitem(value=x, index=$const24.11, fn=<built-in function getitem>) ['$26binary_subscr.12', '$const24.11', 'x']
    $28compare_op.13 = $20binary_subscr.9 <= $26binary_subscr.12 ['$20binary_subscr.9', '$26binary_subscr.12', '$28compare_op.13']
    $30binary_and.14 = $14compare_op.6 & $28compare_op.13 ['$14compare_op.6', '$28compare_op.13', '$30binary_and.14']
    $32return_value.15 = cast(value=$30binary_and.14) ['$30binary_and.14', '$32return_value.15']
    return $32return_value.15                ['$32return_value.15']

DEBUG:httpx:load_ssl_context verify=True cert=None trust_env=True http2=False
DEBUG:httpx:load_verify_locations cafile='C:\\Users\\taskm\\anaconda3\\Library\\ssl\\cacert.pem'
DEBUG:httpx:load_ssl_context verify=True cert=None trust_env=True http2=False
DEBUG:httpx:load_verify_locations cafile='C:\\Users\\taskm\\anaconda3\\Library\\ssl\\cacert.pem'
DEBUG:numba.core.byteflow:bytecode dump:
>          0	NOP(arg=None, lineno=1141)
           2	LOAD_FAST(arg=0, lineno=1144)
           4	LOAD_CONST(arg=1, lineno=1144)
           6	BINARY_SUBSCR(arg=None, lineno=1144)
           8	STORE_FAST(arg=3, lineno=1144)
          10	LOAD_FAST(arg=1, lineno=1145)
          12	UNARY_NEGATIVE(arg=None, lineno=1145)
          14	LOAD_FAST(arg=3, lineno=1145)
          16	DUP_TOP(arg=None, lineno=1145)
          18	ROT_THREE(arg=None, lineno=1145)
          20	COMPARE_OP(arg=1, lineno=1145)
          22	POP_JUMP_IF_FALSE(arg=32, lineno=1145)
          24	LOAD_FAST(arg=1, lineno=1145)
          26	COMPARE_OP(arg=1, lineno=1145)
          28	POP_JUMP_IF_FALSE(arg=40, lineno=1145)
          30	JUMP_FORWARD(arg=4, lineno=1145)
>         32	POP_TOP(arg=None, lineno=1145)
          34	JUMP_FORWARD(arg=4, lineno=1145)
>         36	LOAD_CONST(arg=1, lineno=1146)
          38	STORE_FAST(arg=3, lineno=1146)
>         40	LOAD_FAST(arg=0, lineno=1148)
          42	LOAD_CONST(arg=2, lineno=1148)
          44	BINARY_SUBSCR(arg=None, lineno=1148)
          46	STORE_FAST(arg=4, lineno=1148)
          48	LOAD_FAST(arg=1, lineno=1149)
          50	UNARY_NEGATIVE(arg=None, lineno=1149)
          52	LOAD_FAST(arg=4, lineno=1149)
          54	DUP_TOP(arg=None, lineno=1149)
          56	ROT_THREE(arg=None, lineno=1149)
          58	COMPARE_OP(arg=1, lineno=1149)
          60	POP_JUMP_IF_FALSE(arg=70, lineno=1149)
          62	LOAD_FAST(arg=1, lineno=1149)
          64	COMPARE_OP(arg=1, lineno=1149)
          66	POP_JUMP_IF_FALSE(arg=78, lineno=1149)
          68	JUMP_FORWARD(arg=4, lineno=1149)
>         70	POP_TOP(arg=None, lineno=1149)
          72	JUMP_FORWARD(arg=4, lineno=1149)
>         74	LOAD_CONST(arg=1, lineno=1150)
          76	STORE_FAST(arg=4, lineno=1150)
>         78	LOAD_FAST(arg=2, lineno=1152)
          80	POP_JUMP_IF_FALSE(arg=102, lineno=1152)
          82	LOAD_GLOBAL(arg=0, lineno=1153)
          84	LOAD_METHOD(arg=1, lineno=1153)
          86	LOAD_FAST(arg=3, lineno=1153)
          88	CALL_METHOD(arg=1, lineno=1153)
          90	LOAD_GLOBAL(arg=0, lineno=1153)
          92	LOAD_METHOD(arg=1, lineno=1153)
          94	LOAD_FAST(arg=4, lineno=1153)
          96	CALL_METHOD(arg=1, lineno=1153)
          98	COMPARE_OP(arg=3, lineno=1153)
         100	RETURN_VALUE(arg=None, lineno=1153)
>        102	LOAD_GLOBAL(arg=0, lineno=1155)
         104	LOAD_METHOD(arg=2, lineno=1155)
         106	LOAD_FAST(arg=3, lineno=1155)
         108	CALL_METHOD(arg=1, lineno=1155)
         110	LOAD_GLOBAL(arg=0, lineno=1155)
         112	LOAD_METHOD(arg=2, lineno=1155)
         114	LOAD_FAST(arg=4, lineno=1155)
         116	CALL_METHOD(arg=1, lineno=1155)
         118	COMPARE_OP(arg=3, lineno=1155)
         120	RETURN_VALUE(arg=None, lineno=1155)
         122	LOAD_CONST(arg=3, lineno=1155)
         124	RETURN_VALUE(arg=None, lineno=1155)
DEBUG:numba.core.byteflow:pending: deque([State(pc_initial=0 nstack_initial=0)])
DEBUG:numba.core.byteflow:stack: []
DEBUG:numba.core.byteflow:state.pc_initial: State(pc_initial=0 nstack_initial=0)
DEBUG:numba.core.byteflow:dispatch pc=0, inst=NOP(arg=None, lineno=1141)
DEBUG:numba.core.byteflow:stack []
DEBUG:numba.core.byteflow:dispatch pc=2, inst=LOAD_FAST(arg=0, lineno=1144)
DEBUG:numba.core.byteflow:stack []
DEBUG:numba.core.byteflow:dispatch pc=4, inst=LOAD_CONST(arg=1, lineno=1144)
DEBUG:numba.core.byteflow:stack ['$x2.0']
DEBUG:numba.core.byteflow:dispatch pc=6, inst=BINARY_SUBSCR(arg=None, lineno=1144)
DEBUG:numba.core.byteflow:stack ['$x2.0', '$const4.1']
DEBUG:numba.core.byteflow:dispatch pc=8, inst=STORE_FAST(arg=3, lineno=1144)
DEBUG:numba.core.byteflow:stack ['$6binary_subscr.2']
DEBUG:numba.core.byteflow:dispatch pc=10, inst=LOAD_FAST(arg=1, lineno=1145)
DEBUG:numba.core.byteflow:stack []
DEBUG:numba.core.byteflow:dispatch pc=12, inst=UNARY_NEGATIVE(arg=None, lineno=1145)
DEBUG:numba.core.byteflow:stack ['$threshold10.3']
DEBUG:numba.core.byteflow:dispatch pc=14, inst=LOAD_FAST(arg=3, lineno=1145)
DEBUG:numba.core.byteflow:stack ['$12unary_negative.4']
DEBUG:numba.core.byteflow:dispatch pc=16, inst=DUP_TOP(arg=None, lineno=1145)
DEBUG:numba.core.byteflow:stack ['$12unary_negative.4', '$x014.5']
DEBUG:numba.core.byteflow:dispatch pc=18, inst=ROT_THREE(arg=None, lineno=1145)
DEBUG:numba.core.byteflow:stack ['$12unary_negative.4', '$x014.5', '$16dup_top.6']
DEBUG:numba.core.byteflow:dispatch pc=20, inst=COMPARE_OP(arg=1, lineno=1145)
DEBUG:numba.core.byteflow:stack ['$16dup_top.6', '$12unary_negative.4', '$x014.5']
DEBUG:numba.core.byteflow:dispatch pc=22, inst=POP_JUMP_IF_FALSE(arg=32, lineno=1145)
DEBUG:numba.core.byteflow:stack ['$16dup_top.6', '$20compare_op.7']
DEBUG:numba.core.byteflow:end state. edges=[Edge(pc=24, stack=('$16dup_top.6',), blockstack=(), npush=0), Edge(pc=32, stack=('$16dup_top.6',), blockstack=(), npush=0)]
DEBUG:numba.core.byteflow:pending: deque([State(pc_initial=24 nstack_initial=1), State(pc_initial=32 nstack_initial=1)])
DEBUG:numba.core.byteflow:stack: ['$phi24.0']
DEBUG:numba.core.byteflow:state.pc_initial: State(pc_initial=24 nstack_initial=1)
DEBUG:numba.core.byteflow:dispatch pc=24, inst=LOAD_FAST(arg=1, lineno=1145)
DEBUG:numba.core.byteflow:stack ['$phi24.0']
DEBUG:numba.core.byteflow:dispatch pc=26, inst=COMPARE_OP(arg=1, lineno=1145)
DEBUG:numba.core.byteflow:stack ['$phi24.0', '$threshold24.1']
DEBUG:numba.core.byteflow:dispatch pc=28, inst=POP_JUMP_IF_FALSE(arg=40, lineno=1145)
DEBUG:numba.core.byteflow:stack ['$26compare_op.2']
DEBUG:numba.core.byteflow:end state. edges=[Edge(pc=30, stack=(), blockstack=(), npush=0), Edge(pc=40, stack=(), blockstack=(), npush=0)]
DEBUG:numba.core.byteflow:pending: deque([State(pc_initial=32 nstack_initial=1), State(pc_initial=30 nstack_initial=0), State(pc_initial=40 nstack_initial=0)])
DEBUG:numba.core.byteflow:stack: ['$phi32.0']
DEBUG:numba.core.byteflow:state.pc_initial: State(pc_initial=32 nstack_initial=1)
DEBUG:numba.core.byteflow:dispatch pc=32, inst=POP_TOP(arg=None, lineno=1145)
DEBUG:numba.core.byteflow:stack ['$phi32.0']
DEBUG:numba.core.byteflow:dispatch pc=34, inst=JUMP_FORWARD(arg=4, lineno=1145)
DEBUG:numba.core.byteflow:stack []
DEBUG:numba.core.byteflow:end state. edges=[Edge(pc=40, stack=(), blockstack=(), npush=0)]
DEBUG:numba.core.byteflow:pending: deque([State(pc_initial=30 nstack_initial=0), State(pc_initial=40 nstack_initial=0), State(pc_initial=40 nstack_initial=0)])
DEBUG:numba.core.byteflow:stack: []
DEBUG:numba.core.byteflow:state.pc_initial: State(pc_initial=30 nstack_initial=0)
DEBUG:numba.core.byteflow:dispatch pc=30, inst=JUMP_FORWARD(arg=4, lineno=1145)
DEBUG:numba.core.byteflow:stack []
DEBUG:numba.core.byteflow:end state. edges=[Edge(pc=36, stack=(), blockstack=(), npush=0)]
DEBUG:numba.core.byteflow:pending: deque([State(pc_initial=40 nstack_initial=0), State(pc_initial=40 nstack_initial=0), State(pc_initial=36 nstack_initial=0)])
DEBUG:numba.core.byteflow:stack: []
DEBUG:numba.core.byteflow:state.pc_initial: State(pc_initial=40 nstack_initial=0)
DEBUG:numba.core.byteflow:dispatch pc=40, inst=LOAD_FAST(arg=0, lineno=1148)
DEBUG:numba.core.byteflow:stack []
DEBUG:numba.core.byteflow:dispatch pc=42, inst=LOAD_CONST(arg=2, lineno=1148)
DEBUG:numba.core.byteflow:stack ['$x40.0']
DEBUG:numba.core.byteflow:dispatch pc=44, inst=BINARY_SUBSCR(arg=None, lineno=1148)
DEBUG:numba.core.byteflow:stack ['$x40.0', '$const42.1']
DEBUG:numba.core.byteflow:dispatch pc=46, inst=STORE_FAST(arg=4, lineno=1148)
DEBUG:numba.core.byteflow:stack ['$44binary_subscr.2']
DEBUG:numba.core.byteflow:dispatch pc=48, inst=LOAD_FAST(arg=1, lineno=1149)
DEBUG:numba.core.byteflow:stack []
DEBUG:numba.core.byteflow:dispatch pc=50, inst=UNARY_NEGATIVE(arg=None, lineno=1149)
DEBUG:numba.core.byteflow:stack ['$threshold48.3']
DEBUG:numba.core.byteflow:dispatch pc=52, inst=LOAD_FAST(arg=4, lineno=1149)
DEBUG:numba.core.byteflow:stack ['$50unary_negative.4']
DEBUG:numba.core.byteflow:dispatch pc=54, inst=DUP_TOP(arg=None, lineno=1149)
DEBUG:numba.core.byteflow:stack ['$50unary_negative.4', '$x152.5']
DEBUG:numba.core.byteflow:dispatch pc=56, inst=ROT_THREE(arg=None, lineno=1149)
DEBUG:numba.core.byteflow:stack ['$50unary_negative.4', '$x152.5', '$54dup_top.6']
DEBUG:numba.core.byteflow:dispatch pc=58, inst=COMPARE_OP(arg=1, lineno=1149)
DEBUG:numba.core.byteflow:stack ['$54dup_top.6', '$50unary_negative.4', '$x152.5']
DEBUG:numba.core.byteflow:dispatch pc=60, inst=POP_JUMP_IF_FALSE(arg=70, lineno=1149)
DEBUG:numba.core.byteflow:stack ['$54dup_top.6', '$58compare_op.7']
DEBUG:numba.core.byteflow:end state. edges=[Edge(pc=62, stack=('$54dup_top.6',), blockstack=(), npush=0), Edge(pc=70, stack=('$54dup_top.6',), blockstack=(), npush=0)]
DEBUG:numba.core.byteflow:pending: deque([State(pc_initial=40 nstack_initial=0), State(pc_initial=36 nstack_initial=0), State(pc_initial=62 nstack_initial=1), State(pc_initial=70 nstack_initial=1)])
DEBUG:numba.core.byteflow:pending: deque([State(pc_initial=36 nstack_initial=0), State(pc_initial=62 nstack_initial=1), State(pc_initial=70 nstack_initial=1)])
DEBUG:numba.core.byteflow:stack: []
DEBUG:numba.core.byteflow:state.pc_initial: State(pc_initial=36 nstack_initial=0)
DEBUG:numba.core.byteflow:dispatch pc=36, inst=LOAD_CONST(arg=1, lineno=1146)
DEBUG:numba.core.byteflow:stack []
DEBUG:numba.core.byteflow:dispatch pc=38, inst=STORE_FAST(arg=3, lineno=1146)
DEBUG:numba.core.byteflow:stack ['$const36.0']
DEBUG:numba.core.byteflow:end state. edges=[Edge(pc=40, stack=(), blockstack=(), npush=0)]
DEBUG:numba.core.byteflow:pending: deque([State(pc_initial=62 nstack_initial=1), State(pc_initial=70 nstack_initial=1), State(pc_initial=40 nstack_initial=0)])
DEBUG:numba.core.byteflow:stack: ['$phi62.0']
DEBUG:numba.core.byteflow:state.pc_initial: State(pc_initial=62 nstack_initial=1)
DEBUG:numba.core.byteflow:dispatch pc=62, inst=LOAD_FAST(arg=1, lineno=1149)
DEBUG:numba.core.byteflow:stack ['$phi62.0']
DEBUG:numba.core.byteflow:dispatch pc=64, inst=COMPARE_OP(arg=1, lineno=1149)
DEBUG:numba.core.byteflow:stack ['$phi62.0', '$threshold62.1']
DEBUG:numba.core.byteflow:dispatch pc=66, inst=POP_JUMP_IF_FALSE(arg=78, lineno=1149)
DEBUG:numba.core.byteflow:stack ['$64compare_op.2']
DEBUG:numba.core.byteflow:end state. edges=[Edge(pc=68, stack=(), blockstack=(), npush=0), Edge(pc=78, stack=(), blockstack=(), npush=0)]
DEBUG:numba.core.byteflow:pending: deque([State(pc_initial=70 nstack_initial=1), State(pc_initial=40 nstack_initial=0), State(pc_initial=68 nstack_initial=0), State(pc_initial=78 nstack_initial=0)])
DEBUG:numba.core.byteflow:stack: ['$phi70.0']
DEBUG:numba.core.byteflow:state.pc_initial: State(pc_initial=70 nstack_initial=1)
DEBUG:numba.core.byteflow:dispatch pc=70, inst=POP_TOP(arg=None, lineno=1149)
DEBUG:numba.core.byteflow:stack ['$phi70.0']
DEBUG:numba.core.byteflow:dispatch pc=72, inst=JUMP_FORWARD(arg=4, lineno=1149)
DEBUG:numba.core.byteflow:stack []
DEBUG:numba.core.byteflow:end state. edges=[Edge(pc=78, stack=(), blockstack=(), npush=0)]
DEBUG:numba.core.byteflow:pending: deque([State(pc_initial=40 nstack_initial=0), State(pc_initial=68 nstack_initial=0), State(pc_initial=78 nstack_initial=0), State(pc_initial=78 nstack_initial=0)])
DEBUG:numba.core.byteflow:pending: deque([State(pc_initial=68 nstack_initial=0), State(pc_initial=78 nstack_initial=0), State(pc_initial=78 nstack_initial=0)])
DEBUG:numba.core.byteflow:stack: []
DEBUG:numba.core.byteflow:state.pc_initial: State(pc_initial=68 nstack_initial=0)
DEBUG:numba.core.byteflow:dispatch pc=68, inst=JUMP_FORWARD(arg=4, lineno=1149)
DEBUG:numba.core.byteflow:stack []
DEBUG:numba.core.byteflow:end state. edges=[Edge(pc=74, stack=(), blockstack=(), npush=0)]
DEBUG:numba.core.byteflow:pending: deque([State(pc_initial=78 nstack_initial=0), State(pc_initial=78 nstack_initial=0), State(pc_initial=74 nstack_initial=0)])
DEBUG:numba.core.byteflow:stack: []
DEBUG:numba.core.byteflow:state.pc_initial: State(pc_initial=78 nstack_initial=0)
DEBUG:numba.core.byteflow:dispatch pc=78, inst=LOAD_FAST(arg=2, lineno=1152)
DEBUG:numba.core.byteflow:stack []
DEBUG:numba.core.byteflow:dispatch pc=80, inst=POP_JUMP_IF_FALSE(arg=102, lineno=1152)
DEBUG:numba.core.byteflow:stack ['$zero_pos78.0']
DEBUG:numba.core.byteflow:end state. edges=[Edge(pc=82, stack=(), blockstack=(), npush=0), Edge(pc=102, stack=(), blockstack=(), npush=0)]
DEBUG:numba.core.byteflow:pending: deque([State(pc_initial=78 nstack_initial=0), State(pc_initial=74 nstack_initial=0), State(pc_initial=82 nstack_initial=0), State(pc_initial=102 nstack_initial=0)])
DEBUG:numba.core.byteflow:pending: deque([State(pc_initial=74 nstack_initial=0), State(pc_initial=82 nstack_initial=0), State(pc_initial=102 nstack_initial=0)])
DEBUG:numba.core.byteflow:stack: []
DEBUG:numba.core.byteflow:state.pc_initial: State(pc_initial=74 nstack_initial=0)
DEBUG:numba.core.byteflow:dispatch pc=74, inst=LOAD_CONST(arg=1, lineno=1150)
DEBUG:numba.core.byteflow:stack []
DEBUG:numba.core.byteflow:dispatch pc=76, inst=STORE_FAST(arg=4, lineno=1150)
DEBUG:numba.core.byteflow:stack ['$const74.0']
DEBUG:numba.core.byteflow:end state. edges=[Edge(pc=78, stack=(), blockstack=(), npush=0)]
DEBUG:numba.core.byteflow:pending: deque([State(pc_initial=82 nstack_initial=0), State(pc_initial=102 nstack_initial=0), State(pc_initial=78 nstack_initial=0)])
DEBUG:numba.core.byteflow:stack: []
DEBUG:numba.core.byteflow:state.pc_initial: State(pc_initial=82 nstack_initial=0)
DEBUG:numba.core.byteflow:dispatch pc=82, inst=LOAD_GLOBAL(arg=0, lineno=1153)
DEBUG:numba.core.byteflow:stack []
DEBUG:numba.core.byteflow:dispatch pc=84, inst=LOAD_METHOD(arg=1, lineno=1153)
DEBUG:numba.core.byteflow:stack ['$82load_global.0']
DEBUG:numba.core.byteflow:dispatch pc=86, inst=LOAD_FAST(arg=3, lineno=1153)
DEBUG:numba.core.byteflow:stack ['$84load_method.1']
DEBUG:numba.core.byteflow:dispatch pc=88, inst=CALL_METHOD(arg=1, lineno=1153)
DEBUG:numba.core.byteflow:stack ['$84load_method.1', '$x086.2']
DEBUG:numba.core.byteflow:dispatch pc=90, inst=LOAD_GLOBAL(arg=0, lineno=1153)
DEBUG:numba.core.byteflow:stack ['$88call_method.3']
DEBUG:numba.core.byteflow:dispatch pc=92, inst=LOAD_METHOD(arg=1, lineno=1153)
DEBUG:numba.core.byteflow:stack ['$88call_method.3', '$90load_global.4']
DEBUG:numba.core.byteflow:dispatch pc=94, inst=LOAD_FAST(arg=4, lineno=1153)
DEBUG:numba.core.byteflow:stack ['$88call_method.3', '$92load_method.5']
DEBUG:numba.core.byteflow:dispatch pc=96, inst=CALL_METHOD(arg=1, lineno=1153)
DEBUG:numba.core.byteflow:stack ['$88call_method.3', '$92load_method.5', '$x194.6']
DEBUG:numba.core.byteflow:dispatch pc=98, inst=COMPARE_OP(arg=3, lineno=1153)
DEBUG:numba.core.byteflow:stack ['$88call_method.3', '$96call_method.7']
DEBUG:numba.core.byteflow:dispatch pc=100, inst=RETURN_VALUE(arg=None, lineno=1153)
DEBUG:numba.core.byteflow:stack ['$98compare_op.8']
DEBUG:numba.core.byteflow:end state. edges=[]
DEBUG:numba.core.byteflow:pending: deque([State(pc_initial=102 nstack_initial=0), State(pc_initial=78 nstack_initial=0)])
DEBUG:numba.core.byteflow:stack: []
DEBUG:numba.core.byteflow:state.pc_initial: State(pc_initial=102 nstack_initial=0)
DEBUG:numba.core.byteflow:dispatch pc=102, inst=LOAD_GLOBAL(arg=0, lineno=1155)
DEBUG:numba.core.byteflow:stack []
DEBUG:numba.core.byteflow:dispatch pc=104, inst=LOAD_METHOD(arg=2, lineno=1155)
DEBUG:numba.core.byteflow:stack ['$102load_global.0']
DEBUG:numba.core.byteflow:dispatch pc=106, inst=LOAD_FAST(arg=3, lineno=1155)
DEBUG:numba.core.byteflow:stack ['$104load_method.1']
DEBUG:numba.core.byteflow:dispatch pc=108, inst=CALL_METHOD(arg=1, lineno=1155)
DEBUG:numba.core.byteflow:stack ['$104load_method.1', '$x0106.2']
DEBUG:numba.core.byteflow:dispatch pc=110, inst=LOAD_GLOBAL(arg=0, lineno=1155)
DEBUG:numba.core.byteflow:stack ['$108call_method.3']
DEBUG:numba.core.byteflow:dispatch pc=112, inst=LOAD_METHOD(arg=2, lineno=1155)
DEBUG:numba.core.byteflow:stack ['$108call_method.3', '$110load_global.4']
DEBUG:numba.core.byteflow:dispatch pc=114, inst=LOAD_FAST(arg=4, lineno=1155)
DEBUG:numba.core.byteflow:stack ['$108call_method.3', '$112load_method.5']
DEBUG:numba.core.byteflow:dispatch pc=116, inst=CALL_METHOD(arg=1, lineno=1155)
DEBUG:numba.core.byteflow:stack ['$108call_method.3', '$112load_method.5', '$x1114.6']
DEBUG:numba.core.byteflow:dispatch pc=118, inst=COMPARE_OP(arg=3, lineno=1155)
DEBUG:numba.core.byteflow:stack ['$108call_method.3', '$116call_method.7']
DEBUG:numba.core.byteflow:dispatch pc=120, inst=RETURN_VALUE(arg=None, lineno=1155)
DEBUG:numba.core.byteflow:stack ['$118compare_op.8']
DEBUG:numba.core.byteflow:end state. edges=[]
DEBUG:numba.core.byteflow:pending: deque([State(pc_initial=78 nstack_initial=0)])
DEBUG:numba.core.byteflow:-------------------------Prune PHIs-------------------------
DEBUG:numba.core.byteflow:Used_phis: defaultdict(<class 'set'>,
            {State(pc_initial=0 nstack_initial=0): set(),
             State(pc_initial=24 nstack_initial=1): {'$phi24.0'},
             State(pc_initial=30 nstack_initial=0): set(),
             State(pc_initial=32 nstack_initial=1): set(),
             State(pc_initial=36 nstack_initial=0): set(),
             State(pc_initial=40 nstack_initial=0): set(),
             State(pc_initial=62 nstack_initial=1): {'$phi62.0'},
             State(pc_initial=68 nstack_initial=0): set(),
             State(pc_initial=70 nstack_initial=1): set(),
             State(pc_initial=74 nstack_initial=0): set(),
             State(pc_initial=78 nstack_initial=0): set(),
             State(pc_initial=82 nstack_initial=0): set(),
             State(pc_initial=102 nstack_initial=0): set()})
DEBUG:numba.core.byteflow:defmap: {'$phi24.0': State(pc_initial=0 nstack_initial=0),
 '$phi32.0': State(pc_initial=0 nstack_initial=0),
 '$phi62.0': State(pc_initial=40 nstack_initial=0),
 '$phi70.0': State(pc_initial=40 nstack_initial=0)}
DEBUG:numba.core.byteflow:phismap: defaultdict(<class 'set'>,
            {'$phi24.0': {('$16dup_top.6',
                           State(pc_initial=0 nstack_initial=0))},
             '$phi32.0': {('$16dup_top.6',
                           State(pc_initial=0 nstack_initial=0))},
             '$phi62.0': {('$54dup_top.6',
                           State(pc_initial=40 nstack_initial=0))},
             '$phi70.0': {('$54dup_top.6',
                           State(pc_initial=40 nstack_initial=0))}})
DEBUG:numba.core.byteflow:changing phismap: defaultdict(<class 'set'>,
            {'$phi24.0': {('$16dup_top.6',
                           State(pc_initial=0 nstack_initial=0))},
             '$phi32.0': {('$16dup_top.6',
                           State(pc_initial=0 nstack_initial=0))},
             '$phi62.0': {('$54dup_top.6',
                           State(pc_initial=40 nstack_initial=0))},
             '$phi70.0': {('$54dup_top.6',
                           State(pc_initial=40 nstack_initial=0))}})
DEBUG:numba.core.byteflow:keep phismap: {'$phi24.0': {('$16dup_top.6', State(pc_initial=0 nstack_initial=0))},
 '$phi62.0': {('$54dup_top.6', State(pc_initial=40 nstack_initial=0))}}
DEBUG:numba.core.byteflow:new_out: defaultdict(<class 'dict'>,
            {State(pc_initial=0 nstack_initial=0): {'$phi24.0': '$16dup_top.6'},
             State(pc_initial=40 nstack_initial=0): {'$phi62.0': '$54dup_top.6'}})
DEBUG:numba.core.byteflow:----------------------DONE Prune PHIs-----------------------
DEBUG:numba.core.byteflow:block_infos State(pc_initial=0 nstack_initial=0):
AdaptBlockInfo(insts=((0, {}), (2, {'res': '$x2.0'}), (4, {'res': '$const4.1'}), (6, {'index': '$const4.1', 'target': '$x2.0', 'res': '$6binary_subscr.2'}), (8, {'value': '$6binary_subscr.2'}), (10, {'res': '$threshold10.3'}), (12, {'value': '$threshold10.3', 'res': '$12unary_negative.4'}), (14, {'res': '$x014.5'}), (16, {'orig': ['$x014.5'], 'duped': ['$16dup_top.6']}), (20, {'lhs': '$12unary_negative.4', 'rhs': '$x014.5', 'res': '$20compare_op.7'}), (22, {'pred': '$20compare_op.7'})), outgoing_phis={'$phi24.0': '$16dup_top.6'}, blockstack=(), active_try_block=None, outgoing_edgepushed={24: ('$16dup_top.6',), 32: ('$16dup_top.6',)})
DEBUG:numba.core.byteflow:block_infos State(pc_initial=24 nstack_initial=1):
AdaptBlockInfo(insts=((24, {'res': '$threshold24.1'}), (26, {'lhs': '$phi24.0', 'rhs': '$threshold24.1', 'res': '$26compare_op.2'}), (28, {'pred': '$26compare_op.2'})), outgoing_phis={}, blockstack=(), active_try_block=None, outgoing_edgepushed={30: (), 40: ()})
DEBUG:numba.core.byteflow:block_infos State(pc_initial=30 nstack_initial=0):
AdaptBlockInfo(insts=((30, {}),), outgoing_phis={}, blockstack=(), active_try_block=None, outgoing_edgepushed={36: ()})
DEBUG:numba.core.byteflow:block_infos State(pc_initial=32 nstack_initial=1):
AdaptBlockInfo(insts=((34, {}),), outgoing_phis={}, blockstack=(), active_try_block=None, outgoing_edgepushed={40: ()})
DEBUG:numba.core.byteflow:block_infos State(pc_initial=36 nstack_initial=0):
AdaptBlockInfo(insts=((36, {'res': '$const36.0'}), (38, {'value': '$const36.0'})), outgoing_phis={}, blockstack=(), active_try_block=None, outgoing_edgepushed={40: ()})
DEBUG:numba.core.byteflow:block_infos State(pc_initial=40 nstack_initial=0):
AdaptBlockInfo(insts=((40, {'res': '$x40.0'}), (42, {'res': '$const42.1'}), (44, {'index': '$const42.1', 'target': '$x40.0', 'res': '$44binary_subscr.2'}), (46, {'value': '$44binary_subscr.2'}), (48, {'res': '$threshold48.3'}), (50, {'value': '$threshold48.3', 'res': '$50unary_negative.4'}), (52, {'res': '$x152.5'}), (54, {'orig': ['$x152.5'], 'duped': ['$54dup_top.6']}), (58, {'lhs': '$50unary_negative.4', 'rhs': '$x152.5', 'res': '$58compare_op.7'}), (60, {'pred': '$58compare_op.7'})), outgoing_phis={'$phi62.0': '$54dup_top.6'}, blockstack=(), active_try_block=None, outgoing_edgepushed={62: ('$54dup_top.6',), 70: ('$54dup_top.6',)})
DEBUG:numba.core.byteflow:block_infos State(pc_initial=62 nstack_initial=1):
AdaptBlockInfo(insts=((62, {'res': '$threshold62.1'}), (64, {'lhs': '$phi62.0', 'rhs': '$threshold62.1', 'res': '$64compare_op.2'}), (66, {'pred': '$64compare_op.2'})), outgoing_phis={}, blockstack=(), active_try_block=None, outgoing_edgepushed={68: (), 78: ()})
DEBUG:numba.core.byteflow:block_infos State(pc_initial=68 nstack_initial=0):
AdaptBlockInfo(insts=((68, {}),), outgoing_phis={}, blockstack=(), active_try_block=None, outgoing_edgepushed={74: ()})
DEBUG:numba.core.byteflow:block_infos State(pc_initial=70 nstack_initial=1):
AdaptBlockInfo(insts=((72, {}),), outgoing_phis={}, blockstack=(), active_try_block=None, outgoing_edgepushed={78: ()})
DEBUG:numba.core.byteflow:block_infos State(pc_initial=74 nstack_initial=0):
AdaptBlockInfo(insts=((74, {'res': '$const74.0'}), (76, {'value': '$const74.0'})), outgoing_phis={}, blockstack=(), active_try_block=None, outgoing_edgepushed={78: ()})
DEBUG:numba.core.byteflow:block_infos State(pc_initial=78 nstack_initial=0):
AdaptBlockInfo(insts=((78, {'res': '$zero_pos78.0'}), (80, {'pred': '$zero_pos78.0'})), outgoing_phis={}, blockstack=(), active_try_block=None, outgoing_edgepushed={82: (), 102: ()})
DEBUG:numba.core.byteflow:block_infos State(pc_initial=82 nstack_initial=0):
AdaptBlockInfo(insts=((82, {'res': '$82load_global.0'}), (84, {'item': '$82load_global.0', 'res': '$84load_method.1'}), (86, {'res': '$x086.2'}), (88, {'func': '$84load_method.1', 'args': ['$x086.2'], 'res': '$88call_method.3'}), (90, {'res': '$90load_global.4'}), (92, {'item': '$90load_global.4', 'res': '$92load_method.5'}), (94, {'res': '$x194.6'}), (96, {'func': '$92load_method.5', 'args': ['$x194.6'], 'res': '$96call_method.7'}), (98, {'lhs': '$88call_method.3', 'rhs': '$96call_method.7', 'res': '$98compare_op.8'}), (100, {'retval': '$98compare_op.8', 'castval': '$100return_value.9'})), outgoing_phis={}, blockstack=(), active_try_block=None, outgoing_edgepushed={})
DEBUG:numba.core.byteflow:block_infos State(pc_initial=102 nstack_initial=0):
AdaptBlockInfo(insts=((102, {'res': '$102load_global.0'}), (104, {'item': '$102load_global.0', 'res': '$104load_method.1'}), (106, {'res': '$x0106.2'}), (108, {'func': '$104load_method.1', 'args': ['$x0106.2'], 'res': '$108call_method.3'}), (110, {'res': '$110load_global.4'}), (112, {'item': '$110load_global.4', 'res': '$112load_method.5'}), (114, {'res': '$x1114.6'}), (116, {'func': '$112load_method.5', 'args': ['$x1114.6'], 'res': '$116call_method.7'}), (118, {'lhs': '$108call_method.3', 'rhs': '$116call_method.7', 'res': '$118compare_op.8'}), (120, {'retval': '$118compare_op.8', 'castval': '$120return_value.9'})), outgoing_phis={}, blockstack=(), active_try_block=None, outgoing_edgepushed={})
DEBUG:numba.core.interpreter:label 0:
    x = arg(0, name=x)                       ['x']
    threshold = arg(1, name=threshold)       ['threshold']
    zero_pos = arg(2, name=zero_pos)         ['zero_pos']
    $const4.1 = const(int, 0)                ['$const4.1']
    x0 = getitem(value=x, index=$const4.1, fn=<built-in function getitem>) ['$const4.1', 'x', 'x0']
    $12unary_negative.4 = unary(fn=<built-in function neg>, value=threshold) ['$12unary_negative.4', 'threshold']
    $20compare_op.7 = $12unary_negative.4 <= x0 ['$12unary_negative.4', '$20compare_op.7', 'x0']
    bool22 = global(bool: <class 'bool'>)    ['bool22']
    $22pred = call bool22($20compare_op.7, func=bool22, args=(Var($20compare_op.7, audio.py:1145),), kws=(), vararg=None, varkwarg=None, target=None) ['$20compare_op.7', '$22pred', 'bool22']
    $phi24.0 = x0                            ['$phi24.0', 'x0']
    branch $22pred, 24, 32                   ['$22pred']
label 24:
    $26compare_op.2 = $phi24.0 <= threshold  ['$26compare_op.2', '$phi24.0', 'threshold']
    bool28 = global(bool: <class 'bool'>)    ['bool28']
    $28pred = call bool28($26compare_op.2, func=bool28, args=(Var($26compare_op.2, audio.py:1145),), kws=(), vararg=None, varkwarg=None, target=None) ['$26compare_op.2', '$28pred', 'bool28']
    branch $28pred, 30, 40                   ['$28pred']
label 30:
    jump 36                                  []
label 32:
    jump 40                                  []
label 36:
    x0 = const(int, 0)                       ['x0']
    jump 40                                  []
label 40:
    $const42.1 = const(int, -1)              ['$const42.1']
    x1 = getitem(value=x, index=$const42.1, fn=<built-in function getitem>) ['$const42.1', 'x', 'x1']
    $50unary_negative.4 = unary(fn=<built-in function neg>, value=threshold) ['$50unary_negative.4', 'threshold']
    $58compare_op.7 = $50unary_negative.4 <= x1 ['$50unary_negative.4', '$58compare_op.7', 'x1']
    bool60 = global(bool: <class 'bool'>)    ['bool60']
    $60pred = call bool60($58compare_op.7, func=bool60, args=(Var($58compare_op.7, audio.py:1149),), kws=(), vararg=None, varkwarg=None, target=None) ['$58compare_op.7', '$60pred', 'bool60']
    $phi62.0 = x1                            ['$phi62.0', 'x1']
    branch $60pred, 62, 70                   ['$60pred']
label 62:
    $64compare_op.2 = $phi62.0 <= threshold  ['$64compare_op.2', '$phi62.0', 'threshold']
    bool66 = global(bool: <class 'bool'>)    ['bool66']
    $66pred = call bool66($64compare_op.2, func=bool66, args=(Var($64compare_op.2, audio.py:1149),), kws=(), vararg=None, varkwarg=None, target=None) ['$64compare_op.2', '$66pred', 'bool66']
    branch $66pred, 68, 78                   ['$66pred']
label 68:
    jump 74                                  []
label 70:
    jump 78                                  []
label 74:
    x1 = const(int, 0)                       ['x1']
    jump 78                                  []
label 78:
    bool80 = global(bool: <class 'bool'>)    ['bool80']
    $80pred = call bool80(zero_pos, func=bool80, args=(Var(zero_pos, audio.py:1141),), kws=(), vararg=None, varkwarg=None, target=None) ['$80pred', 'bool80', 'zero_pos']
    branch $80pred, 82, 102                  ['$80pred']
label 82:
    $82load_global.0 = global(np: <module 'numpy' from 'C:\\Users\\taskm\\anaconda3\\envs\\taskai\\lib\\site-packages\\numpy\\__init__.py'>) ['$82load_global.0']
    $84load_method.1 = getattr(value=$82load_global.0, attr=signbit) ['$82load_global.0', '$84load_method.1']
    $88call_method.3 = call $84load_method.1(x0, func=$84load_method.1, args=[Var(x0, audio.py:1144)], kws=(), vararg=None, varkwarg=None, target=None) ['$84load_method.1', '$88call_method.3', 'x0']
    $90load_global.4 = global(np: <module 'numpy' from 'C:\\Users\\taskm\\anaconda3\\envs\\taskai\\lib\\site-packages\\numpy\\__init__.py'>) ['$90load_global.4']
    $92load_method.5 = getattr(value=$90load_global.4, attr=signbit) ['$90load_global.4', '$92load_method.5']
    $96call_method.7 = call $92load_method.5(x1, func=$92load_method.5, args=[Var(x1, audio.py:1148)], kws=(), vararg=None, varkwarg=None, target=None) ['$92load_method.5', '$96call_method.7', 'x1']
    $98compare_op.8 = $88call_method.3 != $96call_method.7 ['$88call_method.3', '$96call_method.7', '$98compare_op.8']
    $100return_value.9 = cast(value=$98compare_op.8) ['$100return_value.9', '$98compare_op.8']
    return $100return_value.9                ['$100return_value.9']
label 102:
    $102load_global.0 = global(np: <module 'numpy' from 'C:\\Users\\taskm\\anaconda3\\envs\\taskai\\lib\\site-packages\\numpy\\__init__.py'>) ['$102load_global.0']
    $104load_method.1 = getattr(value=$102load_global.0, attr=sign) ['$102load_global.0', '$104load_method.1']
    $108call_method.3 = call $104load_method.1(x0, func=$104load_method.1, args=[Var(x0, audio.py:1144)], kws=(), vararg=None, varkwarg=None, target=None) ['$104load_method.1', '$108call_method.3', 'x0']
    $110load_global.4 = global(np: <module 'numpy' from 'C:\\Users\\taskm\\anaconda3\\envs\\taskai\\lib\\site-packages\\numpy\\__init__.py'>) ['$110load_global.4']
    $112load_method.5 = getattr(value=$110load_global.4, attr=sign) ['$110load_global.4', '$112load_method.5']
    $116call_method.7 = call $112load_method.5(x1, func=$112load_method.5, args=[Var(x1, audio.py:1148)], kws=(), vararg=None, varkwarg=None, target=None) ['$112load_method.5', '$116call_method.7', 'x1']
    $118compare_op.8 = $108call_method.3 != $116call_method.7 ['$108call_method.3', '$116call_method.7', '$118compare_op.8']
    $120return_value.9 = cast(value=$118compare_op.8) ['$118compare_op.8', '$120return_value.9']
    return $120return_value.9                ['$120return_value.9']

DEBUG:numba.core.byteflow:bytecode dump:
>          0	NOP(arg=None, lineno=1039)
           2	LOAD_FAST(arg=0, lineno=1042)
           4	LOAD_CONST(arg=1, lineno=1042)
           6	BINARY_SUBSCR(arg=None, lineno=1042)
           8	LOAD_FAST(arg=0, lineno=1042)
          10	LOAD_CONST(arg=2, lineno=1042)
          12	BINARY_SUBSCR(arg=None, lineno=1042)
          14	COMPARE_OP(arg=4, lineno=1042)
          16	LOAD_FAST(arg=0, lineno=1042)
          18	LOAD_CONST(arg=1, lineno=1042)
          20	BINARY_SUBSCR(arg=None, lineno=1042)
          22	LOAD_FAST(arg=0, lineno=1042)
          24	LOAD_CONST(arg=3, lineno=1042)
          26	BINARY_SUBSCR(arg=None, lineno=1042)
          28	COMPARE_OP(arg=5, lineno=1042)
          30	BINARY_AND(arg=None, lineno=1042)
          32	RETURN_VALUE(arg=None, lineno=1042)
DEBUG:numba.core.byteflow:pending: deque([State(pc_initial=0 nstack_initial=0)])
DEBUG:numba.core.byteflow:stack: []
DEBUG:numba.core.byteflow:state.pc_initial: State(pc_initial=0 nstack_initial=0)
DEBUG:numba.core.byteflow:dispatch pc=0, inst=NOP(arg=None, lineno=1039)
DEBUG:numba.core.byteflow:stack []
DEBUG:numba.core.byteflow:dispatch pc=2, inst=LOAD_FAST(arg=0, lineno=1042)
DEBUG:numba.core.byteflow:stack []
DEBUG:numba.core.byteflow:dispatch pc=4, inst=LOAD_CONST(arg=1, lineno=1042)
DEBUG:numba.core.byteflow:stack ['$x2.0']
DEBUG:numba.core.byteflow:dispatch pc=6, inst=BINARY_SUBSCR(arg=None, lineno=1042)
DEBUG:numba.core.byteflow:stack ['$x2.0', '$const4.1']
DEBUG:numba.core.byteflow:dispatch pc=8, inst=LOAD_FAST(arg=0, lineno=1042)
DEBUG:numba.core.byteflow:stack ['$6binary_subscr.2']
DEBUG:numba.core.byteflow:dispatch pc=10, inst=LOAD_CONST(arg=2, lineno=1042)
DEBUG:numba.core.byteflow:stack ['$6binary_subscr.2', '$x8.3']
DEBUG:numba.core.byteflow:dispatch pc=12, inst=BINARY_SUBSCR(arg=None, lineno=1042)
DEBUG:numba.core.byteflow:stack ['$6binary_subscr.2', '$x8.3', '$const10.4']
DEBUG:numba.core.byteflow:dispatch pc=14, inst=COMPARE_OP(arg=4, lineno=1042)
DEBUG:numba.core.byteflow:stack ['$6binary_subscr.2', '$12binary_subscr.5']
DEBUG:numba.core.byteflow:dispatch pc=16, inst=LOAD_FAST(arg=0, lineno=1042)
DEBUG:numba.core.byteflow:stack ['$14compare_op.6']
DEBUG:numba.core.byteflow:dispatch pc=18, inst=LOAD_CONST(arg=1, lineno=1042)
DEBUG:numba.core.byteflow:stack ['$14compare_op.6', '$x16.7']
DEBUG:numba.core.byteflow:dispatch pc=20, inst=BINARY_SUBSCR(arg=None, lineno=1042)
DEBUG:numba.core.byteflow:stack ['$14compare_op.6', '$x16.7', '$const18.8']
DEBUG:numba.core.byteflow:dispatch pc=22, inst=LOAD_FAST(arg=0, lineno=1042)
DEBUG:numba.core.byteflow:stack ['$14compare_op.6', '$20binary_subscr.9']
DEBUG:numba.core.byteflow:dispatch pc=24, inst=LOAD_CONST(arg=3, lineno=1042)
DEBUG:numba.core.byteflow:stack ['$14compare_op.6', '$20binary_subscr.9', '$x22.10']
DEBUG:numba.core.byteflow:dispatch pc=26, inst=BINARY_SUBSCR(arg=None, lineno=1042)
DEBUG:numba.core.byteflow:stack ['$14compare_op.6', '$20binary_subscr.9', '$x22.10', '$const24.11']
DEBUG:numba.core.byteflow:dispatch pc=28, inst=COMPARE_OP(arg=5, lineno=1042)
DEBUG:numba.core.byteflow:stack ['$14compare_op.6', '$20binary_subscr.9', '$26binary_subscr.12']
DEBUG:numba.core.byteflow:dispatch pc=30, inst=BINARY_AND(arg=None, lineno=1042)
DEBUG:numba.core.byteflow:stack ['$14compare_op.6', '$28compare_op.13']
DEBUG:numba.core.byteflow:dispatch pc=32, inst=RETURN_VALUE(arg=None, lineno=1042)
DEBUG:numba.core.byteflow:stack ['$30binary_and.14']
DEBUG:numba.core.byteflow:end state. edges=[]
DEBUG:numba.core.byteflow:-------------------------Prune PHIs-------------------------
DEBUG:numba.core.byteflow:Used_phis: defaultdict(<class 'set'>, {State(pc_initial=0 nstack_initial=0): set()})
DEBUG:numba.core.byteflow:defmap: {}
DEBUG:numba.core.byteflow:phismap: defaultdict(<class 'set'>, {})
DEBUG:numba.core.byteflow:changing phismap: defaultdict(<class 'set'>, {})
DEBUG:numba.core.byteflow:keep phismap: {}
DEBUG:numba.core.byteflow:new_out: defaultdict(<class 'dict'>, {})
DEBUG:numba.core.byteflow:----------------------DONE Prune PHIs-----------------------
DEBUG:numba.core.byteflow:block_infos State(pc_initial=0 nstack_initial=0):
AdaptBlockInfo(insts=((0, {}), (2, {'res': '$x2.0'}), (4, {'res': '$const4.1'}), (6, {'index': '$const4.1', 'target': '$x2.0', 'res': '$6binary_subscr.2'}), (8, {'res': '$x8.3'}), (10, {'res': '$const10.4'}), (12, {'index': '$const10.4', 'target': '$x8.3', 'res': '$12binary_subscr.5'}), (14, {'lhs': '$6binary_subscr.2', 'rhs': '$12binary_subscr.5', 'res': '$14compare_op.6'}), (16, {'res': '$x16.7'}), (18, {'res': '$const18.8'}), (20, {'index': '$const18.8', 'target': '$x16.7', 'res': '$20binary_subscr.9'}), (22, {'res': '$x22.10'}), (24, {'res': '$const24.11'}), (26, {'index': '$const24.11', 'target': '$x22.10', 'res': '$26binary_subscr.12'}), (28, {'lhs': '$20binary_subscr.9', 'rhs': '$26binary_subscr.12', 'res': '$28compare_op.13'}), (30, {'lhs': '$14compare_op.6', 'rhs': '$28compare_op.13', 'res': '$30binary_and.14'}), (32, {'retval': '$30binary_and.14', 'castval': '$32return_value.15'})), outgoing_phis={}, blockstack=(), active_try_block=None, outgoing_edgepushed={})
DEBUG:numba.core.interpreter:label 0:
    x = arg(0, name=x)                       ['x']
    $const4.1 = const(int, 0)                ['$const4.1']
    $6binary_subscr.2 = getitem(value=x, index=$const4.1, fn=<built-in function getitem>) ['$6binary_subscr.2', '$const4.1', 'x']
    $const10.4 = const(int, -1)              ['$const10.4']
    $12binary_subscr.5 = getitem(value=x, index=$const10.4, fn=<built-in function getitem>) ['$12binary_subscr.5', '$const10.4', 'x']
    $14compare_op.6 = $6binary_subscr.2 > $12binary_subscr.5 ['$12binary_subscr.5', '$14compare_op.6', '$6binary_subscr.2']
    $const18.8 = const(int, 0)               ['$const18.8']
    $20binary_subscr.9 = getitem(value=x, index=$const18.8, fn=<built-in function getitem>) ['$20binary_subscr.9', '$const18.8', 'x']
    $const24.11 = const(int, 1)              ['$const24.11']
    $26binary_subscr.12 = getitem(value=x, index=$const24.11, fn=<built-in function getitem>) ['$26binary_subscr.12', '$const24.11', 'x']
    $28compare_op.13 = $20binary_subscr.9 >= $26binary_subscr.12 ['$20binary_subscr.9', '$26binary_subscr.12', '$28compare_op.13']
    $30binary_and.14 = $14compare_op.6 & $28compare_op.13 ['$14compare_op.6', '$28compare_op.13', '$30binary_and.14']
    $32return_value.15 = cast(value=$30binary_and.14) ['$30binary_and.14', '$32return_value.15']
    return $32return_value.15                ['$32return_value.15']

DEBUG:numba.core.byteflow:bytecode dump:
>          0	NOP(arg=None, lineno=1045)
           2	LOAD_FAST(arg=0, lineno=1048)
           4	LOAD_CONST(arg=1, lineno=1048)
           6	BINARY_SUBSCR(arg=None, lineno=1048)
           8	LOAD_FAST(arg=0, lineno=1048)
          10	LOAD_CONST(arg=2, lineno=1048)
          12	BINARY_SUBSCR(arg=None, lineno=1048)
          14	COMPARE_OP(arg=0, lineno=1048)
          16	LOAD_FAST(arg=0, lineno=1048)
          18	LOAD_CONST(arg=1, lineno=1048)
          20	BINARY_SUBSCR(arg=None, lineno=1048)
          22	LOAD_FAST(arg=0, lineno=1048)
          24	LOAD_CONST(arg=3, lineno=1048)
          26	BINARY_SUBSCR(arg=None, lineno=1048)
          28	COMPARE_OP(arg=1, lineno=1048)
          30	BINARY_AND(arg=None, lineno=1048)
          32	RETURN_VALUE(arg=None, lineno=1048)
DEBUG:numba.core.byteflow:pending: deque([State(pc_initial=0 nstack_initial=0)])
DEBUG:numba.core.byteflow:stack: []
DEBUG:numba.core.byteflow:state.pc_initial: State(pc_initial=0 nstack_initial=0)
DEBUG:numba.core.byteflow:dispatch pc=0, inst=NOP(arg=None, lineno=1045)
DEBUG:numba.core.byteflow:stack []
DEBUG:numba.core.byteflow:dispatch pc=2, inst=LOAD_FAST(arg=0, lineno=1048)
DEBUG:numba.core.byteflow:stack []
DEBUG:numba.core.byteflow:dispatch pc=4, inst=LOAD_CONST(arg=1, lineno=1048)
DEBUG:numba.core.byteflow:stack ['$x2.0']
DEBUG:numba.core.byteflow:dispatch pc=6, inst=BINARY_SUBSCR(arg=None, lineno=1048)
DEBUG:numba.core.byteflow:stack ['$x2.0', '$const4.1']
DEBUG:numba.core.byteflow:dispatch pc=8, inst=LOAD_FAST(arg=0, lineno=1048)
DEBUG:numba.core.byteflow:stack ['$6binary_subscr.2']
DEBUG:numba.core.byteflow:dispatch pc=10, inst=LOAD_CONST(arg=2, lineno=1048)
DEBUG:numba.core.byteflow:stack ['$6binary_subscr.2', '$x8.3']
DEBUG:numba.core.byteflow:dispatch pc=12, inst=BINARY_SUBSCR(arg=None, lineno=1048)
DEBUG:numba.core.byteflow:stack ['$6binary_subscr.2', '$x8.3', '$const10.4']
DEBUG:numba.core.byteflow:dispatch pc=14, inst=COMPARE_OP(arg=0, lineno=1048)
DEBUG:numba.core.byteflow:stack ['$6binary_subscr.2', '$12binary_subscr.5']
DEBUG:numba.core.byteflow:dispatch pc=16, inst=LOAD_FAST(arg=0, lineno=1048)
DEBUG:numba.core.byteflow:stack ['$14compare_op.6']
DEBUG:numba.core.byteflow:dispatch pc=18, inst=LOAD_CONST(arg=1, lineno=1048)
DEBUG:numba.core.byteflow:stack ['$14compare_op.6', '$x16.7']
DEBUG:numba.core.byteflow:dispatch pc=20, inst=BINARY_SUBSCR(arg=None, lineno=1048)
DEBUG:numba.core.byteflow:stack ['$14compare_op.6', '$x16.7', '$const18.8']
DEBUG:numba.core.byteflow:dispatch pc=22, inst=LOAD_FAST(arg=0, lineno=1048)
DEBUG:numba.core.byteflow:stack ['$14compare_op.6', '$20binary_subscr.9']
DEBUG:numba.core.byteflow:dispatch pc=24, inst=LOAD_CONST(arg=3, lineno=1048)
DEBUG:numba.core.byteflow:stack ['$14compare_op.6', '$20binary_subscr.9', '$x22.10']
DEBUG:numba.core.byteflow:dispatch pc=26, inst=BINARY_SUBSCR(arg=None, lineno=1048)
DEBUG:numba.core.byteflow:stack ['$14compare_op.6', '$20binary_subscr.9', '$x22.10', '$const24.11']
DEBUG:numba.core.byteflow:dispatch pc=28, inst=COMPARE_OP(arg=1, lineno=1048)
DEBUG:numba.core.byteflow:stack ['$14compare_op.6', '$20binary_subscr.9', '$26binary_subscr.12']
DEBUG:numba.core.byteflow:dispatch pc=30, inst=BINARY_AND(arg=None, lineno=1048)
DEBUG:numba.core.byteflow:stack ['$14compare_op.6', '$28compare_op.13']
DEBUG:numba.core.byteflow:dispatch pc=32, inst=RETURN_VALUE(arg=None, lineno=1048)
DEBUG:numba.core.byteflow:stack ['$30binary_and.14']
DEBUG:numba.core.byteflow:end state. edges=[]
DEBUG:numba.core.byteflow:-------------------------Prune PHIs-------------------------
DEBUG:numba.core.byteflow:Used_phis: defaultdict(<class 'set'>, {State(pc_initial=0 nstack_initial=0): set()})
DEBUG:numba.core.byteflow:defmap: {}
DEBUG:numba.core.byteflow:phismap: defaultdict(<class 'set'>, {})
DEBUG:numba.core.byteflow:changing phismap: defaultdict(<class 'set'>, {})
DEBUG:numba.core.byteflow:keep phismap: {}
DEBUG:numba.core.byteflow:new_out: defaultdict(<class 'dict'>, {})
DEBUG:numba.core.byteflow:----------------------DONE Prune PHIs-----------------------
DEBUG:numba.core.byteflow:block_infos State(pc_initial=0 nstack_initial=0):
AdaptBlockInfo(insts=((0, {}), (2, {'res': '$x2.0'}), (4, {'res': '$const4.1'}), (6, {'index': '$const4.1', 'target': '$x2.0', 'res': '$6binary_subscr.2'}), (8, {'res': '$x8.3'}), (10, {'res': '$const10.4'}), (12, {'index': '$const10.4', 'target': '$x8.3', 'res': '$12binary_subscr.5'}), (14, {'lhs': '$6binary_subscr.2', 'rhs': '$12binary_subscr.5', 'res': '$14compare_op.6'}), (16, {'res': '$x16.7'}), (18, {'res': '$const18.8'}), (20, {'index': '$const18.8', 'target': '$x16.7', 'res': '$20binary_subscr.9'}), (22, {'res': '$x22.10'}), (24, {'res': '$const24.11'}), (26, {'index': '$const24.11', 'target': '$x22.10', 'res': '$26binary_subscr.12'}), (28, {'lhs': '$20binary_subscr.9', 'rhs': '$26binary_subscr.12', 'res': '$28compare_op.13'}), (30, {'lhs': '$14compare_op.6', 'rhs': '$28compare_op.13', 'res': '$30binary_and.14'}), (32, {'retval': '$30binary_and.14', 'castval': '$32return_value.15'})), outgoing_phis={}, blockstack=(), active_try_block=None, outgoing_edgepushed={})
DEBUG:numba.core.interpreter:label 0:
    x = arg(0, name=x)                       ['x']
    $const4.1 = const(int, 0)                ['$const4.1']
    $6binary_subscr.2 = getitem(value=x, index=$const4.1, fn=<built-in function getitem>) ['$6binary_subscr.2', '$const4.1', 'x']
    $const10.4 = const(int, -1)              ['$const10.4']
    $12binary_subscr.5 = getitem(value=x, index=$const10.4, fn=<built-in function getitem>) ['$12binary_subscr.5', '$const10.4', 'x']
    $14compare_op.6 = $6binary_subscr.2 < $12binary_subscr.5 ['$12binary_subscr.5', '$14compare_op.6', '$6binary_subscr.2']
    $const18.8 = const(int, 0)               ['$const18.8']
    $20binary_subscr.9 = getitem(value=x, index=$const18.8, fn=<built-in function getitem>) ['$20binary_subscr.9', '$const18.8', 'x']
    $const24.11 = const(int, 1)              ['$const24.11']
    $26binary_subscr.12 = getitem(value=x, index=$const24.11, fn=<built-in function getitem>) ['$26binary_subscr.12', '$const24.11', 'x']
    $28compare_op.13 = $20binary_subscr.9 <= $26binary_subscr.12 ['$20binary_subscr.9', '$26binary_subscr.12', '$28compare_op.13']
    $30binary_and.14 = $14compare_op.6 & $28compare_op.13 ['$14compare_op.6', '$28compare_op.13', '$30binary_and.14']
    $32return_value.15 = cast(value=$30binary_and.14) ['$30binary_and.14', '$32return_value.15']
    return $32return_value.15                ['$32return_value.15']

DEBUG:openai._base_client:Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'json_data': {'messages': [{'role': 'user', 'content': "Summarize the following chapter in one sentence: put it out but so you said you the way you articulated though i really like that you said a cog in the machine they lost a cog in the machine yo did you see my game of thrones breakdown of the of the um the the the leagues in battle right right now how i framed it with you no i gotta seriously because i don't i try not to watch none of this yeah yeah i know you said you was gonna do that too no i was i was that was a little test along with a question because you say you were stepping away from the whole negativity and all of that and yo the fact that is that everybody's saying that is dying right now i think about you so much but anyway uh but what i pointed out was how like it seems like we're starting to feel the after effects of the lack of innovation from that side so you saying that the deal they made with caffeine was like them selling them a machine but a cog was missing in the machine it looked the same from the outside"}], 'model': 'gpt-3.5-turbo-16k', 'max_tokens': 1000, 'n': 1, 'stream': False, 'temperature': 0.7}}
DEBUG:openai._base_client:Sending HTTP Request: POST https://api.openai.com/v1/chat/completions
DEBUG:httpcore.connection:connect_tcp.started host='api.openai.com' port=443 local_address=None timeout=None socket_options=None
DEBUG:httpcore.connection:connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000002391171FC40>
DEBUG:httpcore.connection:start_tls.started ssl_context=<ssl.SSLContext object at 0x000002390E86E940> server_hostname='api.openai.com' timeout=None
DEBUG:httpcore.connection:start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000002391171FC10>
DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>
DEBUG:httpcore.http11:send_request_headers.complete
DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>
DEBUG:httpcore.http11:send_request_body.complete
DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>
DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Mon, 08 Jul 2024 04:38:31 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'openai-organization', b'machinekingsmedia'), (b'openai-processing-ms', b'1027'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains'), (b'x-ratelimit-limit-requests', b'10000'), (b'x-ratelimit-limit-tokens', b'1000000'), (b'x-ratelimit-remaining-requests', b'9999'), (b'x-ratelimit-remaining-tokens', b'998751'), (b'x-ratelimit-reset-requests', b'6ms'), (b'x-ratelimit-reset-tokens', b'74ms'), (b'x-request-id', b'req_96f4ec1fbc6672fb306d9b2223a32cbb'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Set-Cookie', b'__cf_bm=nmDXcB9aoeFzzpa638wpBCzYBu268I._xdlsVU6SIP4-1720413511-1.0.1.1-a1hPqqwHa9nfg6.Fs0tD6nyVpLvqVUQT7oTBnW5BRX6OTuFIDqAugxWEjl9QJew651NH1TIV_O4TaWSUN1RcCg; path=/; expires=Mon, 08-Jul-24 05:08:31 GMT; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), (b'Set-Cookie', b'_cfuvid=QbGU_f7jpIz2hhf.ldKjAG2m4R1Clp8G3cDTbd40Jmg-1720413511605-0.0.1.1-604800000; path=/; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), (b'Server', b'cloudflare'), (b'CF-RAY', b'89fd6293dcb8823f-IAD'), (b'Content-Encoding', b'br'), (b'alt-svc', b'h3=":443"; ma=86400')])
INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>
DEBUG:httpcore.http11:receive_response_body.complete
DEBUG:httpcore.http11:response_closed.started
DEBUG:httpcore.http11:response_closed.complete
DEBUG:openai._base_client:HTTP Response: POST https://api.openai.com/v1/chat/completions "200 OK" Headers([('date', 'Mon, 08 Jul 2024 04:38:31 GMT'), ('content-type', 'application/json'), ('transfer-encoding', 'chunked'), ('connection', 'keep-alive'), ('openai-organization', 'machinekingsmedia'), ('openai-processing-ms', '1027'), ('openai-version', '2020-10-01'), ('strict-transport-security', 'max-age=31536000; includeSubDomains'), ('x-ratelimit-limit-requests', '10000'), ('x-ratelimit-limit-tokens', '1000000'), ('x-ratelimit-remaining-requests', '9999'), ('x-ratelimit-remaining-tokens', '998751'), ('x-ratelimit-reset-requests', '6ms'), ('x-ratelimit-reset-tokens', '74ms'), ('x-request-id', 'req_96f4ec1fbc6672fb306d9b2223a32cbb'), ('cf-cache-status', 'DYNAMIC'), ('set-cookie', '__cf_bm=nmDXcB9aoeFzzpa638wpBCzYBu268I._xdlsVU6SIP4-1720413511-1.0.1.1-a1hPqqwHa9nfg6.Fs0tD6nyVpLvqVUQT7oTBnW5BRX6OTuFIDqAugxWEjl9QJew651NH1TIV_O4TaWSUN1RcCg; path=/; expires=Mon, 08-Jul-24 05:08:31 GMT; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), ('set-cookie', '_cfuvid=QbGU_f7jpIz2hhf.ldKjAG2m4R1Clp8G3cDTbd40Jmg-1720413511605-0.0.1.1-604800000; path=/; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), ('server', 'cloudflare'), ('cf-ray', '89fd6293dcb8823f-IAD'), ('content-encoding', 'br'), ('alt-svc', 'h3=":443"; ma=86400')])
DEBUG:openai._base_client:request_id: req_96f4ec1fbc6672fb306d9b2223a32cbb
DEBUG:openai._base_client:Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'json_data': {'messages': [{'role': 'user', 'content': "Summarize the following chapter in one sentence: but on the inside an important piece was missing they made it basically look like that i had nothing to do with what went down yeah they framed it like that yeah because you were a talent scout yeah apparently yeah they knew they knew that that's not the case and caffeine knows it's not the case you understand and it's between me and you of course as i was going through this for three years they was sitting in on the meetings with me and my lawyers in this so why would they sit if they think it was important if they they're gonna stand behind these niggas and make it look like a notice just trying to scam these niggas yeah when they realize like hold on my nigga this nigga really did all of this shit and y'all think about it if you could if you acquiring a company and its routes and how it became successful why wouldn't you keep the same team wow yeah yeah i see what you're saying i see what you're saying and i can see that fueling"}], 'model': 'gpt-3.5-turbo-16k', 'max_tokens': 1000, 'n': 1, 'stream': False, 'temperature': 0.7}}
DEBUG:openai._base_client:Sending HTTP Request: POST https://api.openai.com/v1/chat/completions
DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>
DEBUG:httpcore.http11:send_request_headers.complete
DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>
DEBUG:httpcore.http11:send_request_body.complete
DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>
DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Mon, 08 Jul 2024 04:38:33 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'openai-organization', b'machinekingsmedia'), (b'openai-processing-ms', b'748'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains'), (b'x-ratelimit-limit-requests', b'10000'), (b'x-ratelimit-limit-tokens', b'1000000'), (b'x-ratelimit-remaining-requests', b'9999'), (b'x-ratelimit-remaining-tokens', b'998750'), (b'x-ratelimit-reset-requests', b'6ms'), (b'x-ratelimit-reset-tokens', b'75ms'), (b'x-request-id', b'req_538933c1904b721465849026001898f6'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'89fd629fbfc2823f-IAD'), (b'Content-Encoding', b'br'), (b'alt-svc', b'h3=":443"; ma=86400')])
INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>
DEBUG:httpcore.http11:receive_response_body.complete
DEBUG:httpcore.http11:response_closed.started
DEBUG:httpcore.http11:response_closed.complete
DEBUG:openai._base_client:HTTP Response: POST https://api.openai.com/v1/chat/completions "200 OK" Headers({'date': 'Mon, 08 Jul 2024 04:38:33 GMT', 'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'openai-organization': 'machinekingsmedia', 'openai-processing-ms': '748', 'openai-version': '2020-10-01', 'strict-transport-security': 'max-age=31536000; includeSubDomains', 'x-ratelimit-limit-requests': '10000', 'x-ratelimit-limit-tokens': '1000000', 'x-ratelimit-remaining-requests': '9999', 'x-ratelimit-remaining-tokens': '998750', 'x-ratelimit-reset-requests': '6ms', 'x-ratelimit-reset-tokens': '75ms', 'x-request-id': 'req_538933c1904b721465849026001898f6', 'cf-cache-status': 'DYNAMIC', 'server': 'cloudflare', 'cf-ray': '89fd629fbfc2823f-IAD', 'content-encoding': 'br', 'alt-svc': 'h3=":443"; ma=86400'})
DEBUG:openai._base_client:request_id: req_538933c1904b721465849026001898f6
DEBUG:openai._base_client:Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'json_data': {'messages': [{'role': 'user', 'content': "Summarize the following chapter in one sentence: anger amongst their ranks yeah and you gotta think about it sound like it was all a money move with them bro they tried to cut me out they owe me money you know what i'm saying yeah they were wrong and bro when they went to that situation they were like yo you know three ways is better than four yeah yeah the way they looked at it i wanted to give me the credit that i deserved you know i'm saying period yeah yeah my mentality was i'm gonna make this stupid motherfucker look like he's the man sacrifice my own shit you feel me yeah and their mentality was like nah we caught this nigga out we didn't pay and like no like it was just it was just stupid on smack's part bro like you know you know what business you go before i got there and you was falling off the cliff why would you revert to what you was because when i got there this is the highest you've ever been yeah they was with me not with them niggas yeah but but"}], 'model': 'gpt-3.5-turbo-16k', 'max_tokens': 1000, 'n': 1, 'stream': False, 'temperature': 0.7}}
DEBUG:openai._base_client:Sending HTTP Request: POST https://api.openai.com/v1/chat/completions
DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>
DEBUG:httpcore.http11:send_request_headers.complete
DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>
DEBUG:httpcore.http11:send_request_body.complete
DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>
DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Mon, 08 Jul 2024 04:38:34 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'openai-organization', b'machinekingsmedia'), (b'openai-processing-ms', b'774'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains'), (b'x-ratelimit-limit-requests', b'10000'), (b'x-ratelimit-limit-tokens', b'1000000'), (b'x-ratelimit-remaining-requests', b'9999'), (b'x-ratelimit-remaining-tokens', b'998755'), (b'x-ratelimit-reset-requests', b'6ms'), (b'x-ratelimit-reset-tokens', b'74ms'), (b'x-request-id', b'req_99d7450ed56a1b31709906cc00ee37f3'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'89fd62a9a85a823f-IAD'), (b'Content-Encoding', b'br'), (b'alt-svc', b'h3=":443"; ma=86400')])
INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>
DEBUG:httpcore.http11:receive_response_body.complete
DEBUG:httpcore.http11:response_closed.started
DEBUG:httpcore.http11:response_closed.complete
DEBUG:openai._base_client:HTTP Response: POST https://api.openai.com/v1/chat/completions "200 OK" Headers({'date': 'Mon, 08 Jul 2024 04:38:34 GMT', 'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'openai-organization': 'machinekingsmedia', 'openai-processing-ms': '774', 'openai-version': '2020-10-01', 'strict-transport-security': 'max-age=31536000; includeSubDomains', 'x-ratelimit-limit-requests': '10000', 'x-ratelimit-limit-tokens': '1000000', 'x-ratelimit-remaining-requests': '9999', 'x-ratelimit-remaining-tokens': '998755', 'x-ratelimit-reset-requests': '6ms', 'x-ratelimit-reset-tokens': '74ms', 'x-request-id': 'req_99d7450ed56a1b31709906cc00ee37f3', 'cf-cache-status': 'DYNAMIC', 'server': 'cloudflare', 'cf-ray': '89fd62a9a85a823f-IAD', 'content-encoding': 'br', 'alt-svc': 'h3=":443"; ma=86400'})
DEBUG:openai._base_client:request_id: req_99d7450ed56a1b31709906cc00ee37f3
DEBUG:openai._base_client:Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'json_data': {'messages': [{'role': 'user', 'content': "Summarize the following chapter in one sentence: just like just like with uh just like with the dallas cowboys um yeah jerry jerry jerry jones thought that you know he could he could run it was the it was the logo the star whatever he could do it without jimmy johnson and um and they haven't been there since i was so happy they ain't advancing the playoff this year too so we can still run with that it is bro it's like when ufc got bored they didn't give it a thing of white in a way it was the cog he was the yeah but he was the face though he was he was the face i remember too when you were wrong i was one of the faces it was me and smack they asked me to fall back because i was out shining him yeah i remember that but smack wasn't really out there talking to the bloggers you used to go on angry fan and big up smack used to be in here i did so much shit that people don't even know that's what i'm saying like it's just it's just if you see them they don't even know what to do"}], 'model': 'gpt-3.5-turbo-16k', 'max_tokens': 1000, 'n': 1, 'stream': False, 'temperature': 0.7}}
DEBUG:openai._base_client:Sending HTTP Request: POST https://api.openai.com/v1/chat/completions
DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>
DEBUG:httpcore.http11:send_request_headers.complete
DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>
DEBUG:httpcore.http11:send_request_body.complete
DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>
DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Mon, 08 Jul 2024 04:38:36 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'openai-organization', b'machinekingsmedia'), (b'openai-processing-ms', b'782'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains'), (b'x-ratelimit-limit-requests', b'10000'), (b'x-ratelimit-limit-tokens', b'1000000'), (b'x-ratelimit-remaining-requests', b'9999'), (b'x-ratelimit-remaining-tokens', b'998751'), (b'x-ratelimit-reset-requests', b'6ms'), (b'x-ratelimit-reset-tokens', b'74ms'), (b'x-request-id', b'req_2cc8073f9cfa1a5c1137953a33d09157'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'89fd62b3d8b5823f-IAD'), (b'Content-Encoding', b'br'), (b'alt-svc', b'h3=":443"; ma=86400')])
INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>
DEBUG:httpcore.http11:receive_response_body.complete
DEBUG:httpcore.http11:response_closed.started
DEBUG:httpcore.http11:response_closed.complete
DEBUG:openai._base_client:HTTP Response: POST https://api.openai.com/v1/chat/completions "200 OK" Headers({'date': 'Mon, 08 Jul 2024 04:38:36 GMT', 'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'openai-organization': 'machinekingsmedia', 'openai-processing-ms': '782', 'openai-version': '2020-10-01', 'strict-transport-security': 'max-age=31536000; includeSubDomains', 'x-ratelimit-limit-requests': '10000', 'x-ratelimit-limit-tokens': '1000000', 'x-ratelimit-remaining-requests': '9999', 'x-ratelimit-remaining-tokens': '998751', 'x-ratelimit-reset-requests': '6ms', 'x-ratelimit-reset-tokens': '74ms', 'x-request-id': 'req_2cc8073f9cfa1a5c1137953a33d09157', 'cf-cache-status': 'DYNAMIC', 'server': 'cloudflare', 'cf-ray': '89fd62b3d8b5823f-IAD', 'content-encoding': 'br', 'alt-svc': 'h3=":443"; ma=86400'})
DEBUG:openai._base_client:request_id: req_2cc8073f9cfa1a5c1137953a33d09157
DEBUG:openai._base_client:Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'json_data': {'messages': [{'role': 'user', 'content': "Summarize the following chapter in one sentence: they don't know what to say they don't even like yo bro like even in business meetings like bro like i have ideas right now that'll that'll make the shit go crazy but i'd never fuck with you know the amount of money you would have to pay me just to resurrect your company yeah but and i've been hearing the adam rumors getting louder this week too about him retiring like as if he made he's done i told you that shit yeah you did tell me that but i didn't see an official announcement but everybody's talking about it no he was never gonna make an announcement he was just like he's gonna do the last event which was uh that's the full circle oh max out back on his last event okay he's not even there oh and he's just gonna do it he was like he's just tired the same shit i said tired of the culture you're putting up money he said he made enough money in the last event where he's just like yo nigga it's not worth the risk why would i put it"}], 'model': 'gpt-3.5-turbo-16k', 'max_tokens': 1000, 'n': 1, 'stream': False, 'temperature': 0.7}}
DEBUG:openai._base_client:Sending HTTP Request: POST https://api.openai.com/v1/chat/completions
DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>
DEBUG:httpcore.http11:send_request_headers.complete
DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>
DEBUG:httpcore.http11:send_request_body.complete
DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>
DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Mon, 08 Jul 2024 04:38:37 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'openai-organization', b'machinekingsmedia'), (b'openai-processing-ms', b'328'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains'), (b'x-ratelimit-limit-requests', b'10000'), (b'x-ratelimit-limit-tokens', b'1000000'), (b'x-ratelimit-remaining-requests', b'9999'), (b'x-ratelimit-remaining-tokens', b'998750'), (b'x-ratelimit-reset-requests', b'6ms'), (b'x-ratelimit-reset-tokens', b'75ms'), (b'x-request-id', b'req_2f4ea08679141e0abae2f6af7ee5b62e'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'89fd62be1926823f-IAD'), (b'Content-Encoding', b'br'), (b'alt-svc', b'h3=":443"; ma=86400')])
INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>
DEBUG:httpcore.http11:receive_response_body.complete
DEBUG:httpcore.http11:response_closed.started
DEBUG:httpcore.http11:response_closed.complete
DEBUG:openai._base_client:HTTP Response: POST https://api.openai.com/v1/chat/completions "200 OK" Headers({'date': 'Mon, 08 Jul 2024 04:38:37 GMT', 'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'openai-organization': 'machinekingsmedia', 'openai-processing-ms': '328', 'openai-version': '2020-10-01', 'strict-transport-security': 'max-age=31536000; includeSubDomains', 'x-ratelimit-limit-requests': '10000', 'x-ratelimit-limit-tokens': '1000000', 'x-ratelimit-remaining-requests': '9999', 'x-ratelimit-remaining-tokens': '998750', 'x-ratelimit-reset-requests': '6ms', 'x-ratelimit-reset-tokens': '75ms', 'x-request-id': 'req_2f4ea08679141e0abae2f6af7ee5b62e', 'cf-cache-status': 'DYNAMIC', 'server': 'cloudflare', 'cf-ray': '89fd62be1926823f-IAD', 'content-encoding': 'br', 'alt-svc': 'h3=":443"; ma=86400'})
DEBUG:openai._base_client:request_id: req_2f4ea08679141e0abae2f6af7ee5b62e
DEBUG:openai._base_client:Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'json_data': {'messages': [{'role': 'user', 'content': "Summarize the following chapter in one sentence: back in bro these i think the usd shop in sugar selling chicken out of his house wait a minute wait up i thought that let's get one wing straight was a store nigga it's in his crib he's selling chicken plates out of his house my nigga no no i saw somebody say congratulations and then i was okay he got a little store or whatever chicken wings out of his crib no i no all right look for the location yeah i'm going to check the location i'm gonna look that up i'm gonna look that up bro yo dog i'm not even bullshitting him he's selling when he don't have a crib he's selling him out of where he's staying out of the crib just selling wings the nigga got the shotgun sauce like it's crazy let's get one wing straight i i was i was like i was i i imagine like like since nunu got the teeth store i thought he was next up hold on my son is an animal dog it's okay it's okay it's all right"}], 'model': 'gpt-3.5-turbo-16k', 'max_tokens': 1000, 'n': 1, 'stream': False, 'temperature': 0.7}}
DEBUG:openai._base_client:Sending HTTP Request: POST https://api.openai.com/v1/chat/completions
DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>
DEBUG:httpcore.http11:send_request_headers.complete
DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>
DEBUG:httpcore.http11:send_request_body.complete
DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>
DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Mon, 08 Jul 2024 04:38:38 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'openai-organization', b'machinekingsmedia'), (b'openai-processing-ms', b'332'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains'), (b'x-ratelimit-limit-requests', b'10000'), (b'x-ratelimit-limit-tokens', b'1000000'), (b'x-ratelimit-remaining-requests', b'9999'), (b'x-ratelimit-remaining-tokens', b'998764'), (b'x-ratelimit-reset-requests', b'6ms'), (b'x-ratelimit-reset-tokens', b'74ms'), (b'x-request-id', b'req_f9039644ae9192d59659703c08298a0c'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'89fd62c56fbb823f-IAD'), (b'Content-Encoding', b'br'), (b'alt-svc', b'h3=":443"; ma=86400')])
INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>
DEBUG:httpcore.http11:receive_response_body.complete
DEBUG:httpcore.http11:response_closed.started
DEBUG:httpcore.http11:response_closed.complete
DEBUG:openai._base_client:HTTP Response: POST https://api.openai.com/v1/chat/completions "200 OK" Headers({'date': 'Mon, 08 Jul 2024 04:38:38 GMT', 'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'openai-organization': 'machinekingsmedia', 'openai-processing-ms': '332', 'openai-version': '2020-10-01', 'strict-transport-security': 'max-age=31536000; includeSubDomains', 'x-ratelimit-limit-requests': '10000', 'x-ratelimit-limit-tokens': '1000000', 'x-ratelimit-remaining-requests': '9999', 'x-ratelimit-remaining-tokens': '998764', 'x-ratelimit-reset-requests': '6ms', 'x-ratelimit-reset-tokens': '74ms', 'x-request-id': 'req_f9039644ae9192d59659703c08298a0c', 'cf-cache-status': 'DYNAMIC', 'server': 'cloudflare', 'cf-ray': '89fd62c56fbb823f-IAD', 'content-encoding': 'br', 'alt-svc': 'h3=":443"; ma=86400'})
DEBUG:openai._base_client:request_id: req_f9039644ae9192d59659703c08298a0c
DEBUG:openai._base_client:Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'json_data': {'messages': [{'role': 'user', 'content': "Summarize the following chapter in one sentence: i'm gonna check up on that noise i might have to do a little something on that all you gotta do is be like hey shotgun where's the location at i'm trying to pull up to buy some wings no no i ain't gonna ask him now i'm gonna just do my little research that's very yo bro i know uh i'm a thousand percent sure niggas call me and i started laughing i said wait what are you talking about he's selling wings he's supposed to be delivering them but he he can't so he'll be having he'll have to come to the square to pick it up wow yeah and he turning down uh the eternal he turned down the easy battle because of loyalty to url as well like he he priced himself out rather than saying no to look bad in the culture bro he didn't price myself out he was told not to do it and he was like oh i'm scared of smack and now look at him bro them niggas are broke right now them niggas they're broke i don't know"}], 'model': 'gpt-3.5-turbo-16k', 'max_tokens': 1000, 'n': 1, 'stream': False, 'temperature': 0.7}}
DEBUG:openai._base_client:Sending HTTP Request: POST https://api.openai.com/v1/chat/completions
DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>
DEBUG:httpcore.http11:send_request_headers.complete
DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>
DEBUG:httpcore.http11:send_request_body.complete
DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>
DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Mon, 08 Jul 2024 04:38:39 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'openai-organization', b'machinekingsmedia'), (b'openai-processing-ms', b'652'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains'), (b'x-ratelimit-limit-requests', b'10000'), (b'x-ratelimit-limit-tokens', b'1000000'), (b'x-ratelimit-remaining-requests', b'9999'), (b'x-ratelimit-remaining-tokens', b'998760'), (b'x-ratelimit-reset-requests', b'6ms'), (b'x-ratelimit-reset-tokens', b'74ms'), (b'x-request-id', b'req_295bc85b648cb1261da87630dbff02ea'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'89fd62cccda2823f-IAD'), (b'Content-Encoding', b'br'), (b'alt-svc', b'h3=":443"; ma=86400')])
INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>
DEBUG:httpcore.http11:receive_response_body.complete
DEBUG:httpcore.http11:response_closed.started
DEBUG:httpcore.http11:response_closed.complete
DEBUG:openai._base_client:HTTP Response: POST https://api.openai.com/v1/chat/completions "200 OK" Headers({'date': 'Mon, 08 Jul 2024 04:38:39 GMT', 'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'openai-organization': 'machinekingsmedia', 'openai-processing-ms': '652', 'openai-version': '2020-10-01', 'strict-transport-security': 'max-age=31536000; includeSubDomains', 'x-ratelimit-limit-requests': '10000', 'x-ratelimit-limit-tokens': '1000000', 'x-ratelimit-remaining-requests': '9999', 'x-ratelimit-remaining-tokens': '998760', 'x-ratelimit-reset-requests': '6ms', 'x-ratelimit-reset-tokens': '74ms', 'x-request-id': 'req_295bc85b648cb1261da87630dbff02ea', 'cf-cache-status': 'DYNAMIC', 'server': 'cloudflare', 'cf-ray': '89fd62cccda2823f-IAD', 'content-encoding': 'br', 'alt-svc': 'h3=":443"; ma=86400'})
DEBUG:openai._base_client:request_id: req_295bc85b648cb1261da87630dbff02ea
DEBUG:openai._base_client:Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'json_data': {'messages': [{'role': 'user', 'content': "Summarize the following chapter in one sentence: how bad i was gonna eat i heard i heard that nigga tate rockets back at his well he's been back at his mama crib but he's in the basement just chilling like this they got me a cake though yeah and sent sending them to the bahamas or something right come on that's how you satisfy that's how you satisfy a certain section of society yeah you know just a bit of your style you could say that's how you that's how you satisfy them man and and also like like we discussed many many times before um the the blind loyalty to the brand is it's crazy and and i think it has a little bit to do with nostalgia um and the work that you put in and the glory days of old not what's going on right now but the glory days of old but but this app situation just seems like the worst possible thing for the culture as a whole and their mentality it's not even the app"}], 'model': 'gpt-3.5-turbo-16k', 'max_tokens': 1000, 'n': 1, 'stream': False, 'temperature': 0.7}}
DEBUG:openai._base_client:Sending HTTP Request: POST https://api.openai.com/v1/chat/completions
DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>
DEBUG:httpcore.http11:send_request_headers.complete
DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>
DEBUG:httpcore.http11:send_request_body.complete
DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>
DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Mon, 08 Jul 2024 04:38:40 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'openai-organization', b'machinekingsmedia'), (b'openai-processing-ms', b'864'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains'), (b'x-ratelimit-limit-requests', b'10000'), (b'x-ratelimit-limit-tokens', b'1000000'), (b'x-ratelimit-remaining-requests', b'9999'), (b'x-ratelimit-remaining-tokens', b'998773'), (b'x-ratelimit-reset-requests', b'6ms'), (b'x-ratelimit-reset-tokens', b'73ms'), (b'x-request-id', b'req_b69607e5f759f6b146f4897387f03e5d'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'89fd62d19965823f-IAD'), (b'Content-Encoding', b'br'), (b'alt-svc', b'h3=":443"; ma=86400')])
INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>
DEBUG:httpcore.http11:receive_response_body.complete
DEBUG:httpcore.http11:response_closed.started
DEBUG:httpcore.http11:response_closed.complete
DEBUG:openai._base_client:HTTP Response: POST https://api.openai.com/v1/chat/completions "200 OK" Headers({'date': 'Mon, 08 Jul 2024 04:38:40 GMT', 'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'openai-organization': 'machinekingsmedia', 'openai-processing-ms': '864', 'openai-version': '2020-10-01', 'strict-transport-security': 'max-age=31536000; includeSubDomains', 'x-ratelimit-limit-requests': '10000', 'x-ratelimit-limit-tokens': '1000000', 'x-ratelimit-remaining-requests': '9999', 'x-ratelimit-remaining-tokens': '998773', 'x-ratelimit-reset-requests': '6ms', 'x-ratelimit-reset-tokens': '73ms', 'x-request-id': 'req_b69607e5f759f6b146f4897387f03e5d', 'cf-cache-status': 'DYNAMIC', 'server': 'cloudflare', 'cf-ray': '89fd62d19965823f-IAD', 'content-encoding': 'br', 'alt-svc': 'h3=":443"; ma=86400'})
DEBUG:openai._base_client:request_id: req_b69607e5f759f6b146f4897387f03e5d
DEBUG:openai._base_client:Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'json_data': {'messages': [{'role': 'user', 'content': "Summarize the following chapter in one sentence: situation no more bro it's the fact that they don't have no backing they're not gonna pay for shit ain't nobody gonna put up no money for the niggas but they they're making their own money but they're spending it on jewelry and cars and things of that nature that um if they don't have an office that might be a reflection of them not reinvested into the business itself um you don't even need all that son you don't even need to over half of this shit these niggas are so fucking dumb like yo bro it's the ad situation they they they probably gonna try to sell off what they can if they can at this point so but what can they sell off the the the the battles somebody used the library worth it you know they already have but i just don't see them cashing out in that situation but what's hold on let me let me get my let me get these niggas no problem all right"}], 'model': 'gpt-3.5-turbo-16k', 'max_tokens': 1000, 'n': 1, 'stream': False, 'temperature': 0.7}}
DEBUG:openai._base_client:Sending HTTP Request: POST https://api.openai.com/v1/chat/completions
DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>
DEBUG:httpcore.http11:send_request_headers.complete
DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>
DEBUG:httpcore.http11:send_request_body.complete
DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>
DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Mon, 08 Jul 2024 04:38:42 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'openai-organization', b'machinekingsmedia'), (b'openai-processing-ms', b'839'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains'), (b'x-ratelimit-limit-requests', b'10000'), (b'x-ratelimit-limit-tokens', b'1000000'), (b'x-ratelimit-remaining-requests', b'9999'), (b'x-ratelimit-remaining-tokens', b'998771'), (b'x-ratelimit-reset-requests', b'6ms'), (b'x-ratelimit-reset-tokens', b'73ms'), (b'x-request-id', b'req_776bf0901ec537a4fb9381b976429df3'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'89fd62d89ed7823f-IAD'), (b'Content-Encoding', b'br'), (b'alt-svc', b'h3=":443"; ma=86400')])
INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>
DEBUG:httpcore.http11:receive_response_body.complete
DEBUG:httpcore.http11:response_closed.started
DEBUG:httpcore.http11:response_closed.complete
DEBUG:openai._base_client:HTTP Response: POST https://api.openai.com/v1/chat/completions "200 OK" Headers({'date': 'Mon, 08 Jul 2024 04:38:42 GMT', 'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'openai-organization': 'machinekingsmedia', 'openai-processing-ms': '839', 'openai-version': '2020-10-01', 'strict-transport-security': 'max-age=31536000; includeSubDomains', 'x-ratelimit-limit-requests': '10000', 'x-ratelimit-limit-tokens': '1000000', 'x-ratelimit-remaining-requests': '9999', 'x-ratelimit-remaining-tokens': '998771', 'x-ratelimit-reset-requests': '6ms', 'x-ratelimit-reset-tokens': '73ms', 'x-request-id': 'req_776bf0901ec537a4fb9381b976429df3', 'cf-cache-status': 'DYNAMIC', 'server': 'cloudflare', 'cf-ray': '89fd62d89ed7823f-IAD', 'content-encoding': 'br', 'alt-svc': 'h3=":443"; ma=86400'})
DEBUG:openai._base_client:request_id: req_776bf0901ec537a4fb9381b976429df3
DEBUG:openai._base_client:Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'json_data': {'messages': [{'role': 'user', 'content': "Summarize the following chapter in one sentence: and like if i'm if i'm a business dude and i purchased a company and you sold me everything but what made the engine run i'm gonna feel a way yeah when you're telling them like yo this shit got no legs he bullshitting you don't think they did their own research they was just like yo we don't want to get sued and miss this shit because then what happens is you look like y'all was in it with them to kick me out yeah so they was just trying to protect their own ass but they did their own research nigga they knew that i wasn't just a scout too many people saying the same shit like come on bro let me just minimize it bro i've seen their faces i i've seen with smack down when we did the depositions i've seen all that shit on those niggas is a dummy but you know you know so you can just tell like like there's one part like i can't really talk about the deposition shit because it's not whatever but i just recall"}], 'model': 'gpt-3.5-turbo-16k', 'max_tokens': 1000, 'n': 1, 'stream': False, 'temperature': 0.7}}
DEBUG:openai._base_client:Sending HTTP Request: POST https://api.openai.com/v1/chat/completions
DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>
DEBUG:httpcore.http11:send_request_headers.complete
DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>
DEBUG:httpcore.http11:send_request_body.complete
DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>
DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Mon, 08 Jul 2024 04:38:44 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'openai-organization', b'machinekingsmedia'), (b'openai-processing-ms', b'778'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains'), (b'x-ratelimit-limit-requests', b'10000'), (b'x-ratelimit-limit-tokens', b'1000000'), (b'x-ratelimit-remaining-requests', b'9999'), (b'x-ratelimit-remaining-tokens', b'998756'), (b'x-ratelimit-reset-requests', b'6ms'), (b'x-ratelimit-reset-tokens', b'74ms'), (b'x-request-id', b'req_a89cf5a8549e6b675255cb2f01623b99'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'89fd62e31f45823f-IAD'), (b'Content-Encoding', b'br'), (b'alt-svc', b'h3=":443"; ma=86400')])
INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>
DEBUG:httpcore.http11:receive_response_body.complete
DEBUG:httpcore.http11:response_closed.started
DEBUG:httpcore.http11:response_closed.complete
DEBUG:openai._base_client:HTTP Response: POST https://api.openai.com/v1/chat/completions "200 OK" Headers({'date': 'Mon, 08 Jul 2024 04:38:44 GMT', 'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'openai-organization': 'machinekingsmedia', 'openai-processing-ms': '778', 'openai-version': '2020-10-01', 'strict-transport-security': 'max-age=31536000; includeSubDomains', 'x-ratelimit-limit-requests': '10000', 'x-ratelimit-limit-tokens': '1000000', 'x-ratelimit-remaining-requests': '9999', 'x-ratelimit-remaining-tokens': '998756', 'x-ratelimit-reset-requests': '6ms', 'x-ratelimit-reset-tokens': '74ms', 'x-request-id': 'req_a89cf5a8549e6b675255cb2f01623b99', 'cf-cache-status': 'DYNAMIC', 'server': 'cloudflare', 'cf-ray': '89fd62e31f45823f-IAD', 'content-encoding': 'br', 'alt-svc': 'h3=":443"; ma=86400'})
DEBUG:openai._base_client:request_id: req_a89cf5a8549e6b675255cb2f01623b99
DEBUG:openai._base_client:Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'json_data': {'messages': [{'role': 'user', 'content': "Summarize the following chapter in one sentence: yeah don't don't speak on it if you can't speak on it now because i am recording yeah i'm recording now i was gonna say like i could just see it bro i could see it in his face specifically like i know troy and i know what moves troy i know what motivates him and it's it's all stupid shit like you probably say nigga shit you're in your mid-40s my nigga you worried about a chain and a fucking 300 shirt for 500 whatever the fuck you pay for those shirts yeah yeah you can see the way that they dress like the way they dress as of late like they are the accessorized chains yeah you can kind of see that yeah you don't see fucking jay-z doing that like and then there's something about money that's different like when you got money you don't act like you got money you know what i'm saying yeah like it's one of those situations where you kind of like"}], 'model': 'gpt-3.5-turbo-16k', 'max_tokens': 1000, 'n': 1, 'stream': False, 'temperature': 0.7}}
DEBUG:openai._base_client:Sending HTTP Request: POST https://api.openai.com/v1/chat/completions
DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>
DEBUG:httpcore.http11:send_request_headers.complete
DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>
DEBUG:httpcore.http11:send_request_body.complete
DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>
DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Mon, 08 Jul 2024 04:38:45 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'openai-organization', b'machinekingsmedia'), (b'openai-processing-ms', b'284'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains'), (b'x-ratelimit-limit-requests', b'10000'), (b'x-ratelimit-limit-tokens', b'1000000'), (b'x-ratelimit-remaining-requests', b'9999'), (b'x-ratelimit-remaining-tokens', b'998773'), (b'x-ratelimit-reset-requests', b'6ms'), (b'x-ratelimit-reset-tokens', b'73ms'), (b'x-request-id', b'req_40dfb84141045c23c9f420ecdaa27810'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'89fd62ed4807823f-IAD'), (b'Content-Encoding', b'br'), (b'alt-svc', b'h3=":443"; ma=86400')])
INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>
DEBUG:httpcore.http11:receive_response_body.complete
DEBUG:httpcore.http11:response_closed.started
DEBUG:httpcore.http11:response_closed.complete
DEBUG:openai._base_client:HTTP Response: POST https://api.openai.com/v1/chat/completions "200 OK" Headers({'date': 'Mon, 08 Jul 2024 04:38:45 GMT', 'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'openai-organization': 'machinekingsmedia', 'openai-processing-ms': '284', 'openai-version': '2020-10-01', 'strict-transport-security': 'max-age=31536000; includeSubDomains', 'x-ratelimit-limit-requests': '10000', 'x-ratelimit-limit-tokens': '1000000', 'x-ratelimit-remaining-requests': '9999', 'x-ratelimit-remaining-tokens': '998773', 'x-ratelimit-reset-requests': '6ms', 'x-ratelimit-reset-tokens': '73ms', 'x-request-id': 'req_40dfb84141045c23c9f420ecdaa27810', 'cf-cache-status': 'DYNAMIC', 'server': 'cloudflare', 'cf-ray': '89fd62ed4807823f-IAD', 'content-encoding': 'br', 'alt-svc': 'h3=":443"; ma=86400'})
DEBUG:openai._base_client:request_id: req_40dfb84141045c23c9f420ecdaa27810
DEBUG:openai._base_client:Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'json_data': {'messages': [{'role': 'user', 'content': "Summarize the following chapter in one sentence: they were niggas with real money and then you see a nigga like smack Logan they laugh at them because they don't even know how to like you got to show off you got but you don't show this shit off you don't how you live but but they they are a hip-hop culture though so there is that but their business as business owners that that work with these independent contractors it just seems as though from my observation of that the subscription money has got they've gotten so much reliable subscription money over the years that they kind of got comfortable and they just haven't they it seemed like they've even given up trying to innovate no i mean bro you understand bro like it's like who said who's not the best when you win a championship you gotta understand and that you gotta continue what you're doing in fact you gotta do more because not as expectation their only expectation was to to fuck bitches and wear"}], 'model': 'gpt-3.5-turbo-16k', 'max_tokens': 1000, 'n': 1, 'stream': False, 'temperature': 0.7}}
DEBUG:openai._base_client:Sending HTTP Request: POST https://api.openai.com/v1/chat/completions
DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>
DEBUG:httpcore.http11:send_request_headers.complete
DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>
DEBUG:httpcore.http11:send_request_body.complete
DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>
DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Mon, 08 Jul 2024 04:38:46 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'openai-organization', b'machinekingsmedia'), (b'openai-processing-ms', b'923'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains'), (b'x-ratelimit-limit-requests', b'10000'), (b'x-ratelimit-limit-tokens', b'1000000'), (b'x-ratelimit-remaining-requests', b'9999'), (b'x-ratelimit-remaining-tokens', b'998757'), (b'x-ratelimit-reset-requests', b'6ms'), (b'x-ratelimit-reset-tokens', b'74ms'), (b'x-request-id', b'req_55feb248856c1c1c5ad5de5a4b5f7328'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'89fd62f48e1e823f-IAD'), (b'Content-Encoding', b'br'), (b'alt-svc', b'h3=":443"; ma=86400')])
INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>
DEBUG:httpcore.http11:receive_response_body.complete
DEBUG:httpcore.http11:response_closed.started
DEBUG:httpcore.http11:response_closed.complete
DEBUG:openai._base_client:HTTP Response: POST https://api.openai.com/v1/chat/completions "200 OK" Headers({'date': 'Mon, 08 Jul 2024 04:38:46 GMT', 'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'openai-organization': 'machinekingsmedia', 'openai-processing-ms': '923', 'openai-version': '2020-10-01', 'strict-transport-security': 'max-age=31536000; includeSubDomains', 'x-ratelimit-limit-requests': '10000', 'x-ratelimit-limit-tokens': '1000000', 'x-ratelimit-remaining-requests': '9999', 'x-ratelimit-remaining-tokens': '998757', 'x-ratelimit-reset-requests': '6ms', 'x-ratelimit-reset-tokens': '74ms', 'x-request-id': 'req_55feb248856c1c1c5ad5de5a4b5f7328', 'cf-cache-status': 'DYNAMIC', 'server': 'cloudflare', 'cf-ray': '89fd62f48e1e823f-IAD', 'content-encoding': 'br', 'alt-svc': 'h3=":443"; ma=86400'})
DEBUG:openai._base_client:request_id: req_55feb248856c1c1c5ad5de5a4b5f7328
DEBUG:openai._base_client:Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'json_data': {'messages': [{'role': 'user', 'content': "Summarize the following chapter in one sentence: jewelry that's they should yo i'm gonna use that that championship shit for that that um that cowboys that cowboys video that i'm gonna make yeah that's a fire analogy i'm gonna use that analogy for them winning a championship with you pretty much and etching themselves in history with you norbs etching themselves in history but then the expectations are there now and now that without you because yo yo i paid i called you tyrian lannister like you you fuck hold on you watch game of thrones i started through i never finished oh okay okay but you still should watch that shit though norbs because that shit that that's that that's a good ass blog that's a good ass blog and i'll watch i'll hit you back i'm gonna get this nigga started i'll watch it right now all right i'm gonna send it uh maybe like nothing i make is over 12 minutes so it's probably like eight or nine minutes maybe i'll send it to you okay i gotta put this nigga he follow me"}], 'model': 'gpt-3.5-turbo-16k', 'max_tokens': 1000, 'n': 1, 'stream': False, 'temperature': 0.7}}
DEBUG:openai._base_client:Sending HTTP Request: POST https://api.openai.com/v1/chat/completions
DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>
DEBUG:httpcore.http11:send_request_headers.complete
DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>
DEBUG:httpcore.http11:send_request_body.complete
DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>
DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Mon, 08 Jul 2024 04:38:48 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'openai-organization', b'machinekingsmedia'), (b'openai-processing-ms', b'840'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains'), (b'x-ratelimit-limit-requests', b'10000'), (b'x-ratelimit-limit-tokens', b'1000000'), (b'x-ratelimit-remaining-requests', b'9999'), (b'x-ratelimit-remaining-tokens', b'998748'), (b'x-ratelimit-reset-requests', b'6ms'), (b'x-ratelimit-reset-tokens', b'75ms'), (b'x-request-id', b'req_2883a233e02107d77030ac355964a07b'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'89fd62fbcc0d823f-IAD'), (b'Content-Encoding', b'br'), (b'alt-svc', b'h3=":443"; ma=86400')])
INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>
DEBUG:httpcore.http11:receive_response_body.complete
DEBUG:httpcore.http11:response_closed.started
DEBUG:httpcore.http11:response_closed.complete
DEBUG:openai._base_client:HTTP Response: POST https://api.openai.com/v1/chat/completions "200 OK" Headers({'date': 'Mon, 08 Jul 2024 04:38:48 GMT', 'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'openai-organization': 'machinekingsmedia', 'openai-processing-ms': '840', 'openai-version': '2020-10-01', 'strict-transport-security': 'max-age=31536000; includeSubDomains', 'x-ratelimit-limit-requests': '10000', 'x-ratelimit-limit-tokens': '1000000', 'x-ratelimit-remaining-requests': '9999', 'x-ratelimit-remaining-tokens': '998748', 'x-ratelimit-reset-requests': '6ms', 'x-ratelimit-reset-tokens': '75ms', 'x-request-id': 'req_2883a233e02107d77030ac355964a07b', 'cf-cache-status': 'DYNAMIC', 'server': 'cloudflare', 'cf-ray': '89fd62fbcc0d823f-IAD', 'content-encoding': 'br', 'alt-svc': 'h3=":443"; ma=86400'})
DEBUG:openai._base_client:request_id: req_2883a233e02107d77030ac355964a07b
DEBUG:openai._base_client:Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'json_data': {'messages': [{'role': 'user', 'content': "Summarize the following chapter in one sentence: around all right my my baby girl i'm gonna be putting her to bed at any moment so um if not then i'll hit you later on tonight all right"}], 'model': 'gpt-3.5-turbo-16k', 'max_tokens': 1000, 'n': 1, 'stream': False, 'temperature': 0.7}}
DEBUG:openai._base_client:Sending HTTP Request: POST https://api.openai.com/v1/chat/completions
DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>
DEBUG:httpcore.http11:send_request_headers.complete
DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>
DEBUG:httpcore.http11:send_request_body.complete
DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>
DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Mon, 08 Jul 2024 04:38:49 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'openai-organization', b'machinekingsmedia'), (b'openai-processing-ms', b'276'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains'), (b'x-ratelimit-limit-requests', b'10000'), (b'x-ratelimit-limit-tokens', b'1000000'), (b'x-ratelimit-remaining-requests', b'9999'), (b'x-ratelimit-remaining-tokens', b'998952'), (b'x-ratelimit-reset-requests', b'6ms'), (b'x-ratelimit-reset-tokens', b'62ms'), (b'x-request-id', b'req_7850b0a033ef509d6fb01813c30a0b70'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'89fd63066dae823f-IAD'), (b'Content-Encoding', b'br'), (b'alt-svc', b'h3=":443"; ma=86400')])
INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>
DEBUG:httpcore.http11:receive_response_body.complete
DEBUG:httpcore.http11:response_closed.started
DEBUG:httpcore.http11:response_closed.complete
DEBUG:openai._base_client:HTTP Response: POST https://api.openai.com/v1/chat/completions "200 OK" Headers({'date': 'Mon, 08 Jul 2024 04:38:49 GMT', 'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'openai-organization': 'machinekingsmedia', 'openai-processing-ms': '276', 'openai-version': '2020-10-01', 'strict-transport-security': 'max-age=31536000; includeSubDomains', 'x-ratelimit-limit-requests': '10000', 'x-ratelimit-limit-tokens': '1000000', 'x-ratelimit-remaining-requests': '9999', 'x-ratelimit-remaining-tokens': '998952', 'x-ratelimit-reset-requests': '6ms', 'x-ratelimit-reset-tokens': '62ms', 'x-request-id': 'req_7850b0a033ef509d6fb01813c30a0b70', 'cf-cache-status': 'DYNAMIC', 'server': 'cloudflare', 'cf-ray': '89fd63066dae823f-IAD', 'content-encoding': 'br', 'alt-svc': 'h3=":443"; ma=86400'})
DEBUG:openai._base_client:request_id: req_7850b0a033ef509d6fb01813c30a0b70
DEBUG:openai._base_client:Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'json_data': {'messages': [{'role': 'user', 'content': "Provide a brief summary of the following transcript: File: 240204_1940.mp3\nRecording Date: 2024-02-04\nDuration: 00:13:13.60\nTranscription Date: 2024-07-07 06:17:21\n\n1\n00:00:00,000 --> 00:00:04,800\nput it out but so you said you the way you articulated though i really like that you said\n\n2\n00:00:04,800 --> 00:00:09,120\na cog in the machine they lost a cog in the machine yo did you see my game of thrones\n\n3\n00:00:09,120 --> 00:00:15,600\nbreakdown of the of the um the the the leagues in battle right right now how i framed it with you\n\n4\n00:00:16,719 --> 00:00:20,639\nno i gotta seriously because i don't i try not to watch none of this yeah yeah i know you said\n\n5\n00:00:20,639 --> 00:00:24,879\nyou was gonna do that too no i was i was that was a little test along with a question because\n\n6\n00:00:24,879 --> 00:00:30,959\nyou say you were stepping away from the whole negativity and all of that and yo the fact that\n\n7\n00:00:30,959 --> 00:00:37,840\nis that everybody's saying that is dying right now i think about you so much but anyway uh but\n\n8\n00:00:37,840 --> 00:00:44,880\nwhat i pointed out was how like it seems like we're starting to feel the after effects of the\n\n9\n00:00:44,880 --> 00:00:51,919\nlack of innovation from that side so you saying that the deal they made with caffeine was like\n\n10\n00:00:51,919 --> 00:00:56,959\nthem selling them a machine but a cog was missing in the machine it looked the same from the outside\n\n11\n00:00:56,959 --> 00:01:02,000\nbut on the inside an important piece was missing they made it basically look like that i had\n\n12\n00:01:02,000 --> 00:01:07,599\nnothing to do with what went down yeah they framed it like that yeah because you were a talent scout\n\n13\n00:01:08,320 --> 00:01:16,800\nyeah apparently yeah they knew they knew that that's not the case and caffeine knows it's not\n\n14\n00:01:16,800 --> 00:01:22,959\nthe case you understand and it's between me and you of course as i was going through this for three\n\n15\n00:01:22,959 --> 00:01:28,720\nyears they was sitting in on the meetings with me and my lawyers in this so why would they sit\n\n16\n00:01:28,720 --> 00:01:32,320\nif they think it was important if they they're gonna stand behind these niggas and make it look\n\n17\n00:01:32,320 --> 00:01:37,040\nlike a notice just trying to scam these niggas yeah when they realize like hold on my nigga\n\n18\n00:01:38,320 --> 00:01:43,680\nthis nigga really did all of this shit and y'all think about it if you could if you acquiring a\n\n19\n00:01:43,680 --> 00:01:50,000\ncompany and its routes and how it became successful why wouldn't you keep the same team\n\n20\n00:01:51,919 --> 00:01:57,599\nwow yeah yeah i see what you're saying i see what you're saying and i can see that fueling\n\n21\n00:01:57,599 --> 00:02:03,839\nanger amongst their ranks yeah and you gotta think about it sound like it was all a money\n\n22\n00:02:03,839 --> 00:02:07,680\nmove with them bro they tried to cut me out they owe me money you know what i'm saying yeah\n\n23\n00:02:07,680 --> 00:02:14,559\nthey were wrong and bro when they went to that situation they were like yo you know three ways\n\n24\n00:02:14,559 --> 00:02:20,559\nis better than four yeah yeah the way they looked at it i wanted to give me the credit that i\n\n25\n00:02:20,559 --> 00:02:26,720\ndeserved you know i'm saying period yeah yeah my mentality was i'm gonna make this stupid\n\n26\n00:02:26,720 --> 00:02:33,839\nmotherfucker look like he's the man sacrifice my own shit you feel me yeah and their mentality\n\n27\n00:02:33,839 --> 00:02:38,880\nwas like nah we caught this nigga out we didn't pay and like no like it was just it was just\n\n28\n00:02:38,880 --> 00:02:44,399\nstupid on smack's part bro like you know you know what business you go before i got there\n\n29\n00:02:44,399 --> 00:02:50,639\nand you was falling off the cliff why would you revert to what you was because when i got there\n\n30\n00:02:50,639 --> 00:02:58,559\nthis is the highest you've ever been yeah they was with me not with them niggas yeah but but\n\n31\n00:02:58,559 --> 00:03:07,039\njust like just like with uh just like with the dallas cowboys um yeah jerry jerry jerry jones\n\n32\n00:03:07,039 --> 00:03:11,520\nthought that you know he could he could run it was the it was the logo the star whatever he could\n\n33\n00:03:11,520 --> 00:03:16,399\ndo it without jimmy johnson and um and they haven't been there since i was so happy they\n\n34\n00:03:16,399 --> 00:03:23,199\nain't advancing the playoff this year too so we can still run with that it is bro it's like\n\n35\n00:03:23,839 --> 00:03:28,639\nwhen ufc got bored they didn't give it a thing of white in a way it was the cog he was the\n\n36\n00:03:28,639 --> 00:03:34,800\nyeah but he was the face though he was he was the face i remember too when you were wrong i was one\n\n37\n00:03:34,800 --> 00:03:39,600\nof the faces it was me and smack they asked me to fall back because i was out shining him yeah i\n\n38\n00:03:39,600 --> 00:03:45,199\nremember that but smack wasn't really out there talking to the bloggers you used to go on angry\n\n39\n00:03:45,199 --> 00:03:52,080\nfan and big up smack used to be in here i did so much shit that people don't even know that's\n\n40\n00:03:52,080 --> 00:03:57,279\nwhat i'm saying like it's just it's just if you see them they don't even know what to do\n\n41\n00:03:58,160 --> 00:04:02,399\nthey don't know what to say they don't even like yo bro like even in business meetings\n\n42\n00:04:03,039 --> 00:04:08,479\nlike bro like i have ideas right now that'll that'll make the shit go crazy but i'd never\n\n43\n00:04:08,479 --> 00:04:12,559\nfuck with you know the amount of money you would have to pay me just to resurrect your company\n\n44\n00:04:13,759 --> 00:04:20,239\nyeah but and i've been hearing the adam rumors getting louder this week too about him retiring\n\n45\n00:04:20,239 --> 00:04:26,079\nlike as if he made he's done i told you that shit yeah you did tell me that but i didn't see an\n\n46\n00:04:26,079 --> 00:04:30,799\nofficial announcement but everybody's talking about it no he was never gonna make an announcement\n\n47\n00:04:30,799 --> 00:04:35,839\nhe was just like he's gonna do the last event which was uh that's the full circle oh max out\n\n48\n00:04:35,839 --> 00:04:41,839\nback on his last event okay he's not even there oh and he's just gonna do it he was like he's just\n\n49\n00:04:41,839 --> 00:04:46,239\ntired the same shit i said tired of the culture you're putting up money he said he made enough\n\n50\n00:04:46,239 --> 00:04:50,239\nmoney in the last event where he's just like yo nigga it's not worth the risk why would i put it\n\n51\n00:04:50,239 --> 00:04:57,359\nback in bro these i think the usd shop in sugar selling chicken out of his house wait a minute\n\n52\n00:04:57,359 --> 00:05:04,480\nwait up i thought that let's get one wing straight was a store nigga it's in his crib he's selling\n\n53\n00:05:04,480 --> 00:05:12,640\nchicken plates out of his house my nigga no no i saw somebody say congratulations and then i was\n\n54\n00:05:12,640 --> 00:05:21,200\nokay he got a little store or whatever chicken wings out of his crib no i no all right look for\n\n55\n00:05:21,200 --> 00:05:27,519\nthe location yeah i'm going to check the location i'm gonna look that up i'm gonna look that up bro\n\n56\n00:05:28,079 --> 00:05:32,640\nyo dog i'm not even bullshitting him he's selling when he don't have a crib he's selling him out of\n\n57\n00:05:32,640 --> 00:05:36,320\nwhere he's staying out of the crib just selling wings the nigga got the shotgun sauce\n\n58\n00:05:36,480 --> 00:05:44,079\nlike it's crazy let's get one wing straight i i was i was like i was i i imagine like\n\n59\n00:05:44,079 --> 00:05:50,239\nlike since nunu got the teeth store i thought he was next up hold on my son is an animal dog\n\n60\n00:05:52,799 --> 00:05:55,920\nit's okay it's okay it's all right\n\n61\n00:05:57,760 --> 00:06:01,440\ni'm gonna check up on that noise i might have to do a little something on that\n\n62\n00:06:01,519 --> 00:06:06,640\nall you gotta do is be like hey shotgun where's the location at i'm trying to pull up to buy some\n\n63\n00:06:06,640 --> 00:06:11,200\nwings no no i ain't gonna ask him now i'm gonna just do my little research that's very\n\n64\n00:06:11,839 --> 00:06:16,160\nyo bro i know uh i'm a thousand percent sure niggas call me and i started laughing i said\n\n65\n00:06:16,160 --> 00:06:21,839\nwait what are you talking about he's selling wings he's supposed to be delivering them\n\n66\n00:06:21,839 --> 00:06:25,359\nbut he he can't so he'll be having he'll have to come to the square to pick it up\n\n67\n00:06:26,320 --> 00:06:34,320\nwow yeah and he turning down uh the eternal he turned down the easy battle because of loyalty\n\n68\n00:06:34,320 --> 00:06:40,079\nto url as well like he he priced himself out rather than saying no to look bad in the culture\n\n69\n00:06:40,959 --> 00:06:46,239\nbro he didn't price myself out he was told not to do it and he was like oh i'm scared of smack\n\n70\n00:06:47,200 --> 00:06:52,720\nand now look at him bro them niggas are broke right now them niggas they're broke i don't know\n\n71\n00:06:52,720 --> 00:06:57,279\nhow bad i was gonna eat i heard i heard that nigga tate rockets back at his well he's been\n\n72\n00:06:57,279 --> 00:07:02,959\nback at his mama crib but he's in the basement just chilling like this they got me a cake though\n\n73\n00:07:03,839 --> 00:07:06,239\nyeah and sent sending them to the bahamas or something right\n\n74\n00:07:07,679 --> 00:07:15,679\ncome on that's how you satisfy that's how you satisfy a certain section of society\n\n75\n00:07:16,640 --> 00:07:18,480\nyeah you know just a bit of your style you could say\n\n76\n00:07:21,760 --> 00:07:27,519\nthat's how you that's how you satisfy them man and and also like like we discussed many many\n\n77\n00:07:27,519 --> 00:07:36,079\ntimes before um the the blind loyalty to the brand is it's crazy and and i think it has a\n\n78\n00:07:36,079 --> 00:07:43,359\nlittle bit to do with nostalgia um and the work that you put in and the glory days of old not\n\n79\n00:07:43,359 --> 00:07:50,640\nwhat's going on right now but the glory days of old but but this app situation just seems like\n\n80\n00:07:50,640 --> 00:07:58,799\nthe worst possible thing for the culture as a whole and their mentality it's not even the app\n\n81\n00:07:58,799 --> 00:08:03,279\nsituation no more bro it's the fact that they don't have no backing they're not gonna pay for\n\n82\n00:08:03,279 --> 00:08:09,679\nshit ain't nobody gonna put up no money for the niggas but they they're making their own money\n\n83\n00:08:09,679 --> 00:08:15,040\nbut they're spending it on jewelry and cars and things of that nature that um if they don't have\n\n84\n00:08:15,040 --> 00:08:22,160\nan office that might be a reflection of them not reinvested into the business itself um you don't\n\n85\n00:08:22,160 --> 00:08:26,959\neven need all that son you don't even need to over half of this shit these niggas are so fucking dumb\n\n86\n00:08:27,920 --> 00:08:34,479\nlike yo bro it's the ad situation they they they probably gonna try to sell off what they can\n\n87\n00:08:34,479 --> 00:08:43,440\nif they can at this point so but what can they sell off the the the the battles\n\n88\n00:08:44,320 --> 00:08:49,119\nsomebody used the library worth it you know they already have but i just don't see them\n\n89\n00:08:49,119 --> 00:08:57,119\ncashing out in that situation but what's hold on let me let me get my let me get these niggas\n\n90\n00:08:57,919 --> 00:08:58,960\nno problem all right\n\n91\n00:09:06,239 --> 00:09:09,760\nand like if i'm if i'm a business dude and i purchased a company\n\n92\n00:09:10,559 --> 00:09:17,440\nand you sold me everything but what made the engine run i'm gonna feel a way yeah\n\n93\n00:09:18,239 --> 00:09:22,159\nwhen you're telling them like yo this shit got no legs he bullshitting you don't think they did\n\n94\n00:09:22,159 --> 00:09:29,520\ntheir own research they was just like yo we don't want to get sued and miss this shit because then\n\n95\n00:09:29,520 --> 00:09:37,440\nwhat happens is you look like y'all was in it with them to kick me out yeah so they was just\n\n96\n00:09:37,440 --> 00:09:42,159\ntrying to protect their own ass but they did their own research nigga they knew that i wasn't just\n\n97\n00:09:43,280 --> 00:09:51,280\na scout too many people saying the same shit like come on bro let me just minimize it bro\n\n98\n00:09:51,280 --> 00:09:55,760\ni've seen their faces i i've seen with smack down when we did the depositions i've seen all that\n\n99\n00:09:55,760 --> 00:10:04,320\nshit on those niggas is a dummy but you know you know so you can just tell like like there's one\n\n100\n00:10:04,320 --> 00:10:09,760\npart like i can't really talk about the deposition shit because it's not whatever but i just recall\n\n101\n00:10:09,760 --> 00:10:16,640\nyeah don't don't speak on it if you can't speak on it now because i am recording yeah i'm recording\n\n102\n00:10:17,280 --> 00:10:22,400\nnow i was gonna say like i could just see it bro i could see it in his face specifically\n\n103\n00:10:22,400 --> 00:10:28,400\nlike i know troy and i know what moves troy i know what motivates him and it's it's all\n\n104\n00:10:29,200 --> 00:10:31,440\nstupid shit like you probably say nigga shit\n\n105\n00:10:33,440 --> 00:10:36,320\nyou're in your mid-40s my nigga you worried about a chain and a fucking\n\n106\n00:10:36,880 --> 00:10:41,760\n300 shirt for 500 whatever the fuck you pay for those shirts yeah yeah you can see the way that\n\n107\n00:10:42,479 --> 00:10:48,320\nthey dress like the way they dress as of late like they are the accessorized chains yeah you\n\n108\n00:10:48,320 --> 00:10:54,000\ncan kind of see that yeah you don't see fucking jay-z doing that like and then there's something\n\n109\n00:10:54,000 --> 00:11:00,080\nabout money that's different like when you got money you don't act like you got money\n\n110\n00:11:00,640 --> 00:11:04,880\nyou know what i'm saying yeah like it's one of those situations where you kind of like\n\n111\n00:11:06,000 --> 00:11:10,640\nthey were niggas with real money and then you see a nigga like smack Logan they laugh at them because\n\n112\n00:11:11,280 --> 00:11:14,880\nthey don't even know how to like you got to show off you got but you don't show this shit off\n\n113\n00:11:16,080 --> 00:11:22,559\nyou don't how you live but but they they are a hip-hop culture though so there is that but\n\n114\n00:11:22,559 --> 00:11:27,440\ntheir business as business owners that that work with these independent contractors\n\n115\n00:11:28,400 --> 00:11:34,559\nit just seems as though from my observation of that the subscription money has got they've gotten\n\n116\n00:11:34,559 --> 00:11:40,719\nso much reliable subscription money over the years that they kind of got comfortable\n\n117\n00:11:41,599 --> 00:11:46,080\nand they just haven't they it seemed like they've even given up trying to innovate\n\n118\n00:11:47,039 --> 00:11:52,400\nno i mean bro you understand bro like it's like who said who's not the best when you win a\n\n119\n00:11:52,400 --> 00:11:59,679\nchampionship you gotta understand and that you gotta continue what you're doing in fact you\n\n120\n00:11:59,679 --> 00:12:05,520\ngotta do more because not as expectation their only expectation was to to fuck bitches and wear\n\n121\n00:12:05,520 --> 00:12:10,719\njewelry that's they should yo i'm gonna use that that championship shit for that that um that\n\n122\n00:12:10,719 --> 00:12:18,880\ncowboys that cowboys video that i'm gonna make yeah that's a fire analogy i'm gonna use that\n\n123\n00:12:18,880 --> 00:12:24,640\nanalogy for them winning a championship with you pretty much and etching themselves in history\n\n124\n00:12:25,359 --> 00:12:32,000\nwith you norbs etching themselves in history but then the expectations are there now and now that\n\n125\n00:12:32,000 --> 00:12:38,559\nwithout you because yo yo i paid i called you tyrian lannister like you you fuck hold on you\n\n126\n00:12:38,559 --> 00:12:43,919\nwatch game of thrones i started through i never finished oh okay okay but you still should watch\n\n127\n00:12:43,919 --> 00:12:49,119\nthat shit though norbs because that shit that that's that that's a good ass blog that's a good\n\n128\n00:12:49,119 --> 00:12:53,840\nass blog and i'll watch i'll hit you back i'm gonna get this nigga started i'll watch it right\n\n129\n00:12:53,840 --> 00:12:58,799\nnow all right i'm gonna send it uh maybe like nothing i make is over 12 minutes so it's probably\n\n130\n00:12:58,799 --> 00:13:03,679\nlike eight or nine minutes maybe i'll send it to you okay i gotta put this nigga he follow me\n\n131\n00:13:03,679 --> 00:13:09,280\naround all right my my baby girl i'm gonna be putting her to bed at any moment so um if not\n\n132\n00:13:09,280 --> 00:13:13,599\nthen i'll hit you later on tonight all right\n\n\n"}], 'model': 'gpt-3.5-turbo-16k', 'max_tokens': 1000, 'n': 1, 'stream': False, 'temperature': 0.7}}
DEBUG:openai._base_client:Sending HTTP Request: POST https://api.openai.com/v1/chat/completions
DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>
DEBUG:httpcore.http11:send_request_headers.complete
DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>
DEBUG:httpcore.http11:send_request_body.complete
DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>
DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Mon, 08 Jul 2024 04:38:51 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'openai-organization', b'machinekingsmedia'), (b'openai-processing-ms', b'2515'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains'), (b'x-ratelimit-limit-requests', b'10000'), (b'x-ratelimit-limit-tokens', b'1000000'), (b'x-ratelimit-remaining-requests', b'9999'), (b'x-ratelimit-remaining-tokens', b'995834'), (b'x-ratelimit-reset-requests', b'6ms'), (b'x-ratelimit-reset-tokens', b'249ms'), (b'x-request-id', b'req_a819c3ac65d1196dfab07dd2302e30f2'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'89fd630d8b94823f-IAD'), (b'Content-Encoding', b'br'), (b'alt-svc', b'h3=":443"; ma=86400')])
INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>
DEBUG:httpcore.http11:receive_response_body.complete
DEBUG:httpcore.http11:response_closed.started
DEBUG:httpcore.http11:response_closed.complete
DEBUG:openai._base_client:HTTP Response: POST https://api.openai.com/v1/chat/completions "200 OK" Headers({'date': 'Mon, 08 Jul 2024 04:38:51 GMT', 'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'openai-organization': 'machinekingsmedia', 'openai-processing-ms': '2515', 'openai-version': '2020-10-01', 'strict-transport-security': 'max-age=31536000; includeSubDomains', 'x-ratelimit-limit-requests': '10000', 'x-ratelimit-limit-tokens': '1000000', 'x-ratelimit-remaining-requests': '9999', 'x-ratelimit-remaining-tokens': '995834', 'x-ratelimit-reset-requests': '6ms', 'x-ratelimit-reset-tokens': '249ms', 'x-request-id': 'req_a819c3ac65d1196dfab07dd2302e30f2', 'cf-cache-status': 'DYNAMIC', 'server': 'cloudflare', 'cf-ray': '89fd630d8b94823f-IAD', 'content-encoding': 'br', 'alt-svc': 'h3=":443"; ma=86400'})
DEBUG:openai._base_client:request_id: req_a819c3ac65d1196dfab07dd2302e30f2
ERROR:root:Error processing chapters: 'title'
Traceback (most recent call last):
  File "D:\github\chappymedium\chappie.py", line 272, in process_chapters
    self.update_chapter_list()
  File "D:\github\chappymedium\chappie.py", line 331, in update_chapter_list
    self.chapter_list.addItem(f"{chapter['title']} ({seconds_to_time(chapter['start'])} - {seconds_to_time(chapter['end'])})")
KeyError: 'title'
DEBUG:httpx:load_ssl_context verify=True cert=None trust_env=True http2=False
DEBUG:httpx:load_verify_locations cafile='C:\\Users\\taskm\\anaconda3\\Library\\ssl\\cacert.pem'
DEBUG:httpx:load_ssl_context verify=True cert=None trust_env=True http2=False
DEBUG:httpx:load_verify_locations cafile='C:\\Users\\taskm\\anaconda3\\Library\\ssl\\cacert.pem'
DEBUG:numba.core.byteflow:bytecode dump:
>          0	NOP(arg=None, lineno=1141)
           2	LOAD_FAST(arg=0, lineno=1144)
           4	LOAD_CONST(arg=1, lineno=1144)
           6	BINARY_SUBSCR(arg=None, lineno=1144)
           8	STORE_FAST(arg=3, lineno=1144)
          10	LOAD_FAST(arg=1, lineno=1145)
          12	UNARY_NEGATIVE(arg=None, lineno=1145)
          14	LOAD_FAST(arg=3, lineno=1145)
          16	DUP_TOP(arg=None, lineno=1145)
          18	ROT_THREE(arg=None, lineno=1145)
          20	COMPARE_OP(arg=1, lineno=1145)
          22	POP_JUMP_IF_FALSE(arg=32, lineno=1145)
          24	LOAD_FAST(arg=1, lineno=1145)
          26	COMPARE_OP(arg=1, lineno=1145)
          28	POP_JUMP_IF_FALSE(arg=40, lineno=1145)
          30	JUMP_FORWARD(arg=4, lineno=1145)
>         32	POP_TOP(arg=None, lineno=1145)
          34	JUMP_FORWARD(arg=4, lineno=1145)
>         36	LOAD_CONST(arg=1, lineno=1146)
          38	STORE_FAST(arg=3, lineno=1146)
>         40	LOAD_FAST(arg=0, lineno=1148)
          42	LOAD_CONST(arg=2, lineno=1148)
          44	BINARY_SUBSCR(arg=None, lineno=1148)
          46	STORE_FAST(arg=4, lineno=1148)
          48	LOAD_FAST(arg=1, lineno=1149)
          50	UNARY_NEGATIVE(arg=None, lineno=1149)
          52	LOAD_FAST(arg=4, lineno=1149)
          54	DUP_TOP(arg=None, lineno=1149)
          56	ROT_THREE(arg=None, lineno=1149)
          58	COMPARE_OP(arg=1, lineno=1149)
          60	POP_JUMP_IF_FALSE(arg=70, lineno=1149)
          62	LOAD_FAST(arg=1, lineno=1149)
          64	COMPARE_OP(arg=1, lineno=1149)
          66	POP_JUMP_IF_FALSE(arg=78, lineno=1149)
          68	JUMP_FORWARD(arg=4, lineno=1149)
>         70	POP_TOP(arg=None, lineno=1149)
          72	JUMP_FORWARD(arg=4, lineno=1149)
>         74	LOAD_CONST(arg=1, lineno=1150)
          76	STORE_FAST(arg=4, lineno=1150)
>         78	LOAD_FAST(arg=2, lineno=1152)
          80	POP_JUMP_IF_FALSE(arg=102, lineno=1152)
          82	LOAD_GLOBAL(arg=0, lineno=1153)
          84	LOAD_METHOD(arg=1, lineno=1153)
          86	LOAD_FAST(arg=3, lineno=1153)
          88	CALL_METHOD(arg=1, lineno=1153)
          90	LOAD_GLOBAL(arg=0, lineno=1153)
          92	LOAD_METHOD(arg=1, lineno=1153)
          94	LOAD_FAST(arg=4, lineno=1153)
          96	CALL_METHOD(arg=1, lineno=1153)
          98	COMPARE_OP(arg=3, lineno=1153)
         100	RETURN_VALUE(arg=None, lineno=1153)
>        102	LOAD_GLOBAL(arg=0, lineno=1155)
         104	LOAD_METHOD(arg=2, lineno=1155)
         106	LOAD_FAST(arg=3, lineno=1155)
         108	CALL_METHOD(arg=1, lineno=1155)
         110	LOAD_GLOBAL(arg=0, lineno=1155)
         112	LOAD_METHOD(arg=2, lineno=1155)
         114	LOAD_FAST(arg=4, lineno=1155)
         116	CALL_METHOD(arg=1, lineno=1155)
         118	COMPARE_OP(arg=3, lineno=1155)
         120	RETURN_VALUE(arg=None, lineno=1155)
         122	LOAD_CONST(arg=3, lineno=1155)
         124	RETURN_VALUE(arg=None, lineno=1155)
DEBUG:numba.core.byteflow:pending: deque([State(pc_initial=0 nstack_initial=0)])
DEBUG:numba.core.byteflow:stack: []
DEBUG:numba.core.byteflow:state.pc_initial: State(pc_initial=0 nstack_initial=0)
DEBUG:numba.core.byteflow:dispatch pc=0, inst=NOP(arg=None, lineno=1141)
DEBUG:numba.core.byteflow:stack []
DEBUG:numba.core.byteflow:dispatch pc=2, inst=LOAD_FAST(arg=0, lineno=1144)
DEBUG:numba.core.byteflow:stack []
DEBUG:numba.core.byteflow:dispatch pc=4, inst=LOAD_CONST(arg=1, lineno=1144)
DEBUG:numba.core.byteflow:stack ['$x2.0']
DEBUG:numba.core.byteflow:dispatch pc=6, inst=BINARY_SUBSCR(arg=None, lineno=1144)
DEBUG:numba.core.byteflow:stack ['$x2.0', '$const4.1']
DEBUG:numba.core.byteflow:dispatch pc=8, inst=STORE_FAST(arg=3, lineno=1144)
DEBUG:numba.core.byteflow:stack ['$6binary_subscr.2']
DEBUG:numba.core.byteflow:dispatch pc=10, inst=LOAD_FAST(arg=1, lineno=1145)
DEBUG:numba.core.byteflow:stack []
DEBUG:numba.core.byteflow:dispatch pc=12, inst=UNARY_NEGATIVE(arg=None, lineno=1145)
DEBUG:numba.core.byteflow:stack ['$threshold10.3']
DEBUG:numba.core.byteflow:dispatch pc=14, inst=LOAD_FAST(arg=3, lineno=1145)
DEBUG:numba.core.byteflow:stack ['$12unary_negative.4']
DEBUG:numba.core.byteflow:dispatch pc=16, inst=DUP_TOP(arg=None, lineno=1145)
DEBUG:numba.core.byteflow:stack ['$12unary_negative.4', '$x014.5']
DEBUG:numba.core.byteflow:dispatch pc=18, inst=ROT_THREE(arg=None, lineno=1145)
DEBUG:numba.core.byteflow:stack ['$12unary_negative.4', '$x014.5', '$16dup_top.6']
DEBUG:numba.core.byteflow:dispatch pc=20, inst=COMPARE_OP(arg=1, lineno=1145)
DEBUG:numba.core.byteflow:stack ['$16dup_top.6', '$12unary_negative.4', '$x014.5']
DEBUG:numba.core.byteflow:dispatch pc=22, inst=POP_JUMP_IF_FALSE(arg=32, lineno=1145)
DEBUG:numba.core.byteflow:stack ['$16dup_top.6', '$20compare_op.7']
DEBUG:numba.core.byteflow:end state. edges=[Edge(pc=24, stack=('$16dup_top.6',), blockstack=(), npush=0), Edge(pc=32, stack=('$16dup_top.6',), blockstack=(), npush=0)]
DEBUG:numba.core.byteflow:pending: deque([State(pc_initial=24 nstack_initial=1), State(pc_initial=32 nstack_initial=1)])
DEBUG:numba.core.byteflow:stack: ['$phi24.0']
DEBUG:numba.core.byteflow:state.pc_initial: State(pc_initial=24 nstack_initial=1)
DEBUG:numba.core.byteflow:dispatch pc=24, inst=LOAD_FAST(arg=1, lineno=1145)
DEBUG:numba.core.byteflow:stack ['$phi24.0']
DEBUG:numba.core.byteflow:dispatch pc=26, inst=COMPARE_OP(arg=1, lineno=1145)
DEBUG:numba.core.byteflow:stack ['$phi24.0', '$threshold24.1']
DEBUG:numba.core.byteflow:dispatch pc=28, inst=POP_JUMP_IF_FALSE(arg=40, lineno=1145)
DEBUG:numba.core.byteflow:stack ['$26compare_op.2']
DEBUG:numba.core.byteflow:end state. edges=[Edge(pc=30, stack=(), blockstack=(), npush=0), Edge(pc=40, stack=(), blockstack=(), npush=0)]
DEBUG:numba.core.byteflow:pending: deque([State(pc_initial=32 nstack_initial=1), State(pc_initial=30 nstack_initial=0), State(pc_initial=40 nstack_initial=0)])
DEBUG:numba.core.byteflow:stack: ['$phi32.0']
DEBUG:numba.core.byteflow:state.pc_initial: State(pc_initial=32 nstack_initial=1)
DEBUG:numba.core.byteflow:dispatch pc=32, inst=POP_TOP(arg=None, lineno=1145)
DEBUG:numba.core.byteflow:stack ['$phi32.0']
DEBUG:numba.core.byteflow:dispatch pc=34, inst=JUMP_FORWARD(arg=4, lineno=1145)
DEBUG:numba.core.byteflow:stack []
DEBUG:numba.core.byteflow:end state. edges=[Edge(pc=40, stack=(), blockstack=(), npush=0)]
DEBUG:numba.core.byteflow:pending: deque([State(pc_initial=30 nstack_initial=0), State(pc_initial=40 nstack_initial=0), State(pc_initial=40 nstack_initial=0)])
DEBUG:numba.core.byteflow:stack: []
DEBUG:numba.core.byteflow:state.pc_initial: State(pc_initial=30 nstack_initial=0)
DEBUG:numba.core.byteflow:dispatch pc=30, inst=JUMP_FORWARD(arg=4, lineno=1145)
DEBUG:numba.core.byteflow:stack []
DEBUG:numba.core.byteflow:end state. edges=[Edge(pc=36, stack=(), blockstack=(), npush=0)]
DEBUG:numba.core.byteflow:pending: deque([State(pc_initial=40 nstack_initial=0), State(pc_initial=40 nstack_initial=0), State(pc_initial=36 nstack_initial=0)])
DEBUG:numba.core.byteflow:stack: []
DEBUG:numba.core.byteflow:state.pc_initial: State(pc_initial=40 nstack_initial=0)
DEBUG:numba.core.byteflow:dispatch pc=40, inst=LOAD_FAST(arg=0, lineno=1148)
DEBUG:numba.core.byteflow:stack []
DEBUG:numba.core.byteflow:dispatch pc=42, inst=LOAD_CONST(arg=2, lineno=1148)
DEBUG:numba.core.byteflow:stack ['$x40.0']
DEBUG:numba.core.byteflow:dispatch pc=44, inst=BINARY_SUBSCR(arg=None, lineno=1148)
DEBUG:numba.core.byteflow:stack ['$x40.0', '$const42.1']
DEBUG:numba.core.byteflow:dispatch pc=46, inst=STORE_FAST(arg=4, lineno=1148)
DEBUG:numba.core.byteflow:stack ['$44binary_subscr.2']
DEBUG:numba.core.byteflow:dispatch pc=48, inst=LOAD_FAST(arg=1, lineno=1149)
DEBUG:numba.core.byteflow:stack []
DEBUG:numba.core.byteflow:dispatch pc=50, inst=UNARY_NEGATIVE(arg=None, lineno=1149)
DEBUG:numba.core.byteflow:stack ['$threshold48.3']
DEBUG:numba.core.byteflow:dispatch pc=52, inst=LOAD_FAST(arg=4, lineno=1149)
DEBUG:numba.core.byteflow:stack ['$50unary_negative.4']
DEBUG:numba.core.byteflow:dispatch pc=54, inst=DUP_TOP(arg=None, lineno=1149)
DEBUG:numba.core.byteflow:stack ['$50unary_negative.4', '$x152.5']
DEBUG:numba.core.byteflow:dispatch pc=56, inst=ROT_THREE(arg=None, lineno=1149)
DEBUG:numba.core.byteflow:stack ['$50unary_negative.4', '$x152.5', '$54dup_top.6']
DEBUG:numba.core.byteflow:dispatch pc=58, inst=COMPARE_OP(arg=1, lineno=1149)
DEBUG:numba.core.byteflow:stack ['$54dup_top.6', '$50unary_negative.4', '$x152.5']
DEBUG:numba.core.byteflow:dispatch pc=60, inst=POP_JUMP_IF_FALSE(arg=70, lineno=1149)
DEBUG:numba.core.byteflow:stack ['$54dup_top.6', '$58compare_op.7']
DEBUG:numba.core.byteflow:end state. edges=[Edge(pc=62, stack=('$54dup_top.6',), blockstack=(), npush=0), Edge(pc=70, stack=('$54dup_top.6',), blockstack=(), npush=0)]
DEBUG:numba.core.byteflow:pending: deque([State(pc_initial=40 nstack_initial=0), State(pc_initial=36 nstack_initial=0), State(pc_initial=62 nstack_initial=1), State(pc_initial=70 nstack_initial=1)])
DEBUG:numba.core.byteflow:pending: deque([State(pc_initial=36 nstack_initial=0), State(pc_initial=62 nstack_initial=1), State(pc_initial=70 nstack_initial=1)])
DEBUG:numba.core.byteflow:stack: []
DEBUG:numba.core.byteflow:state.pc_initial: State(pc_initial=36 nstack_initial=0)
DEBUG:numba.core.byteflow:dispatch pc=36, inst=LOAD_CONST(arg=1, lineno=1146)
DEBUG:numba.core.byteflow:stack []
DEBUG:numba.core.byteflow:dispatch pc=38, inst=STORE_FAST(arg=3, lineno=1146)
DEBUG:numba.core.byteflow:stack ['$const36.0']
DEBUG:numba.core.byteflow:end state. edges=[Edge(pc=40, stack=(), blockstack=(), npush=0)]
DEBUG:numba.core.byteflow:pending: deque([State(pc_initial=62 nstack_initial=1), State(pc_initial=70 nstack_initial=1), State(pc_initial=40 nstack_initial=0)])
DEBUG:numba.core.byteflow:stack: ['$phi62.0']
DEBUG:numba.core.byteflow:state.pc_initial: State(pc_initial=62 nstack_initial=1)
DEBUG:numba.core.byteflow:dispatch pc=62, inst=LOAD_FAST(arg=1, lineno=1149)
DEBUG:numba.core.byteflow:stack ['$phi62.0']
DEBUG:numba.core.byteflow:dispatch pc=64, inst=COMPARE_OP(arg=1, lineno=1149)
DEBUG:numba.core.byteflow:stack ['$phi62.0', '$threshold62.1']
DEBUG:numba.core.byteflow:dispatch pc=66, inst=POP_JUMP_IF_FALSE(arg=78, lineno=1149)
DEBUG:numba.core.byteflow:stack ['$64compare_op.2']
DEBUG:numba.core.byteflow:end state. edges=[Edge(pc=68, stack=(), blockstack=(), npush=0), Edge(pc=78, stack=(), blockstack=(), npush=0)]
DEBUG:numba.core.byteflow:pending: deque([State(pc_initial=70 nstack_initial=1), State(pc_initial=40 nstack_initial=0), State(pc_initial=68 nstack_initial=0), State(pc_initial=78 nstack_initial=0)])
DEBUG:numba.core.byteflow:stack: ['$phi70.0']
DEBUG:numba.core.byteflow:state.pc_initial: State(pc_initial=70 nstack_initial=1)
DEBUG:numba.core.byteflow:dispatch pc=70, inst=POP_TOP(arg=None, lineno=1149)
DEBUG:numba.core.byteflow:stack ['$phi70.0']
DEBUG:numba.core.byteflow:dispatch pc=72, inst=JUMP_FORWARD(arg=4, lineno=1149)
DEBUG:numba.core.byteflow:stack []
DEBUG:numba.core.byteflow:end state. edges=[Edge(pc=78, stack=(), blockstack=(), npush=0)]
DEBUG:numba.core.byteflow:pending: deque([State(pc_initial=40 nstack_initial=0), State(pc_initial=68 nstack_initial=0), State(pc_initial=78 nstack_initial=0), State(pc_initial=78 nstack_initial=0)])
DEBUG:numba.core.byteflow:pending: deque([State(pc_initial=68 nstack_initial=0), State(pc_initial=78 nstack_initial=0), State(pc_initial=78 nstack_initial=0)])
DEBUG:numba.core.byteflow:stack: []
DEBUG:numba.core.byteflow:state.pc_initial: State(pc_initial=68 nstack_initial=0)
DEBUG:numba.core.byteflow:dispatch pc=68, inst=JUMP_FORWARD(arg=4, lineno=1149)
DEBUG:numba.core.byteflow:stack []
DEBUG:numba.core.byteflow:end state. edges=[Edge(pc=74, stack=(), blockstack=(), npush=0)]
DEBUG:numba.core.byteflow:pending: deque([State(pc_initial=78 nstack_initial=0), State(pc_initial=78 nstack_initial=0), State(pc_initial=74 nstack_initial=0)])
DEBUG:numba.core.byteflow:stack: []
DEBUG:numba.core.byteflow:state.pc_initial: State(pc_initial=78 nstack_initial=0)
DEBUG:numba.core.byteflow:dispatch pc=78, inst=LOAD_FAST(arg=2, lineno=1152)
DEBUG:numba.core.byteflow:stack []
DEBUG:numba.core.byteflow:dispatch pc=80, inst=POP_JUMP_IF_FALSE(arg=102, lineno=1152)
DEBUG:numba.core.byteflow:stack ['$zero_pos78.0']
DEBUG:numba.core.byteflow:end state. edges=[Edge(pc=82, stack=(), blockstack=(), npush=0), Edge(pc=102, stack=(), blockstack=(), npush=0)]
DEBUG:numba.core.byteflow:pending: deque([State(pc_initial=78 nstack_initial=0), State(pc_initial=74 nstack_initial=0), State(pc_initial=82 nstack_initial=0), State(pc_initial=102 nstack_initial=0)])
DEBUG:numba.core.byteflow:pending: deque([State(pc_initial=74 nstack_initial=0), State(pc_initial=82 nstack_initial=0), State(pc_initial=102 nstack_initial=0)])
DEBUG:numba.core.byteflow:stack: []
DEBUG:numba.core.byteflow:state.pc_initial: State(pc_initial=74 nstack_initial=0)
DEBUG:numba.core.byteflow:dispatch pc=74, inst=LOAD_CONST(arg=1, lineno=1150)
DEBUG:numba.core.byteflow:stack []
DEBUG:numba.core.byteflow:dispatch pc=76, inst=STORE_FAST(arg=4, lineno=1150)
DEBUG:numba.core.byteflow:stack ['$const74.0']
DEBUG:numba.core.byteflow:end state. edges=[Edge(pc=78, stack=(), blockstack=(), npush=0)]
DEBUG:numba.core.byteflow:pending: deque([State(pc_initial=82 nstack_initial=0), State(pc_initial=102 nstack_initial=0), State(pc_initial=78 nstack_initial=0)])
DEBUG:numba.core.byteflow:stack: []
DEBUG:numba.core.byteflow:state.pc_initial: State(pc_initial=82 nstack_initial=0)
DEBUG:numba.core.byteflow:dispatch pc=82, inst=LOAD_GLOBAL(arg=0, lineno=1153)
DEBUG:numba.core.byteflow:stack []
DEBUG:numba.core.byteflow:dispatch pc=84, inst=LOAD_METHOD(arg=1, lineno=1153)
DEBUG:numba.core.byteflow:stack ['$82load_global.0']
DEBUG:numba.core.byteflow:dispatch pc=86, inst=LOAD_FAST(arg=3, lineno=1153)
DEBUG:numba.core.byteflow:stack ['$84load_method.1']
DEBUG:numba.core.byteflow:dispatch pc=88, inst=CALL_METHOD(arg=1, lineno=1153)
DEBUG:numba.core.byteflow:stack ['$84load_method.1', '$x086.2']
DEBUG:numba.core.byteflow:dispatch pc=90, inst=LOAD_GLOBAL(arg=0, lineno=1153)
DEBUG:numba.core.byteflow:stack ['$88call_method.3']
DEBUG:numba.core.byteflow:dispatch pc=92, inst=LOAD_METHOD(arg=1, lineno=1153)
DEBUG:numba.core.byteflow:stack ['$88call_method.3', '$90load_global.4']
DEBUG:numba.core.byteflow:dispatch pc=94, inst=LOAD_FAST(arg=4, lineno=1153)
DEBUG:numba.core.byteflow:stack ['$88call_method.3', '$92load_method.5']
DEBUG:numba.core.byteflow:dispatch pc=96, inst=CALL_METHOD(arg=1, lineno=1153)
DEBUG:numba.core.byteflow:stack ['$88call_method.3', '$92load_method.5', '$x194.6']
DEBUG:numba.core.byteflow:dispatch pc=98, inst=COMPARE_OP(arg=3, lineno=1153)
DEBUG:numba.core.byteflow:stack ['$88call_method.3', '$96call_method.7']
DEBUG:numba.core.byteflow:dispatch pc=100, inst=RETURN_VALUE(arg=None, lineno=1153)
DEBUG:numba.core.byteflow:stack ['$98compare_op.8']
DEBUG:numba.core.byteflow:end state. edges=[]
DEBUG:numba.core.byteflow:pending: deque([State(pc_initial=102 nstack_initial=0), State(pc_initial=78 nstack_initial=0)])
DEBUG:numba.core.byteflow:stack: []
DEBUG:numba.core.byteflow:state.pc_initial: State(pc_initial=102 nstack_initial=0)
DEBUG:numba.core.byteflow:dispatch pc=102, inst=LOAD_GLOBAL(arg=0, lineno=1155)
DEBUG:numba.core.byteflow:stack []
DEBUG:numba.core.byteflow:dispatch pc=104, inst=LOAD_METHOD(arg=2, lineno=1155)
DEBUG:numba.core.byteflow:stack ['$102load_global.0']
DEBUG:numba.core.byteflow:dispatch pc=106, inst=LOAD_FAST(arg=3, lineno=1155)
DEBUG:numba.core.byteflow:stack ['$104load_method.1']
DEBUG:numba.core.byteflow:dispatch pc=108, inst=CALL_METHOD(arg=1, lineno=1155)
DEBUG:numba.core.byteflow:stack ['$104load_method.1', '$x0106.2']
DEBUG:numba.core.byteflow:dispatch pc=110, inst=LOAD_GLOBAL(arg=0, lineno=1155)
DEBUG:numba.core.byteflow:stack ['$108call_method.3']
DEBUG:numba.core.byteflow:dispatch pc=112, inst=LOAD_METHOD(arg=2, lineno=1155)
DEBUG:numba.core.byteflow:stack ['$108call_method.3', '$110load_global.4']
DEBUG:numba.core.byteflow:dispatch pc=114, inst=LOAD_FAST(arg=4, lineno=1155)
DEBUG:numba.core.byteflow:stack ['$108call_method.3', '$112load_method.5']
DEBUG:numba.core.byteflow:dispatch pc=116, inst=CALL_METHOD(arg=1, lineno=1155)
DEBUG:numba.core.byteflow:stack ['$108call_method.3', '$112load_method.5', '$x1114.6']
DEBUG:numba.core.byteflow:dispatch pc=118, inst=COMPARE_OP(arg=3, lineno=1155)
DEBUG:numba.core.byteflow:stack ['$108call_method.3', '$116call_method.7']
DEBUG:numba.core.byteflow:dispatch pc=120, inst=RETURN_VALUE(arg=None, lineno=1155)
DEBUG:numba.core.byteflow:stack ['$118compare_op.8']
DEBUG:numba.core.byteflow:end state. edges=[]
DEBUG:numba.core.byteflow:pending: deque([State(pc_initial=78 nstack_initial=0)])
DEBUG:numba.core.byteflow:-------------------------Prune PHIs-------------------------
DEBUG:numba.core.byteflow:Used_phis: defaultdict(<class 'set'>,
            {State(pc_initial=0 nstack_initial=0): set(),
             State(pc_initial=24 nstack_initial=1): {'$phi24.0'},
             State(pc_initial=30 nstack_initial=0): set(),
             State(pc_initial=32 nstack_initial=1): set(),
             State(pc_initial=36 nstack_initial=0): set(),
             State(pc_initial=40 nstack_initial=0): set(),
             State(pc_initial=62 nstack_initial=1): {'$phi62.0'},
             State(pc_initial=68 nstack_initial=0): set(),
             State(pc_initial=70 nstack_initial=1): set(),
             State(pc_initial=74 nstack_initial=0): set(),
             State(pc_initial=78 nstack_initial=0): set(),
             State(pc_initial=82 nstack_initial=0): set(),
             State(pc_initial=102 nstack_initial=0): set()})
DEBUG:numba.core.byteflow:defmap: {'$phi24.0': State(pc_initial=0 nstack_initial=0),
 '$phi32.0': State(pc_initial=0 nstack_initial=0),
 '$phi62.0': State(pc_initial=40 nstack_initial=0),
 '$phi70.0': State(pc_initial=40 nstack_initial=0)}
DEBUG:numba.core.byteflow:phismap: defaultdict(<class 'set'>,
            {'$phi24.0': {('$16dup_top.6',
                           State(pc_initial=0 nstack_initial=0))},
             '$phi32.0': {('$16dup_top.6',
                           State(pc_initial=0 nstack_initial=0))},
             '$phi62.0': {('$54dup_top.6',
                           State(pc_initial=40 nstack_initial=0))},
             '$phi70.0': {('$54dup_top.6',
                           State(pc_initial=40 nstack_initial=0))}})
DEBUG:numba.core.byteflow:changing phismap: defaultdict(<class 'set'>,
            {'$phi24.0': {('$16dup_top.6',
                           State(pc_initial=0 nstack_initial=0))},
             '$phi32.0': {('$16dup_top.6',
                           State(pc_initial=0 nstack_initial=0))},
             '$phi62.0': {('$54dup_top.6',
                           State(pc_initial=40 nstack_initial=0))},
             '$phi70.0': {('$54dup_top.6',
                           State(pc_initial=40 nstack_initial=0))}})
DEBUG:numba.core.byteflow:keep phismap: {'$phi24.0': {('$16dup_top.6', State(pc_initial=0 nstack_initial=0))},
 '$phi62.0': {('$54dup_top.6', State(pc_initial=40 nstack_initial=0))}}
DEBUG:numba.core.byteflow:new_out: defaultdict(<class 'dict'>,
            {State(pc_initial=0 nstack_initial=0): {'$phi24.0': '$16dup_top.6'},
             State(pc_initial=40 nstack_initial=0): {'$phi62.0': '$54dup_top.6'}})
DEBUG:numba.core.byteflow:----------------------DONE Prune PHIs-----------------------
DEBUG:numba.core.byteflow:block_infos State(pc_initial=0 nstack_initial=0):
AdaptBlockInfo(insts=((0, {}), (2, {'res': '$x2.0'}), (4, {'res': '$const4.1'}), (6, {'index': '$const4.1', 'target': '$x2.0', 'res': '$6binary_subscr.2'}), (8, {'value': '$6binary_subscr.2'}), (10, {'res': '$threshold10.3'}), (12, {'value': '$threshold10.3', 'res': '$12unary_negative.4'}), (14, {'res': '$x014.5'}), (16, {'orig': ['$x014.5'], 'duped': ['$16dup_top.6']}), (20, {'lhs': '$12unary_negative.4', 'rhs': '$x014.5', 'res': '$20compare_op.7'}), (22, {'pred': '$20compare_op.7'})), outgoing_phis={'$phi24.0': '$16dup_top.6'}, blockstack=(), active_try_block=None, outgoing_edgepushed={24: ('$16dup_top.6',), 32: ('$16dup_top.6',)})
DEBUG:numba.core.byteflow:block_infos State(pc_initial=24 nstack_initial=1):
AdaptBlockInfo(insts=((24, {'res': '$threshold24.1'}), (26, {'lhs': '$phi24.0', 'rhs': '$threshold24.1', 'res': '$26compare_op.2'}), (28, {'pred': '$26compare_op.2'})), outgoing_phis={}, blockstack=(), active_try_block=None, outgoing_edgepushed={30: (), 40: ()})
DEBUG:numba.core.byteflow:block_infos State(pc_initial=30 nstack_initial=0):
AdaptBlockInfo(insts=((30, {}),), outgoing_phis={}, blockstack=(), active_try_block=None, outgoing_edgepushed={36: ()})
DEBUG:numba.core.byteflow:block_infos State(pc_initial=32 nstack_initial=1):
AdaptBlockInfo(insts=((34, {}),), outgoing_phis={}, blockstack=(), active_try_block=None, outgoing_edgepushed={40: ()})
DEBUG:numba.core.byteflow:block_infos State(pc_initial=36 nstack_initial=0):
AdaptBlockInfo(insts=((36, {'res': '$const36.0'}), (38, {'value': '$const36.0'})), outgoing_phis={}, blockstack=(), active_try_block=None, outgoing_edgepushed={40: ()})
DEBUG:numba.core.byteflow:block_infos State(pc_initial=40 nstack_initial=0):
AdaptBlockInfo(insts=((40, {'res': '$x40.0'}), (42, {'res': '$const42.1'}), (44, {'index': '$const42.1', 'target': '$x40.0', 'res': '$44binary_subscr.2'}), (46, {'value': '$44binary_subscr.2'}), (48, {'res': '$threshold48.3'}), (50, {'value': '$threshold48.3', 'res': '$50unary_negative.4'}), (52, {'res': '$x152.5'}), (54, {'orig': ['$x152.5'], 'duped': ['$54dup_top.6']}), (58, {'lhs': '$50unary_negative.4', 'rhs': '$x152.5', 'res': '$58compare_op.7'}), (60, {'pred': '$58compare_op.7'})), outgoing_phis={'$phi62.0': '$54dup_top.6'}, blockstack=(), active_try_block=None, outgoing_edgepushed={62: ('$54dup_top.6',), 70: ('$54dup_top.6',)})
DEBUG:numba.core.byteflow:block_infos State(pc_initial=62 nstack_initial=1):
AdaptBlockInfo(insts=((62, {'res': '$threshold62.1'}), (64, {'lhs': '$phi62.0', 'rhs': '$threshold62.1', 'res': '$64compare_op.2'}), (66, {'pred': '$64compare_op.2'})), outgoing_phis={}, blockstack=(), active_try_block=None, outgoing_edgepushed={68: (), 78: ()})
DEBUG:numba.core.byteflow:block_infos State(pc_initial=68 nstack_initial=0):
AdaptBlockInfo(insts=((68, {}),), outgoing_phis={}, blockstack=(), active_try_block=None, outgoing_edgepushed={74: ()})
DEBUG:numba.core.byteflow:block_infos State(pc_initial=70 nstack_initial=1):
AdaptBlockInfo(insts=((72, {}),), outgoing_phis={}, blockstack=(), active_try_block=None, outgoing_edgepushed={78: ()})
DEBUG:numba.core.byteflow:block_infos State(pc_initial=74 nstack_initial=0):
AdaptBlockInfo(insts=((74, {'res': '$const74.0'}), (76, {'value': '$const74.0'})), outgoing_phis={}, blockstack=(), active_try_block=None, outgoing_edgepushed={78: ()})
DEBUG:numba.core.byteflow:block_infos State(pc_initial=78 nstack_initial=0):
AdaptBlockInfo(insts=((78, {'res': '$zero_pos78.0'}), (80, {'pred': '$zero_pos78.0'})), outgoing_phis={}, blockstack=(), active_try_block=None, outgoing_edgepushed={82: (), 102: ()})
DEBUG:numba.core.byteflow:block_infos State(pc_initial=82 nstack_initial=0):
AdaptBlockInfo(insts=((82, {'res': '$82load_global.0'}), (84, {'item': '$82load_global.0', 'res': '$84load_method.1'}), (86, {'res': '$x086.2'}), (88, {'func': '$84load_method.1', 'args': ['$x086.2'], 'res': '$88call_method.3'}), (90, {'res': '$90load_global.4'}), (92, {'item': '$90load_global.4', 'res': '$92load_method.5'}), (94, {'res': '$x194.6'}), (96, {'func': '$92load_method.5', 'args': ['$x194.6'], 'res': '$96call_method.7'}), (98, {'lhs': '$88call_method.3', 'rhs': '$96call_method.7', 'res': '$98compare_op.8'}), (100, {'retval': '$98compare_op.8', 'castval': '$100return_value.9'})), outgoing_phis={}, blockstack=(), active_try_block=None, outgoing_edgepushed={})
DEBUG:numba.core.byteflow:block_infos State(pc_initial=102 nstack_initial=0):
AdaptBlockInfo(insts=((102, {'res': '$102load_global.0'}), (104, {'item': '$102load_global.0', 'res': '$104load_method.1'}), (106, {'res': '$x0106.2'}), (108, {'func': '$104load_method.1', 'args': ['$x0106.2'], 'res': '$108call_method.3'}), (110, {'res': '$110load_global.4'}), (112, {'item': '$110load_global.4', 'res': '$112load_method.5'}), (114, {'res': '$x1114.6'}), (116, {'func': '$112load_method.5', 'args': ['$x1114.6'], 'res': '$116call_method.7'}), (118, {'lhs': '$108call_method.3', 'rhs': '$116call_method.7', 'res': '$118compare_op.8'}), (120, {'retval': '$118compare_op.8', 'castval': '$120return_value.9'})), outgoing_phis={}, blockstack=(), active_try_block=None, outgoing_edgepushed={})
DEBUG:numba.core.interpreter:label 0:
    x = arg(0, name=x)                       ['x']
    threshold = arg(1, name=threshold)       ['threshold']
    zero_pos = arg(2, name=zero_pos)         ['zero_pos']
    $const4.1 = const(int, 0)                ['$const4.1']
    x0 = getitem(value=x, index=$const4.1, fn=<built-in function getitem>) ['$const4.1', 'x', 'x0']
    $12unary_negative.4 = unary(fn=<built-in function neg>, value=threshold) ['$12unary_negative.4', 'threshold']
    $20compare_op.7 = $12unary_negative.4 <= x0 ['$12unary_negative.4', '$20compare_op.7', 'x0']
    bool22 = global(bool: <class 'bool'>)    ['bool22']
    $22pred = call bool22($20compare_op.7, func=bool22, args=(Var($20compare_op.7, audio.py:1145),), kws=(), vararg=None, varkwarg=None, target=None) ['$20compare_op.7', '$22pred', 'bool22']
    $phi24.0 = x0                            ['$phi24.0', 'x0']
    branch $22pred, 24, 32                   ['$22pred']
label 24:
    $26compare_op.2 = $phi24.0 <= threshold  ['$26compare_op.2', '$phi24.0', 'threshold']
    bool28 = global(bool: <class 'bool'>)    ['bool28']
    $28pred = call bool28($26compare_op.2, func=bool28, args=(Var($26compare_op.2, audio.py:1145),), kws=(), vararg=None, varkwarg=None, target=None) ['$26compare_op.2', '$28pred', 'bool28']
    branch $28pred, 30, 40                   ['$28pred']
label 30:
    jump 36                                  []
label 32:
    jump 40                                  []
label 36:
    x0 = const(int, 0)                       ['x0']
    jump 40                                  []
label 40:
    $const42.1 = const(int, -1)              ['$const42.1']
    x1 = getitem(value=x, index=$const42.1, fn=<built-in function getitem>) ['$const42.1', 'x', 'x1']
    $50unary_negative.4 = unary(fn=<built-in function neg>, value=threshold) ['$50unary_negative.4', 'threshold']
    $58compare_op.7 = $50unary_negative.4 <= x1 ['$50unary_negative.4', '$58compare_op.7', 'x1']
    bool60 = global(bool: <class 'bool'>)    ['bool60']
    $60pred = call bool60($58compare_op.7, func=bool60, args=(Var($58compare_op.7, audio.py:1149),), kws=(), vararg=None, varkwarg=None, target=None) ['$58compare_op.7', '$60pred', 'bool60']
    $phi62.0 = x1                            ['$phi62.0', 'x1']
    branch $60pred, 62, 70                   ['$60pred']
label 62:
    $64compare_op.2 = $phi62.0 <= threshold  ['$64compare_op.2', '$phi62.0', 'threshold']
    bool66 = global(bool: <class 'bool'>)    ['bool66']
    $66pred = call bool66($64compare_op.2, func=bool66, args=(Var($64compare_op.2, audio.py:1149),), kws=(), vararg=None, varkwarg=None, target=None) ['$64compare_op.2', '$66pred', 'bool66']
    branch $66pred, 68, 78                   ['$66pred']
label 68:
    jump 74                                  []
label 70:
    jump 78                                  []
label 74:
    x1 = const(int, 0)                       ['x1']
    jump 78                                  []
label 78:
    bool80 = global(bool: <class 'bool'>)    ['bool80']
    $80pred = call bool80(zero_pos, func=bool80, args=(Var(zero_pos, audio.py:1141),), kws=(), vararg=None, varkwarg=None, target=None) ['$80pred', 'bool80', 'zero_pos']
    branch $80pred, 82, 102                  ['$80pred']
label 82:
    $82load_global.0 = global(np: <module 'numpy' from 'C:\\Users\\taskm\\anaconda3\\envs\\taskai\\lib\\site-packages\\numpy\\__init__.py'>) ['$82load_global.0']
    $84load_method.1 = getattr(value=$82load_global.0, attr=signbit) ['$82load_global.0', '$84load_method.1']
    $88call_method.3 = call $84load_method.1(x0, func=$84load_method.1, args=[Var(x0, audio.py:1144)], kws=(), vararg=None, varkwarg=None, target=None) ['$84load_method.1', '$88call_method.3', 'x0']
    $90load_global.4 = global(np: <module 'numpy' from 'C:\\Users\\taskm\\anaconda3\\envs\\taskai\\lib\\site-packages\\numpy\\__init__.py'>) ['$90load_global.4']
    $92load_method.5 = getattr(value=$90load_global.4, attr=signbit) ['$90load_global.4', '$92load_method.5']
    $96call_method.7 = call $92load_method.5(x1, func=$92load_method.5, args=[Var(x1, audio.py:1148)], kws=(), vararg=None, varkwarg=None, target=None) ['$92load_method.5', '$96call_method.7', 'x1']
    $98compare_op.8 = $88call_method.3 != $96call_method.7 ['$88call_method.3', '$96call_method.7', '$98compare_op.8']
    $100return_value.9 = cast(value=$98compare_op.8) ['$100return_value.9', '$98compare_op.8']
    return $100return_value.9                ['$100return_value.9']
label 102:
    $102load_global.0 = global(np: <module 'numpy' from 'C:\\Users\\taskm\\anaconda3\\envs\\taskai\\lib\\site-packages\\numpy\\__init__.py'>) ['$102load_global.0']
    $104load_method.1 = getattr(value=$102load_global.0, attr=sign) ['$102load_global.0', '$104load_method.1']
    $108call_method.3 = call $104load_method.1(x0, func=$104load_method.1, args=[Var(x0, audio.py:1144)], kws=(), vararg=None, varkwarg=None, target=None) ['$104load_method.1', '$108call_method.3', 'x0']
    $110load_global.4 = global(np: <module 'numpy' from 'C:\\Users\\taskm\\anaconda3\\envs\\taskai\\lib\\site-packages\\numpy\\__init__.py'>) ['$110load_global.4']
    $112load_method.5 = getattr(value=$110load_global.4, attr=sign) ['$110load_global.4', '$112load_method.5']
    $116call_method.7 = call $112load_method.5(x1, func=$112load_method.5, args=[Var(x1, audio.py:1148)], kws=(), vararg=None, varkwarg=None, target=None) ['$112load_method.5', '$116call_method.7', 'x1']
    $118compare_op.8 = $108call_method.3 != $116call_method.7 ['$108call_method.3', '$116call_method.7', '$118compare_op.8']
    $120return_value.9 = cast(value=$118compare_op.8) ['$118compare_op.8', '$120return_value.9']
    return $120return_value.9                ['$120return_value.9']

DEBUG:numba.core.byteflow:bytecode dump:
>          0	NOP(arg=None, lineno=1039)
           2	LOAD_FAST(arg=0, lineno=1042)
           4	LOAD_CONST(arg=1, lineno=1042)
           6	BINARY_SUBSCR(arg=None, lineno=1042)
           8	LOAD_FAST(arg=0, lineno=1042)
          10	LOAD_CONST(arg=2, lineno=1042)
          12	BINARY_SUBSCR(arg=None, lineno=1042)
          14	COMPARE_OP(arg=4, lineno=1042)
          16	LOAD_FAST(arg=0, lineno=1042)
          18	LOAD_CONST(arg=1, lineno=1042)
          20	BINARY_SUBSCR(arg=None, lineno=1042)
          22	LOAD_FAST(arg=0, lineno=1042)
          24	LOAD_CONST(arg=3, lineno=1042)
          26	BINARY_SUBSCR(arg=None, lineno=1042)
          28	COMPARE_OP(arg=5, lineno=1042)
          30	BINARY_AND(arg=None, lineno=1042)
          32	RETURN_VALUE(arg=None, lineno=1042)
DEBUG:numba.core.byteflow:pending: deque([State(pc_initial=0 nstack_initial=0)])
DEBUG:numba.core.byteflow:stack: []
DEBUG:numba.core.byteflow:state.pc_initial: State(pc_initial=0 nstack_initial=0)
DEBUG:numba.core.byteflow:dispatch pc=0, inst=NOP(arg=None, lineno=1039)
DEBUG:numba.core.byteflow:stack []
DEBUG:numba.core.byteflow:dispatch pc=2, inst=LOAD_FAST(arg=0, lineno=1042)
DEBUG:numba.core.byteflow:stack []
DEBUG:numba.core.byteflow:dispatch pc=4, inst=LOAD_CONST(arg=1, lineno=1042)
DEBUG:numba.core.byteflow:stack ['$x2.0']
DEBUG:numba.core.byteflow:dispatch pc=6, inst=BINARY_SUBSCR(arg=None, lineno=1042)
DEBUG:numba.core.byteflow:stack ['$x2.0', '$const4.1']
DEBUG:numba.core.byteflow:dispatch pc=8, inst=LOAD_FAST(arg=0, lineno=1042)
DEBUG:numba.core.byteflow:stack ['$6binary_subscr.2']
DEBUG:numba.core.byteflow:dispatch pc=10, inst=LOAD_CONST(arg=2, lineno=1042)
DEBUG:numba.core.byteflow:stack ['$6binary_subscr.2', '$x8.3']
DEBUG:numba.core.byteflow:dispatch pc=12, inst=BINARY_SUBSCR(arg=None, lineno=1042)
DEBUG:numba.core.byteflow:stack ['$6binary_subscr.2', '$x8.3', '$const10.4']
DEBUG:numba.core.byteflow:dispatch pc=14, inst=COMPARE_OP(arg=4, lineno=1042)
DEBUG:numba.core.byteflow:stack ['$6binary_subscr.2', '$12binary_subscr.5']
DEBUG:numba.core.byteflow:dispatch pc=16, inst=LOAD_FAST(arg=0, lineno=1042)
DEBUG:numba.core.byteflow:stack ['$14compare_op.6']
DEBUG:numba.core.byteflow:dispatch pc=18, inst=LOAD_CONST(arg=1, lineno=1042)
DEBUG:numba.core.byteflow:stack ['$14compare_op.6', '$x16.7']
DEBUG:numba.core.byteflow:dispatch pc=20, inst=BINARY_SUBSCR(arg=None, lineno=1042)
DEBUG:numba.core.byteflow:stack ['$14compare_op.6', '$x16.7', '$const18.8']
DEBUG:numba.core.byteflow:dispatch pc=22, inst=LOAD_FAST(arg=0, lineno=1042)
DEBUG:numba.core.byteflow:stack ['$14compare_op.6', '$20binary_subscr.9']
DEBUG:numba.core.byteflow:dispatch pc=24, inst=LOAD_CONST(arg=3, lineno=1042)
DEBUG:numba.core.byteflow:stack ['$14compare_op.6', '$20binary_subscr.9', '$x22.10']
DEBUG:numba.core.byteflow:dispatch pc=26, inst=BINARY_SUBSCR(arg=None, lineno=1042)
DEBUG:numba.core.byteflow:stack ['$14compare_op.6', '$20binary_subscr.9', '$x22.10', '$const24.11']
DEBUG:numba.core.byteflow:dispatch pc=28, inst=COMPARE_OP(arg=5, lineno=1042)
DEBUG:numba.core.byteflow:stack ['$14compare_op.6', '$20binary_subscr.9', '$26binary_subscr.12']
DEBUG:numba.core.byteflow:dispatch pc=30, inst=BINARY_AND(arg=None, lineno=1042)
DEBUG:numba.core.byteflow:stack ['$14compare_op.6', '$28compare_op.13']
DEBUG:numba.core.byteflow:dispatch pc=32, inst=RETURN_VALUE(arg=None, lineno=1042)
DEBUG:numba.core.byteflow:stack ['$30binary_and.14']
DEBUG:numba.core.byteflow:end state. edges=[]
DEBUG:numba.core.byteflow:-------------------------Prune PHIs-------------------------
DEBUG:numba.core.byteflow:Used_phis: defaultdict(<class 'set'>, {State(pc_initial=0 nstack_initial=0): set()})
DEBUG:numba.core.byteflow:defmap: {}
DEBUG:numba.core.byteflow:phismap: defaultdict(<class 'set'>, {})
DEBUG:numba.core.byteflow:changing phismap: defaultdict(<class 'set'>, {})
DEBUG:numba.core.byteflow:keep phismap: {}
DEBUG:numba.core.byteflow:new_out: defaultdict(<class 'dict'>, {})
DEBUG:numba.core.byteflow:----------------------DONE Prune PHIs-----------------------
DEBUG:numba.core.byteflow:block_infos State(pc_initial=0 nstack_initial=0):
AdaptBlockInfo(insts=((0, {}), (2, {'res': '$x2.0'}), (4, {'res': '$const4.1'}), (6, {'index': '$const4.1', 'target': '$x2.0', 'res': '$6binary_subscr.2'}), (8, {'res': '$x8.3'}), (10, {'res': '$const10.4'}), (12, {'index': '$const10.4', 'target': '$x8.3', 'res': '$12binary_subscr.5'}), (14, {'lhs': '$6binary_subscr.2', 'rhs': '$12binary_subscr.5', 'res': '$14compare_op.6'}), (16, {'res': '$x16.7'}), (18, {'res': '$const18.8'}), (20, {'index': '$const18.8', 'target': '$x16.7', 'res': '$20binary_subscr.9'}), (22, {'res': '$x22.10'}), (24, {'res': '$const24.11'}), (26, {'index': '$const24.11', 'target': '$x22.10', 'res': '$26binary_subscr.12'}), (28, {'lhs': '$20binary_subscr.9', 'rhs': '$26binary_subscr.12', 'res': '$28compare_op.13'}), (30, {'lhs': '$14compare_op.6', 'rhs': '$28compare_op.13', 'res': '$30binary_and.14'}), (32, {'retval': '$30binary_and.14', 'castval': '$32return_value.15'})), outgoing_phis={}, blockstack=(), active_try_block=None, outgoing_edgepushed={})
DEBUG:numba.core.interpreter:label 0:
    x = arg(0, name=x)                       ['x']
    $const4.1 = const(int, 0)                ['$const4.1']
    $6binary_subscr.2 = getitem(value=x, index=$const4.1, fn=<built-in function getitem>) ['$6binary_subscr.2', '$const4.1', 'x']
    $const10.4 = const(int, -1)              ['$const10.4']
    $12binary_subscr.5 = getitem(value=x, index=$const10.4, fn=<built-in function getitem>) ['$12binary_subscr.5', '$const10.4', 'x']
    $14compare_op.6 = $6binary_subscr.2 > $12binary_subscr.5 ['$12binary_subscr.5', '$14compare_op.6', '$6binary_subscr.2']
    $const18.8 = const(int, 0)               ['$const18.8']
    $20binary_subscr.9 = getitem(value=x, index=$const18.8, fn=<built-in function getitem>) ['$20binary_subscr.9', '$const18.8', 'x']
    $const24.11 = const(int, 1)              ['$const24.11']
    $26binary_subscr.12 = getitem(value=x, index=$const24.11, fn=<built-in function getitem>) ['$26binary_subscr.12', '$const24.11', 'x']
    $28compare_op.13 = $20binary_subscr.9 >= $26binary_subscr.12 ['$20binary_subscr.9', '$26binary_subscr.12', '$28compare_op.13']
    $30binary_and.14 = $14compare_op.6 & $28compare_op.13 ['$14compare_op.6', '$28compare_op.13', '$30binary_and.14']
    $32return_value.15 = cast(value=$30binary_and.14) ['$30binary_and.14', '$32return_value.15']
    return $32return_value.15                ['$32return_value.15']

DEBUG:numba.core.byteflow:bytecode dump:
>          0	NOP(arg=None, lineno=1045)
           2	LOAD_FAST(arg=0, lineno=1048)
           4	LOAD_CONST(arg=1, lineno=1048)
           6	BINARY_SUBSCR(arg=None, lineno=1048)
           8	LOAD_FAST(arg=0, lineno=1048)
          10	LOAD_CONST(arg=2, lineno=1048)
          12	BINARY_SUBSCR(arg=None, lineno=1048)
          14	COMPARE_OP(arg=0, lineno=1048)
          16	LOAD_FAST(arg=0, lineno=1048)
          18	LOAD_CONST(arg=1, lineno=1048)
          20	BINARY_SUBSCR(arg=None, lineno=1048)
          22	LOAD_FAST(arg=0, lineno=1048)
          24	LOAD_CONST(arg=3, lineno=1048)
          26	BINARY_SUBSCR(arg=None, lineno=1048)
          28	COMPARE_OP(arg=1, lineno=1048)
          30	BINARY_AND(arg=None, lineno=1048)
          32	RETURN_VALUE(arg=None, lineno=1048)
DEBUG:numba.core.byteflow:pending: deque([State(pc_initial=0 nstack_initial=0)])
DEBUG:numba.core.byteflow:stack: []
DEBUG:numba.core.byteflow:state.pc_initial: State(pc_initial=0 nstack_initial=0)
DEBUG:numba.core.byteflow:dispatch pc=0, inst=NOP(arg=None, lineno=1045)
DEBUG:numba.core.byteflow:stack []
DEBUG:numba.core.byteflow:dispatch pc=2, inst=LOAD_FAST(arg=0, lineno=1048)
DEBUG:numba.core.byteflow:stack []
DEBUG:numba.core.byteflow:dispatch pc=4, inst=LOAD_CONST(arg=1, lineno=1048)
DEBUG:numba.core.byteflow:stack ['$x2.0']
DEBUG:numba.core.byteflow:dispatch pc=6, inst=BINARY_SUBSCR(arg=None, lineno=1048)
DEBUG:numba.core.byteflow:stack ['$x2.0', '$const4.1']
DEBUG:numba.core.byteflow:dispatch pc=8, inst=LOAD_FAST(arg=0, lineno=1048)
DEBUG:numba.core.byteflow:stack ['$6binary_subscr.2']
DEBUG:numba.core.byteflow:dispatch pc=10, inst=LOAD_CONST(arg=2, lineno=1048)
DEBUG:numba.core.byteflow:stack ['$6binary_subscr.2', '$x8.3']
DEBUG:numba.core.byteflow:dispatch pc=12, inst=BINARY_SUBSCR(arg=None, lineno=1048)
DEBUG:numba.core.byteflow:stack ['$6binary_subscr.2', '$x8.3', '$const10.4']
DEBUG:numba.core.byteflow:dispatch pc=14, inst=COMPARE_OP(arg=0, lineno=1048)
DEBUG:numba.core.byteflow:stack ['$6binary_subscr.2', '$12binary_subscr.5']
DEBUG:numba.core.byteflow:dispatch pc=16, inst=LOAD_FAST(arg=0, lineno=1048)
DEBUG:numba.core.byteflow:stack ['$14compare_op.6']
DEBUG:numba.core.byteflow:dispatch pc=18, inst=LOAD_CONST(arg=1, lineno=1048)
DEBUG:numba.core.byteflow:stack ['$14compare_op.6', '$x16.7']
DEBUG:numba.core.byteflow:dispatch pc=20, inst=BINARY_SUBSCR(arg=None, lineno=1048)
DEBUG:numba.core.byteflow:stack ['$14compare_op.6', '$x16.7', '$const18.8']
DEBUG:numba.core.byteflow:dispatch pc=22, inst=LOAD_FAST(arg=0, lineno=1048)
DEBUG:numba.core.byteflow:stack ['$14compare_op.6', '$20binary_subscr.9']
DEBUG:numba.core.byteflow:dispatch pc=24, inst=LOAD_CONST(arg=3, lineno=1048)
DEBUG:numba.core.byteflow:stack ['$14compare_op.6', '$20binary_subscr.9', '$x22.10']
DEBUG:numba.core.byteflow:dispatch pc=26, inst=BINARY_SUBSCR(arg=None, lineno=1048)
DEBUG:numba.core.byteflow:stack ['$14compare_op.6', '$20binary_subscr.9', '$x22.10', '$const24.11']
DEBUG:numba.core.byteflow:dispatch pc=28, inst=COMPARE_OP(arg=1, lineno=1048)
DEBUG:numba.core.byteflow:stack ['$14compare_op.6', '$20binary_subscr.9', '$26binary_subscr.12']
DEBUG:numba.core.byteflow:dispatch pc=30, inst=BINARY_AND(arg=None, lineno=1048)
DEBUG:numba.core.byteflow:stack ['$14compare_op.6', '$28compare_op.13']
DEBUG:numba.core.byteflow:dispatch pc=32, inst=RETURN_VALUE(arg=None, lineno=1048)
DEBUG:numba.core.byteflow:stack ['$30binary_and.14']
DEBUG:numba.core.byteflow:end state. edges=[]
DEBUG:numba.core.byteflow:-------------------------Prune PHIs-------------------------
DEBUG:numba.core.byteflow:Used_phis: defaultdict(<class 'set'>, {State(pc_initial=0 nstack_initial=0): set()})
DEBUG:numba.core.byteflow:defmap: {}
DEBUG:numba.core.byteflow:phismap: defaultdict(<class 'set'>, {})
DEBUG:numba.core.byteflow:changing phismap: defaultdict(<class 'set'>, {})
DEBUG:numba.core.byteflow:keep phismap: {}
DEBUG:numba.core.byteflow:new_out: defaultdict(<class 'dict'>, {})
DEBUG:numba.core.byteflow:----------------------DONE Prune PHIs-----------------------
DEBUG:numba.core.byteflow:block_infos State(pc_initial=0 nstack_initial=0):
AdaptBlockInfo(insts=((0, {}), (2, {'res': '$x2.0'}), (4, {'res': '$const4.1'}), (6, {'index': '$const4.1', 'target': '$x2.0', 'res': '$6binary_subscr.2'}), (8, {'res': '$x8.3'}), (10, {'res': '$const10.4'}), (12, {'index': '$const10.4', 'target': '$x8.3', 'res': '$12binary_subscr.5'}), (14, {'lhs': '$6binary_subscr.2', 'rhs': '$12binary_subscr.5', 'res': '$14compare_op.6'}), (16, {'res': '$x16.7'}), (18, {'res': '$const18.8'}), (20, {'index': '$const18.8', 'target': '$x16.7', 'res': '$20binary_subscr.9'}), (22, {'res': '$x22.10'}), (24, {'res': '$const24.11'}), (26, {'index': '$const24.11', 'target': '$x22.10', 'res': '$26binary_subscr.12'}), (28, {'lhs': '$20binary_subscr.9', 'rhs': '$26binary_subscr.12', 'res': '$28compare_op.13'}), (30, {'lhs': '$14compare_op.6', 'rhs': '$28compare_op.13', 'res': '$30binary_and.14'}), (32, {'retval': '$30binary_and.14', 'castval': '$32return_value.15'})), outgoing_phis={}, blockstack=(), active_try_block=None, outgoing_edgepushed={})
DEBUG:numba.core.interpreter:label 0:
    x = arg(0, name=x)                       ['x']
    $const4.1 = const(int, 0)                ['$const4.1']
    $6binary_subscr.2 = getitem(value=x, index=$const4.1, fn=<built-in function getitem>) ['$6binary_subscr.2', '$const4.1', 'x']
    $const10.4 = const(int, -1)              ['$const10.4']
    $12binary_subscr.5 = getitem(value=x, index=$const10.4, fn=<built-in function getitem>) ['$12binary_subscr.5', '$const10.4', 'x']
    $14compare_op.6 = $6binary_subscr.2 < $12binary_subscr.5 ['$12binary_subscr.5', '$14compare_op.6', '$6binary_subscr.2']
    $const18.8 = const(int, 0)               ['$const18.8']
    $20binary_subscr.9 = getitem(value=x, index=$const18.8, fn=<built-in function getitem>) ['$20binary_subscr.9', '$const18.8', 'x']
    $const24.11 = const(int, 1)              ['$const24.11']
    $26binary_subscr.12 = getitem(value=x, index=$const24.11, fn=<built-in function getitem>) ['$26binary_subscr.12', '$const24.11', 'x']
    $28compare_op.13 = $20binary_subscr.9 <= $26binary_subscr.12 ['$20binary_subscr.9', '$26binary_subscr.12', '$28compare_op.13']
    $30binary_and.14 = $14compare_op.6 & $28compare_op.13 ['$14compare_op.6', '$28compare_op.13', '$30binary_and.14']
    $32return_value.15 = cast(value=$30binary_and.14) ['$30binary_and.14', '$32return_value.15']
    return $32return_value.15                ['$32return_value.15']

DEBUG:openai._base_client:Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'json_data': {'messages': [{'role': 'user', 'content': "Summarize the following chapter in one sentence: put it out but so you said you the way you articulated though i really like that you said a cog in the machine they lost a cog in the machine yo did you see my game of thrones breakdown of the of the um the the the leagues in battle right right now how i framed it with you no i gotta seriously because i don't i try not to watch none of this yeah yeah i know you said you was gonna do that too no i was i was that was a little test along with a question because you say you were stepping away from the whole negativity and all of that and yo the fact that is that everybody's saying that is dying right now i think about you so much but anyway uh but what i pointed out was how like it seems like we're starting to feel the after effects of the lack of innovation from that side so you saying that the deal they made with caffeine was like them selling them a machine but a cog was missing in the machine it looked the same from the outside"}], 'model': 'gpt-3.5-turbo-16k', 'max_tokens': 1000, 'n': 1, 'stream': False, 'temperature': 0.7}}
DEBUG:openai._base_client:Sending HTTP Request: POST https://api.openai.com/v1/chat/completions
DEBUG:httpcore.connection:connect_tcp.started host='api.openai.com' port=443 local_address=None timeout=None socket_options=None
DEBUG:httpcore.connection:connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000001E200028730>
DEBUG:httpcore.connection:start_tls.started ssl_context=<ssl.SSLContext object at 0x000001E247F42B40> server_hostname='api.openai.com' timeout=None
DEBUG:httpcore.connection:start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000001E200028700>
DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>
DEBUG:httpcore.http11:send_request_headers.complete
DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>
DEBUG:httpcore.http11:send_request_body.complete
DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>
DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Mon, 08 Jul 2024 06:03:36 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'openai-organization', b'machinekingsmedia'), (b'openai-processing-ms', b'771'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains'), (b'x-ratelimit-limit-requests', b'10000'), (b'x-ratelimit-limit-tokens', b'1000000'), (b'x-ratelimit-remaining-requests', b'9999'), (b'x-ratelimit-remaining-tokens', b'998751'), (b'x-ratelimit-reset-requests', b'6ms'), (b'x-ratelimit-reset-tokens', b'74ms'), (b'x-request-id', b'req_be62b86ef3cd988bf2db4c17ed15d323'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Set-Cookie', b'__cf_bm=fpS9ZnI0h1bs.NiUTx0qi2j96wUxd.rQ5dpj0qj3YOs-1720418616-1.0.1.1-uC3.B3MXkKoLG3jid85ZiwHRzZ0DpTUIPUa5Rj.Qf7ekNGIOWa.AYWuSvACjXg.J5GxQaHRJP6PqzyBcg8GQxQ; path=/; expires=Mon, 08-Jul-24 06:33:36 GMT; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), (b'Set-Cookie', b'_cfuvid=DXZHXfUeN1ROb1BxALn0HwG08ovAvEpt7Cpd.rE7TP4-1720418616763-0.0.1.1-604800000; path=/; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), (b'Server', b'cloudflare'), (b'CF-RAY', b'89fddf3d4e502f1e-IAD'), (b'Content-Encoding', b'br'), (b'alt-svc', b'h3=":443"; ma=86400')])
INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>
DEBUG:httpcore.http11:receive_response_body.complete
DEBUG:httpcore.http11:response_closed.started
DEBUG:httpcore.http11:response_closed.complete
DEBUG:openai._base_client:HTTP Response: POST https://api.openai.com/v1/chat/completions "200 OK" Headers([('date', 'Mon, 08 Jul 2024 06:03:36 GMT'), ('content-type', 'application/json'), ('transfer-encoding', 'chunked'), ('connection', 'keep-alive'), ('openai-organization', 'machinekingsmedia'), ('openai-processing-ms', '771'), ('openai-version', '2020-10-01'), ('strict-transport-security', 'max-age=31536000; includeSubDomains'), ('x-ratelimit-limit-requests', '10000'), ('x-ratelimit-limit-tokens', '1000000'), ('x-ratelimit-remaining-requests', '9999'), ('x-ratelimit-remaining-tokens', '998751'), ('x-ratelimit-reset-requests', '6ms'), ('x-ratelimit-reset-tokens', '74ms'), ('x-request-id', 'req_be62b86ef3cd988bf2db4c17ed15d323'), ('cf-cache-status', 'DYNAMIC'), ('set-cookie', '__cf_bm=fpS9ZnI0h1bs.NiUTx0qi2j96wUxd.rQ5dpj0qj3YOs-1720418616-1.0.1.1-uC3.B3MXkKoLG3jid85ZiwHRzZ0DpTUIPUa5Rj.Qf7ekNGIOWa.AYWuSvACjXg.J5GxQaHRJP6PqzyBcg8GQxQ; path=/; expires=Mon, 08-Jul-24 06:33:36 GMT; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), ('set-cookie', '_cfuvid=DXZHXfUeN1ROb1BxALn0HwG08ovAvEpt7Cpd.rE7TP4-1720418616763-0.0.1.1-604800000; path=/; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), ('server', 'cloudflare'), ('cf-ray', '89fddf3d4e502f1e-IAD'), ('content-encoding', 'br'), ('alt-svc', 'h3=":443"; ma=86400')])
DEBUG:openai._base_client:request_id: req_be62b86ef3cd988bf2db4c17ed15d323
DEBUG:openai._base_client:Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'json_data': {'messages': [{'role': 'user', 'content': "Summarize the following chapter in one sentence: but on the inside an important piece was missing they made it basically look like that i had nothing to do with what went down yeah they framed it like that yeah because you were a talent scout yeah apparently yeah they knew they knew that that's not the case and caffeine knows it's not the case you understand and it's between me and you of course as i was going through this for three years they was sitting in on the meetings with me and my lawyers in this so why would they sit if they think it was important if they they're gonna stand behind these niggas and make it look like a notice just trying to scam these niggas yeah when they realize like hold on my nigga this nigga really did all of this shit and y'all think about it if you could if you acquiring a company and its routes and how it became successful why wouldn't you keep the same team wow yeah yeah i see what you're saying i see what you're saying and i can see that fueling"}], 'model': 'gpt-3.5-turbo-16k', 'max_tokens': 1000, 'n': 1, 'stream': False, 'temperature': 0.7}}
DEBUG:openai._base_client:Sending HTTP Request: POST https://api.openai.com/v1/chat/completions
DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>
DEBUG:httpcore.http11:send_request_headers.complete
DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>
DEBUG:httpcore.http11:send_request_body.complete
DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>
DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Mon, 08 Jul 2024 06:03:38 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'openai-organization', b'machinekingsmedia'), (b'openai-processing-ms', b'675'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains'), (b'x-ratelimit-limit-requests', b'10000'), (b'x-ratelimit-limit-tokens', b'1000000'), (b'x-ratelimit-remaining-requests', b'9999'), (b'x-ratelimit-remaining-tokens', b'998750'), (b'x-ratelimit-reset-requests', b'6ms'), (b'x-ratelimit-reset-tokens', b'75ms'), (b'x-request-id', b'req_cb9c67e782ddaeabbaeb70f268dd8082'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'89fddf42fac92f1e-IAD'), (b'Content-Encoding', b'br'), (b'alt-svc', b'h3=":443"; ma=86400')])
INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>
DEBUG:httpcore.http11:receive_response_body.complete
DEBUG:httpcore.http11:response_closed.started
DEBUG:httpcore.http11:response_closed.complete
DEBUG:openai._base_client:HTTP Response: POST https://api.openai.com/v1/chat/completions "200 OK" Headers({'date': 'Mon, 08 Jul 2024 06:03:38 GMT', 'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'openai-organization': 'machinekingsmedia', 'openai-processing-ms': '675', 'openai-version': '2020-10-01', 'strict-transport-security': 'max-age=31536000; includeSubDomains', 'x-ratelimit-limit-requests': '10000', 'x-ratelimit-limit-tokens': '1000000', 'x-ratelimit-remaining-requests': '9999', 'x-ratelimit-remaining-tokens': '998750', 'x-ratelimit-reset-requests': '6ms', 'x-ratelimit-reset-tokens': '75ms', 'x-request-id': 'req_cb9c67e782ddaeabbaeb70f268dd8082', 'cf-cache-status': 'DYNAMIC', 'server': 'cloudflare', 'cf-ray': '89fddf42fac92f1e-IAD', 'content-encoding': 'br', 'alt-svc': 'h3=":443"; ma=86400'})
DEBUG:openai._base_client:request_id: req_cb9c67e782ddaeabbaeb70f268dd8082
DEBUG:openai._base_client:Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'json_data': {'messages': [{'role': 'user', 'content': "Summarize the following chapter in one sentence: anger amongst their ranks yeah and you gotta think about it sound like it was all a money move with them bro they tried to cut me out they owe me money you know what i'm saying yeah they were wrong and bro when they went to that situation they were like yo you know three ways is better than four yeah yeah the way they looked at it i wanted to give me the credit that i deserved you know i'm saying period yeah yeah my mentality was i'm gonna make this stupid motherfucker look like he's the man sacrifice my own shit you feel me yeah and their mentality was like nah we caught this nigga out we didn't pay and like no like it was just it was just stupid on smack's part bro like you know you know what business you go before i got there and you was falling off the cliff why would you revert to what you was because when i got there this is the highest you've ever been yeah they was with me not with them niggas yeah but but"}], 'model': 'gpt-3.5-turbo-16k', 'max_tokens': 1000, 'n': 1, 'stream': False, 'temperature': 0.7}}
DEBUG:openai._base_client:Sending HTTP Request: POST https://api.openai.com/v1/chat/completions
DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>
DEBUG:httpcore.http11:send_request_headers.complete
DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>
DEBUG:httpcore.http11:send_request_body.complete
DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>
DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Mon, 08 Jul 2024 06:03:39 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'openai-organization', b'machinekingsmedia'), (b'openai-processing-ms', b'944'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains'), (b'x-ratelimit-limit-requests', b'10000'), (b'x-ratelimit-limit-tokens', b'1000000'), (b'x-ratelimit-remaining-requests', b'9999'), (b'x-ratelimit-remaining-tokens', b'998755'), (b'x-ratelimit-reset-requests', b'6ms'), (b'x-ratelimit-reset-tokens', b'74ms'), (b'x-request-id', b'req_d93ccce1999cf71a1bcba8d945953fdc'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'89fddf4c9a8e2f1e-IAD'), (b'Content-Encoding', b'br'), (b'alt-svc', b'h3=":443"; ma=86400')])
INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>
DEBUG:httpcore.http11:receive_response_body.complete
DEBUG:httpcore.http11:response_closed.started
DEBUG:httpcore.http11:response_closed.complete
DEBUG:openai._base_client:HTTP Response: POST https://api.openai.com/v1/chat/completions "200 OK" Headers({'date': 'Mon, 08 Jul 2024 06:03:39 GMT', 'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'openai-organization': 'machinekingsmedia', 'openai-processing-ms': '944', 'openai-version': '2020-10-01', 'strict-transport-security': 'max-age=31536000; includeSubDomains', 'x-ratelimit-limit-requests': '10000', 'x-ratelimit-limit-tokens': '1000000', 'x-ratelimit-remaining-requests': '9999', 'x-ratelimit-remaining-tokens': '998755', 'x-ratelimit-reset-requests': '6ms', 'x-ratelimit-reset-tokens': '74ms', 'x-request-id': 'req_d93ccce1999cf71a1bcba8d945953fdc', 'cf-cache-status': 'DYNAMIC', 'server': 'cloudflare', 'cf-ray': '89fddf4c9a8e2f1e-IAD', 'content-encoding': 'br', 'alt-svc': 'h3=":443"; ma=86400'})
DEBUG:openai._base_client:request_id: req_d93ccce1999cf71a1bcba8d945953fdc
DEBUG:openai._base_client:Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'json_data': {'messages': [{'role': 'user', 'content': "Summarize the following chapter in one sentence: just like just like with uh just like with the dallas cowboys um yeah jerry jerry jerry jones thought that you know he could he could run it was the it was the logo the star whatever he could do it without jimmy johnson and um and they haven't been there since i was so happy they ain't advancing the playoff this year too so we can still run with that it is bro it's like when ufc got bored they didn't give it a thing of white in a way it was the cog he was the yeah but he was the face though he was he was the face i remember too when you were wrong i was one of the faces it was me and smack they asked me to fall back because i was out shining him yeah i remember that but smack wasn't really out there talking to the bloggers you used to go on angry fan and big up smack used to be in here i did so much shit that people don't even know that's what i'm saying like it's just it's just if you see them they don't even know what to do"}], 'model': 'gpt-3.5-turbo-16k', 'max_tokens': 1000, 'n': 1, 'stream': False, 'temperature': 0.7}}
DEBUG:openai._base_client:Sending HTTP Request: POST https://api.openai.com/v1/chat/completions
DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>
DEBUG:httpcore.http11:send_request_headers.complete
DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>
DEBUG:httpcore.http11:send_request_body.complete
DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>
DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Mon, 08 Jul 2024 06:03:40 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'openai-organization', b'machinekingsmedia'), (b'openai-processing-ms', b'601'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains'), (b'x-ratelimit-limit-requests', b'10000'), (b'x-ratelimit-limit-tokens', b'1000000'), (b'x-ratelimit-remaining-requests', b'9999'), (b'x-ratelimit-remaining-tokens', b'998752'), (b'x-ratelimit-reset-requests', b'6ms'), (b'x-ratelimit-reset-tokens', b'74ms'), (b'x-request-id', b'req_7ea6059e83b9d0b34763a5ab6ee5396f'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'89fddf53f84f2f1e-IAD'), (b'Content-Encoding', b'br'), (b'alt-svc', b'h3=":443"; ma=86400')])
INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>
DEBUG:httpcore.http11:receive_response_body.complete
DEBUG:httpcore.http11:response_closed.started
DEBUG:httpcore.http11:response_closed.complete
DEBUG:openai._base_client:HTTP Response: POST https://api.openai.com/v1/chat/completions "200 OK" Headers({'date': 'Mon, 08 Jul 2024 06:03:40 GMT', 'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'openai-organization': 'machinekingsmedia', 'openai-processing-ms': '601', 'openai-version': '2020-10-01', 'strict-transport-security': 'max-age=31536000; includeSubDomains', 'x-ratelimit-limit-requests': '10000', 'x-ratelimit-limit-tokens': '1000000', 'x-ratelimit-remaining-requests': '9999', 'x-ratelimit-remaining-tokens': '998752', 'x-ratelimit-reset-requests': '6ms', 'x-ratelimit-reset-tokens': '74ms', 'x-request-id': 'req_7ea6059e83b9d0b34763a5ab6ee5396f', 'cf-cache-status': 'DYNAMIC', 'server': 'cloudflare', 'cf-ray': '89fddf53f84f2f1e-IAD', 'content-encoding': 'br', 'alt-svc': 'h3=":443"; ma=86400'})
DEBUG:openai._base_client:request_id: req_7ea6059e83b9d0b34763a5ab6ee5396f
DEBUG:openai._base_client:Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'json_data': {'messages': [{'role': 'user', 'content': "Summarize the following chapter in one sentence: they don't know what to say they don't even like yo bro like even in business meetings like bro like i have ideas right now that'll that'll make the shit go crazy but i'd never fuck with you know the amount of money you would have to pay me just to resurrect your company yeah but and i've been hearing the adam rumors getting louder this week too about him retiring like as if he made he's done i told you that shit yeah you did tell me that but i didn't see an official announcement but everybody's talking about it no he was never gonna make an announcement he was just like he's gonna do the last event which was uh that's the full circle oh max out back on his last event okay he's not even there oh and he's just gonna do it he was like he's just tired the same shit i said tired of the culture you're putting up money he said he made enough money in the last event where he's just like yo nigga it's not worth the risk why would i put it"}], 'model': 'gpt-3.5-turbo-16k', 'max_tokens': 1000, 'n': 1, 'stream': False, 'temperature': 0.7}}
DEBUG:openai._base_client:Sending HTTP Request: POST https://api.openai.com/v1/chat/completions
DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>
DEBUG:httpcore.http11:send_request_headers.complete
DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>
DEBUG:httpcore.http11:send_request_body.complete
DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>
DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Mon, 08 Jul 2024 06:03:41 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'openai-organization', b'machinekingsmedia'), (b'openai-processing-ms', b'572'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains'), (b'x-ratelimit-limit-requests', b'10000'), (b'x-ratelimit-limit-tokens', b'1000000'), (b'x-ratelimit-remaining-requests', b'9999'), (b'x-ratelimit-remaining-tokens', b'998750'), (b'x-ratelimit-reset-requests', b'6ms'), (b'x-ratelimit-reset-tokens', b'75ms'), (b'x-request-id', b'req_9fab172d2ccbffef03c177fbfb12a015'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'89fddf590c262f1e-IAD'), (b'Content-Encoding', b'br'), (b'alt-svc', b'h3=":443"; ma=86400')])
INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>
DEBUG:httpcore.http11:receive_response_body.complete
DEBUG:httpcore.http11:response_closed.started
DEBUG:httpcore.http11:response_closed.complete
DEBUG:openai._base_client:HTTP Response: POST https://api.openai.com/v1/chat/completions "200 OK" Headers({'date': 'Mon, 08 Jul 2024 06:03:41 GMT', 'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'openai-organization': 'machinekingsmedia', 'openai-processing-ms': '572', 'openai-version': '2020-10-01', 'strict-transport-security': 'max-age=31536000; includeSubDomains', 'x-ratelimit-limit-requests': '10000', 'x-ratelimit-limit-tokens': '1000000', 'x-ratelimit-remaining-requests': '9999', 'x-ratelimit-remaining-tokens': '998750', 'x-ratelimit-reset-requests': '6ms', 'x-ratelimit-reset-tokens': '75ms', 'x-request-id': 'req_9fab172d2ccbffef03c177fbfb12a015', 'cf-cache-status': 'DYNAMIC', 'server': 'cloudflare', 'cf-ray': '89fddf590c262f1e-IAD', 'content-encoding': 'br', 'alt-svc': 'h3=":443"; ma=86400'})
DEBUG:openai._base_client:request_id: req_9fab172d2ccbffef03c177fbfb12a015
DEBUG:openai._base_client:Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'json_data': {'messages': [{'role': 'user', 'content': "Summarize the following chapter in one sentence: back in bro these i think the usd shop in sugar selling chicken out of his house wait a minute wait up i thought that let's get one wing straight was a store nigga it's in his crib he's selling chicken plates out of his house my nigga no no i saw somebody say congratulations and then i was okay he got a little store or whatever chicken wings out of his crib no i no all right look for the location yeah i'm going to check the location i'm gonna look that up i'm gonna look that up bro yo dog i'm not even bullshitting him he's selling when he don't have a crib he's selling him out of where he's staying out of the crib just selling wings the nigga got the shotgun sauce like it's crazy let's get one wing straight i i was i was like i was i i imagine like like since nunu got the teeth store i thought he was next up hold on my son is an animal dog it's okay it's okay it's all right"}], 'model': 'gpt-3.5-turbo-16k', 'max_tokens': 1000, 'n': 1, 'stream': False, 'temperature': 0.7}}
DEBUG:openai._base_client:Sending HTTP Request: POST https://api.openai.com/v1/chat/completions
DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>
DEBUG:httpcore.http11:send_request_headers.complete
DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>
DEBUG:httpcore.http11:send_request_body.complete
DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>
DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Mon, 08 Jul 2024 06:03:42 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'openai-organization', b'machinekingsmedia'), (b'openai-processing-ms', b'630'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains'), (b'x-ratelimit-limit-requests', b'10000'), (b'x-ratelimit-limit-tokens', b'1000000'), (b'x-ratelimit-remaining-requests', b'9999'), (b'x-ratelimit-remaining-tokens', b'998764'), (b'x-ratelimit-reset-requests', b'6ms'), (b'x-ratelimit-reset-tokens', b'74ms'), (b'x-request-id', b'req_84a509407ae1b188c8ce5d3eccc02165'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'89fddf61eb2c2f1e-IAD'), (b'Content-Encoding', b'br'), (b'alt-svc', b'h3=":443"; ma=86400')])
INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>
DEBUG:httpcore.http11:receive_response_body.complete
DEBUG:httpcore.http11:response_closed.started
DEBUG:httpcore.http11:response_closed.complete
DEBUG:openai._base_client:HTTP Response: POST https://api.openai.com/v1/chat/completions "200 OK" Headers({'date': 'Mon, 08 Jul 2024 06:03:42 GMT', 'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'openai-organization': 'machinekingsmedia', 'openai-processing-ms': '630', 'openai-version': '2020-10-01', 'strict-transport-security': 'max-age=31536000; includeSubDomains', 'x-ratelimit-limit-requests': '10000', 'x-ratelimit-limit-tokens': '1000000', 'x-ratelimit-remaining-requests': '9999', 'x-ratelimit-remaining-tokens': '998764', 'x-ratelimit-reset-requests': '6ms', 'x-ratelimit-reset-tokens': '74ms', 'x-request-id': 'req_84a509407ae1b188c8ce5d3eccc02165', 'cf-cache-status': 'DYNAMIC', 'server': 'cloudflare', 'cf-ray': '89fddf61eb2c2f1e-IAD', 'content-encoding': 'br', 'alt-svc': 'h3=":443"; ma=86400'})
DEBUG:openai._base_client:request_id: req_84a509407ae1b188c8ce5d3eccc02165
DEBUG:openai._base_client:Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'json_data': {'messages': [{'role': 'user', 'content': "Summarize the following chapter in one sentence: i'm gonna check up on that noise i might have to do a little something on that all you gotta do is be like hey shotgun where's the location at i'm trying to pull up to buy some wings no no i ain't gonna ask him now i'm gonna just do my little research that's very yo bro i know uh i'm a thousand percent sure niggas call me and i started laughing i said wait what are you talking about he's selling wings he's supposed to be delivering them but he he can't so he'll be having he'll have to come to the square to pick it up wow yeah and he turning down uh the eternal he turned down the easy battle because of loyalty to url as well like he he priced himself out rather than saying no to look bad in the culture bro he didn't price myself out he was told not to do it and he was like oh i'm scared of smack and now look at him bro them niggas are broke right now them niggas they're broke i don't know"}], 'model': 'gpt-3.5-turbo-16k', 'max_tokens': 1000, 'n': 1, 'stream': False, 'temperature': 0.7}}
DEBUG:openai._base_client:Sending HTTP Request: POST https://api.openai.com/v1/chat/completions
DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>
DEBUG:httpcore.http11:send_request_headers.complete
DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>
DEBUG:httpcore.http11:send_request_body.complete
DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>
DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Mon, 08 Jul 2024 06:03:43 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'openai-organization', b'machinekingsmedia'), (b'openai-processing-ms', b'435'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains'), (b'x-ratelimit-limit-requests', b'10000'), (b'x-ratelimit-limit-tokens', b'1000000'), (b'x-ratelimit-remaining-requests', b'9999'), (b'x-ratelimit-remaining-tokens', b'998760'), (b'x-ratelimit-reset-requests', b'6ms'), (b'x-ratelimit-reset-tokens', b'74ms'), (b'x-request-id', b'req_3fe59b8d143494a355add44571358d58'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'89fddf675fcb2f1e-IAD'), (b'Content-Encoding', b'br'), (b'alt-svc', b'h3=":443"; ma=86400')])
INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>
DEBUG:httpcore.http11:receive_response_body.complete
DEBUG:httpcore.http11:response_closed.started
DEBUG:httpcore.http11:response_closed.complete
DEBUG:openai._base_client:HTTP Response: POST https://api.openai.com/v1/chat/completions "200 OK" Headers({'date': 'Mon, 08 Jul 2024 06:03:43 GMT', 'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'openai-organization': 'machinekingsmedia', 'openai-processing-ms': '435', 'openai-version': '2020-10-01', 'strict-transport-security': 'max-age=31536000; includeSubDomains', 'x-ratelimit-limit-requests': '10000', 'x-ratelimit-limit-tokens': '1000000', 'x-ratelimit-remaining-requests': '9999', 'x-ratelimit-remaining-tokens': '998760', 'x-ratelimit-reset-requests': '6ms', 'x-ratelimit-reset-tokens': '74ms', 'x-request-id': 'req_3fe59b8d143494a355add44571358d58', 'cf-cache-status': 'DYNAMIC', 'server': 'cloudflare', 'cf-ray': '89fddf675fcb2f1e-IAD', 'content-encoding': 'br', 'alt-svc': 'h3=":443"; ma=86400'})
DEBUG:openai._base_client:request_id: req_3fe59b8d143494a355add44571358d58
DEBUG:openai._base_client:Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'json_data': {'messages': [{'role': 'user', 'content': "Summarize the following chapter in one sentence: how bad i was gonna eat i heard i heard that nigga tate rockets back at his well he's been back at his mama crib but he's in the basement just chilling like this they got me a cake though yeah and sent sending them to the bahamas or something right come on that's how you satisfy that's how you satisfy a certain section of society yeah you know just a bit of your style you could say that's how you that's how you satisfy them man and and also like like we discussed many many times before um the the blind loyalty to the brand is it's crazy and and i think it has a little bit to do with nostalgia um and the work that you put in and the glory days of old not what's going on right now but the glory days of old but but this app situation just seems like the worst possible thing for the culture as a whole and their mentality it's not even the app"}], 'model': 'gpt-3.5-turbo-16k', 'max_tokens': 1000, 'n': 1, 'stream': False, 'temperature': 0.7}}
DEBUG:openai._base_client:Sending HTTP Request: POST https://api.openai.com/v1/chat/completions
DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>
DEBUG:httpcore.http11:send_request_headers.complete
DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>
DEBUG:httpcore.http11:send_request_body.complete
DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>
DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Mon, 08 Jul 2024 06:03:44 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'openai-organization', b'machinekingsmedia'), (b'openai-processing-ms', b'693'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains'), (b'x-ratelimit-limit-requests', b'10000'), (b'x-ratelimit-limit-tokens', b'1000000'), (b'x-ratelimit-remaining-requests', b'9999'), (b'x-ratelimit-remaining-tokens', b'998773'), (b'x-ratelimit-reset-requests', b'6ms'), (b'x-ratelimit-reset-tokens', b'73ms'), (b'x-request-id', b'req_7c5cde6810368f6ea38588584f79e3b0'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'89fddf6aca922f1e-IAD'), (b'Content-Encoding', b'br'), (b'alt-svc', b'h3=":443"; ma=86400')])
INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>
DEBUG:httpcore.http11:receive_response_body.complete
DEBUG:httpcore.http11:response_closed.started
DEBUG:httpcore.http11:response_closed.complete
DEBUG:openai._base_client:HTTP Response: POST https://api.openai.com/v1/chat/completions "200 OK" Headers({'date': 'Mon, 08 Jul 2024 06:03:44 GMT', 'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'openai-organization': 'machinekingsmedia', 'openai-processing-ms': '693', 'openai-version': '2020-10-01', 'strict-transport-security': 'max-age=31536000; includeSubDomains', 'x-ratelimit-limit-requests': '10000', 'x-ratelimit-limit-tokens': '1000000', 'x-ratelimit-remaining-requests': '9999', 'x-ratelimit-remaining-tokens': '998773', 'x-ratelimit-reset-requests': '6ms', 'x-ratelimit-reset-tokens': '73ms', 'x-request-id': 'req_7c5cde6810368f6ea38588584f79e3b0', 'cf-cache-status': 'DYNAMIC', 'server': 'cloudflare', 'cf-ray': '89fddf6aca922f1e-IAD', 'content-encoding': 'br', 'alt-svc': 'h3=":443"; ma=86400'})
DEBUG:openai._base_client:request_id: req_7c5cde6810368f6ea38588584f79e3b0
DEBUG:openai._base_client:Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'json_data': {'messages': [{'role': 'user', 'content': "Summarize the following chapter in one sentence: situation no more bro it's the fact that they don't have no backing they're not gonna pay for shit ain't nobody gonna put up no money for the niggas but they they're making their own money but they're spending it on jewelry and cars and things of that nature that um if they don't have an office that might be a reflection of them not reinvested into the business itself um you don't even need all that son you don't even need to over half of this shit these niggas are so fucking dumb like yo bro it's the ad situation they they they probably gonna try to sell off what they can if they can at this point so but what can they sell off the the the the battles somebody used the library worth it you know they already have but i just don't see them cashing out in that situation but what's hold on let me let me get my let me get these niggas no problem all right"}], 'model': 'gpt-3.5-turbo-16k', 'max_tokens': 1000, 'n': 1, 'stream': False, 'temperature': 0.7}}
DEBUG:openai._base_client:Sending HTTP Request: POST https://api.openai.com/v1/chat/completions
DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>
DEBUG:httpcore.http11:send_request_headers.complete
DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>
DEBUG:httpcore.http11:send_request_body.complete
DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>
DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Mon, 08 Jul 2024 06:03:45 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'openai-organization', b'machinekingsmedia'), (b'openai-processing-ms', b'634'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains'), (b'x-ratelimit-limit-requests', b'10000'), (b'x-ratelimit-limit-tokens', b'1000000'), (b'x-ratelimit-remaining-requests', b'9999'), (b'x-ratelimit-remaining-tokens', b'998771'), (b'x-ratelimit-reset-requests', b'6ms'), (b'x-ratelimit-reset-tokens', b'73ms'), (b'x-request-id', b'req_24a614c618b9f7a2e7a43c940a7e0060'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'89fddf72b94e2f1e-IAD'), (b'Content-Encoding', b'br'), (b'alt-svc', b'h3=":443"; ma=86400')])
INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>
DEBUG:httpcore.http11:receive_response_body.complete
DEBUG:httpcore.http11:response_closed.started
DEBUG:httpcore.http11:response_closed.complete
DEBUG:openai._base_client:HTTP Response: POST https://api.openai.com/v1/chat/completions "200 OK" Headers({'date': 'Mon, 08 Jul 2024 06:03:45 GMT', 'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'openai-organization': 'machinekingsmedia', 'openai-processing-ms': '634', 'openai-version': '2020-10-01', 'strict-transport-security': 'max-age=31536000; includeSubDomains', 'x-ratelimit-limit-requests': '10000', 'x-ratelimit-limit-tokens': '1000000', 'x-ratelimit-remaining-requests': '9999', 'x-ratelimit-remaining-tokens': '998771', 'x-ratelimit-reset-requests': '6ms', 'x-ratelimit-reset-tokens': '73ms', 'x-request-id': 'req_24a614c618b9f7a2e7a43c940a7e0060', 'cf-cache-status': 'DYNAMIC', 'server': 'cloudflare', 'cf-ray': '89fddf72b94e2f1e-IAD', 'content-encoding': 'br', 'alt-svc': 'h3=":443"; ma=86400'})
DEBUG:openai._base_client:request_id: req_24a614c618b9f7a2e7a43c940a7e0060
DEBUG:openai._base_client:Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'json_data': {'messages': [{'role': 'user', 'content': "Summarize the following chapter in one sentence: and like if i'm if i'm a business dude and i purchased a company and you sold me everything but what made the engine run i'm gonna feel a way yeah when you're telling them like yo this shit got no legs he bullshitting you don't think they did their own research they was just like yo we don't want to get sued and miss this shit because then what happens is you look like y'all was in it with them to kick me out yeah so they was just trying to protect their own ass but they did their own research nigga they knew that i wasn't just a scout too many people saying the same shit like come on bro let me just minimize it bro i've seen their faces i i've seen with smack down when we did the depositions i've seen all that shit on those niggas is a dummy but you know you know so you can just tell like like there's one part like i can't really talk about the deposition shit because it's not whatever but i just recall"}], 'model': 'gpt-3.5-turbo-16k', 'max_tokens': 1000, 'n': 1, 'stream': False, 'temperature': 0.7}}
DEBUG:openai._base_client:Sending HTTP Request: POST https://api.openai.com/v1/chat/completions
DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>
DEBUG:httpcore.http11:send_request_headers.complete
DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>
DEBUG:httpcore.http11:send_request_body.complete
DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>
DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Mon, 08 Jul 2024 06:03:47 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'openai-organization', b'machinekingsmedia'), (b'openai-processing-ms', b'581'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains'), (b'x-ratelimit-limit-requests', b'10000'), (b'x-ratelimit-limit-tokens', b'1000000'), (b'x-ratelimit-remaining-requests', b'9999'), (b'x-ratelimit-remaining-tokens', b'998756'), (b'x-ratelimit-reset-requests', b'6ms'), (b'x-ratelimit-reset-tokens', b'74ms'), (b'x-request-id', b'req_5f7c779ac105f368c9fa08ac7e692871'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'89fddf7bfa312f1e-IAD'), (b'Content-Encoding', b'br'), (b'alt-svc', b'h3=":443"; ma=86400')])
INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>
DEBUG:httpcore.http11:receive_response_body.complete
DEBUG:httpcore.http11:response_closed.started
DEBUG:httpcore.http11:response_closed.complete
DEBUG:openai._base_client:HTTP Response: POST https://api.openai.com/v1/chat/completions "200 OK" Headers({'date': 'Mon, 08 Jul 2024 06:03:47 GMT', 'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'openai-organization': 'machinekingsmedia', 'openai-processing-ms': '581', 'openai-version': '2020-10-01', 'strict-transport-security': 'max-age=31536000; includeSubDomains', 'x-ratelimit-limit-requests': '10000', 'x-ratelimit-limit-tokens': '1000000', 'x-ratelimit-remaining-requests': '9999', 'x-ratelimit-remaining-tokens': '998756', 'x-ratelimit-reset-requests': '6ms', 'x-ratelimit-reset-tokens': '74ms', 'x-request-id': 'req_5f7c779ac105f368c9fa08ac7e692871', 'cf-cache-status': 'DYNAMIC', 'server': 'cloudflare', 'cf-ray': '89fddf7bfa312f1e-IAD', 'content-encoding': 'br', 'alt-svc': 'h3=":443"; ma=86400'})
DEBUG:openai._base_client:request_id: req_5f7c779ac105f368c9fa08ac7e692871
DEBUG:openai._base_client:Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'json_data': {'messages': [{'role': 'user', 'content': "Summarize the following chapter in one sentence: yeah don't don't speak on it if you can't speak on it now because i am recording yeah i'm recording now i was gonna say like i could just see it bro i could see it in his face specifically like i know troy and i know what moves troy i know what motivates him and it's it's all stupid shit like you probably say nigga shit you're in your mid-40s my nigga you worried about a chain and a fucking 300 shirt for 500 whatever the fuck you pay for those shirts yeah yeah you can see the way that they dress like the way they dress as of late like they are the accessorized chains yeah you can kind of see that yeah you don't see fucking jay-z doing that like and then there's something about money that's different like when you got money you don't act like you got money you know what i'm saying yeah like it's one of those situations where you kind of like"}], 'model': 'gpt-3.5-turbo-16k', 'max_tokens': 1000, 'n': 1, 'stream': False, 'temperature': 0.7}}
DEBUG:openai._base_client:Sending HTTP Request: POST https://api.openai.com/v1/chat/completions
DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>
DEBUG:httpcore.http11:send_request_headers.complete
DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>
DEBUG:httpcore.http11:send_request_body.complete
DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>
DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Mon, 08 Jul 2024 06:03:48 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'openai-organization', b'machinekingsmedia'), (b'openai-processing-ms', b'922'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains'), (b'x-ratelimit-limit-requests', b'10000'), (b'x-ratelimit-limit-tokens', b'1000000'), (b'x-ratelimit-remaining-requests', b'9999'), (b'x-ratelimit-remaining-tokens', b'998773'), (b'x-ratelimit-reset-requests', b'6ms'), (b'x-ratelimit-reset-tokens', b'73ms'), (b'x-request-id', b'req_ae4101a1b962cb09d95d235c1821b8f0'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'89fddf84ea2c2f1e-IAD'), (b'Content-Encoding', b'br'), (b'alt-svc', b'h3=":443"; ma=86400')])
INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>
DEBUG:httpcore.http11:receive_response_body.complete
DEBUG:httpcore.http11:response_closed.started
DEBUG:httpcore.http11:response_closed.complete
DEBUG:openai._base_client:HTTP Response: POST https://api.openai.com/v1/chat/completions "200 OK" Headers({'date': 'Mon, 08 Jul 2024 06:03:48 GMT', 'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'openai-organization': 'machinekingsmedia', 'openai-processing-ms': '922', 'openai-version': '2020-10-01', 'strict-transport-security': 'max-age=31536000; includeSubDomains', 'x-ratelimit-limit-requests': '10000', 'x-ratelimit-limit-tokens': '1000000', 'x-ratelimit-remaining-requests': '9999', 'x-ratelimit-remaining-tokens': '998773', 'x-ratelimit-reset-requests': '6ms', 'x-ratelimit-reset-tokens': '73ms', 'x-request-id': 'req_ae4101a1b962cb09d95d235c1821b8f0', 'cf-cache-status': 'DYNAMIC', 'server': 'cloudflare', 'cf-ray': '89fddf84ea2c2f1e-IAD', 'content-encoding': 'br', 'alt-svc': 'h3=":443"; ma=86400'})
DEBUG:openai._base_client:request_id: req_ae4101a1b962cb09d95d235c1821b8f0
DEBUG:openai._base_client:Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'json_data': {'messages': [{'role': 'user', 'content': "Summarize the following chapter in one sentence: they were niggas with real money and then you see a nigga like smack Logan they laugh at them because they don't even know how to like you got to show off you got but you don't show this shit off you don't how you live but but they they are a hip-hop culture though so there is that but their business as business owners that that work with these independent contractors it just seems as though from my observation of that the subscription money has got they've gotten so much reliable subscription money over the years that they kind of got comfortable and they just haven't they it seemed like they've even given up trying to innovate no i mean bro you understand bro like it's like who said who's not the best when you win a championship you gotta understand and that you gotta continue what you're doing in fact you gotta do more because not as expectation their only expectation was to to fuck bitches and wear"}], 'model': 'gpt-3.5-turbo-16k', 'max_tokens': 1000, 'n': 1, 'stream': False, 'temperature': 0.7}}
DEBUG:openai._base_client:Sending HTTP Request: POST https://api.openai.com/v1/chat/completions
DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>
DEBUG:httpcore.http11:send_request_headers.complete
DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>
DEBUG:httpcore.http11:send_request_body.complete
DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>
DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Mon, 08 Jul 2024 06:03:49 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'openai-organization', b'machinekingsmedia'), (b'openai-processing-ms', b'768'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains'), (b'x-ratelimit-limit-requests', b'10000'), (b'x-ratelimit-limit-tokens', b'1000000'), (b'x-ratelimit-remaining-requests', b'9999'), (b'x-ratelimit-remaining-tokens', b'998757'), (b'x-ratelimit-reset-requests', b'6ms'), (b'x-ratelimit-reset-tokens', b'74ms'), (b'x-request-id', b'req_68a8496f7260d9773fd1c0adb4fb154b'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'89fddf8c289c2f1e-IAD'), (b'Content-Encoding', b'br'), (b'alt-svc', b'h3=":443"; ma=86400')])
INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>
DEBUG:httpcore.http11:receive_response_body.complete
DEBUG:httpcore.http11:response_closed.started
DEBUG:httpcore.http11:response_closed.complete
DEBUG:openai._base_client:HTTP Response: POST https://api.openai.com/v1/chat/completions "200 OK" Headers({'date': 'Mon, 08 Jul 2024 06:03:49 GMT', 'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'openai-organization': 'machinekingsmedia', 'openai-processing-ms': '768', 'openai-version': '2020-10-01', 'strict-transport-security': 'max-age=31536000; includeSubDomains', 'x-ratelimit-limit-requests': '10000', 'x-ratelimit-limit-tokens': '1000000', 'x-ratelimit-remaining-requests': '9999', 'x-ratelimit-remaining-tokens': '998757', 'x-ratelimit-reset-requests': '6ms', 'x-ratelimit-reset-tokens': '74ms', 'x-request-id': 'req_68a8496f7260d9773fd1c0adb4fb154b', 'cf-cache-status': 'DYNAMIC', 'server': 'cloudflare', 'cf-ray': '89fddf8c289c2f1e-IAD', 'content-encoding': 'br', 'alt-svc': 'h3=":443"; ma=86400'})
DEBUG:openai._base_client:request_id: req_68a8496f7260d9773fd1c0adb4fb154b
DEBUG:openai._base_client:Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'json_data': {'messages': [{'role': 'user', 'content': "Summarize the following chapter in one sentence: jewelry that's they should yo i'm gonna use that that championship shit for that that um that cowboys that cowboys video that i'm gonna make yeah that's a fire analogy i'm gonna use that analogy for them winning a championship with you pretty much and etching themselves in history with you norbs etching themselves in history but then the expectations are there now and now that without you because yo yo i paid i called you tyrian lannister like you you fuck hold on you watch game of thrones i started through i never finished oh okay okay but you still should watch that shit though norbs because that shit that that's that that's a good ass blog that's a good ass blog and i'll watch i'll hit you back i'm gonna get this nigga started i'll watch it right now all right i'm gonna send it uh maybe like nothing i make is over 12 minutes so it's probably like eight or nine minutes maybe i'll send it to you okay i gotta put this nigga he follow me"}], 'model': 'gpt-3.5-turbo-16k', 'max_tokens': 1000, 'n': 1, 'stream': False, 'temperature': 0.7}}
DEBUG:openai._base_client:Sending HTTP Request: POST https://api.openai.com/v1/chat/completions
DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>
DEBUG:httpcore.http11:send_request_headers.complete
DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>
DEBUG:httpcore.http11:send_request_body.complete
DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>
DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Mon, 08 Jul 2024 06:03:50 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'openai-organization', b'machinekingsmedia'), (b'openai-processing-ms', b'589'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains'), (b'x-ratelimit-limit-requests', b'10000'), (b'x-ratelimit-limit-tokens', b'1000000'), (b'x-ratelimit-remaining-requests', b'9999'), (b'x-ratelimit-remaining-tokens', b'998748'), (b'x-ratelimit-reset-requests', b'6ms'), (b'x-ratelimit-reset-tokens', b'75ms'), (b'x-request-id', b'req_28ed8e4d98103c1856b8b671cc3f1310'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'89fddf91addb2f1e-IAD'), (b'Content-Encoding', b'br'), (b'alt-svc', b'h3=":443"; ma=86400')])
INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>
DEBUG:httpcore.http11:receive_response_body.complete
DEBUG:httpcore.http11:response_closed.started
DEBUG:httpcore.http11:response_closed.complete
DEBUG:openai._base_client:HTTP Response: POST https://api.openai.com/v1/chat/completions "200 OK" Headers({'date': 'Mon, 08 Jul 2024 06:03:50 GMT', 'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'openai-organization': 'machinekingsmedia', 'openai-processing-ms': '589', 'openai-version': '2020-10-01', 'strict-transport-security': 'max-age=31536000; includeSubDomains', 'x-ratelimit-limit-requests': '10000', 'x-ratelimit-limit-tokens': '1000000', 'x-ratelimit-remaining-requests': '9999', 'x-ratelimit-remaining-tokens': '998748', 'x-ratelimit-reset-requests': '6ms', 'x-ratelimit-reset-tokens': '75ms', 'x-request-id': 'req_28ed8e4d98103c1856b8b671cc3f1310', 'cf-cache-status': 'DYNAMIC', 'server': 'cloudflare', 'cf-ray': '89fddf91addb2f1e-IAD', 'content-encoding': 'br', 'alt-svc': 'h3=":443"; ma=86400'})
DEBUG:openai._base_client:request_id: req_28ed8e4d98103c1856b8b671cc3f1310
DEBUG:openai._base_client:Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'json_data': {'messages': [{'role': 'user', 'content': "Summarize the following chapter in one sentence: around all right my my baby girl i'm gonna be putting her to bed at any moment so um if not then i'll hit you later on tonight all right"}], 'model': 'gpt-3.5-turbo-16k', 'max_tokens': 1000, 'n': 1, 'stream': False, 'temperature': 0.7}}
DEBUG:openai._base_client:Sending HTTP Request: POST https://api.openai.com/v1/chat/completions
DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>
DEBUG:httpcore.http11:send_request_headers.complete
DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>
DEBUG:httpcore.http11:send_request_body.complete
DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>
DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Mon, 08 Jul 2024 06:03:50 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'openai-organization', b'machinekingsmedia'), (b'openai-processing-ms', b'448'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains'), (b'x-ratelimit-limit-requests', b'10000'), (b'x-ratelimit-limit-tokens', b'1000000'), (b'x-ratelimit-remaining-requests', b'9999'), (b'x-ratelimit-remaining-tokens', b'998951'), (b'x-ratelimit-reset-requests', b'6ms'), (b'x-ratelimit-reset-tokens', b'62ms'), (b'x-request-id', b'req_1992d151eace8e58f57e965625d2f5d4'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'89fddf96ca6c2f1e-IAD'), (b'Content-Encoding', b'br'), (b'alt-svc', b'h3=":443"; ma=86400')])
INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>
DEBUG:httpcore.http11:receive_response_body.complete
DEBUG:httpcore.http11:response_closed.started
DEBUG:httpcore.http11:response_closed.complete
DEBUG:openai._base_client:HTTP Response: POST https://api.openai.com/v1/chat/completions "200 OK" Headers({'date': 'Mon, 08 Jul 2024 06:03:50 GMT', 'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'openai-organization': 'machinekingsmedia', 'openai-processing-ms': '448', 'openai-version': '2020-10-01', 'strict-transport-security': 'max-age=31536000; includeSubDomains', 'x-ratelimit-limit-requests': '10000', 'x-ratelimit-limit-tokens': '1000000', 'x-ratelimit-remaining-requests': '9999', 'x-ratelimit-remaining-tokens': '998951', 'x-ratelimit-reset-requests': '6ms', 'x-ratelimit-reset-tokens': '62ms', 'x-request-id': 'req_1992d151eace8e58f57e965625d2f5d4', 'cf-cache-status': 'DYNAMIC', 'server': 'cloudflare', 'cf-ray': '89fddf96ca6c2f1e-IAD', 'content-encoding': 'br', 'alt-svc': 'h3=":443"; ma=86400'})
DEBUG:openai._base_client:request_id: req_1992d151eace8e58f57e965625d2f5d4
DEBUG:openai._base_client:Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'json_data': {'messages': [{'role': 'user', 'content': "Provide a brief summary of the following transcript: File: 240204_1940.mp3\nRecording Date: 2024-02-04\nDuration: 00:13:13.60\nTranscription Date: 2024-07-07 06:17:21\n\n1\n00:00:00,000 --> 00:00:04,800\nput it out but so you said you the way you articulated though i really like that you said\n\n2\n00:00:04,800 --> 00:00:09,120\na cog in the machine they lost a cog in the machine yo did you see my game of thrones\n\n3\n00:00:09,120 --> 00:00:15,600\nbreakdown of the of the um the the the leagues in battle right right now how i framed it with you\n\n4\n00:00:16,719 --> 00:00:20,639\nno i gotta seriously because i don't i try not to watch none of this yeah yeah i know you said\n\n5\n00:00:20,639 --> 00:00:24,879\nyou was gonna do that too no i was i was that was a little test along with a question because\n\n6\n00:00:24,879 --> 00:00:30,959\nyou say you were stepping away from the whole negativity and all of that and yo the fact that\n\n7\n00:00:30,959 --> 00:00:37,840\nis that everybody's saying that is dying right now i think about you so much but anyway uh but\n\n8\n00:00:37,840 --> 00:00:44,880\nwhat i pointed out was how like it seems like we're starting to feel the after effects of the\n\n9\n00:00:44,880 --> 00:00:51,919\nlack of innovation from that side so you saying that the deal they made with caffeine was like\n\n10\n00:00:51,919 --> 00:00:56,959\nthem selling them a machine but a cog was missing in the machine it looked the same from the outside\n\n11\n00:00:56,959 --> 00:01:02,000\nbut on the inside an important piece was missing they made it basically look like that i had\n\n12\n00:01:02,000 --> 00:01:07,599\nnothing to do with what went down yeah they framed it like that yeah because you were a talent scout\n\n13\n00:01:08,320 --> 00:01:16,800\nyeah apparently yeah they knew they knew that that's not the case and caffeine knows it's not\n\n14\n00:01:16,800 --> 00:01:22,959\nthe case you understand and it's between me and you of course as i was going through this for three\n\n15\n00:01:22,959 --> 00:01:28,720\nyears they was sitting in on the meetings with me and my lawyers in this so why would they sit\n\n16\n00:01:28,720 --> 00:01:32,320\nif they think it was important if they they're gonna stand behind these niggas and make it look\n\n17\n00:01:32,320 --> 00:01:37,040\nlike a notice just trying to scam these niggas yeah when they realize like hold on my nigga\n\n18\n00:01:38,320 --> 00:01:43,680\nthis nigga really did all of this shit and y'all think about it if you could if you acquiring a\n\n19\n00:01:43,680 --> 00:01:50,000\ncompany and its routes and how it became successful why wouldn't you keep the same team\n\n20\n00:01:51,919 --> 00:01:57,599\nwow yeah yeah i see what you're saying i see what you're saying and i can see that fueling\n\n21\n00:01:57,599 --> 00:02:03,839\nanger amongst their ranks yeah and you gotta think about it sound like it was all a money\n\n22\n00:02:03,839 --> 00:02:07,680\nmove with them bro they tried to cut me out they owe me money you know what i'm saying yeah\n\n23\n00:02:07,680 --> 00:02:14,559\nthey were wrong and bro when they went to that situation they were like yo you know three ways\n\n24\n00:02:14,559 --> 00:02:20,559\nis better than four yeah yeah the way they looked at it i wanted to give me the credit that i\n\n25\n00:02:20,559 --> 00:02:26,720\ndeserved you know i'm saying period yeah yeah my mentality was i'm gonna make this stupid\n\n26\n00:02:26,720 --> 00:02:33,839\nmotherfucker look like he's the man sacrifice my own shit you feel me yeah and their mentality\n\n27\n00:02:33,839 --> 00:02:38,880\nwas like nah we caught this nigga out we didn't pay and like no like it was just it was just\n\n28\n00:02:38,880 --> 00:02:44,399\nstupid on smack's part bro like you know you know what business you go before i got there\n\n29\n00:02:44,399 --> 00:02:50,639\nand you was falling off the cliff why would you revert to what you was because when i got there\n\n30\n00:02:50,639 --> 00:02:58,559\nthis is the highest you've ever been yeah they was with me not with them niggas yeah but but\n\n31\n00:02:58,559 --> 00:03:07,039\njust like just like with uh "}], 'model': 'gpt-3.5-turbo-16k', 'max_tokens': 1000, 'n': 1, 'stream': False, 'temperature': 0.7}}
DEBUG:openai._base_client:Sending HTTP Request: POST https://api.openai.com/v1/chat/completions
DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>
DEBUG:httpcore.http11:send_request_headers.complete
DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>
DEBUG:httpcore.http11:send_request_body.complete
DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>
DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Mon, 08 Jul 2024 06:03:53 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'openai-organization', b'machinekingsmedia'), (b'openai-processing-ms', b'1979'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains'), (b'x-ratelimit-limit-requests', b'10000'), (b'x-ratelimit-limit-tokens', b'1000000'), (b'x-ratelimit-remaining-requests', b'9999'), (b'x-ratelimit-remaining-tokens', b'997984'), (b'x-ratelimit-reset-requests', b'6ms'), (b'x-ratelimit-reset-tokens', b'120ms'), (b'x-request-id', b'req_8b14853abd11f45366c2e253470c3a0f'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'89fddf9b3e732f1e-IAD'), (b'Content-Encoding', b'br'), (b'alt-svc', b'h3=":443"; ma=86400')])
INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>
DEBUG:httpcore.http11:receive_response_body.complete
DEBUG:httpcore.http11:response_closed.started
DEBUG:httpcore.http11:response_closed.complete
DEBUG:openai._base_client:HTTP Response: POST https://api.openai.com/v1/chat/completions "200 OK" Headers({'date': 'Mon, 08 Jul 2024 06:03:53 GMT', 'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'openai-organization': 'machinekingsmedia', 'openai-processing-ms': '1979', 'openai-version': '2020-10-01', 'strict-transport-security': 'max-age=31536000; includeSubDomains', 'x-ratelimit-limit-requests': '10000', 'x-ratelimit-limit-tokens': '1000000', 'x-ratelimit-remaining-requests': '9999', 'x-ratelimit-remaining-tokens': '997984', 'x-ratelimit-reset-requests': '6ms', 'x-ratelimit-reset-tokens': '120ms', 'x-request-id': 'req_8b14853abd11f45366c2e253470c3a0f', 'cf-cache-status': 'DYNAMIC', 'server': 'cloudflare', 'cf-ray': '89fddf9b3e732f1e-IAD', 'content-encoding': 'br', 'alt-svc': 'h3=":443"; ma=86400'})
DEBUG:openai._base_client:request_id: req_8b14853abd11f45366c2e253470c3a0f
DEBUG:numba.core.byteflow:bytecode dump:
>          0	NOP(arg=None, lineno=1141)
           2	LOAD_FAST(arg=0, lineno=1144)
           4	LOAD_CONST(arg=1, lineno=1144)
           6	BINARY_SUBSCR(arg=None, lineno=1144)
           8	STORE_FAST(arg=3, lineno=1144)
          10	LOAD_FAST(arg=1, lineno=1145)
          12	UNARY_NEGATIVE(arg=None, lineno=1145)
          14	LOAD_FAST(arg=3, lineno=1145)
          16	DUP_TOP(arg=None, lineno=1145)
          18	ROT_THREE(arg=None, lineno=1145)
          20	COMPARE_OP(arg=1, lineno=1145)
          22	POP_JUMP_IF_FALSE(arg=32, lineno=1145)
          24	LOAD_FAST(arg=1, lineno=1145)
          26	COMPARE_OP(arg=1, lineno=1145)
          28	POP_JUMP_IF_FALSE(arg=40, lineno=1145)
          30	JUMP_FORWARD(arg=4, lineno=1145)
>         32	POP_TOP(arg=None, lineno=1145)
          34	JUMP_FORWARD(arg=4, lineno=1145)
>         36	LOAD_CONST(arg=1, lineno=1146)
          38	STORE_FAST(arg=3, lineno=1146)
>         40	LOAD_FAST(arg=0, lineno=1148)
          42	LOAD_CONST(arg=2, lineno=1148)
          44	BINARY_SUBSCR(arg=None, lineno=1148)
          46	STORE_FAST(arg=4, lineno=1148)
          48	LOAD_FAST(arg=1, lineno=1149)
          50	UNARY_NEGATIVE(arg=None, lineno=1149)
          52	LOAD_FAST(arg=4, lineno=1149)
          54	DUP_TOP(arg=None, lineno=1149)
          56	ROT_THREE(arg=None, lineno=1149)
          58	COMPARE_OP(arg=1, lineno=1149)
          60	POP_JUMP_IF_FALSE(arg=70, lineno=1149)
          62	LOAD_FAST(arg=1, lineno=1149)
          64	COMPARE_OP(arg=1, lineno=1149)
          66	POP_JUMP_IF_FALSE(arg=78, lineno=1149)
          68	JUMP_FORWARD(arg=4, lineno=1149)
>         70	POP_TOP(arg=None, lineno=1149)
          72	JUMP_FORWARD(arg=4, lineno=1149)
>         74	LOAD_CONST(arg=1, lineno=1150)
          76	STORE_FAST(arg=4, lineno=1150)
>         78	LOAD_FAST(arg=2, lineno=1152)
          80	POP_JUMP_IF_FALSE(arg=102, lineno=1152)
          82	LOAD_GLOBAL(arg=0, lineno=1153)
          84	LOAD_METHOD(arg=1, lineno=1153)
          86	LOAD_FAST(arg=3, lineno=1153)
          88	CALL_METHOD(arg=1, lineno=1153)
          90	LOAD_GLOBAL(arg=0, lineno=1153)
          92	LOAD_METHOD(arg=1, lineno=1153)
          94	LOAD_FAST(arg=4, lineno=1153)
          96	CALL_METHOD(arg=1, lineno=1153)
          98	COMPARE_OP(arg=3, lineno=1153)
         100	RETURN_VALUE(arg=None, lineno=1153)
>        102	LOAD_GLOBAL(arg=0, lineno=1155)
         104	LOAD_METHOD(arg=2, lineno=1155)
         106	LOAD_FAST(arg=3, lineno=1155)
         108	CALL_METHOD(arg=1, lineno=1155)
         110	LOAD_GLOBAL(arg=0, lineno=1155)
         112	LOAD_METHOD(arg=2, lineno=1155)
         114	LOAD_FAST(arg=4, lineno=1155)
         116	CALL_METHOD(arg=1, lineno=1155)
         118	COMPARE_OP(arg=3, lineno=1155)
         120	RETURN_VALUE(arg=None, lineno=1155)
         122	LOAD_CONST(arg=3, lineno=1155)
         124	RETURN_VALUE(arg=None, lineno=1155)
DEBUG:numba.core.byteflow:pending: deque([State(pc_initial=0 nstack_initial=0)])
DEBUG:numba.core.byteflow:stack: []
DEBUG:numba.core.byteflow:state.pc_initial: State(pc_initial=0 nstack_initial=0)
DEBUG:numba.core.byteflow:dispatch pc=0, inst=NOP(arg=None, lineno=1141)
DEBUG:numba.core.byteflow:stack []
DEBUG:numba.core.byteflow:dispatch pc=2, inst=LOAD_FAST(arg=0, lineno=1144)
DEBUG:numba.core.byteflow:stack []
DEBUG:numba.core.byteflow:dispatch pc=4, inst=LOAD_CONST(arg=1, lineno=1144)
DEBUG:numba.core.byteflow:stack ['$x2.0']
DEBUG:numba.core.byteflow:dispatch pc=6, inst=BINARY_SUBSCR(arg=None, lineno=1144)
DEBUG:numba.core.byteflow:stack ['$x2.0', '$const4.1']
DEBUG:numba.core.byteflow:dispatch pc=8, inst=STORE_FAST(arg=3, lineno=1144)
DEBUG:numba.core.byteflow:stack ['$6binary_subscr.2']
DEBUG:numba.core.byteflow:dispatch pc=10, inst=LOAD_FAST(arg=1, lineno=1145)
DEBUG:numba.core.byteflow:stack []
DEBUG:numba.core.byteflow:dispatch pc=12, inst=UNARY_NEGATIVE(arg=None, lineno=1145)
DEBUG:numba.core.byteflow:stack ['$threshold10.3']
DEBUG:numba.core.byteflow:dispatch pc=14, inst=LOAD_FAST(arg=3, lineno=1145)
DEBUG:numba.core.byteflow:stack ['$12unary_negative.4']
DEBUG:numba.core.byteflow:dispatch pc=16, inst=DUP_TOP(arg=None, lineno=1145)
DEBUG:numba.core.byteflow:stack ['$12unary_negative.4', '$x014.5']
DEBUG:numba.core.byteflow:dispatch pc=18, inst=ROT_THREE(arg=None, lineno=1145)
DEBUG:numba.core.byteflow:stack ['$12unary_negative.4', '$x014.5', '$16dup_top.6']
DEBUG:numba.core.byteflow:dispatch pc=20, inst=COMPARE_OP(arg=1, lineno=1145)
DEBUG:numba.core.byteflow:stack ['$16dup_top.6', '$12unary_negative.4', '$x014.5']
DEBUG:numba.core.byteflow:dispatch pc=22, inst=POP_JUMP_IF_FALSE(arg=32, lineno=1145)
DEBUG:numba.core.byteflow:stack ['$16dup_top.6', '$20compare_op.7']
DEBUG:numba.core.byteflow:end state. edges=[Edge(pc=24, stack=('$16dup_top.6',), blockstack=(), npush=0), Edge(pc=32, stack=('$16dup_top.6',), blockstack=(), npush=0)]
DEBUG:numba.core.byteflow:pending: deque([State(pc_initial=24 nstack_initial=1), State(pc_initial=32 nstack_initial=1)])
DEBUG:numba.core.byteflow:stack: ['$phi24.0']
DEBUG:numba.core.byteflow:state.pc_initial: State(pc_initial=24 nstack_initial=1)
DEBUG:numba.core.byteflow:dispatch pc=24, inst=LOAD_FAST(arg=1, lineno=1145)
DEBUG:numba.core.byteflow:stack ['$phi24.0']
DEBUG:numba.core.byteflow:dispatch pc=26, inst=COMPARE_OP(arg=1, lineno=1145)
DEBUG:numba.core.byteflow:stack ['$phi24.0', '$threshold24.1']
DEBUG:numba.core.byteflow:dispatch pc=28, inst=POP_JUMP_IF_FALSE(arg=40, lineno=1145)
DEBUG:numba.core.byteflow:stack ['$26compare_op.2']
DEBUG:numba.core.byteflow:end state. edges=[Edge(pc=30, stack=(), blockstack=(), npush=0), Edge(pc=40, stack=(), blockstack=(), npush=0)]
DEBUG:numba.core.byteflow:pending: deque([State(pc_initial=32 nstack_initial=1), State(pc_initial=30 nstack_initial=0), State(pc_initial=40 nstack_initial=0)])
DEBUG:numba.core.byteflow:stack: ['$phi32.0']
DEBUG:numba.core.byteflow:state.pc_initial: State(pc_initial=32 nstack_initial=1)
DEBUG:numba.core.byteflow:dispatch pc=32, inst=POP_TOP(arg=None, lineno=1145)
DEBUG:numba.core.byteflow:stack ['$phi32.0']
DEBUG:numba.core.byteflow:dispatch pc=34, inst=JUMP_FORWARD(arg=4, lineno=1145)
DEBUG:numba.core.byteflow:stack []
DEBUG:numba.core.byteflow:end state. edges=[Edge(pc=40, stack=(), blockstack=(), npush=0)]
DEBUG:numba.core.byteflow:pending: deque([State(pc_initial=30 nstack_initial=0), State(pc_initial=40 nstack_initial=0), State(pc_initial=40 nstack_initial=0)])
DEBUG:numba.core.byteflow:stack: []
DEBUG:numba.core.byteflow:state.pc_initial: State(pc_initial=30 nstack_initial=0)
DEBUG:numba.core.byteflow:dispatch pc=30, inst=JUMP_FORWARD(arg=4, lineno=1145)
DEBUG:numba.core.byteflow:stack []
DEBUG:numba.core.byteflow:end state. edges=[Edge(pc=36, stack=(), blockstack=(), npush=0)]
DEBUG:numba.core.byteflow:pending: deque([State(pc_initial=40 nstack_initial=0), State(pc_initial=40 nstack_initial=0), State(pc_initial=36 nstack_initial=0)])
DEBUG:numba.core.byteflow:stack: []
DEBUG:numba.core.byteflow:state.pc_initial: State(pc_initial=40 nstack_initial=0)
DEBUG:numba.core.byteflow:dispatch pc=40, inst=LOAD_FAST(arg=0, lineno=1148)
DEBUG:numba.core.byteflow:stack []
DEBUG:numba.core.byteflow:dispatch pc=42, inst=LOAD_CONST(arg=2, lineno=1148)
DEBUG:numba.core.byteflow:stack ['$x40.0']
DEBUG:numba.core.byteflow:dispatch pc=44, inst=BINARY_SUBSCR(arg=None, lineno=1148)
DEBUG:numba.core.byteflow:stack ['$x40.0', '$const42.1']
DEBUG:numba.core.byteflow:dispatch pc=46, inst=STORE_FAST(arg=4, lineno=1148)
DEBUG:numba.core.byteflow:stack ['$44binary_subscr.2']
DEBUG:numba.core.byteflow:dispatch pc=48, inst=LOAD_FAST(arg=1, lineno=1149)
DEBUG:numba.core.byteflow:stack []
DEBUG:numba.core.byteflow:dispatch pc=50, inst=UNARY_NEGATIVE(arg=None, lineno=1149)
DEBUG:numba.core.byteflow:stack ['$threshold48.3']
DEBUG:numba.core.byteflow:dispatch pc=52, inst=LOAD_FAST(arg=4, lineno=1149)
DEBUG:numba.core.byteflow:stack ['$50unary_negative.4']
DEBUG:numba.core.byteflow:dispatch pc=54, inst=DUP_TOP(arg=None, lineno=1149)
DEBUG:numba.core.byteflow:stack ['$50unary_negative.4', '$x152.5']
DEBUG:numba.core.byteflow:dispatch pc=56, inst=ROT_THREE(arg=None, lineno=1149)
DEBUG:numba.core.byteflow:stack ['$50unary_negative.4', '$x152.5', '$54dup_top.6']
DEBUG:numba.core.byteflow:dispatch pc=58, inst=COMPARE_OP(arg=1, lineno=1149)
DEBUG:numba.core.byteflow:stack ['$54dup_top.6', '$50unary_negative.4', '$x152.5']
DEBUG:numba.core.byteflow:dispatch pc=60, inst=POP_JUMP_IF_FALSE(arg=70, lineno=1149)
DEBUG:numba.core.byteflow:stack ['$54dup_top.6', '$58compare_op.7']
DEBUG:numba.core.byteflow:end state. edges=[Edge(pc=62, stack=('$54dup_top.6',), blockstack=(), npush=0), Edge(pc=70, stack=('$54dup_top.6',), blockstack=(), npush=0)]
DEBUG:numba.core.byteflow:pending: deque([State(pc_initial=40 nstack_initial=0), State(pc_initial=36 nstack_initial=0), State(pc_initial=62 nstack_initial=1), State(pc_initial=70 nstack_initial=1)])
DEBUG:numba.core.byteflow:pending: deque([State(pc_initial=36 nstack_initial=0), State(pc_initial=62 nstack_initial=1), State(pc_initial=70 nstack_initial=1)])
DEBUG:numba.core.byteflow:stack: []
DEBUG:numba.core.byteflow:state.pc_initial: State(pc_initial=36 nstack_initial=0)
DEBUG:numba.core.byteflow:dispatch pc=36, inst=LOAD_CONST(arg=1, lineno=1146)
DEBUG:numba.core.byteflow:stack []
DEBUG:numba.core.byteflow:dispatch pc=38, inst=STORE_FAST(arg=3, lineno=1146)
DEBUG:numba.core.byteflow:stack ['$const36.0']
DEBUG:numba.core.byteflow:end state. edges=[Edge(pc=40, stack=(), blockstack=(), npush=0)]
DEBUG:numba.core.byteflow:pending: deque([State(pc_initial=62 nstack_initial=1), State(pc_initial=70 nstack_initial=1), State(pc_initial=40 nstack_initial=0)])
DEBUG:numba.core.byteflow:stack: ['$phi62.0']
DEBUG:numba.core.byteflow:state.pc_initial: State(pc_initial=62 nstack_initial=1)
DEBUG:numba.core.byteflow:dispatch pc=62, inst=LOAD_FAST(arg=1, lineno=1149)
DEBUG:numba.core.byteflow:stack ['$phi62.0']
DEBUG:numba.core.byteflow:dispatch pc=64, inst=COMPARE_OP(arg=1, lineno=1149)
DEBUG:numba.core.byteflow:stack ['$phi62.0', '$threshold62.1']
DEBUG:numba.core.byteflow:dispatch pc=66, inst=POP_JUMP_IF_FALSE(arg=78, lineno=1149)
DEBUG:numba.core.byteflow:stack ['$64compare_op.2']
DEBUG:numba.core.byteflow:end state. edges=[Edge(pc=68, stack=(), blockstack=(), npush=0), Edge(pc=78, stack=(), blockstack=(), npush=0)]
DEBUG:numba.core.byteflow:pending: deque([State(pc_initial=70 nstack_initial=1), State(pc_initial=40 nstack_initial=0), State(pc_initial=68 nstack_initial=0), State(pc_initial=78 nstack_initial=0)])
DEBUG:numba.core.byteflow:stack: ['$phi70.0']
DEBUG:numba.core.byteflow:state.pc_initial: State(pc_initial=70 nstack_initial=1)
DEBUG:numba.core.byteflow:dispatch pc=70, inst=POP_TOP(arg=None, lineno=1149)
DEBUG:numba.core.byteflow:stack ['$phi70.0']
DEBUG:numba.core.byteflow:dispatch pc=72, inst=JUMP_FORWARD(arg=4, lineno=1149)
DEBUG:numba.core.byteflow:stack []
DEBUG:numba.core.byteflow:end state. edges=[Edge(pc=78, stack=(), blockstack=(), npush=0)]
DEBUG:numba.core.byteflow:pending: deque([State(pc_initial=40 nstack_initial=0), State(pc_initial=68 nstack_initial=0), State(pc_initial=78 nstack_initial=0), State(pc_initial=78 nstack_initial=0)])
DEBUG:numba.core.byteflow:pending: deque([State(pc_initial=68 nstack_initial=0), State(pc_initial=78 nstack_initial=0), State(pc_initial=78 nstack_initial=0)])
DEBUG:numba.core.byteflow:stack: []
DEBUG:numba.core.byteflow:state.pc_initial: State(pc_initial=68 nstack_initial=0)
DEBUG:numba.core.byteflow:dispatch pc=68, inst=JUMP_FORWARD(arg=4, lineno=1149)
DEBUG:numba.core.byteflow:stack []
DEBUG:numba.core.byteflow:end state. edges=[Edge(pc=74, stack=(), blockstack=(), npush=0)]
DEBUG:numba.core.byteflow:pending: deque([State(pc_initial=78 nstack_initial=0), State(pc_initial=78 nstack_initial=0), State(pc_initial=74 nstack_initial=0)])
DEBUG:numba.core.byteflow:stack: []
DEBUG:numba.core.byteflow:state.pc_initial: State(pc_initial=78 nstack_initial=0)
DEBUG:numba.core.byteflow:dispatch pc=78, inst=LOAD_FAST(arg=2, lineno=1152)
DEBUG:numba.core.byteflow:stack []
DEBUG:numba.core.byteflow:dispatch pc=80, inst=POP_JUMP_IF_FALSE(arg=102, lineno=1152)
DEBUG:numba.core.byteflow:stack ['$zero_pos78.0']
DEBUG:numba.core.byteflow:end state. edges=[Edge(pc=82, stack=(), blockstack=(), npush=0), Edge(pc=102, stack=(), blockstack=(), npush=0)]
DEBUG:numba.core.byteflow:pending: deque([State(pc_initial=78 nstack_initial=0), State(pc_initial=74 nstack_initial=0), State(pc_initial=82 nstack_initial=0), State(pc_initial=102 nstack_initial=0)])
DEBUG:numba.core.byteflow:pending: deque([State(pc_initial=74 nstack_initial=0), State(pc_initial=82 nstack_initial=0), State(pc_initial=102 nstack_initial=0)])
DEBUG:numba.core.byteflow:stack: []
DEBUG:numba.core.byteflow:state.pc_initial: State(pc_initial=74 nstack_initial=0)
DEBUG:numba.core.byteflow:dispatch pc=74, inst=LOAD_CONST(arg=1, lineno=1150)
DEBUG:numba.core.byteflow:stack []
DEBUG:numba.core.byteflow:dispatch pc=76, inst=STORE_FAST(arg=4, lineno=1150)
DEBUG:numba.core.byteflow:stack ['$const74.0']
DEBUG:numba.core.byteflow:end state. edges=[Edge(pc=78, stack=(), blockstack=(), npush=0)]
DEBUG:numba.core.byteflow:pending: deque([State(pc_initial=82 nstack_initial=0), State(pc_initial=102 nstack_initial=0), State(pc_initial=78 nstack_initial=0)])
DEBUG:numba.core.byteflow:stack: []
DEBUG:numba.core.byteflow:state.pc_initial: State(pc_initial=82 nstack_initial=0)
DEBUG:numba.core.byteflow:dispatch pc=82, inst=LOAD_GLOBAL(arg=0, lineno=1153)
DEBUG:numba.core.byteflow:stack []
DEBUG:numba.core.byteflow:dispatch pc=84, inst=LOAD_METHOD(arg=1, lineno=1153)
DEBUG:numba.core.byteflow:stack ['$82load_global.0']
DEBUG:numba.core.byteflow:dispatch pc=86, inst=LOAD_FAST(arg=3, lineno=1153)
DEBUG:numba.core.byteflow:stack ['$84load_method.1']
DEBUG:numba.core.byteflow:dispatch pc=88, inst=CALL_METHOD(arg=1, lineno=1153)
DEBUG:numba.core.byteflow:stack ['$84load_method.1', '$x086.2']
DEBUG:numba.core.byteflow:dispatch pc=90, inst=LOAD_GLOBAL(arg=0, lineno=1153)
DEBUG:numba.core.byteflow:stack ['$88call_method.3']
DEBUG:numba.core.byteflow:dispatch pc=92, inst=LOAD_METHOD(arg=1, lineno=1153)
DEBUG:numba.core.byteflow:stack ['$88call_method.3', '$90load_global.4']
DEBUG:numba.core.byteflow:dispatch pc=94, inst=LOAD_FAST(arg=4, lineno=1153)
DEBUG:numba.core.byteflow:stack ['$88call_method.3', '$92load_method.5']
DEBUG:numba.core.byteflow:dispatch pc=96, inst=CALL_METHOD(arg=1, lineno=1153)
DEBUG:numba.core.byteflow:stack ['$88call_method.3', '$92load_method.5', '$x194.6']
DEBUG:numba.core.byteflow:dispatch pc=98, inst=COMPARE_OP(arg=3, lineno=1153)
DEBUG:numba.core.byteflow:stack ['$88call_method.3', '$96call_method.7']
DEBUG:numba.core.byteflow:dispatch pc=100, inst=RETURN_VALUE(arg=None, lineno=1153)
DEBUG:numba.core.byteflow:stack ['$98compare_op.8']
DEBUG:numba.core.byteflow:end state. edges=[]
DEBUG:numba.core.byteflow:pending: deque([State(pc_initial=102 nstack_initial=0), State(pc_initial=78 nstack_initial=0)])
DEBUG:numba.core.byteflow:stack: []
DEBUG:numba.core.byteflow:state.pc_initial: State(pc_initial=102 nstack_initial=0)
DEBUG:numba.core.byteflow:dispatch pc=102, inst=LOAD_GLOBAL(arg=0, lineno=1155)
DEBUG:numba.core.byteflow:stack []
DEBUG:numba.core.byteflow:dispatch pc=104, inst=LOAD_METHOD(arg=2, lineno=1155)
DEBUG:numba.core.byteflow:stack ['$102load_global.0']
DEBUG:numba.core.byteflow:dispatch pc=106, inst=LOAD_FAST(arg=3, lineno=1155)
DEBUG:numba.core.byteflow:stack ['$104load_method.1']
DEBUG:numba.core.byteflow:dispatch pc=108, inst=CALL_METHOD(arg=1, lineno=1155)
DEBUG:numba.core.byteflow:stack ['$104load_method.1', '$x0106.2']
DEBUG:numba.core.byteflow:dispatch pc=110, inst=LOAD_GLOBAL(arg=0, lineno=1155)
DEBUG:numba.core.byteflow:stack ['$108call_method.3']
DEBUG:numba.core.byteflow:dispatch pc=112, inst=LOAD_METHOD(arg=2, lineno=1155)
DEBUG:numba.core.byteflow:stack ['$108call_method.3', '$110load_global.4']
DEBUG:numba.core.byteflow:dispatch pc=114, inst=LOAD_FAST(arg=4, lineno=1155)
DEBUG:numba.core.byteflow:stack ['$108call_method.3', '$112load_method.5']
DEBUG:numba.core.byteflow:dispatch pc=116, inst=CALL_METHOD(arg=1, lineno=1155)
DEBUG:numba.core.byteflow:stack ['$108call_method.3', '$112load_method.5', '$x1114.6']
DEBUG:numba.core.byteflow:dispatch pc=118, inst=COMPARE_OP(arg=3, lineno=1155)
DEBUG:numba.core.byteflow:stack ['$108call_method.3', '$116call_method.7']
DEBUG:numba.core.byteflow:dispatch pc=120, inst=RETURN_VALUE(arg=None, lineno=1155)
DEBUG:numba.core.byteflow:stack ['$118compare_op.8']
DEBUG:numba.core.byteflow:end state. edges=[]
DEBUG:numba.core.byteflow:pending: deque([State(pc_initial=78 nstack_initial=0)])
DEBUG:numba.core.byteflow:-------------------------Prune PHIs-------------------------
DEBUG:numba.core.byteflow:Used_phis: defaultdict(<class 'set'>,
            {State(pc_initial=0 nstack_initial=0): set(),
             State(pc_initial=24 nstack_initial=1): {'$phi24.0'},
             State(pc_initial=30 nstack_initial=0): set(),
             State(pc_initial=32 nstack_initial=1): set(),
             State(pc_initial=36 nstack_initial=0): set(),
             State(pc_initial=40 nstack_initial=0): set(),
             State(pc_initial=62 nstack_initial=1): {'$phi62.0'},
             State(pc_initial=68 nstack_initial=0): set(),
             State(pc_initial=70 nstack_initial=1): set(),
             State(pc_initial=74 nstack_initial=0): set(),
             State(pc_initial=78 nstack_initial=0): set(),
             State(pc_initial=82 nstack_initial=0): set(),
             State(pc_initial=102 nstack_initial=0): set()})
DEBUG:numba.core.byteflow:defmap: {'$phi24.0': State(pc_initial=0 nstack_initial=0),
 '$phi32.0': State(pc_initial=0 nstack_initial=0),
 '$phi62.0': State(pc_initial=40 nstack_initial=0),
 '$phi70.0': State(pc_initial=40 nstack_initial=0)}
DEBUG:numba.core.byteflow:phismap: defaultdict(<class 'set'>,
            {'$phi24.0': {('$16dup_top.6',
                           State(pc_initial=0 nstack_initial=0))},
             '$phi32.0': {('$16dup_top.6',
                           State(pc_initial=0 nstack_initial=0))},
             '$phi62.0': {('$54dup_top.6',
                           State(pc_initial=40 nstack_initial=0))},
             '$phi70.0': {('$54dup_top.6',
                           State(pc_initial=40 nstack_initial=0))}})
DEBUG:numba.core.byteflow:changing phismap: defaultdict(<class 'set'>,
            {'$phi24.0': {('$16dup_top.6',
                           State(pc_initial=0 nstack_initial=0))},
             '$phi32.0': {('$16dup_top.6',
                           State(pc_initial=0 nstack_initial=0))},
             '$phi62.0': {('$54dup_top.6',
                           State(pc_initial=40 nstack_initial=0))},
             '$phi70.0': {('$54dup_top.6',
                           State(pc_initial=40 nstack_initial=0))}})
DEBUG:numba.core.byteflow:keep phismap: {'$phi24.0': {('$16dup_top.6', State(pc_initial=0 nstack_initial=0))},
 '$phi62.0': {('$54dup_top.6', State(pc_initial=40 nstack_initial=0))}}
DEBUG:numba.core.byteflow:new_out: defaultdict(<class 'dict'>,
            {State(pc_initial=0 nstack_initial=0): {'$phi24.0': '$16dup_top.6'},
             State(pc_initial=40 nstack_initial=0): {'$phi62.0': '$54dup_top.6'}})
DEBUG:numba.core.byteflow:----------------------DONE Prune PHIs-----------------------
DEBUG:numba.core.byteflow:block_infos State(pc_initial=0 nstack_initial=0):
AdaptBlockInfo(insts=((0, {}), (2, {'res': '$x2.0'}), (4, {'res': '$const4.1'}), (6, {'index': '$const4.1', 'target': '$x2.0', 'res': '$6binary_subscr.2'}), (8, {'value': '$6binary_subscr.2'}), (10, {'res': '$threshold10.3'}), (12, {'value': '$threshold10.3', 'res': '$12unary_negative.4'}), (14, {'res': '$x014.5'}), (16, {'orig': ['$x014.5'], 'duped': ['$16dup_top.6']}), (20, {'lhs': '$12unary_negative.4', 'rhs': '$x014.5', 'res': '$20compare_op.7'}), (22, {'pred': '$20compare_op.7'})), outgoing_phis={'$phi24.0': '$16dup_top.6'}, blockstack=(), active_try_block=None, outgoing_edgepushed={24: ('$16dup_top.6',), 32: ('$16dup_top.6',)})
DEBUG:numba.core.byteflow:block_infos State(pc_initial=24 nstack_initial=1):
AdaptBlockInfo(insts=((24, {'res': '$threshold24.1'}), (26, {'lhs': '$phi24.0', 'rhs': '$threshold24.1', 'res': '$26compare_op.2'}), (28, {'pred': '$26compare_op.2'})), outgoing_phis={}, blockstack=(), active_try_block=None, outgoing_edgepushed={30: (), 40: ()})
DEBUG:numba.core.byteflow:block_infos State(pc_initial=30 nstack_initial=0):
AdaptBlockInfo(insts=((30, {}),), outgoing_phis={}, blockstack=(), active_try_block=None, outgoing_edgepushed={36: ()})
DEBUG:numba.core.byteflow:block_infos State(pc_initial=32 nstack_initial=1):
AdaptBlockInfo(insts=((34, {}),), outgoing_phis={}, blockstack=(), active_try_block=None, outgoing_edgepushed={40: ()})
DEBUG:numba.core.byteflow:block_infos State(pc_initial=36 nstack_initial=0):
AdaptBlockInfo(insts=((36, {'res': '$const36.0'}), (38, {'value': '$const36.0'})), outgoing_phis={}, blockstack=(), active_try_block=None, outgoing_edgepushed={40: ()})
DEBUG:numba.core.byteflow:block_infos State(pc_initial=40 nstack_initial=0):
AdaptBlockInfo(insts=((40, {'res': '$x40.0'}), (42, {'res': '$const42.1'}), (44, {'index': '$const42.1', 'target': '$x40.0', 'res': '$44binary_subscr.2'}), (46, {'value': '$44binary_subscr.2'}), (48, {'res': '$threshold48.3'}), (50, {'value': '$threshold48.3', 'res': '$50unary_negative.4'}), (52, {'res': '$x152.5'}), (54, {'orig': ['$x152.5'], 'duped': ['$54dup_top.6']}), (58, {'lhs': '$50unary_negative.4', 'rhs': '$x152.5', 'res': '$58compare_op.7'}), (60, {'pred': '$58compare_op.7'})), outgoing_phis={'$phi62.0': '$54dup_top.6'}, blockstack=(), active_try_block=None, outgoing_edgepushed={62: ('$54dup_top.6',), 70: ('$54dup_top.6',)})
DEBUG:numba.core.byteflow:block_infos State(pc_initial=62 nstack_initial=1):
AdaptBlockInfo(insts=((62, {'res': '$threshold62.1'}), (64, {'lhs': '$phi62.0', 'rhs': '$threshold62.1', 'res': '$64compare_op.2'}), (66, {'pred': '$64compare_op.2'})), outgoing_phis={}, blockstack=(), active_try_block=None, outgoing_edgepushed={68: (), 78: ()})
DEBUG:numba.core.byteflow:block_infos State(pc_initial=68 nstack_initial=0):
AdaptBlockInfo(insts=((68, {}),), outgoing_phis={}, blockstack=(), active_try_block=None, outgoing_edgepushed={74: ()})
DEBUG:numba.core.byteflow:block_infos State(pc_initial=70 nstack_initial=1):
AdaptBlockInfo(insts=((72, {}),), outgoing_phis={}, blockstack=(), active_try_block=None, outgoing_edgepushed={78: ()})
DEBUG:numba.core.byteflow:block_infos State(pc_initial=74 nstack_initial=0):
AdaptBlockInfo(insts=((74, {'res': '$const74.0'}), (76, {'value': '$const74.0'})), outgoing_phis={}, blockstack=(), active_try_block=None, outgoing_edgepushed={78: ()})
DEBUG:numba.core.byteflow:block_infos State(pc_initial=78 nstack_initial=0):
AdaptBlockInfo(insts=((78, {'res': '$zero_pos78.0'}), (80, {'pred': '$zero_pos78.0'})), outgoing_phis={}, blockstack=(), active_try_block=None, outgoing_edgepushed={82: (), 102: ()})
DEBUG:numba.core.byteflow:block_infos State(pc_initial=82 nstack_initial=0):
AdaptBlockInfo(insts=((82, {'res': '$82load_global.0'}), (84, {'item': '$82load_global.0', 'res': '$84load_method.1'}), (86, {'res': '$x086.2'}), (88, {'func': '$84load_method.1', 'args': ['$x086.2'], 'res': '$88call_method.3'}), (90, {'res': '$90load_global.4'}), (92, {'item': '$90load_global.4', 'res': '$92load_method.5'}), (94, {'res': '$x194.6'}), (96, {'func': '$92load_method.5', 'args': ['$x194.6'], 'res': '$96call_method.7'}), (98, {'lhs': '$88call_method.3', 'rhs': '$96call_method.7', 'res': '$98compare_op.8'}), (100, {'retval': '$98compare_op.8', 'castval': '$100return_value.9'})), outgoing_phis={}, blockstack=(), active_try_block=None, outgoing_edgepushed={})
DEBUG:numba.core.byteflow:block_infos State(pc_initial=102 nstack_initial=0):
AdaptBlockInfo(insts=((102, {'res': '$102load_global.0'}), (104, {'item': '$102load_global.0', 'res': '$104load_method.1'}), (106, {'res': '$x0106.2'}), (108, {'func': '$104load_method.1', 'args': ['$x0106.2'], 'res': '$108call_method.3'}), (110, {'res': '$110load_global.4'}), (112, {'item': '$110load_global.4', 'res': '$112load_method.5'}), (114, {'res': '$x1114.6'}), (116, {'func': '$112load_method.5', 'args': ['$x1114.6'], 'res': '$116call_method.7'}), (118, {'lhs': '$108call_method.3', 'rhs': '$116call_method.7', 'res': '$118compare_op.8'}), (120, {'retval': '$118compare_op.8', 'castval': '$120return_value.9'})), outgoing_phis={}, blockstack=(), active_try_block=None, outgoing_edgepushed={})
DEBUG:numba.core.interpreter:label 0:
    x = arg(0, name=x)                       ['x']
    threshold = arg(1, name=threshold)       ['threshold']
    zero_pos = arg(2, name=zero_pos)         ['zero_pos']
    $const4.1 = const(int, 0)                ['$const4.1']
    x0 = getitem(value=x, index=$const4.1, fn=<built-in function getitem>) ['$const4.1', 'x', 'x0']
    $12unary_negative.4 = unary(fn=<built-in function neg>, value=threshold) ['$12unary_negative.4', 'threshold']
    $20compare_op.7 = $12unary_negative.4 <= x0 ['$12unary_negative.4', '$20compare_op.7', 'x0']
    bool22 = global(bool: <class 'bool'>)    ['bool22']
    $22pred = call bool22($20compare_op.7, func=bool22, args=(Var($20compare_op.7, audio.py:1145),), kws=(), vararg=None, varkwarg=None, target=None) ['$20compare_op.7', '$22pred', 'bool22']
    $phi24.0 = x0                            ['$phi24.0', 'x0']
    branch $22pred, 24, 32                   ['$22pred']
label 24:
    $26compare_op.2 = $phi24.0 <= threshold  ['$26compare_op.2', '$phi24.0', 'threshold']
    bool28 = global(bool: <class 'bool'>)    ['bool28']
    $28pred = call bool28($26compare_op.2, func=bool28, args=(Var($26compare_op.2, audio.py:1145),), kws=(), vararg=None, varkwarg=None, target=None) ['$26compare_op.2', '$28pred', 'bool28']
    branch $28pred, 30, 40                   ['$28pred']
label 30:
    jump 36                                  []
label 32:
    jump 40                                  []
label 36:
    x0 = const(int, 0)                       ['x0']
    jump 40                                  []
label 40:
    $const42.1 = const(int, -1)              ['$const42.1']
    x1 = getitem(value=x, index=$const42.1, fn=<built-in function getitem>) ['$const42.1', 'x', 'x1']
    $50unary_negative.4 = unary(fn=<built-in function neg>, value=threshold) ['$50unary_negative.4', 'threshold']
    $58compare_op.7 = $50unary_negative.4 <= x1 ['$50unary_negative.4', '$58compare_op.7', 'x1']
    bool60 = global(bool: <class 'bool'>)    ['bool60']
    $60pred = call bool60($58compare_op.7, func=bool60, args=(Var($58compare_op.7, audio.py:1149),), kws=(), vararg=None, varkwarg=None, target=None) ['$58compare_op.7', '$60pred', 'bool60']
    $phi62.0 = x1                            ['$phi62.0', 'x1']
    branch $60pred, 62, 70                   ['$60pred']
label 62:
    $64compare_op.2 = $phi62.0 <= threshold  ['$64compare_op.2', '$phi62.0', 'threshold']
    bool66 = global(bool: <class 'bool'>)    ['bool66']
    $66pred = call bool66($64compare_op.2, func=bool66, args=(Var($64compare_op.2, audio.py:1149),), kws=(), vararg=None, varkwarg=None, target=None) ['$64compare_op.2', '$66pred', 'bool66']
    branch $66pred, 68, 78                   ['$66pred']
label 68:
    jump 74                                  []
label 70:
    jump 78                                  []
label 74:
    x1 = const(int, 0)                       ['x1']
    jump 78                                  []
label 78:
    bool80 = global(bool: <class 'bool'>)    ['bool80']
    $80pred = call bool80(zero_pos, func=bool80, args=(Var(zero_pos, audio.py:1141),), kws=(), vararg=None, varkwarg=None, target=None) ['$80pred', 'bool80', 'zero_pos']
    branch $80pred, 82, 102                  ['$80pred']
label 82:
    $82load_global.0 = global(np: <module 'numpy' from 'C:\\Users\\taskm\\anaconda3\\envs\\taskai\\lib\\site-packages\\numpy\\__init__.py'>) ['$82load_global.0']
    $84load_method.1 = getattr(value=$82load_global.0, attr=signbit) ['$82load_global.0', '$84load_method.1']
    $88call_method.3 = call $84load_method.1(x0, func=$84load_method.1, args=[Var(x0, audio.py:1144)], kws=(), vararg=None, varkwarg=None, target=None) ['$84load_method.1', '$88call_method.3', 'x0']
    $90load_global.4 = global(np: <module 'numpy' from 'C:\\Users\\taskm\\anaconda3\\envs\\taskai\\lib\\site-packages\\numpy\\__init__.py'>) ['$90load_global.4']
    $92load_method.5 = getattr(value=$90load_global.4, attr=signbit) ['$90load_global.4', '$92load_method.5']
    $96call_method.7 = call $92load_method.5(x1, func=$92load_method.5, args=[Var(x1, audio.py:1148)], kws=(), vararg=None, varkwarg=None, target=None) ['$92load_method.5', '$96call_method.7', 'x1']
    $98compare_op.8 = $88call_method.3 != $96call_method.7 ['$88call_method.3', '$96call_method.7', '$98compare_op.8']
    $100return_value.9 = cast(value=$98compare_op.8) ['$100return_value.9', '$98compare_op.8']
    return $100return_value.9                ['$100return_value.9']
label 102:
    $102load_global.0 = global(np: <module 'numpy' from 'C:\\Users\\taskm\\anaconda3\\envs\\taskai\\lib\\site-packages\\numpy\\__init__.py'>) ['$102load_global.0']
    $104load_method.1 = getattr(value=$102load_global.0, attr=sign) ['$102load_global.0', '$104load_method.1']
    $108call_method.3 = call $104load_method.1(x0, func=$104load_method.1, args=[Var(x0, audio.py:1144)], kws=(), vararg=None, varkwarg=None, target=None) ['$104load_method.1', '$108call_method.3', 'x0']
    $110load_global.4 = global(np: <module 'numpy' from 'C:\\Users\\taskm\\anaconda3\\envs\\taskai\\lib\\site-packages\\numpy\\__init__.py'>) ['$110load_global.4']
    $112load_method.5 = getattr(value=$110load_global.4, attr=sign) ['$110load_global.4', '$112load_method.5']
    $116call_method.7 = call $112load_method.5(x1, func=$112load_method.5, args=[Var(x1, audio.py:1148)], kws=(), vararg=None, varkwarg=None, target=None) ['$112load_method.5', '$116call_method.7', 'x1']
    $118compare_op.8 = $108call_method.3 != $116call_method.7 ['$108call_method.3', '$116call_method.7', '$118compare_op.8']
    $120return_value.9 = cast(value=$118compare_op.8) ['$118compare_op.8', '$120return_value.9']
    return $120return_value.9                ['$120return_value.9']

DEBUG:numba.core.byteflow:bytecode dump:
>          0	NOP(arg=None, lineno=1039)
           2	LOAD_FAST(arg=0, lineno=1042)
           4	LOAD_CONST(arg=1, lineno=1042)
           6	BINARY_SUBSCR(arg=None, lineno=1042)
           8	LOAD_FAST(arg=0, lineno=1042)
          10	LOAD_CONST(arg=2, lineno=1042)
          12	BINARY_SUBSCR(arg=None, lineno=1042)
          14	COMPARE_OP(arg=4, lineno=1042)
          16	LOAD_FAST(arg=0, lineno=1042)
          18	LOAD_CONST(arg=1, lineno=1042)
          20	BINARY_SUBSCR(arg=None, lineno=1042)
          22	LOAD_FAST(arg=0, lineno=1042)
          24	LOAD_CONST(arg=3, lineno=1042)
          26	BINARY_SUBSCR(arg=None, lineno=1042)
          28	COMPARE_OP(arg=5, lineno=1042)
          30	BINARY_AND(arg=None, lineno=1042)
          32	RETURN_VALUE(arg=None, lineno=1042)
DEBUG:numba.core.byteflow:pending: deque([State(pc_initial=0 nstack_initial=0)])
DEBUG:numba.core.byteflow:stack: []
DEBUG:numba.core.byteflow:state.pc_initial: State(pc_initial=0 nstack_initial=0)
DEBUG:numba.core.byteflow:dispatch pc=0, inst=NOP(arg=None, lineno=1039)
DEBUG:numba.core.byteflow:stack []
DEBUG:numba.core.byteflow:dispatch pc=2, inst=LOAD_FAST(arg=0, lineno=1042)
DEBUG:numba.core.byteflow:stack []
DEBUG:numba.core.byteflow:dispatch pc=4, inst=LOAD_CONST(arg=1, lineno=1042)
DEBUG:numba.core.byteflow:stack ['$x2.0']
DEBUG:numba.core.byteflow:dispatch pc=6, inst=BINARY_SUBSCR(arg=None, lineno=1042)
DEBUG:numba.core.byteflow:stack ['$x2.0', '$const4.1']
DEBUG:numba.core.byteflow:dispatch pc=8, inst=LOAD_FAST(arg=0, lineno=1042)
DEBUG:numba.core.byteflow:stack ['$6binary_subscr.2']
DEBUG:numba.core.byteflow:dispatch pc=10, inst=LOAD_CONST(arg=2, lineno=1042)
DEBUG:numba.core.byteflow:stack ['$6binary_subscr.2', '$x8.3']
DEBUG:numba.core.byteflow:dispatch pc=12, inst=BINARY_SUBSCR(arg=None, lineno=1042)
DEBUG:numba.core.byteflow:stack ['$6binary_subscr.2', '$x8.3', '$const10.4']
DEBUG:numba.core.byteflow:dispatch pc=14, inst=COMPARE_OP(arg=4, lineno=1042)
DEBUG:numba.core.byteflow:stack ['$6binary_subscr.2', '$12binary_subscr.5']
DEBUG:numba.core.byteflow:dispatch pc=16, inst=LOAD_FAST(arg=0, lineno=1042)
DEBUG:numba.core.byteflow:stack ['$14compare_op.6']
DEBUG:numba.core.byteflow:dispatch pc=18, inst=LOAD_CONST(arg=1, lineno=1042)
DEBUG:numba.core.byteflow:stack ['$14compare_op.6', '$x16.7']
DEBUG:numba.core.byteflow:dispatch pc=20, inst=BINARY_SUBSCR(arg=None, lineno=1042)
DEBUG:numba.core.byteflow:stack ['$14compare_op.6', '$x16.7', '$const18.8']
DEBUG:numba.core.byteflow:dispatch pc=22, inst=LOAD_FAST(arg=0, lineno=1042)
DEBUG:numba.core.byteflow:stack ['$14compare_op.6', '$20binary_subscr.9']
DEBUG:numba.core.byteflow:dispatch pc=24, inst=LOAD_CONST(arg=3, lineno=1042)
DEBUG:numba.core.byteflow:stack ['$14compare_op.6', '$20binary_subscr.9', '$x22.10']
DEBUG:numba.core.byteflow:dispatch pc=26, inst=BINARY_SUBSCR(arg=None, lineno=1042)
DEBUG:numba.core.byteflow:stack ['$14compare_op.6', '$20binary_subscr.9', '$x22.10', '$const24.11']
DEBUG:numba.core.byteflow:dispatch pc=28, inst=COMPARE_OP(arg=5, lineno=1042)
DEBUG:numba.core.byteflow:stack ['$14compare_op.6', '$20binary_subscr.9', '$26binary_subscr.12']
DEBUG:numba.core.byteflow:dispatch pc=30, inst=BINARY_AND(arg=None, lineno=1042)
DEBUG:numba.core.byteflow:stack ['$14compare_op.6', '$28compare_op.13']
DEBUG:numba.core.byteflow:dispatch pc=32, inst=RETURN_VALUE(arg=None, lineno=1042)
DEBUG:numba.core.byteflow:stack ['$30binary_and.14']
DEBUG:numba.core.byteflow:end state. edges=[]
DEBUG:numba.core.byteflow:-------------------------Prune PHIs-------------------------
DEBUG:numba.core.byteflow:Used_phis: defaultdict(<class 'set'>, {State(pc_initial=0 nstack_initial=0): set()})
DEBUG:numba.core.byteflow:defmap: {}
DEBUG:numba.core.byteflow:phismap: defaultdict(<class 'set'>, {})
DEBUG:numba.core.byteflow:changing phismap: defaultdict(<class 'set'>, {})
DEBUG:numba.core.byteflow:keep phismap: {}
DEBUG:numba.core.byteflow:new_out: defaultdict(<class 'dict'>, {})
DEBUG:numba.core.byteflow:----------------------DONE Prune PHIs-----------------------
DEBUG:numba.core.byteflow:block_infos State(pc_initial=0 nstack_initial=0):
AdaptBlockInfo(insts=((0, {}), (2, {'res': '$x2.0'}), (4, {'res': '$const4.1'}), (6, {'index': '$const4.1', 'target': '$x2.0', 'res': '$6binary_subscr.2'}), (8, {'res': '$x8.3'}), (10, {'res': '$const10.4'}), (12, {'index': '$const10.4', 'target': '$x8.3', 'res': '$12binary_subscr.5'}), (14, {'lhs': '$6binary_subscr.2', 'rhs': '$12binary_subscr.5', 'res': '$14compare_op.6'}), (16, {'res': '$x16.7'}), (18, {'res': '$const18.8'}), (20, {'index': '$const18.8', 'target': '$x16.7', 'res': '$20binary_subscr.9'}), (22, {'res': '$x22.10'}), (24, {'res': '$const24.11'}), (26, {'index': '$const24.11', 'target': '$x22.10', 'res': '$26binary_subscr.12'}), (28, {'lhs': '$20binary_subscr.9', 'rhs': '$26binary_subscr.12', 'res': '$28compare_op.13'}), (30, {'lhs': '$14compare_op.6', 'rhs': '$28compare_op.13', 'res': '$30binary_and.14'}), (32, {'retval': '$30binary_and.14', 'castval': '$32return_value.15'})), outgoing_phis={}, blockstack=(), active_try_block=None, outgoing_edgepushed={})
DEBUG:numba.core.interpreter:label 0:
    x = arg(0, name=x)                       ['x']
    $const4.1 = const(int, 0)                ['$const4.1']
    $6binary_subscr.2 = getitem(value=x, index=$const4.1, fn=<built-in function getitem>) ['$6binary_subscr.2', '$const4.1', 'x']
    $const10.4 = const(int, -1)              ['$const10.4']
    $12binary_subscr.5 = getitem(value=x, index=$const10.4, fn=<built-in function getitem>) ['$12binary_subscr.5', '$const10.4', 'x']
    $14compare_op.6 = $6binary_subscr.2 > $12binary_subscr.5 ['$12binary_subscr.5', '$14compare_op.6', '$6binary_subscr.2']
    $const18.8 = const(int, 0)               ['$const18.8']
    $20binary_subscr.9 = getitem(value=x, index=$const18.8, fn=<built-in function getitem>) ['$20binary_subscr.9', '$const18.8', 'x']
    $const24.11 = const(int, 1)              ['$const24.11']
    $26binary_subscr.12 = getitem(value=x, index=$const24.11, fn=<built-in function getitem>) ['$26binary_subscr.12', '$const24.11', 'x']
    $28compare_op.13 = $20binary_subscr.9 >= $26binary_subscr.12 ['$20binary_subscr.9', '$26binary_subscr.12', '$28compare_op.13']
    $30binary_and.14 = $14compare_op.6 & $28compare_op.13 ['$14compare_op.6', '$28compare_op.13', '$30binary_and.14']
    $32return_value.15 = cast(value=$30binary_and.14) ['$30binary_and.14', '$32return_value.15']
    return $32return_value.15                ['$32return_value.15']

DEBUG:numba.core.byteflow:bytecode dump:
>          0	NOP(arg=None, lineno=1045)
           2	LOAD_FAST(arg=0, lineno=1048)
           4	LOAD_CONST(arg=1, lineno=1048)
           6	BINARY_SUBSCR(arg=None, lineno=1048)
           8	LOAD_FAST(arg=0, lineno=1048)
          10	LOAD_CONST(arg=2, lineno=1048)
          12	BINARY_SUBSCR(arg=None, lineno=1048)
          14	COMPARE_OP(arg=0, lineno=1048)
          16	LOAD_FAST(arg=0, lineno=1048)
          18	LOAD_CONST(arg=1, lineno=1048)
          20	BINARY_SUBSCR(arg=None, lineno=1048)
          22	LOAD_FAST(arg=0, lineno=1048)
          24	LOAD_CONST(arg=3, lineno=1048)
          26	BINARY_SUBSCR(arg=None, lineno=1048)
          28	COMPARE_OP(arg=1, lineno=1048)
          30	BINARY_AND(arg=None, lineno=1048)
          32	RETURN_VALUE(arg=None, lineno=1048)
DEBUG:numba.core.byteflow:pending: deque([State(pc_initial=0 nstack_initial=0)])
DEBUG:numba.core.byteflow:stack: []
DEBUG:numba.core.byteflow:state.pc_initial: State(pc_initial=0 nstack_initial=0)
DEBUG:numba.core.byteflow:dispatch pc=0, inst=NOP(arg=None, lineno=1045)
DEBUG:numba.core.byteflow:stack []
DEBUG:numba.core.byteflow:dispatch pc=2, inst=LOAD_FAST(arg=0, lineno=1048)
DEBUG:numba.core.byteflow:stack []
DEBUG:numba.core.byteflow:dispatch pc=4, inst=LOAD_CONST(arg=1, lineno=1048)
DEBUG:numba.core.byteflow:stack ['$x2.0']
DEBUG:numba.core.byteflow:dispatch pc=6, inst=BINARY_SUBSCR(arg=None, lineno=1048)
DEBUG:numba.core.byteflow:stack ['$x2.0', '$const4.1']
DEBUG:numba.core.byteflow:dispatch pc=8, inst=LOAD_FAST(arg=0, lineno=1048)
DEBUG:numba.core.byteflow:stack ['$6binary_subscr.2']
DEBUG:numba.core.byteflow:dispatch pc=10, inst=LOAD_CONST(arg=2, lineno=1048)
DEBUG:numba.core.byteflow:stack ['$6binary_subscr.2', '$x8.3']
DEBUG:numba.core.byteflow:dispatch pc=12, inst=BINARY_SUBSCR(arg=None, lineno=1048)
DEBUG:numba.core.byteflow:stack ['$6binary_subscr.2', '$x8.3', '$const10.4']
DEBUG:numba.core.byteflow:dispatch pc=14, inst=COMPARE_OP(arg=0, lineno=1048)
DEBUG:numba.core.byteflow:stack ['$6binary_subscr.2', '$12binary_subscr.5']
DEBUG:numba.core.byteflow:dispatch pc=16, inst=LOAD_FAST(arg=0, lineno=1048)
DEBUG:numba.core.byteflow:stack ['$14compare_op.6']
DEBUG:numba.core.byteflow:dispatch pc=18, inst=LOAD_CONST(arg=1, lineno=1048)
DEBUG:numba.core.byteflow:stack ['$14compare_op.6', '$x16.7']
DEBUG:numba.core.byteflow:dispatch pc=20, inst=BINARY_SUBSCR(arg=None, lineno=1048)
DEBUG:numba.core.byteflow:stack ['$14compare_op.6', '$x16.7', '$const18.8']
DEBUG:numba.core.byteflow:dispatch pc=22, inst=LOAD_FAST(arg=0, lineno=1048)
DEBUG:numba.core.byteflow:stack ['$14compare_op.6', '$20binary_subscr.9']
DEBUG:numba.core.byteflow:dispatch pc=24, inst=LOAD_CONST(arg=3, lineno=1048)
DEBUG:numba.core.byteflow:stack ['$14compare_op.6', '$20binary_subscr.9', '$x22.10']
DEBUG:numba.core.byteflow:dispatch pc=26, inst=BINARY_SUBSCR(arg=None, lineno=1048)
DEBUG:numba.core.byteflow:stack ['$14compare_op.6', '$20binary_subscr.9', '$x22.10', '$const24.11']
DEBUG:numba.core.byteflow:dispatch pc=28, inst=COMPARE_OP(arg=1, lineno=1048)
DEBUG:numba.core.byteflow:stack ['$14compare_op.6', '$20binary_subscr.9', '$26binary_subscr.12']
DEBUG:numba.core.byteflow:dispatch pc=30, inst=BINARY_AND(arg=None, lineno=1048)
DEBUG:numba.core.byteflow:stack ['$14compare_op.6', '$28compare_op.13']
DEBUG:numba.core.byteflow:dispatch pc=32, inst=RETURN_VALUE(arg=None, lineno=1048)
DEBUG:numba.core.byteflow:stack ['$30binary_and.14']
DEBUG:numba.core.byteflow:end state. edges=[]
DEBUG:numba.core.byteflow:-------------------------Prune PHIs-------------------------
DEBUG:numba.core.byteflow:Used_phis: defaultdict(<class 'set'>, {State(pc_initial=0 nstack_initial=0): set()})
DEBUG:numba.core.byteflow:defmap: {}
DEBUG:numba.core.byteflow:phismap: defaultdict(<class 'set'>, {})
DEBUG:numba.core.byteflow:changing phismap: defaultdict(<class 'set'>, {})
DEBUG:numba.core.byteflow:keep phismap: {}
DEBUG:numba.core.byteflow:new_out: defaultdict(<class 'dict'>, {})
DEBUG:numba.core.byteflow:----------------------DONE Prune PHIs-----------------------
DEBUG:numba.core.byteflow:block_infos State(pc_initial=0 nstack_initial=0):
AdaptBlockInfo(insts=((0, {}), (2, {'res': '$x2.0'}), (4, {'res': '$const4.1'}), (6, {'index': '$const4.1', 'target': '$x2.0', 'res': '$6binary_subscr.2'}), (8, {'res': '$x8.3'}), (10, {'res': '$const10.4'}), (12, {'index': '$const10.4', 'target': '$x8.3', 'res': '$12binary_subscr.5'}), (14, {'lhs': '$6binary_subscr.2', 'rhs': '$12binary_subscr.5', 'res': '$14compare_op.6'}), (16, {'res': '$x16.7'}), (18, {'res': '$const18.8'}), (20, {'index': '$const18.8', 'target': '$x16.7', 'res': '$20binary_subscr.9'}), (22, {'res': '$x22.10'}), (24, {'res': '$const24.11'}), (26, {'index': '$const24.11', 'target': '$x22.10', 'res': '$26binary_subscr.12'}), (28, {'lhs': '$20binary_subscr.9', 'rhs': '$26binary_subscr.12', 'res': '$28compare_op.13'}), (30, {'lhs': '$14compare_op.6', 'rhs': '$28compare_op.13', 'res': '$30binary_and.14'}), (32, {'retval': '$30binary_and.14', 'castval': '$32return_value.15'})), outgoing_phis={}, blockstack=(), active_try_block=None, outgoing_edgepushed={})
DEBUG:numba.core.interpreter:label 0:
    x = arg(0, name=x)                       ['x']
    $const4.1 = const(int, 0)                ['$const4.1']
    $6binary_subscr.2 = getitem(value=x, index=$const4.1, fn=<built-in function getitem>) ['$6binary_subscr.2', '$const4.1', 'x']
    $const10.4 = const(int, -1)              ['$const10.4']
    $12binary_subscr.5 = getitem(value=x, index=$const10.4, fn=<built-in function getitem>) ['$12binary_subscr.5', '$const10.4', 'x']
    $14compare_op.6 = $6binary_subscr.2 < $12binary_subscr.5 ['$12binary_subscr.5', '$14compare_op.6', '$6binary_subscr.2']
    $const18.8 = const(int, 0)               ['$const18.8']
    $20binary_subscr.9 = getitem(value=x, index=$const18.8, fn=<built-in function getitem>) ['$20binary_subscr.9', '$const18.8', 'x']
    $const24.11 = const(int, 1)              ['$const24.11']
    $26binary_subscr.12 = getitem(value=x, index=$const24.11, fn=<built-in function getitem>) ['$26binary_subscr.12', '$const24.11', 'x']
    $28compare_op.13 = $20binary_subscr.9 <= $26binary_subscr.12 ['$20binary_subscr.9', '$26binary_subscr.12', '$28compare_op.13']
    $30binary_and.14 = $14compare_op.6 & $28compare_op.13 ['$14compare_op.6', '$28compare_op.13', '$30binary_and.14']
    $32return_value.15 = cast(value=$30binary_and.14) ['$30binary_and.14', '$32return_value.15']
    return $32return_value.15                ['$32return_value.15']

DEBUG:httpx:load_ssl_context verify=True cert=None trust_env=True http2=False
DEBUG:httpx:load_verify_locations cafile='C:\\Users\\taskm\\anaconda3\\Library\\ssl\\cacert.pem'
DEBUG:httpx:load_ssl_context verify=True cert=None trust_env=True http2=False
DEBUG:httpx:load_verify_locations cafile='C:\\Users\\taskm\\anaconda3\\Library\\ssl\\cacert.pem'
DEBUG:openai._base_client:Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'json_data': {'messages': [{'content': "Summarize the following chapter in one sentence: put it out but so you said you the way you articulated though i really like that you said a cog in the machine they lost a cog in the machine yo did you see my game of thrones breakdown of the of the um the the the leagues in battle right right now how i framed it with you no i gotta seriously because i don't i try not to watch none of this yeah yeah i know you said you was gonna do that too no i was i was that was a little test along with a question because you say you were stepping away from the whole negativity and all of that and yo the fact that is that everybody's saying that is dying right now i think about you so much but anyway uh but what i pointed out was how like it seems like we're starting to feel the after effects of the lack of innovation from that side so you saying that the deal they made with caffeine was like them selling them a machine but a cog was missing in the machine it looked the same from the outside", 'role': 'user'}], 'model': 'gpt-3.5-turbo-16k', 'max_tokens': 1000, 'n': 1, 'stream': False, 'temperature': 0.7}}
DEBUG:openai._base_client:Sending HTTP Request: POST https://api.openai.com/v1/chat/completions
DEBUG:httpcore.connection:connect_tcp.started host='api.openai.com' port=443 local_address=None timeout=None socket_options=None
DEBUG:httpcore.connection:connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x00000164C04A9A00>
DEBUG:httpcore.connection:start_tls.started ssl_context=<ssl.SSLContext object at 0x00000164C053BEC0> server_hostname='api.openai.com' timeout=None
DEBUG:httpcore.connection:start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x00000164C04DD310>
DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>
DEBUG:httpcore.http11:send_request_headers.complete
DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>
DEBUG:httpcore.http11:send_request_body.complete
DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>
DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Mon, 08 Jul 2024 07:21:13 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'openai-organization', b'machinekingsmedia'), (b'openai-processing-ms', b'583'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains'), (b'x-ratelimit-limit-requests', b'10000'), (b'x-ratelimit-limit-tokens', b'1000000'), (b'x-ratelimit-remaining-requests', b'9999'), (b'x-ratelimit-remaining-tokens', b'998751'), (b'x-ratelimit-reset-requests', b'6ms'), (b'x-ratelimit-reset-tokens', b'74ms'), (b'x-request-id', b'req_ed1ac03772b06836fe0dc07b80ee5e9f'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Set-Cookie', b'__cf_bm=8LOE68ipAWmBr5TudWvsVv9So_sM2FyHXQn22NuzR8Q-1720423273-1.0.1.1-0u3eG48hxil9p68xYAn6LFlSxC.5GeWmhv2iwaJ0lD25Y8mx8Rcf2bWXynXI14ajj_cNMrBLIdZy7iiJmW1gFQ; path=/; expires=Mon, 08-Jul-24 07:51:13 GMT; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), (b'Set-Cookie', b'_cfuvid=uD5mq4WMmpaetzHu2EtTC1a9XU5j9rx2bjhmijFl1nw-1720423273448-0.0.1.1-604800000; path=/; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), (b'Server', b'cloudflare'), (b'CF-RAY', b'89fe50ed6fb80598-IAD'), (b'Content-Encoding', b'br'), (b'alt-svc', b'h3=":443"; ma=86400')])
INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>
DEBUG:httpcore.http11:receive_response_body.complete
DEBUG:httpcore.http11:response_closed.started
DEBUG:httpcore.http11:response_closed.complete
DEBUG:openai._base_client:HTTP Response: POST https://api.openai.com/v1/chat/completions "200 OK" Headers([('date', 'Mon, 08 Jul 2024 07:21:13 GMT'), ('content-type', 'application/json'), ('transfer-encoding', 'chunked'), ('connection', 'keep-alive'), ('openai-organization', 'machinekingsmedia'), ('openai-processing-ms', '583'), ('openai-version', '2020-10-01'), ('strict-transport-security', 'max-age=31536000; includeSubDomains'), ('x-ratelimit-limit-requests', '10000'), ('x-ratelimit-limit-tokens', '1000000'), ('x-ratelimit-remaining-requests', '9999'), ('x-ratelimit-remaining-tokens', '998751'), ('x-ratelimit-reset-requests', '6ms'), ('x-ratelimit-reset-tokens', '74ms'), ('x-request-id', 'req_ed1ac03772b06836fe0dc07b80ee5e9f'), ('cf-cache-status', 'DYNAMIC'), ('set-cookie', '__cf_bm=8LOE68ipAWmBr5TudWvsVv9So_sM2FyHXQn22NuzR8Q-1720423273-1.0.1.1-0u3eG48hxil9p68xYAn6LFlSxC.5GeWmhv2iwaJ0lD25Y8mx8Rcf2bWXynXI14ajj_cNMrBLIdZy7iiJmW1gFQ; path=/; expires=Mon, 08-Jul-24 07:51:13 GMT; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), ('set-cookie', '_cfuvid=uD5mq4WMmpaetzHu2EtTC1a9XU5j9rx2bjhmijFl1nw-1720423273448-0.0.1.1-604800000; path=/; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), ('server', 'cloudflare'), ('cf-ray', '89fe50ed6fb80598-IAD'), ('content-encoding', 'br'), ('alt-svc', 'h3=":443"; ma=86400')])
DEBUG:openai._base_client:request_id: req_ed1ac03772b06836fe0dc07b80ee5e9f
DEBUG:openai._base_client:Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'json_data': {'messages': [{'content': "Summarize the following chapter in one sentence: but on the inside an important piece was missing they made it basically look like that i had nothing to do with what went down yeah they framed it like that yeah because you were a talent scout yeah apparently yeah they knew they knew that that's not the case and caffeine knows it's not the case you understand and it's between me and you of course as i was going through this for three years they was sitting in on the meetings with me and my lawyers in this so why would they sit if they think it was important if they they're gonna stand behind these niggas and make it look like a notice just trying to scam these niggas yeah when they realize like hold on my nigga this nigga really did all of this shit and y'all think about it if you could if you acquiring a company and its routes and how it became successful why wouldn't you keep the same team wow yeah yeah i see what you're saying i see what you're saying and i can see that fueling", 'role': 'user'}], 'model': 'gpt-3.5-turbo-16k', 'max_tokens': 1000, 'n': 1, 'stream': False, 'temperature': 0.7}}
DEBUG:openai._base_client:Sending HTTP Request: POST https://api.openai.com/v1/chat/completions
DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>
DEBUG:httpcore.http11:send_request_headers.complete
DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>
DEBUG:httpcore.http11:send_request_body.complete
DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>
DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Mon, 08 Jul 2024 07:21:15 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'openai-organization', b'machinekingsmedia'), (b'openai-processing-ms', b'1033'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains'), (b'x-ratelimit-limit-requests', b'10000'), (b'x-ratelimit-limit-tokens', b'1000000'), (b'x-ratelimit-remaining-requests', b'9999'), (b'x-ratelimit-remaining-tokens', b'998750'), (b'x-ratelimit-reset-requests', b'6ms'), (b'x-ratelimit-reset-tokens', b'75ms'), (b'x-request-id', b'req_601f97f4d6de76a635456c0bdd837f2f'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'89fe50f33bbe0598-IAD'), (b'Content-Encoding', b'br'), (b'alt-svc', b'h3=":443"; ma=86400')])
INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>
DEBUG:httpcore.http11:receive_response_body.complete
DEBUG:httpcore.http11:response_closed.started
DEBUG:httpcore.http11:response_closed.complete
DEBUG:openai._base_client:HTTP Response: POST https://api.openai.com/v1/chat/completions "200 OK" Headers({'date': 'Mon, 08 Jul 2024 07:21:15 GMT', 'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'openai-organization': 'machinekingsmedia', 'openai-processing-ms': '1033', 'openai-version': '2020-10-01', 'strict-transport-security': 'max-age=31536000; includeSubDomains', 'x-ratelimit-limit-requests': '10000', 'x-ratelimit-limit-tokens': '1000000', 'x-ratelimit-remaining-requests': '9999', 'x-ratelimit-remaining-tokens': '998750', 'x-ratelimit-reset-requests': '6ms', 'x-ratelimit-reset-tokens': '75ms', 'x-request-id': 'req_601f97f4d6de76a635456c0bdd837f2f', 'cf-cache-status': 'DYNAMIC', 'server': 'cloudflare', 'cf-ray': '89fe50f33bbe0598-IAD', 'content-encoding': 'br', 'alt-svc': 'h3=":443"; ma=86400'})
DEBUG:openai._base_client:request_id: req_601f97f4d6de76a635456c0bdd837f2f
DEBUG:openai._base_client:Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'json_data': {'messages': [{'content': "Summarize the following chapter in one sentence: anger amongst their ranks yeah and you gotta think about it sound like it was all a money move with them bro they tried to cut me out they owe me money you know what i'm saying yeah they were wrong and bro when they went to that situation they were like yo you know three ways is better than four yeah yeah the way they looked at it i wanted to give me the credit that i deserved you know i'm saying period yeah yeah my mentality was i'm gonna make this stupid motherfucker look like he's the man sacrifice my own shit you feel me yeah and their mentality was like nah we caught this nigga out we didn't pay and like no like it was just it was just stupid on smack's part bro like you know you know what business you go before i got there and you was falling off the cliff why would you revert to what you was because when i got there this is the highest you've ever been yeah they was with me not with them niggas yeah but but", 'role': 'user'}], 'model': 'gpt-3.5-turbo-16k', 'max_tokens': 1000, 'n': 1, 'stream': False, 'temperature': 0.7}}
DEBUG:openai._base_client:Sending HTTP Request: POST https://api.openai.com/v1/chat/completions
DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>
DEBUG:httpcore.http11:send_request_headers.complete
DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>
DEBUG:httpcore.http11:send_request_body.complete
DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>
DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Mon, 08 Jul 2024 07:21:16 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'openai-organization', b'machinekingsmedia'), (b'openai-processing-ms', b'486'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains'), (b'x-ratelimit-limit-requests', b'10000'), (b'x-ratelimit-limit-tokens', b'1000000'), (b'x-ratelimit-remaining-requests', b'9999'), (b'x-ratelimit-remaining-tokens', b'998755'), (b'x-ratelimit-reset-requests', b'6ms'), (b'x-ratelimit-reset-tokens', b'74ms'), (b'x-request-id', b'req_34eac7377130b5c8382f0e139d273d02'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'89fe50ff3b480598-IAD'), (b'Content-Encoding', b'br'), (b'alt-svc', b'h3=":443"; ma=86400')])
INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>
DEBUG:httpcore.http11:receive_response_body.complete
DEBUG:httpcore.http11:response_closed.started
DEBUG:httpcore.http11:response_closed.complete
DEBUG:openai._base_client:HTTP Response: POST https://api.openai.com/v1/chat/completions "200 OK" Headers({'date': 'Mon, 08 Jul 2024 07:21:16 GMT', 'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'openai-organization': 'machinekingsmedia', 'openai-processing-ms': '486', 'openai-version': '2020-10-01', 'strict-transport-security': 'max-age=31536000; includeSubDomains', 'x-ratelimit-limit-requests': '10000', 'x-ratelimit-limit-tokens': '1000000', 'x-ratelimit-remaining-requests': '9999', 'x-ratelimit-remaining-tokens': '998755', 'x-ratelimit-reset-requests': '6ms', 'x-ratelimit-reset-tokens': '74ms', 'x-request-id': 'req_34eac7377130b5c8382f0e139d273d02', 'cf-cache-status': 'DYNAMIC', 'server': 'cloudflare', 'cf-ray': '89fe50ff3b480598-IAD', 'content-encoding': 'br', 'alt-svc': 'h3=":443"; ma=86400'})
DEBUG:openai._base_client:request_id: req_34eac7377130b5c8382f0e139d273d02
DEBUG:openai._base_client:Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'json_data': {'messages': [{'content': "Summarize the following chapter in one sentence: just like just like with uh just like with the dallas cowboys um yeah jerry jerry jerry jones thought that you know he could he could run it was the it was the logo the star whatever he could do it without jimmy johnson and um and they haven't been there since i was so happy they ain't advancing the playoff this year too so we can still run with that it is bro it's like when ufc got bored they didn't give it a thing of white in a way it was the cog he was the yeah but he was the face though he was he was the face i remember too when you were wrong i was one of the faces it was me and smack they asked me to fall back because i was out shining him yeah i remember that but smack wasn't really out there talking to the bloggers you used to go on angry fan and big up smack used to be in here i did so much shit that people don't even know that's what i'm saying like it's just it's just if you see them they don't even know what to do", 'role': 'user'}], 'model': 'gpt-3.5-turbo-16k', 'max_tokens': 1000, 'n': 1, 'stream': False, 'temperature': 0.7}}
DEBUG:openai._base_client:Sending HTTP Request: POST https://api.openai.com/v1/chat/completions
DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>
DEBUG:httpcore.http11:send_request_headers.complete
DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>
DEBUG:httpcore.http11:send_request_body.complete
DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>
DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Mon, 08 Jul 2024 07:21:16 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'openai-organization', b'machinekingsmedia'), (b'openai-processing-ms', b'487'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains'), (b'x-ratelimit-limit-requests', b'10000'), (b'x-ratelimit-limit-tokens', b'1000000'), (b'x-ratelimit-remaining-requests', b'9999'), (b'x-ratelimit-remaining-tokens', b'998752'), (b'x-ratelimit-reset-requests', b'6ms'), (b'x-ratelimit-reset-tokens', b'74ms'), (b'x-request-id', b'req_42658e87bdc41394cd8b40654ada6077'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'89fe5103ce150598-IAD'), (b'Content-Encoding', b'br'), (b'alt-svc', b'h3=":443"; ma=86400')])
INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>
DEBUG:httpcore.http11:receive_response_body.complete
DEBUG:httpcore.http11:response_closed.started
DEBUG:httpcore.http11:response_closed.complete
DEBUG:openai._base_client:HTTP Response: POST https://api.openai.com/v1/chat/completions "200 OK" Headers({'date': 'Mon, 08 Jul 2024 07:21:16 GMT', 'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'openai-organization': 'machinekingsmedia', 'openai-processing-ms': '487', 'openai-version': '2020-10-01', 'strict-transport-security': 'max-age=31536000; includeSubDomains', 'x-ratelimit-limit-requests': '10000', 'x-ratelimit-limit-tokens': '1000000', 'x-ratelimit-remaining-requests': '9999', 'x-ratelimit-remaining-tokens': '998752', 'x-ratelimit-reset-requests': '6ms', 'x-ratelimit-reset-tokens': '74ms', 'x-request-id': 'req_42658e87bdc41394cd8b40654ada6077', 'cf-cache-status': 'DYNAMIC', 'server': 'cloudflare', 'cf-ray': '89fe5103ce150598-IAD', 'content-encoding': 'br', 'alt-svc': 'h3=":443"; ma=86400'})
DEBUG:openai._base_client:request_id: req_42658e87bdc41394cd8b40654ada6077
DEBUG:openai._base_client:Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'json_data': {'messages': [{'content': "Summarize the following chapter in one sentence: they don't know what to say they don't even like yo bro like even in business meetings like bro like i have ideas right now that'll that'll make the shit go crazy but i'd never fuck with you know the amount of money you would have to pay me just to resurrect your company yeah but and i've been hearing the adam rumors getting louder this week too about him retiring like as if he made he's done i told you that shit yeah you did tell me that but i didn't see an official announcement but everybody's talking about it no he was never gonna make an announcement he was just like he's gonna do the last event which was uh that's the full circle oh max out back on his last event okay he's not even there oh and he's just gonna do it he was like he's just tired the same shit i said tired of the culture you're putting up money he said he made enough money in the last event where he's just like yo nigga it's not worth the risk why would i put it", 'role': 'user'}], 'model': 'gpt-3.5-turbo-16k', 'max_tokens': 1000, 'n': 1, 'stream': False, 'temperature': 0.7}}
DEBUG:openai._base_client:Sending HTTP Request: POST https://api.openai.com/v1/chat/completions
DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>
DEBUG:httpcore.http11:send_request_headers.complete
DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>
DEBUG:httpcore.http11:send_request_body.complete
DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>
DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Mon, 08 Jul 2024 07:21:17 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'openai-organization', b'machinekingsmedia'), (b'openai-processing-ms', b'546'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains'), (b'x-ratelimit-limit-requests', b'10000'), (b'x-ratelimit-limit-tokens', b'1000000'), (b'x-ratelimit-remaining-requests', b'9999'), (b'x-ratelimit-remaining-tokens', b'998750'), (b'x-ratelimit-reset-requests', b'6ms'), (b'x-ratelimit-reset-tokens', b'75ms'), (b'x-request-id', b'req_9c261222fe9d13bc432effada0541e7c'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'89fe510788300598-IAD'), (b'Content-Encoding', b'br'), (b'alt-svc', b'h3=":443"; ma=86400')])
INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>
DEBUG:httpcore.http11:receive_response_body.complete
DEBUG:httpcore.http11:response_closed.started
DEBUG:httpcore.http11:response_closed.complete
DEBUG:openai._base_client:HTTP Response: POST https://api.openai.com/v1/chat/completions "200 OK" Headers({'date': 'Mon, 08 Jul 2024 07:21:17 GMT', 'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'openai-organization': 'machinekingsmedia', 'openai-processing-ms': '546', 'openai-version': '2020-10-01', 'strict-transport-security': 'max-age=31536000; includeSubDomains', 'x-ratelimit-limit-requests': '10000', 'x-ratelimit-limit-tokens': '1000000', 'x-ratelimit-remaining-requests': '9999', 'x-ratelimit-remaining-tokens': '998750', 'x-ratelimit-reset-requests': '6ms', 'x-ratelimit-reset-tokens': '75ms', 'x-request-id': 'req_9c261222fe9d13bc432effada0541e7c', 'cf-cache-status': 'DYNAMIC', 'server': 'cloudflare', 'cf-ray': '89fe510788300598-IAD', 'content-encoding': 'br', 'alt-svc': 'h3=":443"; ma=86400'})
DEBUG:openai._base_client:request_id: req_9c261222fe9d13bc432effada0541e7c
DEBUG:openai._base_client:Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'json_data': {'messages': [{'content': "Summarize the following chapter in one sentence: back in bro these i think the usd shop in sugar selling chicken out of his house wait a minute wait up i thought that let's get one wing straight was a store nigga it's in his crib he's selling chicken plates out of his house my nigga no no i saw somebody say congratulations and then i was okay he got a little store or whatever chicken wings out of his crib no i no all right look for the location yeah i'm going to check the location i'm gonna look that up i'm gonna look that up bro yo dog i'm not even bullshitting him he's selling when he don't have a crib he's selling him out of where he's staying out of the crib just selling wings the nigga got the shotgun sauce like it's crazy let's get one wing straight i i was i was like i was i i imagine like like since nunu got the teeth store i thought he was next up hold on my son is an animal dog it's okay it's okay it's all right", 'role': 'user'}], 'model': 'gpt-3.5-turbo-16k', 'max_tokens': 1000, 'n': 1, 'stream': False, 'temperature': 0.7}}
DEBUG:openai._base_client:Sending HTTP Request: POST https://api.openai.com/v1/chat/completions
DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>
DEBUG:httpcore.http11:send_request_headers.complete
DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>
DEBUG:httpcore.http11:send_request_body.complete
DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>
DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Mon, 08 Jul 2024 07:21:18 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'openai-organization', b'machinekingsmedia'), (b'openai-processing-ms', b'482'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains'), (b'x-ratelimit-limit-requests', b'10000'), (b'x-ratelimit-limit-tokens', b'1000000'), (b'x-ratelimit-remaining-requests', b'9999'), (b'x-ratelimit-remaining-tokens', b'998765'), (b'x-ratelimit-reset-requests', b'6ms'), (b'x-ratelimit-reset-tokens', b'74ms'), (b'x-request-id', b'req_b5ba3727376f8b40ed1c695e3ff358cd'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'89fe510cfb720598-IAD'), (b'Content-Encoding', b'br'), (b'alt-svc', b'h3=":443"; ma=86400')])
INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>
DEBUG:httpcore.http11:receive_response_body.complete
DEBUG:httpcore.http11:response_closed.started
DEBUG:httpcore.http11:response_closed.complete
DEBUG:openai._base_client:HTTP Response: POST https://api.openai.com/v1/chat/completions "200 OK" Headers({'date': 'Mon, 08 Jul 2024 07:21:18 GMT', 'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'openai-organization': 'machinekingsmedia', 'openai-processing-ms': '482', 'openai-version': '2020-10-01', 'strict-transport-security': 'max-age=31536000; includeSubDomains', 'x-ratelimit-limit-requests': '10000', 'x-ratelimit-limit-tokens': '1000000', 'x-ratelimit-remaining-requests': '9999', 'x-ratelimit-remaining-tokens': '998765', 'x-ratelimit-reset-requests': '6ms', 'x-ratelimit-reset-tokens': '74ms', 'x-request-id': 'req_b5ba3727376f8b40ed1c695e3ff358cd', 'cf-cache-status': 'DYNAMIC', 'server': 'cloudflare', 'cf-ray': '89fe510cfb720598-IAD', 'content-encoding': 'br', 'alt-svc': 'h3=":443"; ma=86400'})
DEBUG:openai._base_client:request_id: req_b5ba3727376f8b40ed1c695e3ff358cd
DEBUG:openai._base_client:Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'json_data': {'messages': [{'content': "Summarize the following chapter in one sentence: i'm gonna check up on that noise i might have to do a little something on that all you gotta do is be like hey shotgun where's the location at i'm trying to pull up to buy some wings no no i ain't gonna ask him now i'm gonna just do my little research that's very yo bro i know uh i'm a thousand percent sure niggas call me and i started laughing i said wait what are you talking about he's selling wings he's supposed to be delivering them but he he can't so he'll be having he'll have to come to the square to pick it up wow yeah and he turning down uh the eternal he turned down the easy battle because of loyalty to url as well like he he priced himself out rather than saying no to look bad in the culture bro he didn't price myself out he was told not to do it and he was like oh i'm scared of smack and now look at him bro them niggas are broke right now them niggas they're broke i don't know", 'role': 'user'}], 'model': 'gpt-3.5-turbo-16k', 'max_tokens': 1000, 'n': 1, 'stream': False, 'temperature': 0.7}}
DEBUG:openai._base_client:Sending HTTP Request: POST https://api.openai.com/v1/chat/completions
DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>
DEBUG:httpcore.http11:send_request_headers.complete
DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>
DEBUG:httpcore.http11:send_request_body.complete
DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>
DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Mon, 08 Jul 2024 07:21:19 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'openai-organization', b'machinekingsmedia'), (b'openai-processing-ms', b'379'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains'), (b'x-ratelimit-limit-requests', b'10000'), (b'x-ratelimit-limit-tokens', b'1000000'), (b'x-ratelimit-remaining-requests', b'9999'), (b'x-ratelimit-remaining-tokens', b'998760'), (b'x-ratelimit-reset-requests', b'6ms'), (b'x-ratelimit-reset-tokens', b'74ms'), (b'x-request-id', b'req_2e68236474069a4960c90f36dfe7c9a2'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'89fe5110bda70598-IAD'), (b'Content-Encoding', b'br'), (b'alt-svc', b'h3=":443"; ma=86400')])
INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>
DEBUG:httpcore.http11:receive_response_body.complete
DEBUG:httpcore.http11:response_closed.started
DEBUG:httpcore.http11:response_closed.complete
DEBUG:openai._base_client:HTTP Response: POST https://api.openai.com/v1/chat/completions "200 OK" Headers({'date': 'Mon, 08 Jul 2024 07:21:19 GMT', 'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'openai-organization': 'machinekingsmedia', 'openai-processing-ms': '379', 'openai-version': '2020-10-01', 'strict-transport-security': 'max-age=31536000; includeSubDomains', 'x-ratelimit-limit-requests': '10000', 'x-ratelimit-limit-tokens': '1000000', 'x-ratelimit-remaining-requests': '9999', 'x-ratelimit-remaining-tokens': '998760', 'x-ratelimit-reset-requests': '6ms', 'x-ratelimit-reset-tokens': '74ms', 'x-request-id': 'req_2e68236474069a4960c90f36dfe7c9a2', 'cf-cache-status': 'DYNAMIC', 'server': 'cloudflare', 'cf-ray': '89fe5110bda70598-IAD', 'content-encoding': 'br', 'alt-svc': 'h3=":443"; ma=86400'})
DEBUG:openai._base_client:request_id: req_2e68236474069a4960c90f36dfe7c9a2
DEBUG:openai._base_client:Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'json_data': {'messages': [{'content': "Summarize the following chapter in one sentence: how bad i was gonna eat i heard i heard that nigga tate rockets back at his well he's been back at his mama crib but he's in the basement just chilling like this they got me a cake though yeah and sent sending them to the bahamas or something right come on that's how you satisfy that's how you satisfy a certain section of society yeah you know just a bit of your style you could say that's how you that's how you satisfy them man and and also like like we discussed many many times before um the the blind loyalty to the brand is it's crazy and and i think it has a little bit to do with nostalgia um and the work that you put in and the glory days of old not what's going on right now but the glory days of old but but this app situation just seems like the worst possible thing for the culture as a whole and their mentality it's not even the app", 'role': 'user'}], 'model': 'gpt-3.5-turbo-16k', 'max_tokens': 1000, 'n': 1, 'stream': False, 'temperature': 0.7}}
DEBUG:openai._base_client:Sending HTTP Request: POST https://api.openai.com/v1/chat/completions
DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>
DEBUG:httpcore.http11:send_request_headers.complete
DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>
DEBUG:httpcore.http11:send_request_body.complete
DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>
DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Mon, 08 Jul 2024 07:21:20 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'openai-organization', b'machinekingsmedia'), (b'openai-processing-ms', b'529'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains'), (b'x-ratelimit-limit-requests', b'10000'), (b'x-ratelimit-limit-tokens', b'1000000'), (b'x-ratelimit-remaining-requests', b'9999'), (b'x-ratelimit-remaining-tokens', b'998773'), (b'x-ratelimit-reset-requests', b'6ms'), (b'x-ratelimit-reset-tokens', b'73ms'), (b'x-request-id', b'req_6a8800b522483dffef7d899976878ca3'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'89fe51186ab70598-IAD'), (b'Content-Encoding', b'br'), (b'alt-svc', b'h3=":443"; ma=86400')])
INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>
DEBUG:httpcore.http11:receive_response_body.complete
DEBUG:httpcore.http11:response_closed.started
DEBUG:httpcore.http11:response_closed.complete
DEBUG:openai._base_client:HTTP Response: POST https://api.openai.com/v1/chat/completions "200 OK" Headers({'date': 'Mon, 08 Jul 2024 07:21:20 GMT', 'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'openai-organization': 'machinekingsmedia', 'openai-processing-ms': '529', 'openai-version': '2020-10-01', 'strict-transport-security': 'max-age=31536000; includeSubDomains', 'x-ratelimit-limit-requests': '10000', 'x-ratelimit-limit-tokens': '1000000', 'x-ratelimit-remaining-requests': '9999', 'x-ratelimit-remaining-tokens': '998773', 'x-ratelimit-reset-requests': '6ms', 'x-ratelimit-reset-tokens': '73ms', 'x-request-id': 'req_6a8800b522483dffef7d899976878ca3', 'cf-cache-status': 'DYNAMIC', 'server': 'cloudflare', 'cf-ray': '89fe51186ab70598-IAD', 'content-encoding': 'br', 'alt-svc': 'h3=":443"; ma=86400'})
DEBUG:openai._base_client:request_id: req_6a8800b522483dffef7d899976878ca3
DEBUG:openai._base_client:Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'json_data': {'messages': [{'content': "Summarize the following chapter in one sentence: situation no more bro it's the fact that they don't have no backing they're not gonna pay for shit ain't nobody gonna put up no money for the niggas but they they're making their own money but they're spending it on jewelry and cars and things of that nature that um if they don't have an office that might be a reflection of them not reinvested into the business itself um you don't even need all that son you don't even need to over half of this shit these niggas are so fucking dumb like yo bro it's the ad situation they they they probably gonna try to sell off what they can if they can at this point so but what can they sell off the the the the battles somebody used the library worth it you know they already have but i just don't see them cashing out in that situation but what's hold on let me let me get my let me get these niggas no problem all right", 'role': 'user'}], 'model': 'gpt-3.5-turbo-16k', 'max_tokens': 1000, 'n': 1, 'stream': False, 'temperature': 0.7}}
DEBUG:openai._base_client:Sending HTTP Request: POST https://api.openai.com/v1/chat/completions
DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>
DEBUG:httpcore.http11:send_request_headers.complete
DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>
DEBUG:httpcore.http11:send_request_body.complete
DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>
DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Mon, 08 Jul 2024 07:21:21 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'openai-organization', b'machinekingsmedia'), (b'openai-processing-ms', b'967'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains'), (b'x-ratelimit-limit-requests', b'10000'), (b'x-ratelimit-limit-tokens', b'1000000'), (b'x-ratelimit-remaining-requests', b'9999'), (b'x-ratelimit-remaining-tokens', b'998771'), (b'x-ratelimit-reset-requests', b'6ms'), (b'x-ratelimit-reset-tokens', b'73ms'), (b'x-request-id', b'req_09f5078fea041c8961d0be3ef7f8cfbc'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'89fe511d3d940598-IAD'), (b'Content-Encoding', b'br'), (b'alt-svc', b'h3=":443"; ma=86400')])
INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>
DEBUG:httpcore.http11:receive_response_body.complete
DEBUG:httpcore.http11:response_closed.started
DEBUG:httpcore.http11:response_closed.complete
DEBUG:openai._base_client:HTTP Response: POST https://api.openai.com/v1/chat/completions "200 OK" Headers({'date': 'Mon, 08 Jul 2024 07:21:21 GMT', 'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'openai-organization': 'machinekingsmedia', 'openai-processing-ms': '967', 'openai-version': '2020-10-01', 'strict-transport-security': 'max-age=31536000; includeSubDomains', 'x-ratelimit-limit-requests': '10000', 'x-ratelimit-limit-tokens': '1000000', 'x-ratelimit-remaining-requests': '9999', 'x-ratelimit-remaining-tokens': '998771', 'x-ratelimit-reset-requests': '6ms', 'x-ratelimit-reset-tokens': '73ms', 'x-request-id': 'req_09f5078fea041c8961d0be3ef7f8cfbc', 'cf-cache-status': 'DYNAMIC', 'server': 'cloudflare', 'cf-ray': '89fe511d3d940598-IAD', 'content-encoding': 'br', 'alt-svc': 'h3=":443"; ma=86400'})
DEBUG:openai._base_client:request_id: req_09f5078fea041c8961d0be3ef7f8cfbc
DEBUG:openai._base_client:Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'json_data': {'messages': [{'content': "Summarize the following chapter in one sentence: and like if i'm if i'm a business dude and i purchased a company and you sold me everything but what made the engine run i'm gonna feel a way yeah when you're telling them like yo this shit got no legs he bullshitting you don't think they did their own research they was just like yo we don't want to get sued and miss this shit because then what happens is you look like y'all was in it with them to kick me out yeah so they was just trying to protect their own ass but they did their own research nigga they knew that i wasn't just a scout too many people saying the same shit like come on bro let me just minimize it bro i've seen their faces i i've seen with smack down when we did the depositions i've seen all that shit on those niggas is a dummy but you know you know so you can just tell like like there's one part like i can't really talk about the deposition shit because it's not whatever but i just recall", 'role': 'user'}], 'model': 'gpt-3.5-turbo-16k', 'max_tokens': 1000, 'n': 1, 'stream': False, 'temperature': 0.7}}
DEBUG:openai._base_client:Sending HTTP Request: POST https://api.openai.com/v1/chat/completions
DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>
DEBUG:httpcore.http11:send_request_headers.complete
DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>
DEBUG:httpcore.http11:send_request_body.complete
DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>
DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Mon, 08 Jul 2024 07:21:22 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'openai-organization', b'machinekingsmedia'), (b'openai-processing-ms', b'591'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains'), (b'x-ratelimit-limit-requests', b'10000'), (b'x-ratelimit-limit-tokens', b'1000000'), (b'x-ratelimit-remaining-requests', b'9999'), (b'x-ratelimit-remaining-tokens', b'998756'), (b'x-ratelimit-reset-requests', b'6ms'), (b'x-ratelimit-reset-tokens', b'74ms'), (b'x-request-id', b'req_62148c1639007e15c9f3486d7087cb69'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'89fe5124cae30598-IAD'), (b'Content-Encoding', b'br'), (b'alt-svc', b'h3=":443"; ma=86400')])
INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>
DEBUG:httpcore.http11:receive_response_body.complete
DEBUG:httpcore.http11:response_closed.started
DEBUG:httpcore.http11:response_closed.complete
DEBUG:openai._base_client:HTTP Response: POST https://api.openai.com/v1/chat/completions "200 OK" Headers({'date': 'Mon, 08 Jul 2024 07:21:22 GMT', 'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'openai-organization': 'machinekingsmedia', 'openai-processing-ms': '591', 'openai-version': '2020-10-01', 'strict-transport-security': 'max-age=31536000; includeSubDomains', 'x-ratelimit-limit-requests': '10000', 'x-ratelimit-limit-tokens': '1000000', 'x-ratelimit-remaining-requests': '9999', 'x-ratelimit-remaining-tokens': '998756', 'x-ratelimit-reset-requests': '6ms', 'x-ratelimit-reset-tokens': '74ms', 'x-request-id': 'req_62148c1639007e15c9f3486d7087cb69', 'cf-cache-status': 'DYNAMIC', 'server': 'cloudflare', 'cf-ray': '89fe5124cae30598-IAD', 'content-encoding': 'br', 'alt-svc': 'h3=":443"; ma=86400'})
DEBUG:openai._base_client:request_id: req_62148c1639007e15c9f3486d7087cb69
DEBUG:openai._base_client:Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'json_data': {'messages': [{'content': "Summarize the following chapter in one sentence: yeah don't don't speak on it if you can't speak on it now because i am recording yeah i'm recording now i was gonna say like i could just see it bro i could see it in his face specifically like i know troy and i know what moves troy i know what motivates him and it's it's all stupid shit like you probably say nigga shit you're in your mid-40s my nigga you worried about a chain and a fucking 300 shirt for 500 whatever the fuck you pay for those shirts yeah yeah you can see the way that they dress like the way they dress as of late like they are the accessorized chains yeah you can kind of see that yeah you don't see fucking jay-z doing that like and then there's something about money that's different like when you got money you don't act like you got money you know what i'm saying yeah like it's one of those situations where you kind of like", 'role': 'user'}], 'model': 'gpt-3.5-turbo-16k', 'max_tokens': 1000, 'n': 1, 'stream': False, 'temperature': 0.7}}
DEBUG:openai._base_client:Sending HTTP Request: POST https://api.openai.com/v1/chat/completions
DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>
DEBUG:httpcore.http11:send_request_headers.complete
DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>
DEBUG:httpcore.http11:send_request_body.complete
DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>
DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Mon, 08 Jul 2024 07:21:22 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'openai-organization', b'machinekingsmedia'), (b'openai-processing-ms', b'487'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains'), (b'x-ratelimit-limit-requests', b'10000'), (b'x-ratelimit-limit-tokens', b'1000000'), (b'x-ratelimit-remaining-requests', b'9999'), (b'x-ratelimit-remaining-tokens', b'998773'), (b'x-ratelimit-reset-requests', b'6ms'), (b'x-ratelimit-reset-tokens', b'73ms'), (b'x-request-id', b'req_931bb7592e4cf1aab38e4179af310bc7'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'89fe5129adbc0598-IAD'), (b'Content-Encoding', b'br'), (b'alt-svc', b'h3=":443"; ma=86400')])
INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>
DEBUG:httpcore.http11:receive_response_body.complete
DEBUG:httpcore.http11:response_closed.started
DEBUG:httpcore.http11:response_closed.complete
DEBUG:openai._base_client:HTTP Response: POST https://api.openai.com/v1/chat/completions "200 OK" Headers({'date': 'Mon, 08 Jul 2024 07:21:22 GMT', 'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'openai-organization': 'machinekingsmedia', 'openai-processing-ms': '487', 'openai-version': '2020-10-01', 'strict-transport-security': 'max-age=31536000; includeSubDomains', 'x-ratelimit-limit-requests': '10000', 'x-ratelimit-limit-tokens': '1000000', 'x-ratelimit-remaining-requests': '9999', 'x-ratelimit-remaining-tokens': '998773', 'x-ratelimit-reset-requests': '6ms', 'x-ratelimit-reset-tokens': '73ms', 'x-request-id': 'req_931bb7592e4cf1aab38e4179af310bc7', 'cf-cache-status': 'DYNAMIC', 'server': 'cloudflare', 'cf-ray': '89fe5129adbc0598-IAD', 'content-encoding': 'br', 'alt-svc': 'h3=":443"; ma=86400'})
DEBUG:openai._base_client:request_id: req_931bb7592e4cf1aab38e4179af310bc7
DEBUG:openai._base_client:Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'json_data': {'messages': [{'content': "Summarize the following chapter in one sentence: they were niggas with real money and then you see a nigga like smack Logan they laugh at them because they don't even know how to like you got to show off you got but you don't show this shit off you don't how you live but but they they are a hip-hop culture though so there is that but their business as business owners that that work with these independent contractors it just seems as though from my observation of that the subscription money has got they've gotten so much reliable subscription money over the years that they kind of got comfortable and they just haven't they it seemed like they've even given up trying to innovate no i mean bro you understand bro like it's like who said who's not the best when you win a championship you gotta understand and that you gotta continue what you're doing in fact you gotta do more because not as expectation their only expectation was to to fuck bitches and wear", 'role': 'user'}], 'model': 'gpt-3.5-turbo-16k', 'max_tokens': 1000, 'n': 1, 'stream': False, 'temperature': 0.7}}
DEBUG:openai._base_client:Sending HTTP Request: POST https://api.openai.com/v1/chat/completions
DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>
DEBUG:httpcore.http11:send_request_headers.complete
DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>
DEBUG:httpcore.http11:send_request_body.complete
DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>
DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Mon, 08 Jul 2024 07:21:24 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'openai-organization', b'machinekingsmedia'), (b'openai-processing-ms', b'952'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains'), (b'x-ratelimit-limit-requests', b'10000'), (b'x-ratelimit-limit-tokens', b'1000000'), (b'x-ratelimit-remaining-requests', b'9999'), (b'x-ratelimit-remaining-tokens', b'998757'), (b'x-ratelimit-reset-requests', b'6ms'), (b'x-ratelimit-reset-tokens', b'74ms'), (b'x-request-id', b'req_f0946ad4d25f43d0bd0dcb34b0b18cbe'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'89fe512ec8870598-IAD'), (b'Content-Encoding', b'br'), (b'alt-svc', b'h3=":443"; ma=86400')])
INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>
DEBUG:httpcore.http11:receive_response_body.complete
DEBUG:httpcore.http11:response_closed.started
DEBUG:httpcore.http11:response_closed.complete
DEBUG:openai._base_client:HTTP Response: POST https://api.openai.com/v1/chat/completions "200 OK" Headers({'date': 'Mon, 08 Jul 2024 07:21:24 GMT', 'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'openai-organization': 'machinekingsmedia', 'openai-processing-ms': '952', 'openai-version': '2020-10-01', 'strict-transport-security': 'max-age=31536000; includeSubDomains', 'x-ratelimit-limit-requests': '10000', 'x-ratelimit-limit-tokens': '1000000', 'x-ratelimit-remaining-requests': '9999', 'x-ratelimit-remaining-tokens': '998757', 'x-ratelimit-reset-requests': '6ms', 'x-ratelimit-reset-tokens': '74ms', 'x-request-id': 'req_f0946ad4d25f43d0bd0dcb34b0b18cbe', 'cf-cache-status': 'DYNAMIC', 'server': 'cloudflare', 'cf-ray': '89fe512ec8870598-IAD', 'content-encoding': 'br', 'alt-svc': 'h3=":443"; ma=86400'})
DEBUG:openai._base_client:request_id: req_f0946ad4d25f43d0bd0dcb34b0b18cbe
DEBUG:openai._base_client:Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'json_data': {'messages': [{'content': "Summarize the following chapter in one sentence: jewelry that's they should yo i'm gonna use that that championship shit for that that um that cowboys that cowboys video that i'm gonna make yeah that's a fire analogy i'm gonna use that analogy for them winning a championship with you pretty much and etching themselves in history with you norbs etching themselves in history but then the expectations are there now and now that without you because yo yo i paid i called you tyrian lannister like you you fuck hold on you watch game of thrones i started through i never finished oh okay okay but you still should watch that shit though norbs because that shit that that's that that's a good ass blog that's a good ass blog and i'll watch i'll hit you back i'm gonna get this nigga started i'll watch it right now all right i'm gonna send it uh maybe like nothing i make is over 12 minutes so it's probably like eight or nine minutes maybe i'll send it to you okay i gotta put this nigga he follow me", 'role': 'user'}], 'model': 'gpt-3.5-turbo-16k', 'max_tokens': 1000, 'n': 1, 'stream': False, 'temperature': 0.7}}
DEBUG:openai._base_client:Sending HTTP Request: POST https://api.openai.com/v1/chat/completions
DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>
DEBUG:httpcore.http11:send_request_headers.complete
DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>
DEBUG:httpcore.http11:send_request_body.complete
DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>
DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Mon, 08 Jul 2024 07:21:26 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'openai-organization', b'machinekingsmedia'), (b'openai-processing-ms', b'803'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains'), (b'x-ratelimit-limit-requests', b'10000'), (b'x-ratelimit-limit-tokens', b'1000000'), (b'x-ratelimit-remaining-requests', b'9999'), (b'x-ratelimit-remaining-tokens', b'998748'), (b'x-ratelimit-reset-requests', b'6ms'), (b'x-ratelimit-reset-tokens', b'75ms'), (b'x-request-id', b'req_3ae20f2e9ee5ba2f34920ee401dd0b74'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'89fe513a0f4c0598-IAD'), (b'Content-Encoding', b'br'), (b'alt-svc', b'h3=":443"; ma=86400')])
INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>
DEBUG:httpcore.http11:receive_response_body.complete
DEBUG:httpcore.http11:response_closed.started
DEBUG:httpcore.http11:response_closed.complete
DEBUG:openai._base_client:HTTP Response: POST https://api.openai.com/v1/chat/completions "200 OK" Headers({'date': 'Mon, 08 Jul 2024 07:21:26 GMT', 'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'openai-organization': 'machinekingsmedia', 'openai-processing-ms': '803', 'openai-version': '2020-10-01', 'strict-transport-security': 'max-age=31536000; includeSubDomains', 'x-ratelimit-limit-requests': '10000', 'x-ratelimit-limit-tokens': '1000000', 'x-ratelimit-remaining-requests': '9999', 'x-ratelimit-remaining-tokens': '998748', 'x-ratelimit-reset-requests': '6ms', 'x-ratelimit-reset-tokens': '75ms', 'x-request-id': 'req_3ae20f2e9ee5ba2f34920ee401dd0b74', 'cf-cache-status': 'DYNAMIC', 'server': 'cloudflare', 'cf-ray': '89fe513a0f4c0598-IAD', 'content-encoding': 'br', 'alt-svc': 'h3=":443"; ma=86400'})
DEBUG:openai._base_client:request_id: req_3ae20f2e9ee5ba2f34920ee401dd0b74
DEBUG:openai._base_client:Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'json_data': {'messages': [{'content': "Summarize the following chapter in one sentence: around all right my my baby girl i'm gonna be putting her to bed at any moment so um if not then i'll hit you later on tonight all right", 'role': 'user'}], 'model': 'gpt-3.5-turbo-16k', 'max_tokens': 1000, 'n': 1, 'stream': False, 'temperature': 0.7}}
DEBUG:openai._base_client:Sending HTTP Request: POST https://api.openai.com/v1/chat/completions
DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>
DEBUG:httpcore.http11:send_request_headers.complete
DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>
DEBUG:httpcore.http11:send_request_body.complete
DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>
DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Mon, 08 Jul 2024 07:21:27 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'openai-organization', b'machinekingsmedia'), (b'openai-processing-ms', b'762'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains'), (b'x-ratelimit-limit-requests', b'10000'), (b'x-ratelimit-limit-tokens', b'1000000'), (b'x-ratelimit-remaining-requests', b'9999'), (b'x-ratelimit-remaining-tokens', b'998951'), (b'x-ratelimit-reset-requests', b'6ms'), (b'x-ratelimit-reset-tokens', b'62ms'), (b'x-request-id', b'req_3922ba54e2624f3cca80a4a6ef6828d3'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'89fe51444d670598-IAD'), (b'Content-Encoding', b'br'), (b'alt-svc', b'h3=":443"; ma=86400')])
INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>
DEBUG:httpcore.http11:receive_response_body.complete
DEBUG:httpcore.http11:response_closed.started
DEBUG:httpcore.http11:response_closed.complete
DEBUG:openai._base_client:HTTP Response: POST https://api.openai.com/v1/chat/completions "200 OK" Headers({'date': 'Mon, 08 Jul 2024 07:21:27 GMT', 'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'openai-organization': 'machinekingsmedia', 'openai-processing-ms': '762', 'openai-version': '2020-10-01', 'strict-transport-security': 'max-age=31536000; includeSubDomains', 'x-ratelimit-limit-requests': '10000', 'x-ratelimit-limit-tokens': '1000000', 'x-ratelimit-remaining-requests': '9999', 'x-ratelimit-remaining-tokens': '998951', 'x-ratelimit-reset-requests': '6ms', 'x-ratelimit-reset-tokens': '62ms', 'x-request-id': 'req_3922ba54e2624f3cca80a4a6ef6828d3', 'cf-cache-status': 'DYNAMIC', 'server': 'cloudflare', 'cf-ray': '89fe51444d670598-IAD', 'content-encoding': 'br', 'alt-svc': 'h3=":443"; ma=86400'})
DEBUG:openai._base_client:request_id: req_3922ba54e2624f3cca80a4a6ef6828d3
DEBUG:openai._base_client:Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'json_data': {'messages': [{'content': "Provide a brief summary of the following transcript: File: 240204_1940.mp3\nRecording Date: 2024-02-04\nDuration: 00:13:13.60\nTranscription Date: 2024-07-07 06:17:21\n\n1\n00:00:00,000 --> 00:00:04,800\nput it out but so you said you the way you articulated though i really like that you said\n\n2\n00:00:04,800 --> 00:00:09,120\na cog in the machine they lost a cog in the machine yo did you see my game of thrones\n\n3\n00:00:09,120 --> 00:00:15,600\nbreakdown of the of the um the the the leagues in battle right right now how i framed it with you\n\n4\n00:00:16,719 --> 00:00:20,639\nno i gotta seriously because i don't i try not to watch none of this yeah yeah i know you said\n\n5\n00:00:20,639 --> 00:00:24,879\nyou was gonna do that too no i was i was that was a little test along with a question because\n\n6\n00:00:24,879 --> 00:00:30,959\nyou say you were stepping away from the whole negativity and all of that and yo the fact that\n\n7\n00:00:30,959 --> 00:00:37,840\nis that everybody's saying that is dying right now i think about you so much but anyway uh but\n\n8\n00:00:37,840 --> 00:00:44,880\nwhat i pointed out was how like it seems like we're starting to feel the after effects of the\n\n9\n00:00:44,880 --> 00:00:51,919\nlack of innovation from that side so you saying that the deal they made with caffeine was like\n\n10\n00:00:51,919 --> 00:00:56,959\nthem selling them a machine but a cog was missing in the machine it looked the same from the outside\n\n11\n00:00:56,959 --> 00:01:02,000\nbut on the inside an important piece was missing they made it basically look like that i had\n\n12\n00:01:02,000 --> 00:01:07,599\nnothing to do with what went down yeah they framed it like that yeah because you were a talent scout\n\n13\n00:01:08,320 --> 00:01:16,800\nyeah apparently yeah they knew they knew that that's not the case and caffeine knows it's not\n\n14\n00:01:16,800 --> 00:01:22,959\nthe case you understand and it's between me and you of course as i was going through this for three\n\n15\n00:01:22,959 --> 00:01:28,720\nyears they was sitting in on the meetings with me and my lawyers in this so why would they sit\n\n16\n00:01:28,720 --> 00:01:32,320\nif they think it was important if they they're gonna stand behind these niggas and make it look\n\n17\n00:01:32,320 --> 00:01:37,040\nlike a notice just trying to scam these niggas yeah when they realize like hold on my nigga\n\n18\n00:01:38,320 --> 00:01:43,680\nthis nigga really did all of this shit and y'all think about it if you could if you acquiring a\n\n19\n00:01:43,680 --> 00:01:50,000\ncompany and its routes and how it became successful why wouldn't you keep the same team\n\n20\n00:01:51,919 --> 00:01:57,599\nwow yeah yeah i see what you're saying i see what you're saying and i can see that fueling\n\n21\n00:01:57,599 --> 00:02:03,839\nanger amongst their ranks yeah and you gotta think about it sound like it was all a money\n\n22\n00:02:03,839 --> 00:02:07,680\nmove with them bro they tried to cut me out they owe me money you know what i'm saying yeah\n\n23\n00:02:07,680 --> 00:02:14,559\nthey were wrong and bro when they went to that situation they were like yo you know three ways\n\n24\n00:02:14,559 --> 00:02:20,559\nis better than four yeah yeah the way they looked at it i wanted to give me the credit that i\n\n25\n00:02:20,559 --> 00:02:26,720\ndeserved you know i'm saying period yeah yeah my mentality was i'm gonna make this stupid\n\n26\n00:02:26,720 --> 00:02:33,839\nmotherfucker look like he's the man sacrifice my own shit you feel me yeah and their mentality\n\n27\n00:02:33,839 --> 00:02:38,880\nwas like nah we caught this nigga out we didn't pay and like no like it was just it was just\n\n28\n00:02:38,880 --> 00:02:44,399\nstupid on smack's part bro like you know you know what business you go before i got there\n\n29\n00:02:44,399 --> 00:02:50,639\nand you was falling off the cliff why would you revert to what you was because when i got there\n\n30\n00:02:50,639 --> 00:02:58,559\nthis is the highest you've ever been yeah they was with me not with them niggas yeah but but\n\n31\n00:02:58,559 --> 00:03:07,039\njust like just like with uh ", 'role': 'user'}], 'model': 'gpt-3.5-turbo-16k', 'max_tokens': 1000, 'n': 1, 'stream': False, 'temperature': 0.7}}
DEBUG:openai._base_client:Sending HTTP Request: POST https://api.openai.com/v1/chat/completions
DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>
DEBUG:httpcore.http11:send_request_headers.complete
DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>
DEBUG:httpcore.http11:send_request_body.complete
DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>
DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Mon, 08 Jul 2024 07:21:30 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'openai-organization', b'machinekingsmedia'), (b'openai-processing-ms', b'1785'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains'), (b'x-ratelimit-limit-requests', b'10000'), (b'x-ratelimit-limit-tokens', b'1000000'), (b'x-ratelimit-remaining-requests', b'9999'), (b'x-ratelimit-remaining-tokens', b'997985'), (b'x-ratelimit-reset-requests', b'6ms'), (b'x-ratelimit-reset-tokens', b'120ms'), (b'x-request-id', b'req_ce2f8b52250f0f5555d819641b0a7f1e'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'89fe514b19800598-IAD'), (b'Content-Encoding', b'br'), (b'alt-svc', b'h3=":443"; ma=86400')])
INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>
DEBUG:httpcore.http11:receive_response_body.complete
DEBUG:httpcore.http11:response_closed.started
DEBUG:httpcore.http11:response_closed.complete
DEBUG:openai._base_client:HTTP Response: POST https://api.openai.com/v1/chat/completions "200 OK" Headers({'date': 'Mon, 08 Jul 2024 07:21:30 GMT', 'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'openai-organization': 'machinekingsmedia', 'openai-processing-ms': '1785', 'openai-version': '2020-10-01', 'strict-transport-security': 'max-age=31536000; includeSubDomains', 'x-ratelimit-limit-requests': '10000', 'x-ratelimit-limit-tokens': '1000000', 'x-ratelimit-remaining-requests': '9999', 'x-ratelimit-remaining-tokens': '997985', 'x-ratelimit-reset-requests': '6ms', 'x-ratelimit-reset-tokens': '120ms', 'x-request-id': 'req_ce2f8b52250f0f5555d819641b0a7f1e', 'cf-cache-status': 'DYNAMIC', 'server': 'cloudflare', 'cf-ray': '89fe514b19800598-IAD', 'content-encoding': 'br', 'alt-svc': 'h3=":443"; ma=86400'})
DEBUG:openai._base_client:request_id: req_ce2f8b52250f0f5555d819641b0a7f1e
DEBUG:openai._base_client:Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'json_data': {'messages': [{'content': "Generate a short, descriptive title for the following chapter content: put it out but so you said you the way you articulated though i really like that you said a cog in the machine they lost a cog in the machine yo did you see my game of thrones breakdown of the of the um the the the leagues in battle right right now how i framed it with you no i gotta seriously because i don't i try not to watch none of this yeah yeah i know you said you was gonna do that too no i was i was that was a little test along with a question because you say you were stepping away from the whole negativity and all of that and yo the fact that is that everybody's saying that is dying right now i think about you so much but anyway uh but what i pointed out was how like it seems like we're starting to feel the after effects of the lack of innovation from that side so you saying that the deal they made with caffeine was like them selling them a machine but a cog was missing in the machine it looked the same from the outside", 'role': 'user'}], 'model': 'gpt-3.5-turbo-16k', 'max_tokens': 1000, 'n': 1, 'stream': False, 'temperature': 0.7}}
DEBUG:openai._base_client:Sending HTTP Request: POST https://api.openai.com/v1/chat/completions
DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>
DEBUG:httpcore.http11:send_request_headers.complete
DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>
DEBUG:httpcore.http11:send_request_body.complete
DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>
DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Mon, 08 Jul 2024 07:21:30 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'openai-organization', b'machinekingsmedia'), (b'openai-processing-ms', b'295'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains'), (b'x-ratelimit-limit-requests', b'10000'), (b'x-ratelimit-limit-tokens', b'1000000'), (b'x-ratelimit-remaining-requests', b'9999'), (b'x-ratelimit-remaining-tokens', b'998746'), (b'x-ratelimit-reset-requests', b'6ms'), (b'x-ratelimit-reset-tokens', b'75ms'), (b'x-request-id', b'req_82be9fd3e7cc89738b2e5331d9ae5ffd'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'89fe515d2c530598-IAD'), (b'Content-Encoding', b'br'), (b'alt-svc', b'h3=":443"; ma=86400')])
INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>
DEBUG:httpcore.http11:receive_response_body.complete
DEBUG:httpcore.http11:response_closed.started
DEBUG:httpcore.http11:response_closed.complete
DEBUG:openai._base_client:HTTP Response: POST https://api.openai.com/v1/chat/completions "200 OK" Headers({'date': 'Mon, 08 Jul 2024 07:21:30 GMT', 'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'openai-organization': 'machinekingsmedia', 'openai-processing-ms': '295', 'openai-version': '2020-10-01', 'strict-transport-security': 'max-age=31536000; includeSubDomains', 'x-ratelimit-limit-requests': '10000', 'x-ratelimit-limit-tokens': '1000000', 'x-ratelimit-remaining-requests': '9999', 'x-ratelimit-remaining-tokens': '998746', 'x-ratelimit-reset-requests': '6ms', 'x-ratelimit-reset-tokens': '75ms', 'x-request-id': 'req_82be9fd3e7cc89738b2e5331d9ae5ffd', 'cf-cache-status': 'DYNAMIC', 'server': 'cloudflare', 'cf-ray': '89fe515d2c530598-IAD', 'content-encoding': 'br', 'alt-svc': 'h3=":443"; ma=86400'})
DEBUG:openai._base_client:request_id: req_82be9fd3e7cc89738b2e5331d9ae5ffd
DEBUG:openai._base_client:Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'json_data': {'messages': [{'content': "Generate a short, descriptive title for the following chapter content: but on the inside an important piece was missing they made it basically look like that i had nothing to do with what went down yeah they framed it like that yeah because you were a talent scout yeah apparently yeah they knew they knew that that's not the case and caffeine knows it's not the case you understand and it's between me and you of course as i was going through this for three years they was sitting in on the meetings with me and my lawyers in this so why would they sit if they think it was important if they they're gonna stand behind these niggas and make it look like a notice just trying to scam these niggas yeah when they realize like hold on my nigga this nigga really did all of this shit and y'all think about it if you could if you acquiring a company and its routes and how it became successful why wouldn't you keep the same team wow yeah yeah i see what you're saying i see what you're saying and i can see that fueling", 'role': 'user'}], 'model': 'gpt-3.5-turbo-16k', 'max_tokens': 1000, 'n': 1, 'stream': False, 'temperature': 0.7}}
DEBUG:openai._base_client:Sending HTTP Request: POST https://api.openai.com/v1/chat/completions
DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>
DEBUG:httpcore.http11:send_request_headers.complete
DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>
DEBUG:httpcore.http11:send_request_body.complete
DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>
DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Mon, 08 Jul 2024 07:21:31 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'openai-organization', b'machinekingsmedia'), (b'openai-processing-ms', b'350'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains'), (b'x-ratelimit-limit-requests', b'10000'), (b'x-ratelimit-limit-tokens', b'1000000'), (b'x-ratelimit-remaining-requests', b'9999'), (b'x-ratelimit-remaining-tokens', b'998744'), (b'x-ratelimit-reset-requests', b'6ms'), (b'x-ratelimit-reset-tokens', b'75ms'), (b'x-request-id', b'req_cb22aed3f739631e09cd166907d4eb31'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'89fe51608ede0598-IAD'), (b'Content-Encoding', b'br'), (b'alt-svc', b'h3=":443"; ma=86400')])
INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>
DEBUG:httpcore.http11:receive_response_body.complete
DEBUG:httpcore.http11:response_closed.started
DEBUG:httpcore.http11:response_closed.complete
DEBUG:openai._base_client:HTTP Response: POST https://api.openai.com/v1/chat/completions "200 OK" Headers({'date': 'Mon, 08 Jul 2024 07:21:31 GMT', 'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'openai-organization': 'machinekingsmedia', 'openai-processing-ms': '350', 'openai-version': '2020-10-01', 'strict-transport-security': 'max-age=31536000; includeSubDomains', 'x-ratelimit-limit-requests': '10000', 'x-ratelimit-limit-tokens': '1000000', 'x-ratelimit-remaining-requests': '9999', 'x-ratelimit-remaining-tokens': '998744', 'x-ratelimit-reset-requests': '6ms', 'x-ratelimit-reset-tokens': '75ms', 'x-request-id': 'req_cb22aed3f739631e09cd166907d4eb31', 'cf-cache-status': 'DYNAMIC', 'server': 'cloudflare', 'cf-ray': '89fe51608ede0598-IAD', 'content-encoding': 'br', 'alt-svc': 'h3=":443"; ma=86400'})
DEBUG:openai._base_client:request_id: req_cb22aed3f739631e09cd166907d4eb31
DEBUG:openai._base_client:Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'json_data': {'messages': [{'content': "Generate a short, descriptive title for the following chapter content: anger amongst their ranks yeah and you gotta think about it sound like it was all a money move with them bro they tried to cut me out they owe me money you know what i'm saying yeah they were wrong and bro when they went to that situation they were like yo you know three ways is better than four yeah yeah the way they looked at it i wanted to give me the credit that i deserved you know i'm saying period yeah yeah my mentality was i'm gonna make this stupid motherfucker look like he's the man sacrifice my own shit you feel me yeah and their mentality was like nah we caught this nigga out we didn't pay and like no like it was just it was just stupid on smack's part bro like you know you know what business you go before i got there and you was falling off the cliff why would you revert to what you was because when i got there this is the highest you've ever been yeah they was with me not with them niggas yeah but but", 'role': 'user'}], 'model': 'gpt-3.5-turbo-16k', 'max_tokens': 1000, 'n': 1, 'stream': False, 'temperature': 0.7}}
DEBUG:openai._base_client:Sending HTTP Request: POST https://api.openai.com/v1/chat/completions
DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>
DEBUG:httpcore.http11:send_request_headers.complete
DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>
DEBUG:httpcore.http11:send_request_body.complete
DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>
DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Mon, 08 Jul 2024 07:21:31 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'openai-organization', b'machinekingsmedia'), (b'openai-processing-ms', b'330'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains'), (b'x-ratelimit-limit-requests', b'10000'), (b'x-ratelimit-limit-tokens', b'1000000'), (b'x-ratelimit-remaining-requests', b'9999'), (b'x-ratelimit-remaining-tokens', b'998748'), (b'x-ratelimit-reset-requests', b'6ms'), (b'x-ratelimit-reset-tokens', b'75ms'), (b'x-request-id', b'req_337bfd1ceb3d81229480ef29d4a60eeb'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'89fe516368c20598-IAD'), (b'Content-Encoding', b'br'), (b'alt-svc', b'h3=":443"; ma=86400')])
INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>
DEBUG:httpcore.http11:receive_response_body.complete
DEBUG:httpcore.http11:response_closed.started
DEBUG:httpcore.http11:response_closed.complete
DEBUG:openai._base_client:HTTP Response: POST https://api.openai.com/v1/chat/completions "200 OK" Headers({'date': 'Mon, 08 Jul 2024 07:21:31 GMT', 'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'openai-organization': 'machinekingsmedia', 'openai-processing-ms': '330', 'openai-version': '2020-10-01', 'strict-transport-security': 'max-age=31536000; includeSubDomains', 'x-ratelimit-limit-requests': '10000', 'x-ratelimit-limit-tokens': '1000000', 'x-ratelimit-remaining-requests': '9999', 'x-ratelimit-remaining-tokens': '998748', 'x-ratelimit-reset-requests': '6ms', 'x-ratelimit-reset-tokens': '75ms', 'x-request-id': 'req_337bfd1ceb3d81229480ef29d4a60eeb', 'cf-cache-status': 'DYNAMIC', 'server': 'cloudflare', 'cf-ray': '89fe516368c20598-IAD', 'content-encoding': 'br', 'alt-svc': 'h3=":443"; ma=86400'})
DEBUG:openai._base_client:request_id: req_337bfd1ceb3d81229480ef29d4a60eeb
DEBUG:openai._base_client:Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'json_data': {'messages': [{'content': "Generate a short, descriptive title for the following chapter content: just like just like with uh just like with the dallas cowboys um yeah jerry jerry jerry jones thought that you know he could he could run it was the it was the logo the star whatever he could do it without jimmy johnson and um and they haven't been there since i was so happy they ain't advancing the playoff this year too so we can still run with that it is bro it's like when ufc got bored they didn't give it a thing of white in a way it was the cog he was the yeah but he was the face though he was he was the face i remember too when you were wrong i was one of the faces it was me and smack they asked me to fall back because i was out shining him yeah i remember that but smack wasn't really out there talking to the bloggers you used to go on angry fan and big up smack used to be in here i did so much shit that people don't even know that's what i'm saying like it's just it's just if you see them they don't even know what to do", 'role': 'user'}], 'model': 'gpt-3.5-turbo-16k', 'max_tokens': 1000, 'n': 1, 'stream': False, 'temperature': 0.7}}
DEBUG:openai._base_client:Sending HTTP Request: POST https://api.openai.com/v1/chat/completions
DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>
DEBUG:httpcore.http11:send_request_headers.complete
DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>
DEBUG:httpcore.http11:send_request_body.complete
DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>
DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Mon, 08 Jul 2024 07:21:32 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'openai-organization', b'machinekingsmedia'), (b'openai-processing-ms', b'357'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains'), (b'x-ratelimit-limit-requests', b'10000'), (b'x-ratelimit-limit-tokens', b'1000000'), (b'x-ratelimit-remaining-requests', b'9999'), (b'x-ratelimit-remaining-tokens', b'998746'), (b'x-ratelimit-reset-requests', b'6ms'), (b'x-ratelimit-reset-tokens', b'75ms'), (b'x-request-id', b'req_2c84e94f11fa1edcae0ca0a1d8126be6'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'89fe5166fb000598-IAD'), (b'Content-Encoding', b'br'), (b'alt-svc', b'h3=":443"; ma=86400')])
INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>
DEBUG:httpcore.http11:receive_response_body.complete
DEBUG:httpcore.http11:response_closed.started
DEBUG:httpcore.http11:response_closed.complete
DEBUG:openai._base_client:HTTP Response: POST https://api.openai.com/v1/chat/completions "200 OK" Headers({'date': 'Mon, 08 Jul 2024 07:21:32 GMT', 'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'openai-organization': 'machinekingsmedia', 'openai-processing-ms': '357', 'openai-version': '2020-10-01', 'strict-transport-security': 'max-age=31536000; includeSubDomains', 'x-ratelimit-limit-requests': '10000', 'x-ratelimit-limit-tokens': '1000000', 'x-ratelimit-remaining-requests': '9999', 'x-ratelimit-remaining-tokens': '998746', 'x-ratelimit-reset-requests': '6ms', 'x-ratelimit-reset-tokens': '75ms', 'x-request-id': 'req_2c84e94f11fa1edcae0ca0a1d8126be6', 'cf-cache-status': 'DYNAMIC', 'server': 'cloudflare', 'cf-ray': '89fe5166fb000598-IAD', 'content-encoding': 'br', 'alt-svc': 'h3=":443"; ma=86400'})
DEBUG:openai._base_client:request_id: req_2c84e94f11fa1edcae0ca0a1d8126be6
DEBUG:openai._base_client:Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'json_data': {'messages': [{'content': "Generate a short, descriptive title for the following chapter content: they don't know what to say they don't even like yo bro like even in business meetings like bro like i have ideas right now that'll that'll make the shit go crazy but i'd never fuck with you know the amount of money you would have to pay me just to resurrect your company yeah but and i've been hearing the adam rumors getting louder this week too about him retiring like as if he made he's done i told you that shit yeah you did tell me that but i didn't see an official announcement but everybody's talking about it no he was never gonna make an announcement he was just like he's gonna do the last event which was uh that's the full circle oh max out back on his last event okay he's not even there oh and he's just gonna do it he was like he's just tired the same shit i said tired of the culture you're putting up money he said he made enough money in the last event where he's just like yo nigga it's not worth the risk why would i put it", 'role': 'user'}], 'model': 'gpt-3.5-turbo-16k', 'max_tokens': 1000, 'n': 1, 'stream': False, 'temperature': 0.7}}
DEBUG:openai._base_client:Sending HTTP Request: POST https://api.openai.com/v1/chat/completions
DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>
DEBUG:httpcore.http11:send_request_headers.complete
DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>
DEBUG:httpcore.http11:send_request_body.complete
DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>
DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Mon, 08 Jul 2024 07:21:33 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'openai-organization', b'machinekingsmedia'), (b'openai-processing-ms', b'225'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains'), (b'x-ratelimit-limit-requests', b'10000'), (b'x-ratelimit-limit-tokens', b'1000000'), (b'x-ratelimit-remaining-requests', b'9999'), (b'x-ratelimit-remaining-tokens', b'998744'), (b'x-ratelimit-reset-requests', b'6ms'), (b'x-ratelimit-reset-tokens', b'75ms'), (b'x-request-id', b'req_4e5f6306c41a36bd697e9007cb9dbcca'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'89fe516abd420598-IAD'), (b'Content-Encoding', b'br'), (b'alt-svc', b'h3=":443"; ma=86400')])
INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>
DEBUG:httpcore.http11:receive_response_body.complete
DEBUG:httpcore.http11:response_closed.started
DEBUG:httpcore.http11:response_closed.complete
DEBUG:openai._base_client:HTTP Response: POST https://api.openai.com/v1/chat/completions "200 OK" Headers({'date': 'Mon, 08 Jul 2024 07:21:33 GMT', 'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'openai-organization': 'machinekingsmedia', 'openai-processing-ms': '225', 'openai-version': '2020-10-01', 'strict-transport-security': 'max-age=31536000; includeSubDomains', 'x-ratelimit-limit-requests': '10000', 'x-ratelimit-limit-tokens': '1000000', 'x-ratelimit-remaining-requests': '9999', 'x-ratelimit-remaining-tokens': '998744', 'x-ratelimit-reset-requests': '6ms', 'x-ratelimit-reset-tokens': '75ms', 'x-request-id': 'req_4e5f6306c41a36bd697e9007cb9dbcca', 'cf-cache-status': 'DYNAMIC', 'server': 'cloudflare', 'cf-ray': '89fe516abd420598-IAD', 'content-encoding': 'br', 'alt-svc': 'h3=":443"; ma=86400'})
DEBUG:openai._base_client:request_id: req_4e5f6306c41a36bd697e9007cb9dbcca
DEBUG:openai._base_client:Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'json_data': {'messages': [{'content': "Generate a short, descriptive title for the following chapter content: back in bro these i think the usd shop in sugar selling chicken out of his house wait a minute wait up i thought that let's get one wing straight was a store nigga it's in his crib he's selling chicken plates out of his house my nigga no no i saw somebody say congratulations and then i was okay he got a little store or whatever chicken wings out of his crib no i no all right look for the location yeah i'm going to check the location i'm gonna look that up i'm gonna look that up bro yo dog i'm not even bullshitting him he's selling when he don't have a crib he's selling him out of where he's staying out of the crib just selling wings the nigga got the shotgun sauce like it's crazy let's get one wing straight i i was i was like i was i i imagine like like since nunu got the teeth store i thought he was next up hold on my son is an animal dog it's okay it's okay it's all right", 'role': 'user'}], 'model': 'gpt-3.5-turbo-16k', 'max_tokens': 1000, 'n': 1, 'stream': False, 'temperature': 0.7}}
DEBUG:openai._base_client:Sending HTTP Request: POST https://api.openai.com/v1/chat/completions
DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>
DEBUG:httpcore.http11:send_request_headers.complete
DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>
DEBUG:httpcore.http11:send_request_body.complete
DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>
DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Mon, 08 Jul 2024 07:21:34 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'openai-organization', b'machinekingsmedia'), (b'openai-processing-ms', b'211'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains'), (b'x-ratelimit-limit-requests', b'10000'), (b'x-ratelimit-limit-tokens', b'1000000'), (b'x-ratelimit-remaining-requests', b'9999'), (b'x-ratelimit-remaining-tokens', b'998759'), (b'x-ratelimit-reset-requests', b'6ms'), (b'x-ratelimit-reset-tokens', b'74ms'), (b'x-request-id', b'req_1801f6c8bff0c4b907039deb386675bb'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'89fe517189d70598-IAD'), (b'Content-Encoding', b'br'), (b'alt-svc', b'h3=":443"; ma=86400')])
INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>
DEBUG:httpcore.http11:receive_response_body.complete
DEBUG:httpcore.http11:response_closed.started
DEBUG:httpcore.http11:response_closed.complete
DEBUG:openai._base_client:HTTP Response: POST https://api.openai.com/v1/chat/completions "200 OK" Headers({'date': 'Mon, 08 Jul 2024 07:21:34 GMT', 'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'openai-organization': 'machinekingsmedia', 'openai-processing-ms': '211', 'openai-version': '2020-10-01', 'strict-transport-security': 'max-age=31536000; includeSubDomains', 'x-ratelimit-limit-requests': '10000', 'x-ratelimit-limit-tokens': '1000000', 'x-ratelimit-remaining-requests': '9999', 'x-ratelimit-remaining-tokens': '998759', 'x-ratelimit-reset-requests': '6ms', 'x-ratelimit-reset-tokens': '74ms', 'x-request-id': 'req_1801f6c8bff0c4b907039deb386675bb', 'cf-cache-status': 'DYNAMIC', 'server': 'cloudflare', 'cf-ray': '89fe517189d70598-IAD', 'content-encoding': 'br', 'alt-svc': 'h3=":443"; ma=86400'})
DEBUG:openai._base_client:request_id: req_1801f6c8bff0c4b907039deb386675bb
DEBUG:openai._base_client:Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'json_data': {'messages': [{'content': "Generate a short, descriptive title for the following chapter content: i'm gonna check up on that noise i might have to do a little something on that all you gotta do is be like hey shotgun where's the location at i'm trying to pull up to buy some wings no no i ain't gonna ask him now i'm gonna just do my little research that's very yo bro i know uh i'm a thousand percent sure niggas call me and i started laughing i said wait what are you talking about he's selling wings he's supposed to be delivering them but he he can't so he'll be having he'll have to come to the square to pick it up wow yeah and he turning down uh the eternal he turned down the easy battle because of loyalty to url as well like he he priced himself out rather than saying no to look bad in the culture bro he didn't price myself out he was told not to do it and he was like oh i'm scared of smack and now look at him bro them niggas are broke right now them niggas they're broke i don't know", 'role': 'user'}], 'model': 'gpt-3.5-turbo-16k', 'max_tokens': 1000, 'n': 1, 'stream': False, 'temperature': 0.7}}
DEBUG:openai._base_client:Sending HTTP Request: POST https://api.openai.com/v1/chat/completions
DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>
DEBUG:httpcore.http11:send_request_headers.complete
DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>
DEBUG:httpcore.http11:send_request_body.complete
DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>
DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Mon, 08 Jul 2024 07:21:35 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'openai-organization', b'machinekingsmedia'), (b'openai-processing-ms', b'203'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains'), (b'x-ratelimit-limit-requests', b'10000'), (b'x-ratelimit-limit-tokens', b'1000000'), (b'x-ratelimit-remaining-requests', b'9999'), (b'x-ratelimit-remaining-tokens', b'998756'), (b'x-ratelimit-reset-requests', b'6ms'), (b'x-ratelimit-reset-tokens', b'74ms'), (b'x-request-id', b'req_35aba97fa251eb56ccfd1fccbfae7ecc'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'89fe5174dc380598-IAD'), (b'Content-Encoding', b'br'), (b'alt-svc', b'h3=":443"; ma=86400')])
INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>
DEBUG:httpcore.http11:receive_response_body.complete
DEBUG:httpcore.http11:response_closed.started
DEBUG:httpcore.http11:response_closed.complete
DEBUG:openai._base_client:HTTP Response: POST https://api.openai.com/v1/chat/completions "200 OK" Headers({'date': 'Mon, 08 Jul 2024 07:21:35 GMT', 'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'openai-organization': 'machinekingsmedia', 'openai-processing-ms': '203', 'openai-version': '2020-10-01', 'strict-transport-security': 'max-age=31536000; includeSubDomains', 'x-ratelimit-limit-requests': '10000', 'x-ratelimit-limit-tokens': '1000000', 'x-ratelimit-remaining-requests': '9999', 'x-ratelimit-remaining-tokens': '998756', 'x-ratelimit-reset-requests': '6ms', 'x-ratelimit-reset-tokens': '74ms', 'x-request-id': 'req_35aba97fa251eb56ccfd1fccbfae7ecc', 'cf-cache-status': 'DYNAMIC', 'server': 'cloudflare', 'cf-ray': '89fe5174dc380598-IAD', 'content-encoding': 'br', 'alt-svc': 'h3=":443"; ma=86400'})
DEBUG:openai._base_client:request_id: req_35aba97fa251eb56ccfd1fccbfae7ecc
DEBUG:openai._base_client:Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'json_data': {'messages': [{'content': "Generate a short, descriptive title for the following chapter content: how bad i was gonna eat i heard i heard that nigga tate rockets back at his well he's been back at his mama crib but he's in the basement just chilling like this they got me a cake though yeah and sent sending them to the bahamas or something right come on that's how you satisfy that's how you satisfy a certain section of society yeah you know just a bit of your style you could say that's how you that's how you satisfy them man and and also like like we discussed many many times before um the the blind loyalty to the brand is it's crazy and and i think it has a little bit to do with nostalgia um and the work that you put in and the glory days of old not what's going on right now but the glory days of old but but this app situation just seems like the worst possible thing for the culture as a whole and their mentality it's not even the app", 'role': 'user'}], 'model': 'gpt-3.5-turbo-16k', 'max_tokens': 1000, 'n': 1, 'stream': False, 'temperature': 0.7}}
DEBUG:openai._base_client:Sending HTTP Request: POST https://api.openai.com/v1/chat/completions
DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>
DEBUG:httpcore.http11:send_request_headers.complete
DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>
DEBUG:httpcore.http11:send_request_body.complete
DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>
DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Mon, 08 Jul 2024 07:21:35 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'openai-organization', b'machinekingsmedia'), (b'openai-processing-ms', b'393'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains'), (b'x-ratelimit-limit-requests', b'10000'), (b'x-ratelimit-limit-tokens', b'1000000'), (b'x-ratelimit-remaining-requests', b'9999'), (b'x-ratelimit-remaining-tokens', b'998768'), (b'x-ratelimit-reset-requests', b'6ms'), (b'x-ratelimit-reset-tokens', b'73ms'), (b'x-request-id', b'req_190f14abe83981ddd7990084cf1f293f'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'89fe517b68dd0598-IAD'), (b'Content-Encoding', b'br'), (b'alt-svc', b'h3=":443"; ma=86400')])
INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>
DEBUG:httpcore.http11:receive_response_body.complete
DEBUG:httpcore.http11:response_closed.started
DEBUG:httpcore.http11:response_closed.complete
DEBUG:openai._base_client:HTTP Response: POST https://api.openai.com/v1/chat/completions "200 OK" Headers({'date': 'Mon, 08 Jul 2024 07:21:35 GMT', 'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'openai-organization': 'machinekingsmedia', 'openai-processing-ms': '393', 'openai-version': '2020-10-01', 'strict-transport-security': 'max-age=31536000; includeSubDomains', 'x-ratelimit-limit-requests': '10000', 'x-ratelimit-limit-tokens': '1000000', 'x-ratelimit-remaining-requests': '9999', 'x-ratelimit-remaining-tokens': '998768', 'x-ratelimit-reset-requests': '6ms', 'x-ratelimit-reset-tokens': '73ms', 'x-request-id': 'req_190f14abe83981ddd7990084cf1f293f', 'cf-cache-status': 'DYNAMIC', 'server': 'cloudflare', 'cf-ray': '89fe517b68dd0598-IAD', 'content-encoding': 'br', 'alt-svc': 'h3=":443"; ma=86400'})
DEBUG:openai._base_client:request_id: req_190f14abe83981ddd7990084cf1f293f
DEBUG:openai._base_client:Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'json_data': {'messages': [{'content': "Generate a short, descriptive title for the following chapter content: situation no more bro it's the fact that they don't have no backing they're not gonna pay for shit ain't nobody gonna put up no money for the niggas but they they're making their own money but they're spending it on jewelry and cars and things of that nature that um if they don't have an office that might be a reflection of them not reinvested into the business itself um you don't even need all that son you don't even need to over half of this shit these niggas are so fucking dumb like yo bro it's the ad situation they they they probably gonna try to sell off what they can if they can at this point so but what can they sell off the the the the battles somebody used the library worth it you know they already have but i just don't see them cashing out in that situation but what's hold on let me let me get my let me get these niggas no problem all right", 'role': 'user'}], 'model': 'gpt-3.5-turbo-16k', 'max_tokens': 1000, 'n': 1, 'stream': False, 'temperature': 0.7}}
DEBUG:openai._base_client:Sending HTTP Request: POST https://api.openai.com/v1/chat/completions
DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>
DEBUG:httpcore.http11:send_request_headers.complete
DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>
DEBUG:httpcore.http11:send_request_body.complete
DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>
DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Mon, 08 Jul 2024 07:21:37 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'openai-organization', b'machinekingsmedia'), (b'openai-processing-ms', b'542'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains'), (b'x-ratelimit-limit-requests', b'10000'), (b'x-ratelimit-limit-tokens', b'1000000'), (b'x-ratelimit-remaining-requests', b'9999'), (b'x-ratelimit-remaining-tokens', b'998764'), (b'x-ratelimit-reset-requests', b'6ms'), (b'x-ratelimit-reset-tokens', b'74ms'), (b'x-request-id', b'req_57c66d0a256e68a6bde8115356036679'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'89fe517f5b100598-IAD'), (b'Content-Encoding', b'br'), (b'alt-svc', b'h3=":443"; ma=86400')])
INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>
DEBUG:httpcore.http11:receive_response_body.complete
DEBUG:httpcore.http11:response_closed.started
DEBUG:httpcore.http11:response_closed.complete
DEBUG:openai._base_client:HTTP Response: POST https://api.openai.com/v1/chat/completions "200 OK" Headers({'date': 'Mon, 08 Jul 2024 07:21:37 GMT', 'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'openai-organization': 'machinekingsmedia', 'openai-processing-ms': '542', 'openai-version': '2020-10-01', 'strict-transport-security': 'max-age=31536000; includeSubDomains', 'x-ratelimit-limit-requests': '10000', 'x-ratelimit-limit-tokens': '1000000', 'x-ratelimit-remaining-requests': '9999', 'x-ratelimit-remaining-tokens': '998764', 'x-ratelimit-reset-requests': '6ms', 'x-ratelimit-reset-tokens': '74ms', 'x-request-id': 'req_57c66d0a256e68a6bde8115356036679', 'cf-cache-status': 'DYNAMIC', 'server': 'cloudflare', 'cf-ray': '89fe517f5b100598-IAD', 'content-encoding': 'br', 'alt-svc': 'h3=":443"; ma=86400'})
DEBUG:openai._base_client:request_id: req_57c66d0a256e68a6bde8115356036679
DEBUG:openai._base_client:Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'json_data': {'messages': [{'content': "Generate a short, descriptive title for the following chapter content: and like if i'm if i'm a business dude and i purchased a company and you sold me everything but what made the engine run i'm gonna feel a way yeah when you're telling them like yo this shit got no legs he bullshitting you don't think they did their own research they was just like yo we don't want to get sued and miss this shit because then what happens is you look like y'all was in it with them to kick me out yeah so they was just trying to protect their own ass but they did their own research nigga they knew that i wasn't just a scout too many people saying the same shit like come on bro let me just minimize it bro i've seen their faces i i've seen with smack down when we did the depositions i've seen all that shit on those niggas is a dummy but you know you know so you can just tell like like there's one part like i can't really talk about the deposition shit because it's not whatever but i just recall", 'role': 'user'}], 'model': 'gpt-3.5-turbo-16k', 'max_tokens': 1000, 'n': 1, 'stream': False, 'temperature': 0.7}}
DEBUG:openai._base_client:Sending HTTP Request: POST https://api.openai.com/v1/chat/completions
DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>
DEBUG:httpcore.http11:send_request_headers.complete
DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>
DEBUG:httpcore.http11:send_request_body.complete
DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>
DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Mon, 08 Jul 2024 07:21:37 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'openai-organization', b'machinekingsmedia'), (b'openai-processing-ms', b'329'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains'), (b'x-ratelimit-limit-requests', b'10000'), (b'x-ratelimit-limit-tokens', b'1000000'), (b'x-ratelimit-remaining-requests', b'9999'), (b'x-ratelimit-remaining-tokens', b'998752'), (b'x-ratelimit-reset-requests', b'6ms'), (b'x-ratelimit-reset-tokens', b'74ms'), (b'x-request-id', b'req_467489e9d2e9d6c1b2595bf8d2d69f91'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'89fe51880fb60598-IAD'), (b'Content-Encoding', b'br'), (b'alt-svc', b'h3=":443"; ma=86400')])
INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>
DEBUG:httpcore.http11:receive_response_body.complete
DEBUG:httpcore.http11:response_closed.started
DEBUG:httpcore.http11:response_closed.complete
DEBUG:openai._base_client:HTTP Response: POST https://api.openai.com/v1/chat/completions "200 OK" Headers({'date': 'Mon, 08 Jul 2024 07:21:37 GMT', 'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'openai-organization': 'machinekingsmedia', 'openai-processing-ms': '329', 'openai-version': '2020-10-01', 'strict-transport-security': 'max-age=31536000; includeSubDomains', 'x-ratelimit-limit-requests': '10000', 'x-ratelimit-limit-tokens': '1000000', 'x-ratelimit-remaining-requests': '9999', 'x-ratelimit-remaining-tokens': '998752', 'x-ratelimit-reset-requests': '6ms', 'x-ratelimit-reset-tokens': '74ms', 'x-request-id': 'req_467489e9d2e9d6c1b2595bf8d2d69f91', 'cf-cache-status': 'DYNAMIC', 'server': 'cloudflare', 'cf-ray': '89fe51880fb60598-IAD', 'content-encoding': 'br', 'alt-svc': 'h3=":443"; ma=86400'})
DEBUG:openai._base_client:request_id: req_467489e9d2e9d6c1b2595bf8d2d69f91
DEBUG:openai._base_client:Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'json_data': {'messages': [{'content': "Generate a short, descriptive title for the following chapter content: yeah don't don't speak on it if you can't speak on it now because i am recording yeah i'm recording now i was gonna say like i could just see it bro i could see it in his face specifically like i know troy and i know what moves troy i know what motivates him and it's it's all stupid shit like you probably say nigga shit you're in your mid-40s my nigga you worried about a chain and a fucking 300 shirt for 500 whatever the fuck you pay for those shirts yeah yeah you can see the way that they dress like the way they dress as of late like they are the accessorized chains yeah you can kind of see that yeah you don't see fucking jay-z doing that like and then there's something about money that's different like when you got money you don't act like you got money you know what i'm saying yeah like it's one of those situations where you kind of like", 'role': 'user'}], 'model': 'gpt-3.5-turbo-16k', 'max_tokens': 1000, 'n': 1, 'stream': False, 'temperature': 0.7}}
DEBUG:openai._base_client:Sending HTTP Request: POST https://api.openai.com/v1/chat/completions
DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>
DEBUG:httpcore.http11:send_request_headers.complete
DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>
DEBUG:httpcore.http11:send_request_body.complete
DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>
DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Mon, 08 Jul 2024 07:21:39 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'openai-organization', b'machinekingsmedia'), (b'openai-processing-ms', b'391'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains'), (b'x-ratelimit-limit-requests', b'10000'), (b'x-ratelimit-limit-tokens', b'1000000'), (b'x-ratelimit-remaining-requests', b'9999'), (b'x-ratelimit-remaining-tokens', b'998767'), (b'x-ratelimit-reset-requests', b'6ms'), (b'x-ratelimit-reset-tokens', b'73ms'), (b'x-request-id', b'req_4149b9bbe903de8a276d4128c5d9d6e4'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'89fe518ba9dd0598-IAD'), (b'Content-Encoding', b'br'), (b'alt-svc', b'h3=":443"; ma=86400')])
INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>
DEBUG:httpcore.http11:receive_response_body.complete
DEBUG:httpcore.http11:response_closed.started
DEBUG:httpcore.http11:response_closed.complete
DEBUG:openai._base_client:HTTP Response: POST https://api.openai.com/v1/chat/completions "200 OK" Headers({'date': 'Mon, 08 Jul 2024 07:21:39 GMT', 'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'openai-organization': 'machinekingsmedia', 'openai-processing-ms': '391', 'openai-version': '2020-10-01', 'strict-transport-security': 'max-age=31536000; includeSubDomains', 'x-ratelimit-limit-requests': '10000', 'x-ratelimit-limit-tokens': '1000000', 'x-ratelimit-remaining-requests': '9999', 'x-ratelimit-remaining-tokens': '998767', 'x-ratelimit-reset-requests': '6ms', 'x-ratelimit-reset-tokens': '73ms', 'x-request-id': 'req_4149b9bbe903de8a276d4128c5d9d6e4', 'cf-cache-status': 'DYNAMIC', 'server': 'cloudflare', 'cf-ray': '89fe518ba9dd0598-IAD', 'content-encoding': 'br', 'alt-svc': 'h3=":443"; ma=86400'})
DEBUG:openai._base_client:request_id: req_4149b9bbe903de8a276d4128c5d9d6e4
DEBUG:openai._base_client:Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'json_data': {'messages': [{'content': "Generate a short, descriptive title for the following chapter content: they were niggas with real money and then you see a nigga like smack Logan they laugh at them because they don't even know how to like you got to show off you got but you don't show this shit off you don't how you live but but they they are a hip-hop culture though so there is that but their business as business owners that that work with these independent contractors it just seems as though from my observation of that the subscription money has got they've gotten so much reliable subscription money over the years that they kind of got comfortable and they just haven't they it seemed like they've even given up trying to innovate no i mean bro you understand bro like it's like who said who's not the best when you win a championship you gotta understand and that you gotta continue what you're doing in fact you gotta do more because not as expectation their only expectation was to to fuck bitches and wear", 'role': 'user'}], 'model': 'gpt-3.5-turbo-16k', 'max_tokens': 1000, 'n': 1, 'stream': False, 'temperature': 0.7}}
DEBUG:openai._base_client:Sending HTTP Request: POST https://api.openai.com/v1/chat/completions
DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>
DEBUG:httpcore.http11:send_request_headers.complete
DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>
DEBUG:httpcore.http11:send_request_body.complete
DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>
DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Mon, 08 Jul 2024 07:21:39 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'openai-organization', b'machinekingsmedia'), (b'openai-processing-ms', b'495'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains'), (b'x-ratelimit-limit-requests', b'10000'), (b'x-ratelimit-limit-tokens', b'1000000'), (b'x-ratelimit-remaining-requests', b'9999'), (b'x-ratelimit-remaining-tokens', b'998752'), (b'x-ratelimit-reset-requests', b'6ms'), (b'x-ratelimit-reset-tokens', b'74ms'), (b'x-request-id', b'req_c96bf26fdd5e30b78eafd2bc7b0e539e'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'89fe51936e900598-IAD'), (b'Content-Encoding', b'br'), (b'alt-svc', b'h3=":443"; ma=86400')])
INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>
DEBUG:httpcore.http11:receive_response_body.complete
DEBUG:httpcore.http11:response_closed.started
DEBUG:httpcore.http11:response_closed.complete
DEBUG:openai._base_client:HTTP Response: POST https://api.openai.com/v1/chat/completions "200 OK" Headers({'date': 'Mon, 08 Jul 2024 07:21:39 GMT', 'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'openai-organization': 'machinekingsmedia', 'openai-processing-ms': '495', 'openai-version': '2020-10-01', 'strict-transport-security': 'max-age=31536000; includeSubDomains', 'x-ratelimit-limit-requests': '10000', 'x-ratelimit-limit-tokens': '1000000', 'x-ratelimit-remaining-requests': '9999', 'x-ratelimit-remaining-tokens': '998752', 'x-ratelimit-reset-requests': '6ms', 'x-ratelimit-reset-tokens': '74ms', 'x-request-id': 'req_c96bf26fdd5e30b78eafd2bc7b0e539e', 'cf-cache-status': 'DYNAMIC', 'server': 'cloudflare', 'cf-ray': '89fe51936e900598-IAD', 'content-encoding': 'br', 'alt-svc': 'h3=":443"; ma=86400'})
DEBUG:openai._base_client:request_id: req_c96bf26fdd5e30b78eafd2bc7b0e539e
DEBUG:openai._base_client:Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'json_data': {'messages': [{'content': "Generate a short, descriptive title for the following chapter content: jewelry that's they should yo i'm gonna use that that championship shit for that that um that cowboys that cowboys video that i'm gonna make yeah that's a fire analogy i'm gonna use that analogy for them winning a championship with you pretty much and etching themselves in history with you norbs etching themselves in history but then the expectations are there now and now that without you because yo yo i paid i called you tyrian lannister like you you fuck hold on you watch game of thrones i started through i never finished oh okay okay but you still should watch that shit though norbs because that shit that that's that that's a good ass blog that's a good ass blog and i'll watch i'll hit you back i'm gonna get this nigga started i'll watch it right now all right i'm gonna send it uh maybe like nothing i make is over 12 minutes so it's probably like eight or nine minutes maybe i'll send it to you okay i gotta put this nigga he follow me", 'role': 'user'}], 'model': 'gpt-3.5-turbo-16k', 'max_tokens': 1000, 'n': 1, 'stream': False, 'temperature': 0.7}}
DEBUG:openai._base_client:Sending HTTP Request: POST https://api.openai.com/v1/chat/completions
DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>
DEBUG:httpcore.http11:send_request_headers.complete
DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>
DEBUG:httpcore.http11:send_request_body.complete
DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>
DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Mon, 08 Jul 2024 07:21:40 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'openai-organization', b'machinekingsmedia'), (b'openai-processing-ms', b'337'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains'), (b'x-ratelimit-limit-requests', b'10000'), (b'x-ratelimit-limit-tokens', b'1000000'), (b'x-ratelimit-remaining-requests', b'9999'), (b'x-ratelimit-remaining-tokens', b'998743'), (b'x-ratelimit-reset-requests', b'6ms'), (b'x-ratelimit-reset-tokens', b'75ms'), (b'x-request-id', b'req_90f104c685bac372caaa97e235567452'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'89fe519728fe0598-IAD'), (b'Content-Encoding', b'br'), (b'alt-svc', b'h3=":443"; ma=86400')])
INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>
DEBUG:httpcore.http11:receive_response_body.complete
DEBUG:httpcore.http11:response_closed.started
DEBUG:httpcore.http11:response_closed.complete
DEBUG:openai._base_client:HTTP Response: POST https://api.openai.com/v1/chat/completions "200 OK" Headers({'date': 'Mon, 08 Jul 2024 07:21:40 GMT', 'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'openai-organization': 'machinekingsmedia', 'openai-processing-ms': '337', 'openai-version': '2020-10-01', 'strict-transport-security': 'max-age=31536000; includeSubDomains', 'x-ratelimit-limit-requests': '10000', 'x-ratelimit-limit-tokens': '1000000', 'x-ratelimit-remaining-requests': '9999', 'x-ratelimit-remaining-tokens': '998743', 'x-ratelimit-reset-requests': '6ms', 'x-ratelimit-reset-tokens': '75ms', 'x-request-id': 'req_90f104c685bac372caaa97e235567452', 'cf-cache-status': 'DYNAMIC', 'server': 'cloudflare', 'cf-ray': '89fe519728fe0598-IAD', 'content-encoding': 'br', 'alt-svc': 'h3=":443"; ma=86400'})
DEBUG:openai._base_client:request_id: req_90f104c685bac372caaa97e235567452
DEBUG:openai._base_client:Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'json_data': {'messages': [{'content': "Generate a short, descriptive title for the following chapter content: around all right my my baby girl i'm gonna be putting her to bed at any moment so um if not then i'll hit you later on tonight all right", 'role': 'user'}], 'model': 'gpt-3.5-turbo-16k', 'max_tokens': 1000, 'n': 1, 'stream': False, 'temperature': 0.7}}
DEBUG:openai._base_client:Sending HTTP Request: POST https://api.openai.com/v1/chat/completions
DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>
DEBUG:httpcore.http11:send_request_headers.complete
DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>
DEBUG:httpcore.http11:send_request_body.complete
DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>
DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Mon, 08 Jul 2024 07:21:40 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'openai-organization', b'machinekingsmedia'), (b'openai-processing-ms', b'452'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains'), (b'x-ratelimit-limit-requests', b'10000'), (b'x-ratelimit-limit-tokens', b'1000000'), (b'x-ratelimit-remaining-requests', b'9999'), (b'x-ratelimit-remaining-tokens', b'998947'), (b'x-ratelimit-reset-requests', b'6ms'), (b'x-ratelimit-reset-tokens', b'63ms'), (b'x-request-id', b'req_0f686e535afad9d60443518ff2b8bb85'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'89fe519abba80598-IAD'), (b'Content-Encoding', b'br'), (b'alt-svc', b'h3=":443"; ma=86400')])
INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>
DEBUG:httpcore.http11:receive_response_body.complete
DEBUG:httpcore.http11:response_closed.started
DEBUG:httpcore.http11:response_closed.complete
DEBUG:openai._base_client:HTTP Response: POST https://api.openai.com/v1/chat/completions "200 OK" Headers({'date': 'Mon, 08 Jul 2024 07:21:40 GMT', 'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'openai-organization': 'machinekingsmedia', 'openai-processing-ms': '452', 'openai-version': '2020-10-01', 'strict-transport-security': 'max-age=31536000; includeSubDomains', 'x-ratelimit-limit-requests': '10000', 'x-ratelimit-limit-tokens': '1000000', 'x-ratelimit-remaining-requests': '9999', 'x-ratelimit-remaining-tokens': '998947', 'x-ratelimit-reset-requests': '6ms', 'x-ratelimit-reset-tokens': '63ms', 'x-request-id': 'req_0f686e535afad9d60443518ff2b8bb85', 'cf-cache-status': 'DYNAMIC', 'server': 'cloudflare', 'cf-ray': '89fe519abba80598-IAD', 'content-encoding': 'br', 'alt-svc': 'h3=":443"; ma=86400'})
DEBUG:openai._base_client:request_id: req_0f686e535afad9d60443518ff2b8bb85
DEBUG:openai._base_client:Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'json_data': {'messages': [{'content': 'Summarize the following chapter in one sentence: Okay, so I want to talk about ADEPT HNIC and we want to talk about him because he is a part of a subculture within Battlerite. He is a part of the same aura that Alex Jones is a part of. He uses the same formula that Alex Jones does within Battlerite. Just like I use the same formula that an Unsolved Mystery or a 1090 Jake or a Trap Laurel Ross would use and I bring that to Battlerite. So what ADEPT does is he brings his obsession with sex. He is obsessed with sex. Not only is he obsessed with sex, he is obsessed with deviant sexual behavior. But his obsession is manifested in his blogs that he does.', 'role': 'user'}], 'model': 'gpt-3.5-turbo-16k', 'max_tokens': 1000, 'n': 1, 'stream': False, 'temperature': 0.7}}
DEBUG:openai._base_client:Sending HTTP Request: POST https://api.openai.com/v1/chat/completions
DEBUG:httpcore.connection:close.started
DEBUG:httpcore.connection:close.complete
DEBUG:httpcore.connection:connect_tcp.started host='api.openai.com' port=443 local_address=None timeout=None socket_options=None
DEBUG:httpcore.connection:connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x00000164846EFFD0>
DEBUG:httpcore.connection:start_tls.started ssl_context=<ssl.SSLContext object at 0x00000164C053BEC0> server_hostname='api.openai.com' timeout=None
DEBUG:httpcore.connection:start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x00000164846EF4C0>
DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>
DEBUG:httpcore.http11:send_request_headers.complete
DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>
DEBUG:httpcore.http11:send_request_body.complete
DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>
DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Mon, 08 Jul 2024 07:23:53 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'openai-organization', b'machinekingsmedia'), (b'openai-processing-ms', b'641'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains'), (b'x-ratelimit-limit-requests', b'10000'), (b'x-ratelimit-limit-tokens', b'1000000'), (b'x-ratelimit-remaining-requests', b'9999'), (b'x-ratelimit-remaining-tokens', b'998834'), (b'x-ratelimit-reset-requests', b'6ms'), (b'x-ratelimit-reset-tokens', b'69ms'), (b'x-request-id', b'req_ee9f336d45321db54d99a78a25f9b57c'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'89fe54d3b9fd0594-IAD'), (b'Content-Encoding', b'br'), (b'alt-svc', b'h3=":443"; ma=86400')])
INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>
DEBUG:httpcore.http11:receive_response_body.complete
DEBUG:httpcore.http11:response_closed.started
DEBUG:httpcore.http11:response_closed.complete
DEBUG:openai._base_client:HTTP Response: POST https://api.openai.com/v1/chat/completions "200 OK" Headers({'date': 'Mon, 08 Jul 2024 07:23:53 GMT', 'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'openai-organization': 'machinekingsmedia', 'openai-processing-ms': '641', 'openai-version': '2020-10-01', 'strict-transport-security': 'max-age=31536000; includeSubDomains', 'x-ratelimit-limit-requests': '10000', 'x-ratelimit-limit-tokens': '1000000', 'x-ratelimit-remaining-requests': '9999', 'x-ratelimit-remaining-tokens': '998834', 'x-ratelimit-reset-requests': '6ms', 'x-ratelimit-reset-tokens': '69ms', 'x-request-id': 'req_ee9f336d45321db54d99a78a25f9b57c', 'cf-cache-status': 'DYNAMIC', 'server': 'cloudflare', 'cf-ray': '89fe54d3b9fd0594-IAD', 'content-encoding': 'br', 'alt-svc': 'h3=":443"; ma=86400'})
DEBUG:openai._base_client:request_id: req_ee9f336d45321db54d99a78a25f9b57c
DEBUG:openai._base_client:Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'json_data': {'messages': [{'content': "Summarize the following chapter in one sentence: So what we end up with is everything within Battlerite is somehow tied to the occult because it's not just sex that he uses. Just like Alex Jones, they navigate in between sex and Satan worship. Two things that when you put them on someone, when you accuse them of that, it's very difficult for them to prove that they are not what they are accusing. ADEPT has brought the same vibe from conservative media. The exact same vibe from conservative media which is a pit of lies and they play to the smallest common denominator within their audience. They play to the least informed, they play to the curious, and honestly they play to the poor and uneducated.", 'role': 'user'}], 'model': 'gpt-3.5-turbo-16k', 'max_tokens': 1000, 'n': 1, 'stream': False, 'temperature': 0.7}}
DEBUG:openai._base_client:Sending HTTP Request: POST https://api.openai.com/v1/chat/completions
DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>
DEBUG:httpcore.http11:send_request_headers.complete
DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>
DEBUG:httpcore.http11:send_request_body.complete
DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>
DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Mon, 08 Jul 2024 07:23:54 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'openai-organization', b'machinekingsmedia'), (b'openai-processing-ms', b'709'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains'), (b'x-ratelimit-limit-requests', b'10000'), (b'x-ratelimit-limit-tokens', b'1000000'), (b'x-ratelimit-remaining-requests', b'9999'), (b'x-ratelimit-remaining-tokens', b'998822'), (b'x-ratelimit-reset-requests', b'6ms'), (b'x-ratelimit-reset-tokens', b'70ms'), (b'x-request-id', b'req_8a562147250dcbec13a0700ec2c5fcc6'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'89fe54dd18420594-IAD'), (b'Content-Encoding', b'br'), (b'alt-svc', b'h3=":443"; ma=86400')])
INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>
DEBUG:httpcore.http11:receive_response_body.complete
DEBUG:httpcore.http11:response_closed.started
DEBUG:httpcore.http11:response_closed.complete
DEBUG:openai._base_client:HTTP Response: POST https://api.openai.com/v1/chat/completions "200 OK" Headers({'date': 'Mon, 08 Jul 2024 07:23:54 GMT', 'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'openai-organization': 'machinekingsmedia', 'openai-processing-ms': '709', 'openai-version': '2020-10-01', 'strict-transport-security': 'max-age=31536000; includeSubDomains', 'x-ratelimit-limit-requests': '10000', 'x-ratelimit-limit-tokens': '1000000', 'x-ratelimit-remaining-requests': '9999', 'x-ratelimit-remaining-tokens': '998822', 'x-ratelimit-reset-requests': '6ms', 'x-ratelimit-reset-tokens': '70ms', 'x-request-id': 'req_8a562147250dcbec13a0700ec2c5fcc6', 'cf-cache-status': 'DYNAMIC', 'server': 'cloudflare', 'cf-ray': '89fe54dd18420594-IAD', 'content-encoding': 'br', 'alt-svc': 'h3=":443"; ma=86400'})
DEBUG:openai._base_client:request_id: req_8a562147250dcbec13a0700ec2c5fcc6
DEBUG:openai._base_client:Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'json_data': {'messages': [{'content': "Summarize the following chapter in one sentence: Not saying that they are poor and uneducated, but that is their audience. Everything someone does is attributed to one of those two things. Now when he wants to get views on his YouTube videos, he also has another thing within Battlerite that he goes to and that is to... What he also does is he will target another blogger by the name of Chris Unbiased. Chris Unbiased is a blogger, another Battlerite blogger, and what makes him a good target for ADEPT is he was accused of some sexual crime and it's a complicated situation. So when a situation is complicated, that's ideal for him because he can inject his aura into it, drum up his fans, and get a couple views. It's important to note that he also is one of these Illuminati people.", 'role': 'user'}], 'model': 'gpt-3.5-turbo-16k', 'max_tokens': 1000, 'n': 1, 'stream': False, 'temperature': 0.7}}
DEBUG:openai._base_client:Sending HTTP Request: POST https://api.openai.com/v1/chat/completions
DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>
DEBUG:httpcore.http11:send_request_headers.complete
DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>
DEBUG:httpcore.http11:send_request_body.complete
DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>
DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Mon, 08 Jul 2024 07:23:55 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'openai-organization', b'machinekingsmedia'), (b'openai-processing-ms', b'582'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains'), (b'x-ratelimit-limit-requests', b'10000'), (b'x-ratelimit-limit-tokens', b'1000000'), (b'x-ratelimit-remaining-requests', b'9999'), (b'x-ratelimit-remaining-tokens', b'998801'), (b'x-ratelimit-reset-requests', b'6ms'), (b'x-ratelimit-reset-tokens', b'71ms'), (b'x-request-id', b'req_9eaed4d80c6189e4dce42e49a9d36832'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'89fe54e30c3a0594-IAD'), (b'Content-Encoding', b'br'), (b'alt-svc', b'h3=":443"; ma=86400')])
INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>
DEBUG:httpcore.http11:receive_response_body.complete
DEBUG:httpcore.http11:response_closed.started
DEBUG:httpcore.http11:response_closed.complete
DEBUG:openai._base_client:HTTP Response: POST https://api.openai.com/v1/chat/completions "200 OK" Headers({'date': 'Mon, 08 Jul 2024 07:23:55 GMT', 'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'openai-organization': 'machinekingsmedia', 'openai-processing-ms': '582', 'openai-version': '2020-10-01', 'strict-transport-security': 'max-age=31536000; includeSubDomains', 'x-ratelimit-limit-requests': '10000', 'x-ratelimit-limit-tokens': '1000000', 'x-ratelimit-remaining-requests': '9999', 'x-ratelimit-remaining-tokens': '998801', 'x-ratelimit-reset-requests': '6ms', 'x-ratelimit-reset-tokens': '71ms', 'x-request-id': 'req_9eaed4d80c6189e4dce42e49a9d36832', 'cf-cache-status': 'DYNAMIC', 'server': 'cloudflare', 'cf-ray': '89fe54e30c3a0594-IAD', 'content-encoding': 'br', 'alt-svc': 'h3=":443"; ma=86400'})
DEBUG:openai._base_client:request_id: req_9eaed4d80c6189e4dce42e49a9d36832
DEBUG:openai._base_client:Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'json_data': {'messages': [{'content': "Summarize the following chapter in one sentence: He believes that battle rappers are somehow in the Illuminati, but again, this falls in line with his Alex Jones approach, secret societies, sects, and cults. That is the way that he moves. What he also does is he has aligned himself with other bloggers because there are other bloggers who have gravitated towards him and they've created a circle of hatred for Chris Unbiased. These are all bottom fields. Saying something about Chris Unbiased will guarantee you a few thousand views. Within their world, a few thousand views is valuable because it's better than a few hundred views and that is what they average when talking about actual battle rap content.", 'role': 'user'}], 'model': 'gpt-3.5-turbo-16k', 'max_tokens': 1000, 'n': 1, 'stream': False, 'temperature': 0.7}}
DEBUG:openai._base_client:Sending HTTP Request: POST https://api.openai.com/v1/chat/completions
DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>
DEBUG:httpcore.http11:send_request_headers.complete
DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>
DEBUG:httpcore.http11:send_request_body.complete
DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>
DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Mon, 08 Jul 2024 07:23:57 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'openai-organization', b'machinekingsmedia'), (b'openai-processing-ms', b'642'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains'), (b'x-ratelimit-limit-requests', b'10000'), (b'x-ratelimit-limit-tokens', b'1000000'), (b'x-ratelimit-remaining-requests', b'9999'), (b'x-ratelimit-remaining-tokens', b'998822'), (b'x-ratelimit-reset-requests', b'6ms'), (b'x-ratelimit-reset-tokens', b'70ms'), (b'x-request-id', b'req_d699365607b5a458ccfe80083c72d362'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'89fe54e8cff60594-IAD'), (b'Content-Encoding', b'br'), (b'alt-svc', b'h3=":443"; ma=86400')])
INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>
DEBUG:httpcore.http11:receive_response_body.complete
DEBUG:httpcore.http11:response_closed.started
DEBUG:httpcore.http11:response_closed.complete
DEBUG:openai._base_client:HTTP Response: POST https://api.openai.com/v1/chat/completions "200 OK" Headers({'date': 'Mon, 08 Jul 2024 07:23:57 GMT', 'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'openai-organization': 'machinekingsmedia', 'openai-processing-ms': '642', 'openai-version': '2020-10-01', 'strict-transport-security': 'max-age=31536000; includeSubDomains', 'x-ratelimit-limit-requests': '10000', 'x-ratelimit-limit-tokens': '1000000', 'x-ratelimit-remaining-requests': '9999', 'x-ratelimit-remaining-tokens': '998822', 'x-ratelimit-reset-requests': '6ms', 'x-ratelimit-reset-tokens': '70ms', 'x-request-id': 'req_d699365607b5a458ccfe80083c72d362', 'cf-cache-status': 'DYNAMIC', 'server': 'cloudflare', 'cf-ray': '89fe54e8cff60594-IAD', 'content-encoding': 'br', 'alt-svc': 'h3=":443"; ma=86400'})
DEBUG:openai._base_client:request_id: req_d699365607b5a458ccfe80083c72d362
DEBUG:openai._base_client:Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'json_data': {'messages': [{'content': "Summarize the following chapter in one sentence: What we like to call them is the sub-1,000 view club, but these people are the gutter of battle rap blogging. These people are the people that ensure that crackpots, that conspiracy theories, that those things have a place to go, or those people have a place to go. The other person that is a part of A-Dub's circle also consists of people who don't embrace his conspiracy theories as much, but they embrace the hatred for Chris Unbiased because that generates a couple thousand views. This hatred over the years has bordered on obsession. They have been proven wrong dozens of times, but they never acknowledge it, but instead what they do is ignore it, wait a couple of months, make up something new, rent, and repeat.", 'role': 'user'}], 'model': 'gpt-3.5-turbo-16k', 'max_tokens': 1000, 'n': 1, 'stream': False, 'temperature': 0.7}}
DEBUG:openai._base_client:Sending HTTP Request: POST https://api.openai.com/v1/chat/completions
DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>
DEBUG:httpcore.http11:send_request_headers.complete
DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>
DEBUG:httpcore.http11:send_request_body.complete
DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>
DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Mon, 08 Jul 2024 07:23:58 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'openai-organization', b'machinekingsmedia'), (b'openai-processing-ms', b'1130'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains'), (b'x-ratelimit-limit-requests', b'10000'), (b'x-ratelimit-limit-tokens', b'1000000'), (b'x-ratelimit-remaining-requests', b'9999'), (b'x-ratelimit-remaining-tokens', b'998805'), (b'x-ratelimit-reset-requests', b'6ms'), (b'x-ratelimit-reset-tokens', b'71ms'), (b'x-request-id', b'req_5ca6b5f77c71fe4d2d3f5a2d63c142e8'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'89fe54f1fdc90594-IAD'), (b'Content-Encoding', b'br'), (b'alt-svc', b'h3=":443"; ma=86400')])
INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>
DEBUG:httpcore.http11:receive_response_body.complete
DEBUG:httpcore.http11:response_closed.started
DEBUG:httpcore.http11:response_closed.complete
DEBUG:openai._base_client:HTTP Response: POST https://api.openai.com/v1/chat/completions "200 OK" Headers({'date': 'Mon, 08 Jul 2024 07:23:58 GMT', 'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'openai-organization': 'machinekingsmedia', 'openai-processing-ms': '1130', 'openai-version': '2020-10-01', 'strict-transport-security': 'max-age=31536000; includeSubDomains', 'x-ratelimit-limit-requests': '10000', 'x-ratelimit-limit-tokens': '1000000', 'x-ratelimit-remaining-requests': '9999', 'x-ratelimit-remaining-tokens': '998805', 'x-ratelimit-reset-requests': '6ms', 'x-ratelimit-reset-tokens': '71ms', 'x-request-id': 'req_5ca6b5f77c71fe4d2d3f5a2d63c142e8', 'cf-cache-status': 'DYNAMIC', 'server': 'cloudflare', 'cf-ray': '89fe54f1fdc90594-IAD', 'content-encoding': 'br', 'alt-svc': 'h3=":443"; ma=86400'})
DEBUG:openai._base_client:request_id: req_5ca6b5f77c71fe4d2d3f5a2d63c142e8
DEBUG:openai._base_client:Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'json_data': {'messages': [{'content': "Summarize the following chapter in one sentence: What you end up with is A-Dub. H-N-I-C recently, at the end of 2023, what he tried to do was he tried to normalize himself and participate in an actual battle event that involved Geechee Gotti. Geechee Gotti courageously refused to battle in the presence of A-Dub and H-N-I-C. The person that he was battling, Dee Money, allegedly paid $3,000 and here's what Geechee Gotti said, and then I'm going to play with what Geechee Gotti said. I want you to give me a placeholder so that I can just fill it in and write what exactly what he did. This stopped A-Dub culling his tracks. This prevented him from being able to normalize his lies.", 'role': 'user'}], 'model': 'gpt-3.5-turbo-16k', 'max_tokens': 1000, 'n': 1, 'stream': False, 'temperature': 0.7}}
DEBUG:openai._base_client:Sending HTTP Request: POST https://api.openai.com/v1/chat/completions
DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>
DEBUG:httpcore.http11:send_request_headers.complete
DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>
DEBUG:httpcore.http11:send_request_body.complete
DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>
DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Mon, 08 Jul 2024 07:23:59 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'openai-organization', b'machinekingsmedia'), (b'openai-processing-ms', b'970'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains'), (b'x-ratelimit-limit-requests', b'10000'), (b'x-ratelimit-limit-tokens', b'1000000'), (b'x-ratelimit-remaining-requests', b'9999'), (b'x-ratelimit-remaining-tokens', b'998827'), (b'x-ratelimit-reset-requests', b'6ms'), (b'x-ratelimit-reset-tokens', b'70ms'), (b'x-request-id', b'req_0f9f46b2759eaf39e867467015aad837'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'89fe54fb3baa0594-IAD'), (b'Content-Encoding', b'br'), (b'alt-svc', b'h3=":443"; ma=86400')])
INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>
DEBUG:httpcore.http11:receive_response_body.complete
DEBUG:httpcore.http11:response_closed.started
DEBUG:httpcore.http11:response_closed.complete
DEBUG:openai._base_client:HTTP Response: POST https://api.openai.com/v1/chat/completions "200 OK" Headers({'date': 'Mon, 08 Jul 2024 07:23:59 GMT', 'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'openai-organization': 'machinekingsmedia', 'openai-processing-ms': '970', 'openai-version': '2020-10-01', 'strict-transport-security': 'max-age=31536000; includeSubDomains', 'x-ratelimit-limit-requests': '10000', 'x-ratelimit-limit-tokens': '1000000', 'x-ratelimit-remaining-requests': '9999', 'x-ratelimit-remaining-tokens': '998827', 'x-ratelimit-reset-requests': '6ms', 'x-ratelimit-reset-tokens': '70ms', 'x-request-id': 'req_0f9f46b2759eaf39e867467015aad837', 'cf-cache-status': 'DYNAMIC', 'server': 'cloudflare', 'cf-ray': '89fe54fb3baa0594-IAD', 'content-encoding': 'br', 'alt-svc': 'h3=":443"; ma=86400'})
DEBUG:openai._base_client:request_id: req_0f9f46b2759eaf39e867467015aad837
DEBUG:openai._base_client:Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'json_data': {'messages': [{'content': "Summarize the following chapter in one sentence: He has fought with other bloggers. He has created multiple documentaries about crystal bias. A-Dub has gotten close to other bloggers like DME Detroit. He's gotten close to other bloggers like... I forgot the name of that other blog. A-Dub is someone who knows the name of every demon. He knows the name of every symbol, every symbol that has demonic value. He'll show you the definition of the symbol. He'll compare it to a clothing logo that someone may have. He'll rewind videos ten times in order to paint a picture that isn't there.", 'role': 'user'}], 'model': 'gpt-3.5-turbo-16k', 'max_tokens': 1000, 'n': 1, 'stream': False, 'temperature': 0.7}}
DEBUG:openai._base_client:Sending HTTP Request: POST https://api.openai.com/v1/chat/completions
DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>
DEBUG:httpcore.http11:send_request_headers.complete
DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>
DEBUG:httpcore.http11:send_request_body.complete
DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>
DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Mon, 08 Jul 2024 07:24:01 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'openai-organization', b'machinekingsmedia'), (b'openai-processing-ms', b'721'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains'), (b'x-ratelimit-limit-requests', b'10000'), (b'x-ratelimit-limit-tokens', b'1000000'), (b'x-ratelimit-remaining-requests', b'9999'), (b'x-ratelimit-remaining-tokens', b'998851'), (b'x-ratelimit-reset-requests', b'6ms'), (b'x-ratelimit-reset-tokens', b'68ms'), (b'x-request-id', b'req_54771f6381e84bbfceb0f2161864e260'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'89fe5502b8920594-IAD'), (b'Content-Encoding', b'br'), (b'alt-svc', b'h3=":443"; ma=86400')])
INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>
DEBUG:httpcore.http11:receive_response_body.complete
DEBUG:httpcore.http11:response_closed.started
DEBUG:httpcore.http11:response_closed.complete
DEBUG:openai._base_client:HTTP Response: POST https://api.openai.com/v1/chat/completions "200 OK" Headers({'date': 'Mon, 08 Jul 2024 07:24:01 GMT', 'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'openai-organization': 'machinekingsmedia', 'openai-processing-ms': '721', 'openai-version': '2020-10-01', 'strict-transport-security': 'max-age=31536000; includeSubDomains', 'x-ratelimit-limit-requests': '10000', 'x-ratelimit-limit-tokens': '1000000', 'x-ratelimit-remaining-requests': '9999', 'x-ratelimit-remaining-tokens': '998851', 'x-ratelimit-reset-requests': '6ms', 'x-ratelimit-reset-tokens': '68ms', 'x-request-id': 'req_54771f6381e84bbfceb0f2161864e260', 'cf-cache-status': 'DYNAMIC', 'server': 'cloudflare', 'cf-ray': '89fe5502b8920594-IAD', 'content-encoding': 'br', 'alt-svc': 'h3=":443"; ma=86400'})
DEBUG:openai._base_client:request_id: req_54771f6381e84bbfceb0f2161864e260
DEBUG:openai._base_client:Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'json_data': {'messages': [{'content': "Summarize the following chapter in one sentence: He'll add context that isn't there. He has been successful with taking this approach. What that success has gotten him is allies. Not views, but allies. That is what people like him need. Allies. They need to be legitimized. The only way they can be legitimized is with allies. It's with people who are not as fringe as him. Who are not as out there as he is to embrace him.", 'role': 'user'}], 'model': 'gpt-3.5-turbo-16k', 'max_tokens': 1000, 'n': 1, 'stream': False, 'temperature': 0.7}}
DEBUG:openai._base_client:Sending HTTP Request: POST https://api.openai.com/v1/chat/completions
DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>
DEBUG:httpcore.http11:send_request_headers.complete
DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>
DEBUG:httpcore.http11:send_request_body.complete
DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>
DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Mon, 08 Jul 2024 07:24:02 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'openai-organization', b'machinekingsmedia'), (b'openai-processing-ms', b'629'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains'), (b'x-ratelimit-limit-requests', b'10000'), (b'x-ratelimit-limit-tokens', b'1000000'), (b'x-ratelimit-remaining-requests', b'9999'), (b'x-ratelimit-remaining-tokens', b'998893'), (b'x-ratelimit-reset-requests', b'6ms'), (b'x-ratelimit-reset-tokens', b'66ms'), (b'x-request-id', b'req_cc0a30c3a3e1262164d6a8f5c4833a34'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'89fe550ccfaa0594-IAD'), (b'Content-Encoding', b'br'), (b'alt-svc', b'h3=":443"; ma=86400')])
INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>
DEBUG:httpcore.http11:receive_response_body.complete
DEBUG:httpcore.http11:response_closed.started
DEBUG:httpcore.http11:response_closed.complete
DEBUG:openai._base_client:HTTP Response: POST https://api.openai.com/v1/chat/completions "200 OK" Headers({'date': 'Mon, 08 Jul 2024 07:24:02 GMT', 'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'openai-organization': 'machinekingsmedia', 'openai-processing-ms': '629', 'openai-version': '2020-10-01', 'strict-transport-security': 'max-age=31536000; includeSubDomains', 'x-ratelimit-limit-requests': '10000', 'x-ratelimit-limit-tokens': '1000000', 'x-ratelimit-remaining-requests': '9999', 'x-ratelimit-remaining-tokens': '998893', 'x-ratelimit-reset-requests': '6ms', 'x-ratelimit-reset-tokens': '66ms', 'x-request-id': 'req_cc0a30c3a3e1262164d6a8f5c4833a34', 'cf-cache-status': 'DYNAMIC', 'server': 'cloudflare', 'cf-ray': '89fe550ccfaa0594-IAD', 'content-encoding': 'br', 'alt-svc': 'h3=":443"; ma=86400'})
DEBUG:openai._base_client:request_id: req_cc0a30c3a3e1262164d6a8f5c4833a34
DEBUG:openai._base_client:Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'json_data': {'messages': [{'content': "Summarize the following chapter in one sentence: That's what he tried to do with Geechee Gotti. One of the most powerful battle rappers in battle rap. When the attempt to be normalized and embraced by Geechee Gotti failed... It was an illustration of... It was an example of... It can be likened to someone like Alex Jones trying to normalize themselves. And when it failed flat... It was a reminder for us that lies... That fiction has consequences.", 'role': 'user'}], 'model': 'gpt-3.5-turbo-16k', 'max_tokens': 1000, 'n': 1, 'stream': False, 'temperature': 0.7}}
DEBUG:openai._base_client:Sending HTTP Request: POST https://api.openai.com/v1/chat/completions
DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>
DEBUG:httpcore.http11:send_request_headers.complete
DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>
DEBUG:httpcore.http11:send_request_body.complete
DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>
DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Mon, 08 Jul 2024 07:24:03 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'openai-organization', b'machinekingsmedia'), (b'openai-processing-ms', b'570'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains'), (b'x-ratelimit-limit-requests', b'10000'), (b'x-ratelimit-limit-tokens', b'1000000'), (b'x-ratelimit-remaining-requests', b'9999'), (b'x-ratelimit-remaining-tokens', b'998885'), (b'x-ratelimit-reset-requests', b'6ms'), (b'x-ratelimit-reset-tokens', b'66ms'), (b'x-request-id', b'req_76f17a9b228c8274bc7873d65426cbe3'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'89fe55124b3c0594-IAD'), (b'Content-Encoding', b'br'), (b'alt-svc', b'h3=":443"; ma=86400')])
INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>
DEBUG:httpcore.http11:receive_response_body.complete
DEBUG:httpcore.http11:response_closed.started
DEBUG:httpcore.http11:response_closed.complete
DEBUG:openai._base_client:HTTP Response: POST https://api.openai.com/v1/chat/completions "200 OK" Headers({'date': 'Mon, 08 Jul 2024 07:24:03 GMT', 'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'openai-organization': 'machinekingsmedia', 'openai-processing-ms': '570', 'openai-version': '2020-10-01', 'strict-transport-security': 'max-age=31536000; includeSubDomains', 'x-ratelimit-limit-requests': '10000', 'x-ratelimit-limit-tokens': '1000000', 'x-ratelimit-remaining-requests': '9999', 'x-ratelimit-remaining-tokens': '998885', 'x-ratelimit-reset-requests': '6ms', 'x-ratelimit-reset-tokens': '66ms', 'x-request-id': 'req_76f17a9b228c8274bc7873d65426cbe3', 'cf-cache-status': 'DYNAMIC', 'server': 'cloudflare', 'cf-ray': '89fe55124b3c0594-IAD', 'content-encoding': 'br', 'alt-svc': 'h3=":443"; ma=86400'})
DEBUG:openai._base_client:request_id: req_76f17a9b228c8274bc7873d65426cbe3
DEBUG:openai._base_client:Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'json_data': {'messages': [{'content': "Provide a brief summary of the following transcript: File: 231225_0706.mp3\nRecording Date: 2023-12-25\nDuration: 00:16:08.36\nTranscription Date: 2024-07-07 06:14:26\n\n1\n00:00:00,000 --> 00:00:27,840\nOkay, so I want to talk about ADEPT HNIC and we want to talk about him because he is a\n\n2\n00:00:27,840 --> 00:00:33,520\npart of a subculture within Battlerite.\n\n3\n00:00:33,520 --> 00:00:44,160\nHe is a part of the same aura that Alex Jones is a part of.\n\n4\n00:00:44,160 --> 00:00:51,880\nHe uses the same formula that Alex Jones does within Battlerite.\n\n5\n00:00:51,880 --> 00:01:04,440\nJust like I use the same formula that an Unsolved Mystery or a 1090 Jake or a Trap\n\n6\n00:01:04,440 --> 00:01:11,599\nLaurel Ross would use and I bring that to Battlerite.\n\n7\n00:01:11,599 --> 00:01:22,000\nSo what ADEPT does is he brings his obsession with sex.\n\n8\n00:01:22,000 --> 00:01:28,400\nHe is obsessed with sex.\n\n9\n00:01:28,400 --> 00:01:40,360\nNot only is he obsessed with sex, he is obsessed with deviant sexual behavior.\n\n10\n00:01:40,360 --> 00:01:48,080\nBut his obsession is manifested in his blogs that he does.\n\n11\n00:01:48,080 --> 00:02:01,160\nSo what we end up with is everything within Battlerite is somehow tied to the occult because\n\n12\n00:02:01,160 --> 00:02:03,320\nit's not just sex that he uses.\n\n13\n00:02:03,320 --> 00:02:23,399\nJust like Alex Jones, they navigate in between sex and Satan worship.\n\n14\n00:02:23,399 --> 00:02:33,240\nTwo things that when you put them on someone, when you accuse them of that, it's very difficult\n\n15\n00:02:33,240 --> 00:02:42,679\nfor them to prove that they are not what they are accusing.\n\n16\n00:02:42,679 --> 00:02:50,639\nADEPT has brought the same vibe from conservative media.\n\n17\n00:02:50,639 --> 00:03:01,160\nThe exact same vibe from conservative media which is a pit of lies and they play to the\n\n18\n00:03:01,160 --> 00:03:07,960\nsmallest common denominator within their audience.\n\n19\n00:03:07,960 --> 00:03:24,399\nThey play to the least informed, they play to the curious, and honestly they play to\n\n20\n00:03:24,399 --> 00:03:34,559\nthe poor and uneducated.\n\n21\n00:03:34,559 --> 00:03:44,800\nNot saying that they are poor and uneducated, but that is their audience.\n\n22\n00:03:44,800 --> 00:03:58,440\nEverything someone does is attributed to one of those two things.\n\n23\n00:03:58,440 --> 00:04:10,720\nNow when he wants to get views on his YouTube videos, he also has another thing within Battlerite\n\n24\n00:04:10,720 --> 00:04:27,239\nthat he goes to and that is to...\n\n25\n00:04:27,239 --> 00:04:38,839\nWhat he also does is he will target another blogger by the name of Chris Unbiased.\n\n26\n00:04:38,839 --> 00:04:52,959\nChris Unbiased is a blogger, another Battlerite blogger, and what makes him a good target\n\n27\n00:04:52,959 --> 00:05:06,679\nfor ADEPT is he was accused of some sexual crime and it's a complicated situation.\n\n28\n00:05:06,679 --> 00:05:25,079\nSo when a situation is complicated, that's ideal for him because he can inject his aura\n\n29\n00:05:25,079 --> 00:05:38,720\ninto it, drum up his fans, and get a couple views.\n\n30\n00:05:38,720 --> 00:05:49,839\nIt's important to note that he also is one of these Illuminati people.\n\n31\n00:05:49,839 --> 00:05:56,000\nHe believes that battle rappers are somehow in the Illuminati, but again, this falls\n\n32\n00:05:56,000 --> 00:06:08,320\nin line with his Alex Jones approach, secret societies, sects, and cults.\n\n33\n00:06:08,320 --> 00:06:17,239\nThat is the way that he moves.\n\n34\n00:06:17,239 --> 00:06:37,399\nWhat he also does is he has aligned himself with other bloggers because there are other\n\n35\n00:06:37,399 --> 00:06:59,279\nbloggers who have gravitated towards him and they've created a circle of hatred for\n\n36\n00:06:59,279 --> 00:07:02,279\nChris Unbiased.\n\n37\n00:07:02,279 --> 00:07:07,720\nThese are all bottom fields.\n\n38\n00:07:07,720 --> 00:07:19,880\nSaying something about Chris Unbiased will guarantee you a few thousand views.\n\n39\n00:07:19,880 --> 00:07:28,959\nWithin their world, a few thousand views is valuable because it's better than a few", 'role': 'user'}], 'model': 'gpt-3.5-turbo-16k', 'max_tokens': 1000, 'n': 1, 'stream': False, 'temperature': 0.7}}
DEBUG:openai._base_client:Sending HTTP Request: POST https://api.openai.com/v1/chat/completions
DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>
DEBUG:httpcore.http11:send_request_headers.complete
DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>
DEBUG:httpcore.http11:send_request_body.complete
DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>
DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Mon, 08 Jul 2024 07:24:06 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'openai-organization', b'machinekingsmedia'), (b'openai-processing-ms', b'3438'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains'), (b'x-ratelimit-limit-requests', b'10000'), (b'x-ratelimit-limit-tokens', b'1000000'), (b'x-ratelimit-remaining-requests', b'9999'), (b'x-ratelimit-remaining-tokens', b'997985'), (b'x-ratelimit-reset-requests', b'6ms'), (b'x-ratelimit-reset-tokens', b'120ms'), (b'x-request-id', b'req_80fbbfd4470349b23a1ca45b62a79567'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'89fe55175e5e0594-IAD'), (b'Content-Encoding', b'br'), (b'alt-svc', b'h3=":443"; ma=86400')])
INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>
DEBUG:httpcore.http11:receive_response_body.complete
DEBUG:httpcore.http11:response_closed.started
DEBUG:httpcore.http11:response_closed.complete
DEBUG:openai._base_client:HTTP Response: POST https://api.openai.com/v1/chat/completions "200 OK" Headers({'date': 'Mon, 08 Jul 2024 07:24:06 GMT', 'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'openai-organization': 'machinekingsmedia', 'openai-processing-ms': '3438', 'openai-version': '2020-10-01', 'strict-transport-security': 'max-age=31536000; includeSubDomains', 'x-ratelimit-limit-requests': '10000', 'x-ratelimit-limit-tokens': '1000000', 'x-ratelimit-remaining-requests': '9999', 'x-ratelimit-remaining-tokens': '997985', 'x-ratelimit-reset-requests': '6ms', 'x-ratelimit-reset-tokens': '120ms', 'x-request-id': 'req_80fbbfd4470349b23a1ca45b62a79567', 'cf-cache-status': 'DYNAMIC', 'server': 'cloudflare', 'cf-ray': '89fe55175e5e0594-IAD', 'content-encoding': 'br', 'alt-svc': 'h3=":443"; ma=86400'})
DEBUG:openai._base_client:request_id: req_80fbbfd4470349b23a1ca45b62a79567
DEBUG:openai._base_client:Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'json_data': {'messages': [{'content': 'Generate a short, descriptive title for the following chapter content: Okay, so I want to talk about ADEPT HNIC and we want to talk about him because he is a part of a subculture within Battlerite. He is a part of the same aura that Alex Jones is a part of. He uses the same formula that Alex Jones does within Battlerite. Just like I use the same formula that an Unsolved Mystery or a 1090 Jake or a Trap Laurel Ross would use and I bring that to Battlerite. So what ADEPT does is he brings his obsession with sex. He is obsessed with sex. Not only is he obsessed with sex, he is obsessed with deviant sexual behavior. But his obsession is manifested in his blogs that he does.', 'role': 'user'}], 'model': 'gpt-3.5-turbo-16k', 'max_tokens': 1000, 'n': 1, 'stream': False, 'temperature': 0.7}}
DEBUG:openai._base_client:Sending HTTP Request: POST https://api.openai.com/v1/chat/completions
DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>
DEBUG:httpcore.http11:send_request_headers.complete
DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>
DEBUG:httpcore.http11:send_request_body.complete
DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>
DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Mon, 08 Jul 2024 07:24:07 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'openai-organization', b'machinekingsmedia'), (b'openai-processing-ms', b'401'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains'), (b'x-ratelimit-limit-requests', b'10000'), (b'x-ratelimit-limit-tokens', b'1000000'), (b'x-ratelimit-remaining-requests', b'9999'), (b'x-ratelimit-remaining-tokens', b'998828'), (b'x-ratelimit-reset-requests', b'6ms'), (b'x-ratelimit-reset-tokens', b'70ms'), (b'x-request-id', b'req_16851b2a5e58a49dbfd7dffed0836dc8'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'89fe552d8cfe0594-IAD'), (b'Content-Encoding', b'br'), (b'alt-svc', b'h3=":443"; ma=86400')])
INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>
DEBUG:httpcore.http11:receive_response_body.complete
DEBUG:httpcore.http11:response_closed.started
DEBUG:httpcore.http11:response_closed.complete
DEBUG:openai._base_client:HTTP Response: POST https://api.openai.com/v1/chat/completions "200 OK" Headers({'date': 'Mon, 08 Jul 2024 07:24:07 GMT', 'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'openai-organization': 'machinekingsmedia', 'openai-processing-ms': '401', 'openai-version': '2020-10-01', 'strict-transport-security': 'max-age=31536000; includeSubDomains', 'x-ratelimit-limit-requests': '10000', 'x-ratelimit-limit-tokens': '1000000', 'x-ratelimit-remaining-requests': '9999', 'x-ratelimit-remaining-tokens': '998828', 'x-ratelimit-reset-requests': '6ms', 'x-ratelimit-reset-tokens': '70ms', 'x-request-id': 'req_16851b2a5e58a49dbfd7dffed0836dc8', 'cf-cache-status': 'DYNAMIC', 'server': 'cloudflare', 'cf-ray': '89fe552d8cfe0594-IAD', 'content-encoding': 'br', 'alt-svc': 'h3=":443"; ma=86400'})
DEBUG:openai._base_client:request_id: req_16851b2a5e58a49dbfd7dffed0836dc8
DEBUG:openai._base_client:Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'json_data': {'messages': [{'content': "Generate a short, descriptive title for the following chapter content: So what we end up with is everything within Battlerite is somehow tied to the occult because it's not just sex that he uses. Just like Alex Jones, they navigate in between sex and Satan worship. Two things that when you put them on someone, when you accuse them of that, it's very difficult for them to prove that they are not what they are accusing. ADEPT has brought the same vibe from conservative media. The exact same vibe from conservative media which is a pit of lies and they play to the smallest common denominator within their audience. They play to the least informed, they play to the curious, and honestly they play to the poor and uneducated.", 'role': 'user'}], 'model': 'gpt-3.5-turbo-16k', 'max_tokens': 1000, 'n': 1, 'stream': False, 'temperature': 0.7}}
DEBUG:openai._base_client:Sending HTTP Request: POST https://api.openai.com/v1/chat/completions
DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>
DEBUG:httpcore.http11:send_request_headers.complete
DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>
DEBUG:httpcore.http11:send_request_body.complete
DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>
DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Mon, 08 Jul 2024 07:24:08 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'openai-organization', b'machinekingsmedia'), (b'openai-processing-ms', b'436'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains'), (b'x-ratelimit-limit-requests', b'10000'), (b'x-ratelimit-limit-tokens', b'1000000'), (b'x-ratelimit-remaining-requests', b'9999'), (b'x-ratelimit-remaining-tokens', b'998817'), (b'x-ratelimit-reset-requests', b'6ms'), (b'x-ratelimit-reset-tokens', b'70ms'), (b'x-request-id', b'req_d0152097acc9bb0592cca0ab15c649a3'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'89fe55355a370594-IAD'), (b'Content-Encoding', b'br'), (b'alt-svc', b'h3=":443"; ma=86400')])
INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>
DEBUG:httpcore.http11:receive_response_body.complete
DEBUG:httpcore.http11:response_closed.started
DEBUG:httpcore.http11:response_closed.complete
DEBUG:openai._base_client:HTTP Response: POST https://api.openai.com/v1/chat/completions "200 OK" Headers({'date': 'Mon, 08 Jul 2024 07:24:08 GMT', 'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'openai-organization': 'machinekingsmedia', 'openai-processing-ms': '436', 'openai-version': '2020-10-01', 'strict-transport-security': 'max-age=31536000; includeSubDomains', 'x-ratelimit-limit-requests': '10000', 'x-ratelimit-limit-tokens': '1000000', 'x-ratelimit-remaining-requests': '9999', 'x-ratelimit-remaining-tokens': '998817', 'x-ratelimit-reset-requests': '6ms', 'x-ratelimit-reset-tokens': '70ms', 'x-request-id': 'req_d0152097acc9bb0592cca0ab15c649a3', 'cf-cache-status': 'DYNAMIC', 'server': 'cloudflare', 'cf-ray': '89fe55355a370594-IAD', 'content-encoding': 'br', 'alt-svc': 'h3=":443"; ma=86400'})
DEBUG:openai._base_client:request_id: req_d0152097acc9bb0592cca0ab15c649a3
DEBUG:openai._base_client:Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'json_data': {'messages': [{'content': "Generate a short, descriptive title for the following chapter content: Not saying that they are poor and uneducated, but that is their audience. Everything someone does is attributed to one of those two things. Now when he wants to get views on his YouTube videos, he also has another thing within Battlerite that he goes to and that is to... What he also does is he will target another blogger by the name of Chris Unbiased. Chris Unbiased is a blogger, another Battlerite blogger, and what makes him a good target for ADEPT is he was accused of some sexual crime and it's a complicated situation. So when a situation is complicated, that's ideal for him because he can inject his aura into it, drum up his fans, and get a couple views. It's important to note that he also is one of these Illuminati people.", 'role': 'user'}], 'model': 'gpt-3.5-turbo-16k', 'max_tokens': 1000, 'n': 1, 'stream': False, 'temperature': 0.7}}
DEBUG:openai._base_client:Sending HTTP Request: POST https://api.openai.com/v1/chat/completions
DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>
DEBUG:httpcore.http11:send_request_headers.complete
DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>
DEBUG:httpcore.http11:send_request_body.complete
DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>
DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Mon, 08 Jul 2024 07:24:09 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'openai-organization', b'machinekingsmedia'), (b'openai-processing-ms', b'362'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains'), (b'x-ratelimit-limit-requests', b'10000'), (b'x-ratelimit-limit-tokens', b'1000000'), (b'x-ratelimit-remaining-requests', b'9999'), (b'x-ratelimit-remaining-tokens', b'998797'), (b'x-ratelimit-reset-requests', b'6ms'), (b'x-ratelimit-reset-tokens', b'72ms'), (b'x-request-id', b'req_82806a8acd2b89f02aca6982fae3a46f'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'89fe55398cff0594-IAD'), (b'Content-Encoding', b'br'), (b'alt-svc', b'h3=":443"; ma=86400')])
INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>
DEBUG:httpcore.http11:receive_response_body.complete
DEBUG:httpcore.http11:response_closed.started
DEBUG:httpcore.http11:response_closed.complete
DEBUG:openai._base_client:HTTP Response: POST https://api.openai.com/v1/chat/completions "200 OK" Headers({'date': 'Mon, 08 Jul 2024 07:24:09 GMT', 'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'openai-organization': 'machinekingsmedia', 'openai-processing-ms': '362', 'openai-version': '2020-10-01', 'strict-transport-security': 'max-age=31536000; includeSubDomains', 'x-ratelimit-limit-requests': '10000', 'x-ratelimit-limit-tokens': '1000000', 'x-ratelimit-remaining-requests': '9999', 'x-ratelimit-remaining-tokens': '998797', 'x-ratelimit-reset-requests': '6ms', 'x-ratelimit-reset-tokens': '72ms', 'x-request-id': 'req_82806a8acd2b89f02aca6982fae3a46f', 'cf-cache-status': 'DYNAMIC', 'server': 'cloudflare', 'cf-ray': '89fe55398cff0594-IAD', 'content-encoding': 'br', 'alt-svc': 'h3=":443"; ma=86400'})
DEBUG:openai._base_client:request_id: req_82806a8acd2b89f02aca6982fae3a46f
DEBUG:openai._base_client:Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'json_data': {'messages': [{'content': "Generate a short, descriptive title for the following chapter content: He believes that battle rappers are somehow in the Illuminati, but again, this falls in line with his Alex Jones approach, secret societies, sects, and cults. That is the way that he moves. What he also does is he has aligned himself with other bloggers because there are other bloggers who have gravitated towards him and they've created a circle of hatred for Chris Unbiased. These are all bottom fields. Saying something about Chris Unbiased will guarantee you a few thousand views. Within their world, a few thousand views is valuable because it's better than a few hundred views and that is what they average when talking about actual battle rap content.", 'role': 'user'}], 'model': 'gpt-3.5-turbo-16k', 'max_tokens': 1000, 'n': 1, 'stream': False, 'temperature': 0.7}}
DEBUG:openai._base_client:Sending HTTP Request: POST https://api.openai.com/v1/chat/completions
DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>
DEBUG:httpcore.http11:send_request_headers.complete
DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>
DEBUG:httpcore.http11:send_request_body.complete
DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>
DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Mon, 08 Jul 2024 07:24:09 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'openai-organization', b'machinekingsmedia'), (b'openai-processing-ms', b'290'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains'), (b'x-ratelimit-limit-requests', b'10000'), (b'x-ratelimit-limit-tokens', b'1000000'), (b'x-ratelimit-remaining-requests', b'9999'), (b'x-ratelimit-remaining-tokens', b'998815'), (b'x-ratelimit-reset-requests', b'6ms'), (b'x-ratelimit-reset-tokens', b'71ms'), (b'x-request-id', b'req_8a8ab6e8c84e6ebc5ee1654c4e6e45d7'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'89fe553c9f2e0594-IAD'), (b'Content-Encoding', b'br'), (b'alt-svc', b'h3=":443"; ma=86400')])
INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>
DEBUG:httpcore.http11:receive_response_body.complete
DEBUG:httpcore.http11:response_closed.started
DEBUG:httpcore.http11:response_closed.complete
DEBUG:openai._base_client:HTTP Response: POST https://api.openai.com/v1/chat/completions "200 OK" Headers({'date': 'Mon, 08 Jul 2024 07:24:09 GMT', 'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'openai-organization': 'machinekingsmedia', 'openai-processing-ms': '290', 'openai-version': '2020-10-01', 'strict-transport-security': 'max-age=31536000; includeSubDomains', 'x-ratelimit-limit-requests': '10000', 'x-ratelimit-limit-tokens': '1000000', 'x-ratelimit-remaining-requests': '9999', 'x-ratelimit-remaining-tokens': '998815', 'x-ratelimit-reset-requests': '6ms', 'x-ratelimit-reset-tokens': '71ms', 'x-request-id': 'req_8a8ab6e8c84e6ebc5ee1654c4e6e45d7', 'cf-cache-status': 'DYNAMIC', 'server': 'cloudflare', 'cf-ray': '89fe553c9f2e0594-IAD', 'content-encoding': 'br', 'alt-svc': 'h3=":443"; ma=86400'})
DEBUG:openai._base_client:request_id: req_8a8ab6e8c84e6ebc5ee1654c4e6e45d7
DEBUG:openai._base_client:Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'json_data': {'messages': [{'content': "Generate a short, descriptive title for the following chapter content: What we like to call them is the sub-1,000 view club, but these people are the gutter of battle rap blogging. These people are the people that ensure that crackpots, that conspiracy theories, that those things have a place to go, or those people have a place to go. The other person that is a part of A-Dub's circle also consists of people who don't embrace his conspiracy theories as much, but they embrace the hatred for Chris Unbiased because that generates a couple thousand views. This hatred over the years has bordered on obsession. They have been proven wrong dozens of times, but they never acknowledge it, but instead what they do is ignore it, wait a couple of months, make up something new, rent, and repeat.", 'role': 'user'}], 'model': 'gpt-3.5-turbo-16k', 'max_tokens': 1000, 'n': 1, 'stream': False, 'temperature': 0.7}}
DEBUG:openai._base_client:Sending HTTP Request: POST https://api.openai.com/v1/chat/completions
DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>
DEBUG:httpcore.http11:send_request_headers.complete
DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>
DEBUG:httpcore.http11:send_request_body.complete
DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>
DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Mon, 08 Jul 2024 07:24:10 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'openai-organization', b'machinekingsmedia'), (b'openai-processing-ms', b'498'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains'), (b'x-ratelimit-limit-requests', b'10000'), (b'x-ratelimit-limit-tokens', b'1000000'), (b'x-ratelimit-remaining-requests', b'9999'), (b'x-ratelimit-remaining-tokens', b'998801'), (b'x-ratelimit-reset-requests', b'6ms'), (b'x-ratelimit-reset-tokens', b'71ms'), (b'x-request-id', b'req_bea1775ad07a77b30777e23fd85b6c5d'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'89fe553f08d10594-IAD'), (b'Content-Encoding', b'br'), (b'alt-svc', b'h3=":443"; ma=86400')])
INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>
DEBUG:httpcore.http11:receive_response_body.complete
DEBUG:httpcore.http11:response_closed.started
DEBUG:httpcore.http11:response_closed.complete
DEBUG:openai._base_client:HTTP Response: POST https://api.openai.com/v1/chat/completions "200 OK" Headers({'date': 'Mon, 08 Jul 2024 07:24:10 GMT', 'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'openai-organization': 'machinekingsmedia', 'openai-processing-ms': '498', 'openai-version': '2020-10-01', 'strict-transport-security': 'max-age=31536000; includeSubDomains', 'x-ratelimit-limit-requests': '10000', 'x-ratelimit-limit-tokens': '1000000', 'x-ratelimit-remaining-requests': '9999', 'x-ratelimit-remaining-tokens': '998801', 'x-ratelimit-reset-requests': '6ms', 'x-ratelimit-reset-tokens': '71ms', 'x-request-id': 'req_bea1775ad07a77b30777e23fd85b6c5d', 'cf-cache-status': 'DYNAMIC', 'server': 'cloudflare', 'cf-ray': '89fe553f08d10594-IAD', 'content-encoding': 'br', 'alt-svc': 'h3=":443"; ma=86400'})
DEBUG:openai._base_client:request_id: req_bea1775ad07a77b30777e23fd85b6c5d
DEBUG:openai._base_client:Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'json_data': {'messages': [{'content': "Generate a short, descriptive title for the following chapter content: What you end up with is A-Dub. H-N-I-C recently, at the end of 2023, what he tried to do was he tried to normalize himself and participate in an actual battle event that involved Geechee Gotti. Geechee Gotti courageously refused to battle in the presence of A-Dub and H-N-I-C. The person that he was battling, Dee Money, allegedly paid $3,000 and here's what Geechee Gotti said, and then I'm going to play with what Geechee Gotti said. I want you to give me a placeholder so that I can just fill it in and write what exactly what he did. This stopped A-Dub culling his tracks. This prevented him from being able to normalize his lies.", 'role': 'user'}], 'model': 'gpt-3.5-turbo-16k', 'max_tokens': 1000, 'n': 1, 'stream': False, 'temperature': 0.7}}
DEBUG:openai._base_client:Sending HTTP Request: POST https://api.openai.com/v1/chat/completions
DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>
DEBUG:httpcore.http11:send_request_headers.complete
DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>
DEBUG:httpcore.http11:send_request_body.complete
DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>
DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Mon, 08 Jul 2024 07:24:11 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'openai-organization', b'machinekingsmedia'), (b'openai-processing-ms', b'354'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains'), (b'x-ratelimit-limit-requests', b'10000'), (b'x-ratelimit-limit-tokens', b'1000000'), (b'x-ratelimit-remaining-requests', b'9999'), (b'x-ratelimit-remaining-tokens', b'998822'), (b'x-ratelimit-reset-requests', b'6ms'), (b'x-ratelimit-reset-tokens', b'70ms'), (b'x-request-id', b'req_da905c75a060c8b3bcbfec1bc4314f66'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'89fe55476f410594-IAD'), (b'Content-Encoding', b'br'), (b'alt-svc', b'h3=":443"; ma=86400')])
INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>
DEBUG:httpcore.http11:receive_response_body.complete
DEBUG:httpcore.http11:response_closed.started
DEBUG:httpcore.http11:response_closed.complete
DEBUG:openai._base_client:HTTP Response: POST https://api.openai.com/v1/chat/completions "200 OK" Headers({'date': 'Mon, 08 Jul 2024 07:24:11 GMT', 'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'openai-organization': 'machinekingsmedia', 'openai-processing-ms': '354', 'openai-version': '2020-10-01', 'strict-transport-security': 'max-age=31536000; includeSubDomains', 'x-ratelimit-limit-requests': '10000', 'x-ratelimit-limit-tokens': '1000000', 'x-ratelimit-remaining-requests': '9999', 'x-ratelimit-remaining-tokens': '998822', 'x-ratelimit-reset-requests': '6ms', 'x-ratelimit-reset-tokens': '70ms', 'x-request-id': 'req_da905c75a060c8b3bcbfec1bc4314f66', 'cf-cache-status': 'DYNAMIC', 'server': 'cloudflare', 'cf-ray': '89fe55476f410594-IAD', 'content-encoding': 'br', 'alt-svc': 'h3=":443"; ma=86400'})
DEBUG:openai._base_client:request_id: req_da905c75a060c8b3bcbfec1bc4314f66
DEBUG:openai._base_client:Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'json_data': {'messages': [{'content': "Generate a short, descriptive title for the following chapter content: He has fought with other bloggers. He has created multiple documentaries about crystal bias. A-Dub has gotten close to other bloggers like DME Detroit. He's gotten close to other bloggers like... I forgot the name of that other blog. A-Dub is someone who knows the name of every demon. He knows the name of every symbol, every symbol that has demonic value. He'll show you the definition of the symbol. He'll compare it to a clothing logo that someone may have. He'll rewind videos ten times in order to paint a picture that isn't there.", 'role': 'user'}], 'model': 'gpt-3.5-turbo-16k', 'max_tokens': 1000, 'n': 1, 'stream': False, 'temperature': 0.7}}
DEBUG:openai._base_client:Sending HTTP Request: POST https://api.openai.com/v1/chat/completions
DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>
DEBUG:httpcore.http11:send_request_headers.complete
DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>
DEBUG:httpcore.http11:send_request_body.complete
DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>
DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Mon, 08 Jul 2024 07:24:12 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'openai-organization', b'machinekingsmedia'), (b'openai-processing-ms', b'310'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains'), (b'x-ratelimit-limit-requests', b'10000'), (b'x-ratelimit-limit-tokens', b'1000000'), (b'x-ratelimit-remaining-requests', b'9999'), (b'x-ratelimit-remaining-tokens', b'998847'), (b'x-ratelimit-reset-requests', b'6ms'), (b'x-ratelimit-reset-tokens', b'69ms'), (b'x-request-id', b'req_67b635abf02047193ec851cd8c7adbf0'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'89fe554eebe20594-IAD'), (b'Content-Encoding', b'br'), (b'alt-svc', b'h3=":443"; ma=86400')])
INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>
DEBUG:httpcore.http11:receive_response_body.complete
DEBUG:httpcore.http11:response_closed.started
DEBUG:httpcore.http11:response_closed.complete
DEBUG:openai._base_client:HTTP Response: POST https://api.openai.com/v1/chat/completions "200 OK" Headers({'date': 'Mon, 08 Jul 2024 07:24:12 GMT', 'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'openai-organization': 'machinekingsmedia', 'openai-processing-ms': '310', 'openai-version': '2020-10-01', 'strict-transport-security': 'max-age=31536000; includeSubDomains', 'x-ratelimit-limit-requests': '10000', 'x-ratelimit-limit-tokens': '1000000', 'x-ratelimit-remaining-requests': '9999', 'x-ratelimit-remaining-tokens': '998847', 'x-ratelimit-reset-requests': '6ms', 'x-ratelimit-reset-tokens': '69ms', 'x-request-id': 'req_67b635abf02047193ec851cd8c7adbf0', 'cf-cache-status': 'DYNAMIC', 'server': 'cloudflare', 'cf-ray': '89fe554eebe20594-IAD', 'content-encoding': 'br', 'alt-svc': 'h3=":443"; ma=86400'})
DEBUG:openai._base_client:request_id: req_67b635abf02047193ec851cd8c7adbf0
DEBUG:openai._base_client:Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'json_data': {'messages': [{'content': "Generate a short, descriptive title for the following chapter content: He'll add context that isn't there. He has been successful with taking this approach. What that success has gotten him is allies. Not views, but allies. That is what people like him need. Allies. They need to be legitimized. The only way they can be legitimized is with allies. It's with people who are not as fringe as him. Who are not as out there as he is to embrace him.", 'role': 'user'}], 'model': 'gpt-3.5-turbo-16k', 'max_tokens': 1000, 'n': 1, 'stream': False, 'temperature': 0.7}}
DEBUG:openai._base_client:Sending HTTP Request: POST https://api.openai.com/v1/chat/completions
DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>
DEBUG:httpcore.http11:send_request_headers.complete
DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>
DEBUG:httpcore.http11:send_request_body.complete
DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>
DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Mon, 08 Jul 2024 07:24:13 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'openai-organization', b'machinekingsmedia'), (b'openai-processing-ms', b'413'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains'), (b'x-ratelimit-limit-requests', b'10000'), (b'x-ratelimit-limit-tokens', b'1000000'), (b'x-ratelimit-remaining-requests', b'9999'), (b'x-ratelimit-remaining-tokens', b'998886'), (b'x-ratelimit-reset-requests', b'6ms'), (b'x-ratelimit-reset-tokens', b'66ms'), (b'x-request-id', b'req_93d638a283bb4923c12691bd6ca75d63'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'89fe55528e1a0594-IAD'), (b'Content-Encoding', b'br'), (b'alt-svc', b'h3=":443"; ma=86400')])
INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>
DEBUG:httpcore.http11:receive_response_body.complete
DEBUG:httpcore.http11:response_closed.started
DEBUG:httpcore.http11:response_closed.complete
DEBUG:openai._base_client:HTTP Response: POST https://api.openai.com/v1/chat/completions "200 OK" Headers({'date': 'Mon, 08 Jul 2024 07:24:13 GMT', 'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'openai-organization': 'machinekingsmedia', 'openai-processing-ms': '413', 'openai-version': '2020-10-01', 'strict-transport-security': 'max-age=31536000; includeSubDomains', 'x-ratelimit-limit-requests': '10000', 'x-ratelimit-limit-tokens': '1000000', 'x-ratelimit-remaining-requests': '9999', 'x-ratelimit-remaining-tokens': '998886', 'x-ratelimit-reset-requests': '6ms', 'x-ratelimit-reset-tokens': '66ms', 'x-request-id': 'req_93d638a283bb4923c12691bd6ca75d63', 'cf-cache-status': 'DYNAMIC', 'server': 'cloudflare', 'cf-ray': '89fe55528e1a0594-IAD', 'content-encoding': 'br', 'alt-svc': 'h3=":443"; ma=86400'})
DEBUG:openai._base_client:request_id: req_93d638a283bb4923c12691bd6ca75d63
DEBUG:openai._base_client:Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'json_data': {'messages': [{'content': "Generate a short, descriptive title for the following chapter content: That's what he tried to do with Geechee Gotti. One of the most powerful battle rappers in battle rap. When the attempt to be normalized and embraced by Geechee Gotti failed... It was an illustration of... It was an example of... It can be likened to someone like Alex Jones trying to normalize themselves. And when it failed flat... It was a reminder for us that lies... That fiction has consequences.", 'role': 'user'}], 'model': 'gpt-3.5-turbo-16k', 'max_tokens': 1000, 'n': 1, 'stream': False, 'temperature': 0.7}}
DEBUG:openai._base_client:Sending HTTP Request: POST https://api.openai.com/v1/chat/completions
DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>
DEBUG:httpcore.http11:send_request_headers.complete
DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>
DEBUG:httpcore.http11:send_request_body.complete
DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>
DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Mon, 08 Jul 2024 07:24:14 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'openai-organization', b'machinekingsmedia'), (b'openai-processing-ms', b'242'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains'), (b'x-ratelimit-limit-requests', b'10000'), (b'x-ratelimit-limit-tokens', b'1000000'), (b'x-ratelimit-remaining-requests', b'9999'), (b'x-ratelimit-remaining-tokens', b'998880'), (b'x-ratelimit-reset-requests', b'6ms'), (b'x-ratelimit-reset-tokens', b'67ms'), (b'x-request-id', b'req_3e26f9719626efc946ff94b4377a77d9'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'89fe5555c8610594-IAD'), (b'Content-Encoding', b'br'), (b'alt-svc', b'h3=":443"; ma=86400')])
INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>
DEBUG:httpcore.http11:receive_response_body.complete
DEBUG:httpcore.http11:response_closed.started
DEBUG:httpcore.http11:response_closed.complete
DEBUG:openai._base_client:HTTP Response: POST https://api.openai.com/v1/chat/completions "200 OK" Headers({'date': 'Mon, 08 Jul 2024 07:24:14 GMT', 'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'openai-organization': 'machinekingsmedia', 'openai-processing-ms': '242', 'openai-version': '2020-10-01', 'strict-transport-security': 'max-age=31536000; includeSubDomains', 'x-ratelimit-limit-requests': '10000', 'x-ratelimit-limit-tokens': '1000000', 'x-ratelimit-remaining-requests': '9999', 'x-ratelimit-remaining-tokens': '998880', 'x-ratelimit-reset-requests': '6ms', 'x-ratelimit-reset-tokens': '67ms', 'x-request-id': 'req_3e26f9719626efc946ff94b4377a77d9', 'cf-cache-status': 'DYNAMIC', 'server': 'cloudflare', 'cf-ray': '89fe5555c8610594-IAD', 'content-encoding': 'br', 'alt-svc': 'h3=":443"; ma=86400'})
DEBUG:openai._base_client:request_id: req_3e26f9719626efc946ff94b4377a77d9
DEBUG:openai._base_client:Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'json_data': {'messages': [{'content': "Summarize the following chapter in one sentence: okay let's talk about DME Detroit. DME Detroit is a part of the underbelly of battle rap blogging. He uses his iPad as a soundboard and he uses it for comedic effect but he also has a button there where he says the n-word well not he says it but he has it to say the n-word because he can't say it so he uses that as a button that he can press and he really gets over on his fans by being able to use that type of language under the guise of hey I didn't say it guys it's just a button that I press and it says it. He also is very very close to adept. This underbelly of hip-hop I'm sorry this underbelly of battle rap blogging is not a place where people don't get views or they aren't seen these are the bottom feeders that feed off of rumors they feed off of", 'role': 'user'}], 'model': 'gpt-3.5-turbo-16k', 'max_tokens': 1000, 'n': 1, 'stream': False, 'temperature': 0.7}}
DEBUG:openai._base_client:Sending HTTP Request: POST https://api.openai.com/v1/chat/completions
DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>
DEBUG:httpcore.http11:send_request_headers.complete
DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>
DEBUG:httpcore.http11:send_request_body.complete
DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>
DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Mon, 08 Jul 2024 07:24:14 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'openai-organization', b'machinekingsmedia'), (b'openai-processing-ms', b'257'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains'), (b'x-ratelimit-limit-requests', b'10000'), (b'x-ratelimit-limit-tokens', b'1000000'), (b'x-ratelimit-remaining-requests', b'9999'), (b'x-ratelimit-remaining-tokens', b'998796'), (b'x-ratelimit-reset-requests', b'6ms'), (b'x-ratelimit-reset-tokens', b'72ms'), (b'x-request-id', b'req_5b8d3a7db13eac568884e1608e528fbb'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'89fe555cacf20594-IAD'), (b'Content-Encoding', b'br'), (b'alt-svc', b'h3=":443"; ma=86400')])
INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>
DEBUG:httpcore.http11:receive_response_body.complete
DEBUG:httpcore.http11:response_closed.started
DEBUG:httpcore.http11:response_closed.complete
DEBUG:openai._base_client:HTTP Response: POST https://api.openai.com/v1/chat/completions "200 OK" Headers({'date': 'Mon, 08 Jul 2024 07:24:14 GMT', 'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'openai-organization': 'machinekingsmedia', 'openai-processing-ms': '257', 'openai-version': '2020-10-01', 'strict-transport-security': 'max-age=31536000; includeSubDomains', 'x-ratelimit-limit-requests': '10000', 'x-ratelimit-limit-tokens': '1000000', 'x-ratelimit-remaining-requests': '9999', 'x-ratelimit-remaining-tokens': '998796', 'x-ratelimit-reset-requests': '6ms', 'x-ratelimit-reset-tokens': '72ms', 'x-request-id': 'req_5b8d3a7db13eac568884e1608e528fbb', 'cf-cache-status': 'DYNAMIC', 'server': 'cloudflare', 'cf-ray': '89fe555cacf20594-IAD', 'content-encoding': 'br', 'alt-svc': 'h3=":443"; ma=86400'})
DEBUG:openai._base_client:request_id: req_5b8d3a7db13eac568884e1608e528fbb
DEBUG:openai._base_client:Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'json_data': {'messages': [{'content': "Summarize the following chapter in one sentence: controversy and they feed off of repeating lies amongst themselves where if one of them says it another one of them will repeat it to give it legs and then the person that originally said it feels as though now what they said has some type of legitimacy in their mind DME Detroit is also one of those people who is not the most responsible man in the world. If you go to his page he has a dmedetroit.com that dmedetroit.com he lost that domain some time ago it says domain not claimed so DME Detroit while he is very active on face on on YouTube he is not he spends a lot of his time making very long live streams two-hour videos about Chris on bias but he's made yes 10,000 10.6 subscriber yes 10.6 subscribers", 'role': 'user'}], 'model': 'gpt-3.5-turbo-16k', 'max_tokens': 1000, 'n': 1, 'stream': False, 'temperature': 0.7}}
DEBUG:openai._base_client:Sending HTTP Request: POST https://api.openai.com/v1/chat/completions
DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>
DEBUG:httpcore.http11:send_request_headers.complete
DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>
DEBUG:httpcore.http11:send_request_body.complete
DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>
DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Mon, 08 Jul 2024 07:24:15 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'openai-organization', b'machinekingsmedia'), (b'openai-processing-ms', b'802'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains'), (b'x-ratelimit-limit-requests', b'10000'), (b'x-ratelimit-limit-tokens', b'1000000'), (b'x-ratelimit-remaining-requests', b'9999'), (b'x-ratelimit-remaining-tokens', b'998809'), (b'x-ratelimit-reset-requests', b'6ms'), (b'x-ratelimit-reset-tokens', b'71ms'), (b'x-request-id', b'req_43b55875c37a990617bbc10c5c87bb7a'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'89fe555fced00594-IAD'), (b'Content-Encoding', b'br'), (b'alt-svc', b'h3=":443"; ma=86400')])
INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>
DEBUG:httpcore.http11:receive_response_body.complete
DEBUG:httpcore.http11:response_closed.started
DEBUG:httpcore.http11:response_closed.complete
DEBUG:openai._base_client:HTTP Response: POST https://api.openai.com/v1/chat/completions "200 OK" Headers({'date': 'Mon, 08 Jul 2024 07:24:15 GMT', 'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'openai-organization': 'machinekingsmedia', 'openai-processing-ms': '802', 'openai-version': '2020-10-01', 'strict-transport-security': 'max-age=31536000; includeSubDomains', 'x-ratelimit-limit-requests': '10000', 'x-ratelimit-limit-tokens': '1000000', 'x-ratelimit-remaining-requests': '9999', 'x-ratelimit-remaining-tokens': '998809', 'x-ratelimit-reset-requests': '6ms', 'x-ratelimit-reset-tokens': '71ms', 'x-request-id': 'req_43b55875c37a990617bbc10c5c87bb7a', 'cf-cache-status': 'DYNAMIC', 'server': 'cloudflare', 'cf-ray': '89fe555fced00594-IAD', 'content-encoding': 'br', 'alt-svc': 'h3=":443"; ma=86400'})
DEBUG:openai._base_client:request_id: req_43b55875c37a990617bbc10c5c87bb7a
DEBUG:openai._base_client:Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'json_data': {'messages': [{'content': "Summarize the following chapter in one sentence: with 2.2 thousand videos yes love muffin probably the reason for DME Detroit making so many videos is because it's been so long since he's had a video that did well on YouTube so he's seen greatness or he's seen some return on all his effort but he hasn't seen that return in over four years and over four years he hasn't had a video go over 16k 16,000 views and those videos that he made two of them are regarding the Tech N9ne situation you know what Tech N9ne was accused of what Tech N9ne was pretty much guilty of some heinous crimes and he ended up committing suicide in DME Detroit he attached himself to that controversy and he saw success but that was four years ago and now he rarely breaks over 3,000 views he does break over 3,000 views but it's very", 'role': 'user'}], 'model': 'gpt-3.5-turbo-16k', 'max_tokens': 1000, 'n': 1, 'stream': False, 'temperature': 0.7}}
DEBUG:openai._base_client:Sending HTTP Request: POST https://api.openai.com/v1/chat/completions
DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>
DEBUG:httpcore.http11:send_request_headers.complete
DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>
DEBUG:httpcore.http11:send_request_body.complete
DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>
DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Mon, 08 Jul 2024 07:24:16 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'openai-organization', b'machinekingsmedia'), (b'openai-processing-ms', b'885'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains'), (b'x-ratelimit-limit-requests', b'10000'), (b'x-ratelimit-limit-tokens', b'1000000'), (b'x-ratelimit-remaining-requests', b'9999'), (b'x-ratelimit-remaining-tokens', b'998796'), (b'x-ratelimit-reset-requests', b'6ms'), (b'x-ratelimit-reset-tokens', b'72ms'), (b'x-request-id', b'req_fb44b1717a5fb54a57f9385f71e05d66'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'89fe55664a990594-IAD'), (b'Content-Encoding', b'br'), (b'alt-svc', b'h3=":443"; ma=86400')])
INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>
DEBUG:httpcore.http11:receive_response_body.complete
DEBUG:httpcore.http11:response_closed.started
DEBUG:httpcore.http11:response_closed.complete
DEBUG:openai._base_client:HTTP Response: POST https://api.openai.com/v1/chat/completions "200 OK" Headers({'date': 'Mon, 08 Jul 2024 07:24:16 GMT', 'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'openai-organization': 'machinekingsmedia', 'openai-processing-ms': '885', 'openai-version': '2020-10-01', 'strict-transport-security': 'max-age=31536000; includeSubDomains', 'x-ratelimit-limit-requests': '10000', 'x-ratelimit-limit-tokens': '1000000', 'x-ratelimit-remaining-requests': '9999', 'x-ratelimit-remaining-tokens': '998796', 'x-ratelimit-reset-requests': '6ms', 'x-ratelimit-reset-tokens': '72ms', 'x-request-id': 'req_fb44b1717a5fb54a57f9385f71e05d66', 'cf-cache-status': 'DYNAMIC', 'server': 'cloudflare', 'cf-ray': '89fe55664a990594-IAD', 'content-encoding': 'br', 'alt-svc': 'h3=":443"; ma=86400'})
DEBUG:openai._base_client:request_id: req_fb44b1717a5fb54a57f9385f71e05d66
DEBUG:openai._base_client:Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'json_data': {'messages': [{'content': "Summarize the following chapter in one sentence: rare for him DME Detroit one one thing that he is credited with is being one of the early people to say that the URL app numbers were wrong he was one of the first people to really sniff out the problem with the with the numbers DME Detroit mentions ADEPT in almost every other video it's almost his sister podcast for his sister channel where he has ADEPT running it and then he just does his thing DME Detroit's channel has run its course it is widely believed that during a feud with PZ world he tried to get PZ's channel taken down the methods that he used are currently unknown at this time but DME Detroit ADEPT are tethered together", 'role': 'user'}], 'model': 'gpt-3.5-turbo-16k', 'max_tokens': 1000, 'n': 1, 'stream': False, 'temperature': 0.7}}
DEBUG:openai._base_client:Sending HTTP Request: POST https://api.openai.com/v1/chat/completions
DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>
DEBUG:httpcore.http11:send_request_headers.complete
DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>
DEBUG:httpcore.http11:send_request_body.complete
DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>
DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Mon, 08 Jul 2024 07:24:18 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'openai-organization', b'machinekingsmedia'), (b'openai-processing-ms', b'906'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains'), (b'x-ratelimit-limit-requests', b'10000'), (b'x-ratelimit-limit-tokens', b'1000000'), (b'x-ratelimit-remaining-requests', b'9999'), (b'x-ratelimit-remaining-tokens', b'998826'), (b'x-ratelimit-reset-requests', b'6ms'), (b'x-ratelimit-reset-tokens', b'70ms'), (b'x-request-id', b'req_a8698de169bb919bdb7ec265bba3d455'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'89fe556d3ee70594-IAD'), (b'Content-Encoding', b'br'), (b'alt-svc', b'h3=":443"; ma=86400')])
INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>
DEBUG:httpcore.http11:receive_response_body.complete
DEBUG:httpcore.http11:response_closed.started
DEBUG:httpcore.http11:response_closed.complete
DEBUG:openai._base_client:HTTP Response: POST https://api.openai.com/v1/chat/completions "200 OK" Headers({'date': 'Mon, 08 Jul 2024 07:24:18 GMT', 'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'openai-organization': 'machinekingsmedia', 'openai-processing-ms': '906', 'openai-version': '2020-10-01', 'strict-transport-security': 'max-age=31536000; includeSubDomains', 'x-ratelimit-limit-requests': '10000', 'x-ratelimit-limit-tokens': '1000000', 'x-ratelimit-remaining-requests': '9999', 'x-ratelimit-remaining-tokens': '998826', 'x-ratelimit-reset-requests': '6ms', 'x-ratelimit-reset-tokens': '70ms', 'x-request-id': 'req_a8698de169bb919bdb7ec265bba3d455', 'cf-cache-status': 'DYNAMIC', 'server': 'cloudflare', 'cf-ray': '89fe556d3ee70594-IAD', 'content-encoding': 'br', 'alt-svc': 'h3=":443"; ma=86400'})
DEBUG:openai._base_client:request_id: req_a8698de169bb919bdb7ec265bba3d455
DEBUG:openai._base_client:Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'json_data': {'messages': [{'content': "Summarize the following chapter in one sentence: and they find common ground in that world that they're in", 'role': 'user'}], 'model': 'gpt-3.5-turbo-16k', 'max_tokens': 1000, 'n': 1, 'stream': False, 'temperature': 0.7}}
DEBUG:openai._base_client:Sending HTTP Request: POST https://api.openai.com/v1/chat/completions
DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>
DEBUG:httpcore.http11:send_request_headers.complete
DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>
DEBUG:httpcore.http11:send_request_body.complete
DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>
DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Mon, 08 Jul 2024 07:24:19 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'openai-organization', b'machinekingsmedia'), (b'openai-processing-ms', b'533'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains'), (b'x-ratelimit-limit-requests', b'10000'), (b'x-ratelimit-limit-tokens', b'1000000'), (b'x-ratelimit-remaining-requests', b'9999'), (b'x-ratelimit-remaining-tokens', b'998972'), (b'x-ratelimit-reset-requests', b'6ms'), (b'x-ratelimit-reset-tokens', b'61ms'), (b'x-request-id', b'req_18c7a7a01e9172558a9269f3329ee7fb'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'89fe55785e460594-IAD'), (b'Content-Encoding', b'br'), (b'alt-svc', b'h3=":443"; ma=86400')])
INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>
DEBUG:httpcore.http11:receive_response_body.complete
DEBUG:httpcore.http11:response_closed.started
DEBUG:httpcore.http11:response_closed.complete
DEBUG:openai._base_client:HTTP Response: POST https://api.openai.com/v1/chat/completions "200 OK" Headers({'date': 'Mon, 08 Jul 2024 07:24:19 GMT', 'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'openai-organization': 'machinekingsmedia', 'openai-processing-ms': '533', 'openai-version': '2020-10-01', 'strict-transport-security': 'max-age=31536000; includeSubDomains', 'x-ratelimit-limit-requests': '10000', 'x-ratelimit-limit-tokens': '1000000', 'x-ratelimit-remaining-requests': '9999', 'x-ratelimit-remaining-tokens': '998972', 'x-ratelimit-reset-requests': '6ms', 'x-ratelimit-reset-tokens': '61ms', 'x-request-id': 'req_18c7a7a01e9172558a9269f3329ee7fb', 'cf-cache-status': 'DYNAMIC', 'server': 'cloudflare', 'cf-ray': '89fe55785e460594-IAD', 'content-encoding': 'br', 'alt-svc': 'h3=":443"; ma=86400'})
DEBUG:openai._base_client:request_id: req_18c7a7a01e9172558a9269f3329ee7fb
DEBUG:openai._base_client:Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'json_data': {'messages': [{'content': "Provide a brief summary of the following transcript: File: 231230_1910.mp3\nRecording Date: 2023-12-30\nDuration: 00:07:26.48\nTranscription Date: 2024-07-07 06:16:19\n\n1\n00:00:00,000 --> 00:00:15,360\nokay let's talk about DME Detroit. DME Detroit is a part of the underbelly of\n\n2\n00:00:15,360 --> 00:00:33,439\nbattle rap blogging. He uses his iPad as a soundboard and he uses it for comedic\n\n3\n00:00:33,439 --> 00:00:40,080\neffect but he also has a button there where he says the n-word well not he\n\n4\n00:00:40,080 --> 00:00:48,959\nsays it but he has it to say the n-word because he can't say it so he uses that\n\n5\n00:00:48,959 --> 00:00:56,759\nas a button that he can press and he really gets over on his fans by being\n\n6\n00:00:56,759 --> 00:01:05,199\nable to use that type of language under the guise of hey I didn't say it guys\n\n7\n00:01:05,199 --> 00:01:13,760\nit's just a button that I press and it says it. He also is very very close to\n\n8\n00:01:13,760 --> 00:01:21,320\nadept. This underbelly of hip-hop I'm sorry this underbelly of battle rap\n\n9\n00:01:21,320 --> 00:01:30,919\nblogging is not a place where people don't get views or they aren't seen\n\n10\n00:01:30,919 --> 00:01:40,480\nthese are the bottom feeders that feed off of rumors they feed off of\n\n11\n00:01:40,480 --> 00:01:50,440\ncontroversy and they feed off of repeating lies amongst themselves where\n\n12\n00:01:50,440 --> 00:01:59,320\nif one of them says it another one of them will repeat it to give it legs and\n\n13\n00:01:59,320 --> 00:02:05,440\nthen the person that originally said it feels as though now what they said has\n\n14\n00:02:05,440 --> 00:02:14,800\nsome type of legitimacy in their mind DME Detroit is also one of those people\n\n15\n00:02:14,800 --> 00:02:23,279\nwho is not the most responsible man in the world. If you go to his page he has a\n\n16\n00:02:23,440 --> 00:02:33,800\ndmedetroit.com that dmedetroit.com he lost that domain some time ago\n\n17\n00:02:33,800 --> 00:02:43,279\nit says domain not claimed\n\n18\n00:02:47,080 --> 00:03:00,399\nso DME Detroit while he is very active on face on on YouTube he is not he\n\n19\n00:03:00,399 --> 00:03:10,199\nspends a lot of his time making very long live streams two-hour videos about\n\n20\n00:03:10,199 --> 00:03:34,199\nChris on bias but he's made yes 10,000 10.6 subscriber yes 10.6 subscribers\n\n21\n00:03:34,199 --> 00:03:51,360\nwith 2.2 thousand videos yes love muffin probably the reason for DME Detroit\n\n22\n00:03:51,360 --> 00:04:01,600\nmaking so many videos is because it's been so long since he's had a video that\n\n23\n00:04:01,600 --> 00:04:10,679\ndid well on YouTube so he's seen greatness or he's seen some return on\n\n24\n00:04:10,679 --> 00:04:19,200\nall his effort but he hasn't seen that return in over four years and over four\n\n25\n00:04:19,200 --> 00:04:29,320\nyears he hasn't had a video go over 16k 16,000 views and those videos that he\n\n26\n00:04:29,399 --> 00:04:35,320\nmade two of them are regarding the Tech N9ne situation you know what Tech N9ne was\n\n27\n00:04:35,320 --> 00:04:46,679\naccused of what Tech N9ne was pretty much guilty of some heinous crimes and he\n\n28\n00:04:46,679 --> 00:04:58,040\nended up committing suicide in DME Detroit he attached himself to that\n\n29\n00:04:58,040 --> 00:05:09,720\ncontroversy and he saw success but that was four years ago and now he\n\n30\n00:05:09,720 --> 00:05:28,920\nrarely breaks over 3,000 views he does break over 3,000 views but it's very\n\n31\n00:05:28,920 --> 00:05:31,760\nrare for him\n\n32\n00:05:40,559 --> 00:05:48,160\nDME Detroit one one thing that he is credited with is being one of the early\n\n33\n00:05:48,160 --> 00:05:55,559\npeople to say that the URL app numbers were wrong he was one of the first\n\n34\n00:05:55,559 --> 00:06:05,880\npeople to really sniff out the problem with the with the numbers DME Detroit\n\n35\n00:06:05,920 --> 00:06:19,799\nmentions ADEPT in almost every other video it's almost his sister podcast for\n\n36\n00:06:19,799 --> 00:06:28,959\nhis sister channel where he has ADEPT running it and then he just does his\n\n37\n00:06:28,959 --> 00:06:48,279\nthing DME", 'role': 'user'}], 'model': 'gpt-3.5-turbo-16k', 'max_tokens': 1000, 'n': 1, 'stream': False, 'temperature': 0.7}}
DEBUG:openai._base_client:Sending HTTP Request: POST https://api.openai.com/v1/chat/completions
DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>
DEBUG:httpcore.http11:send_request_headers.complete
DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>
DEBUG:httpcore.http11:send_request_body.complete
DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>
DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Mon, 08 Jul 2024 07:24:22 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'openai-organization', b'machinekingsmedia'), (b'openai-processing-ms', b'2226'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains'), (b'x-ratelimit-limit-requests', b'10000'), (b'x-ratelimit-limit-tokens', b'1000000'), (b'x-ratelimit-remaining-requests', b'9999'), (b'x-ratelimit-remaining-tokens', b'997985'), (b'x-ratelimit-reset-requests', b'6ms'), (b'x-ratelimit-reset-tokens', b'120ms'), (b'x-request-id', b'req_18ba2d8c7f0b18eeb1d3c6d41aae7241'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'89fe557c69000594-IAD'), (b'Content-Encoding', b'br'), (b'alt-svc', b'h3=":443"; ma=86400')])
INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>
DEBUG:httpcore.http11:receive_response_body.complete
DEBUG:httpcore.http11:response_closed.started
DEBUG:httpcore.http11:response_closed.complete
DEBUG:openai._base_client:HTTP Response: POST https://api.openai.com/v1/chat/completions "200 OK" Headers({'date': 'Mon, 08 Jul 2024 07:24:22 GMT', 'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'openai-organization': 'machinekingsmedia', 'openai-processing-ms': '2226', 'openai-version': '2020-10-01', 'strict-transport-security': 'max-age=31536000; includeSubDomains', 'x-ratelimit-limit-requests': '10000', 'x-ratelimit-limit-tokens': '1000000', 'x-ratelimit-remaining-requests': '9999', 'x-ratelimit-remaining-tokens': '997985', 'x-ratelimit-reset-requests': '6ms', 'x-ratelimit-reset-tokens': '120ms', 'x-request-id': 'req_18ba2d8c7f0b18eeb1d3c6d41aae7241', 'cf-cache-status': 'DYNAMIC', 'server': 'cloudflare', 'cf-ray': '89fe557c69000594-IAD', 'content-encoding': 'br', 'alt-svc': 'h3=":443"; ma=86400'})
DEBUG:openai._base_client:request_id: req_18ba2d8c7f0b18eeb1d3c6d41aae7241
DEBUG:openai._base_client:Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'json_data': {'messages': [{'content': "Generate a short, descriptive title for the following chapter content: okay let's talk about DME Detroit. DME Detroit is a part of the underbelly of battle rap blogging. He uses his iPad as a soundboard and he uses it for comedic effect but he also has a button there where he says the n-word well not he says it but he has it to say the n-word because he can't say it so he uses that as a button that he can press and he really gets over on his fans by being able to use that type of language under the guise of hey I didn't say it guys it's just a button that I press and it says it. He also is very very close to adept. This underbelly of hip-hop I'm sorry this underbelly of battle rap blogging is not a place where people don't get views or they aren't seen these are the bottom feeders that feed off of rumors they feed off of", 'role': 'user'}], 'model': 'gpt-3.5-turbo-16k', 'max_tokens': 1000, 'n': 1, 'stream': False, 'temperature': 0.7}}
DEBUG:openai._base_client:Sending HTTP Request: POST https://api.openai.com/v1/chat/completions
DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>
DEBUG:httpcore.http11:send_request_headers.complete
DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>
DEBUG:httpcore.http11:send_request_body.complete
DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>
DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Mon, 08 Jul 2024 07:24:23 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'openai-organization', b'machinekingsmedia'), (b'openai-processing-ms', b'401'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains'), (b'x-ratelimit-limit-requests', b'10000'), (b'x-ratelimit-limit-tokens', b'1000000'), (b'x-ratelimit-remaining-requests', b'9999'), (b'x-ratelimit-remaining-tokens', b'998790'), (b'x-ratelimit-reset-requests', b'6ms'), (b'x-ratelimit-reset-tokens', b'72ms'), (b'x-request-id', b'req_6b582cb96211bbc6575b87a3ea8cb42d'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'89fe55913e940594-IAD'), (b'Content-Encoding', b'br'), (b'alt-svc', b'h3=":443"; ma=86400')])
INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>
DEBUG:httpcore.http11:receive_response_body.complete
DEBUG:httpcore.http11:response_closed.started
DEBUG:httpcore.http11:response_closed.complete
DEBUG:openai._base_client:HTTP Response: POST https://api.openai.com/v1/chat/completions "200 OK" Headers({'date': 'Mon, 08 Jul 2024 07:24:23 GMT', 'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'openai-organization': 'machinekingsmedia', 'openai-processing-ms': '401', 'openai-version': '2020-10-01', 'strict-transport-security': 'max-age=31536000; includeSubDomains', 'x-ratelimit-limit-requests': '10000', 'x-ratelimit-limit-tokens': '1000000', 'x-ratelimit-remaining-requests': '9999', 'x-ratelimit-remaining-tokens': '998790', 'x-ratelimit-reset-requests': '6ms', 'x-ratelimit-reset-tokens': '72ms', 'x-request-id': 'req_6b582cb96211bbc6575b87a3ea8cb42d', 'cf-cache-status': 'DYNAMIC', 'server': 'cloudflare', 'cf-ray': '89fe55913e940594-IAD', 'content-encoding': 'br', 'alt-svc': 'h3=":443"; ma=86400'})
DEBUG:openai._base_client:request_id: req_6b582cb96211bbc6575b87a3ea8cb42d
DEBUG:openai._base_client:Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'json_data': {'messages': [{'content': "Generate a short, descriptive title for the following chapter content: controversy and they feed off of repeating lies amongst themselves where if one of them says it another one of them will repeat it to give it legs and then the person that originally said it feels as though now what they said has some type of legitimacy in their mind DME Detroit is also one of those people who is not the most responsible man in the world. If you go to his page he has a dmedetroit.com that dmedetroit.com he lost that domain some time ago it says domain not claimed so DME Detroit while he is very active on face on on YouTube he is not he spends a lot of his time making very long live streams two-hour videos about Chris on bias but he's made yes 10,000 10.6 subscriber yes 10.6 subscribers", 'role': 'user'}], 'model': 'gpt-3.5-turbo-16k', 'max_tokens': 1000, 'n': 1, 'stream': False, 'temperature': 0.7}}
DEBUG:openai._base_client:Sending HTTP Request: POST https://api.openai.com/v1/chat/completions
DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>
DEBUG:httpcore.http11:send_request_headers.complete
DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>
DEBUG:httpcore.http11:send_request_body.complete
DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>
DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Mon, 08 Jul 2024 07:24:24 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'openai-organization', b'machinekingsmedia'), (b'openai-processing-ms', b'389'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains'), (b'x-ratelimit-limit-requests', b'10000'), (b'x-ratelimit-limit-tokens', b'1000000'), (b'x-ratelimit-remaining-requests', b'9999'), (b'x-ratelimit-remaining-tokens', b'998802'), (b'x-ratelimit-reset-requests', b'6ms'), (b'x-ratelimit-reset-tokens', b'71ms'), (b'x-request-id', b'req_24bcf801544bbb463691a3a37460c268'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'89fe559489110594-IAD'), (b'Content-Encoding', b'br'), (b'alt-svc', b'h3=":443"; ma=86400')])
INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>
DEBUG:httpcore.http11:receive_response_body.complete
DEBUG:httpcore.http11:response_closed.started
DEBUG:httpcore.http11:response_closed.complete
DEBUG:openai._base_client:HTTP Response: POST https://api.openai.com/v1/chat/completions "200 OK" Headers({'date': 'Mon, 08 Jul 2024 07:24:24 GMT', 'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'openai-organization': 'machinekingsmedia', 'openai-processing-ms': '389', 'openai-version': '2020-10-01', 'strict-transport-security': 'max-age=31536000; includeSubDomains', 'x-ratelimit-limit-requests': '10000', 'x-ratelimit-limit-tokens': '1000000', 'x-ratelimit-remaining-requests': '9999', 'x-ratelimit-remaining-tokens': '998802', 'x-ratelimit-reset-requests': '6ms', 'x-ratelimit-reset-tokens': '71ms', 'x-request-id': 'req_24bcf801544bbb463691a3a37460c268', 'cf-cache-status': 'DYNAMIC', 'server': 'cloudflare', 'cf-ray': '89fe559489110594-IAD', 'content-encoding': 'br', 'alt-svc': 'h3=":443"; ma=86400'})
DEBUG:openai._base_client:request_id: req_24bcf801544bbb463691a3a37460c268
DEBUG:openai._base_client:Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'json_data': {'messages': [{'content': "Generate a short, descriptive title for the following chapter content: with 2.2 thousand videos yes love muffin probably the reason for DME Detroit making so many videos is because it's been so long since he's had a video that did well on YouTube so he's seen greatness or he's seen some return on all his effort but he hasn't seen that return in over four years and over four years he hasn't had a video go over 16k 16,000 views and those videos that he made two of them are regarding the Tech N9ne situation you know what Tech N9ne was accused of what Tech N9ne was pretty much guilty of some heinous crimes and he ended up committing suicide in DME Detroit he attached himself to that controversy and he saw success but that was four years ago and now he rarely breaks over 3,000 views he does break over 3,000 views but it's very", 'role': 'user'}], 'model': 'gpt-3.5-turbo-16k', 'max_tokens': 1000, 'n': 1, 'stream': False, 'temperature': 0.7}}
DEBUG:openai._base_client:Sending HTTP Request: POST https://api.openai.com/v1/chat/completions
DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>
DEBUG:httpcore.http11:send_request_headers.complete
DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>
DEBUG:httpcore.http11:send_request_body.complete
DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>
DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Mon, 08 Jul 2024 07:24:25 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'openai-organization', b'machinekingsmedia'), (b'openai-processing-ms', b'398'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains'), (b'x-ratelimit-limit-requests', b'10000'), (b'x-ratelimit-limit-tokens', b'1000000'), (b'x-ratelimit-remaining-requests', b'9999'), (b'x-ratelimit-remaining-tokens', b'998790'), (b'x-ratelimit-reset-requests', b'6ms'), (b'x-ratelimit-reset-tokens', b'72ms'), (b'x-request-id', b'req_59e16dc8998ac8b676f1ac2cba2ef26d'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'89fe559c7f790594-IAD'), (b'Content-Encoding', b'br'), (b'alt-svc', b'h3=":443"; ma=86400')])
INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>
DEBUG:httpcore.http11:receive_response_body.complete
DEBUG:httpcore.http11:response_closed.started
DEBUG:httpcore.http11:response_closed.complete
DEBUG:openai._base_client:HTTP Response: POST https://api.openai.com/v1/chat/completions "200 OK" Headers({'date': 'Mon, 08 Jul 2024 07:24:25 GMT', 'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'openai-organization': 'machinekingsmedia', 'openai-processing-ms': '398', 'openai-version': '2020-10-01', 'strict-transport-security': 'max-age=31536000; includeSubDomains', 'x-ratelimit-limit-requests': '10000', 'x-ratelimit-limit-tokens': '1000000', 'x-ratelimit-remaining-requests': '9999', 'x-ratelimit-remaining-tokens': '998790', 'x-ratelimit-reset-requests': '6ms', 'x-ratelimit-reset-tokens': '72ms', 'x-request-id': 'req_59e16dc8998ac8b676f1ac2cba2ef26d', 'cf-cache-status': 'DYNAMIC', 'server': 'cloudflare', 'cf-ray': '89fe559c7f790594-IAD', 'content-encoding': 'br', 'alt-svc': 'h3=":443"; ma=86400'})
DEBUG:openai._base_client:request_id: req_59e16dc8998ac8b676f1ac2cba2ef26d
DEBUG:openai._base_client:Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'json_data': {'messages': [{'content': "Generate a short, descriptive title for the following chapter content: rare for him DME Detroit one one thing that he is credited with is being one of the early people to say that the URL app numbers were wrong he was one of the first people to really sniff out the problem with the with the numbers DME Detroit mentions ADEPT in almost every other video it's almost his sister podcast for his sister channel where he has ADEPT running it and then he just does his thing DME Detroit's channel has run its course it is widely believed that during a feud with PZ world he tried to get PZ's channel taken down the methods that he used are currently unknown at this time but DME Detroit ADEPT are tethered together", 'role': 'user'}], 'model': 'gpt-3.5-turbo-16k', 'max_tokens': 1000, 'n': 1, 'stream': False, 'temperature': 0.7}}
DEBUG:openai._base_client:Sending HTTP Request: POST https://api.openai.com/v1/chat/completions
DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>
DEBUG:httpcore.http11:send_request_headers.complete
DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>
DEBUG:httpcore.http11:send_request_body.complete
DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>
DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Mon, 08 Jul 2024 07:24:25 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'openai-organization', b'machinekingsmedia'), (b'openai-processing-ms', b'368'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains'), (b'x-ratelimit-limit-requests', b'10000'), (b'x-ratelimit-limit-tokens', b'1000000'), (b'x-ratelimit-remaining-requests', b'9999'), (b'x-ratelimit-remaining-tokens', b'998821'), (b'x-ratelimit-reset-requests', b'6ms'), (b'x-ratelimit-reset-tokens', b'70ms'), (b'x-request-id', b'req_ba7cd8cfb120cced31f3e0f76fc2a832'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'89fe55a10b060594-IAD'), (b'Content-Encoding', b'br'), (b'alt-svc', b'h3=":443"; ma=86400')])
INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>
DEBUG:httpcore.http11:receive_response_body.complete
DEBUG:httpcore.http11:response_closed.started
DEBUG:httpcore.http11:response_closed.complete
DEBUG:openai._base_client:HTTP Response: POST https://api.openai.com/v1/chat/completions "200 OK" Headers({'date': 'Mon, 08 Jul 2024 07:24:25 GMT', 'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'openai-organization': 'machinekingsmedia', 'openai-processing-ms': '368', 'openai-version': '2020-10-01', 'strict-transport-security': 'max-age=31536000; includeSubDomains', 'x-ratelimit-limit-requests': '10000', 'x-ratelimit-limit-tokens': '1000000', 'x-ratelimit-remaining-requests': '9999', 'x-ratelimit-remaining-tokens': '998821', 'x-ratelimit-reset-requests': '6ms', 'x-ratelimit-reset-tokens': '70ms', 'x-request-id': 'req_ba7cd8cfb120cced31f3e0f76fc2a832', 'cf-cache-status': 'DYNAMIC', 'server': 'cloudflare', 'cf-ray': '89fe55a10b060594-IAD', 'content-encoding': 'br', 'alt-svc': 'h3=":443"; ma=86400'})
DEBUG:openai._base_client:request_id: req_ba7cd8cfb120cced31f3e0f76fc2a832
DEBUG:openai._base_client:Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'json_data': {'messages': [{'content': "Generate a short, descriptive title for the following chapter content: and they find common ground in that world that they're in", 'role': 'user'}], 'model': 'gpt-3.5-turbo-16k', 'max_tokens': 1000, 'n': 1, 'stream': False, 'temperature': 0.7}}
DEBUG:openai._base_client:Sending HTTP Request: POST https://api.openai.com/v1/chat/completions
DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>
DEBUG:httpcore.http11:send_request_headers.complete
DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>
DEBUG:httpcore.http11:send_request_body.complete
DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>
DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Mon, 08 Jul 2024 07:24:26 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'openai-organization', b'machinekingsmedia'), (b'openai-processing-ms', b'199'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains'), (b'x-ratelimit-limit-requests', b'10000'), (b'x-ratelimit-limit-tokens', b'1000000'), (b'x-ratelimit-remaining-requests', b'9999'), (b'x-ratelimit-remaining-tokens', b'998967'), (b'x-ratelimit-reset-requests', b'6ms'), (b'x-ratelimit-reset-tokens', b'61ms'), (b'x-request-id', b'req_9675ed6ed8d5b291eb8749ae598d25b4'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'89fe55a4edc40594-IAD'), (b'Content-Encoding', b'br'), (b'alt-svc', b'h3=":443"; ma=86400')])
INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>
DEBUG:httpcore.http11:receive_response_body.complete
DEBUG:httpcore.http11:response_closed.started
DEBUG:httpcore.http11:response_closed.complete
DEBUG:openai._base_client:HTTP Response: POST https://api.openai.com/v1/chat/completions "200 OK" Headers({'date': 'Mon, 08 Jul 2024 07:24:26 GMT', 'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'openai-organization': 'machinekingsmedia', 'openai-processing-ms': '199', 'openai-version': '2020-10-01', 'strict-transport-security': 'max-age=31536000; includeSubDomains', 'x-ratelimit-limit-requests': '10000', 'x-ratelimit-limit-tokens': '1000000', 'x-ratelimit-remaining-requests': '9999', 'x-ratelimit-remaining-tokens': '998967', 'x-ratelimit-reset-requests': '6ms', 'x-ratelimit-reset-tokens': '61ms', 'x-request-id': 'req_9675ed6ed8d5b291eb8749ae598d25b4', 'cf-cache-status': 'DYNAMIC', 'server': 'cloudflare', 'cf-ray': '89fe55a4edc40594-IAD', 'content-encoding': 'br', 'alt-svc': 'h3=":443"; ma=86400'})
DEBUG:openai._base_client:request_id: req_9675ed6ed8d5b291eb8749ae598d25b4
DEBUG:openai._base_client:Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'json_data': {'messages': [{'content': "Summarize the following chapter in one sentence: this is how tech 9 joined champion this is him talking about it well I'm the major Jay Black helped me with the trailer for what's that born legacy 3 I believe and we had a good conversation we chopped it up yeah hinted towards he was doing something but at then I was so focused on a Jerry West battle that kind of like stuff just like okay okay yeah yeah yeah okay so tech now was about to battle Jerry West and then he was shooting a trailer Jay Black helped him with the trailer that's what he's referring to just being in that position you know I'm saying um what's that born legacy 3 I believe and we had a good conversation we chopped it up yeah hinted towards he was doing something but at then I was so focused on a Jerry West battle that", 'role': 'user'}], 'model': 'gpt-3.5-turbo-16k', 'max_tokens': 1000, 'n': 1, 'stream': False, 'temperature': 0.7}}
DEBUG:openai._base_client:Sending HTTP Request: POST https://api.openai.com/v1/chat/completions
DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>
DEBUG:httpcore.http11:send_request_headers.complete
DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>
DEBUG:httpcore.http11:send_request_body.complete
DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>
DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Mon, 08 Jul 2024 07:24:28 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'openai-organization', b'machinekingsmedia'), (b'openai-processing-ms', b'1538'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains'), (b'x-ratelimit-limit-requests', b'10000'), (b'x-ratelimit-limit-tokens', b'1000000'), (b'x-ratelimit-remaining-requests', b'9999'), (b'x-ratelimit-remaining-tokens', b'998800'), (b'x-ratelimit-reset-requests', b'6ms'), (b'x-ratelimit-reset-tokens', b'72ms'), (b'x-request-id', b'req_1b177db9d24e2991e638d995471a90f7'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'89fe55a858130594-IAD'), (b'Content-Encoding', b'br'), (b'alt-svc', b'h3=":443"; ma=86400')])
INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>
DEBUG:httpcore.http11:receive_response_body.complete
DEBUG:httpcore.http11:response_closed.started
DEBUG:httpcore.http11:response_closed.complete
DEBUG:openai._base_client:HTTP Response: POST https://api.openai.com/v1/chat/completions "200 OK" Headers({'date': 'Mon, 08 Jul 2024 07:24:28 GMT', 'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'openai-organization': 'machinekingsmedia', 'openai-processing-ms': '1538', 'openai-version': '2020-10-01', 'strict-transport-security': 'max-age=31536000; includeSubDomains', 'x-ratelimit-limit-requests': '10000', 'x-ratelimit-limit-tokens': '1000000', 'x-ratelimit-remaining-requests': '9999', 'x-ratelimit-remaining-tokens': '998800', 'x-ratelimit-reset-requests': '6ms', 'x-ratelimit-reset-tokens': '72ms', 'x-request-id': 'req_1b177db9d24e2991e638d995471a90f7', 'cf-cache-status': 'DYNAMIC', 'server': 'cloudflare', 'cf-ray': '89fe55a858130594-IAD', 'content-encoding': 'br', 'alt-svc': 'h3=":443"; ma=86400'})
DEBUG:openai._base_client:request_id: req_1b177db9d24e2991e638d995471a90f7
DEBUG:openai._base_client:Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'json_data': {'messages': [{'content': "Summarize the following chapter in one sentence: kind of like stuff just like okay okay yeah yeah yeah okay you know and just keep the conversation going but um I want to say last year sometime early in the year he hit me up and was like yo I'm doing a show I'm going to show champion I want you to come up and I want I want you to come up and talk about the Philly battle rap scene so initially I was supposed to go up there talk about Philly battle rap me and big hand go up there talk about Philly battle rap and that was it he just wanted to get my tape on the Philly battle rap scene okay I got champion and I was like yo yo I'm like I like this you know I mean I did the Philly I did a Philly battle rap scene we talked about that the initial reaction from the fans was good", 'role': 'user'}], 'model': 'gpt-3.5-turbo-16k', 'max_tokens': 1000, 'n': 1, 'stream': False, 'temperature': 0.7}}
DEBUG:openai._base_client:Sending HTTP Request: POST https://api.openai.com/v1/chat/completions
DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>
DEBUG:httpcore.http11:send_request_headers.complete
DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>
DEBUG:httpcore.http11:send_request_body.complete
DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>
DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Mon, 08 Jul 2024 07:24:28 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'openai-organization', b'machinekingsmedia'), (b'openai-processing-ms', b'555'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains'), (b'x-ratelimit-limit-requests', b'10000'), (b'x-ratelimit-limit-tokens', b'1000000'), (b'x-ratelimit-remaining-requests', b'9999'), (b'x-ratelimit-remaining-tokens', b'998803'), (b'x-ratelimit-reset-requests', b'6ms'), (b'x-ratelimit-reset-tokens', b'71ms'), (b'x-request-id', b'req_b6b60e2581aad24907c9cb18296ef302'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'89fe55b40f740594-IAD'), (b'Content-Encoding', b'br'), (b'alt-svc', b'h3=":443"; ma=86400')])
INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>
DEBUG:httpcore.http11:receive_response_body.complete
DEBUG:httpcore.http11:response_closed.started
DEBUG:httpcore.http11:response_closed.complete
DEBUG:openai._base_client:HTTP Response: POST https://api.openai.com/v1/chat/completions "200 OK" Headers({'date': 'Mon, 08 Jul 2024 07:24:28 GMT', 'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'openai-organization': 'machinekingsmedia', 'openai-processing-ms': '555', 'openai-version': '2020-10-01', 'strict-transport-security': 'max-age=31536000; includeSubDomains', 'x-ratelimit-limit-requests': '10000', 'x-ratelimit-limit-tokens': '1000000', 'x-ratelimit-remaining-requests': '9999', 'x-ratelimit-remaining-tokens': '998803', 'x-ratelimit-reset-requests': '6ms', 'x-ratelimit-reset-tokens': '71ms', 'x-request-id': 'req_b6b60e2581aad24907c9cb18296ef302', 'cf-cache-status': 'DYNAMIC', 'server': 'cloudflare', 'cf-ray': '89fe55b40f740594-IAD', 'content-encoding': 'br', 'alt-svc': 'h3=":443"; ma=86400'})
DEBUG:openai._base_client:request_id: req_b6b60e2581aad24907c9cb18296ef302
DEBUG:openai._base_client:Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'json_data': {'messages': [{'content': "Summarize the following chapter in one sentence: Oh tech 9 is great you need to have him on the show blah blah blah blah blah so we counted like black was like already leaning towards oh I want you to come back up and I was like yeah I want to come back up and we both kind of like agreed that it'd be best if I come back up now it wasn't like permanent or nothing it was just like yeah we just need you to come back up so I came back up I started to do a couple more episodes of course the fans hated me you know but black seen something like now the fans even though they hate you now like new perfect for this like I know what I'm seeing I know I know what I'm seeing right here so I'm not worried about what they're saying like this is perfect so I started to you know read the comments and listen to what they", 'role': 'user'}], 'model': 'gpt-3.5-turbo-16k', 'max_tokens': 1000, 'n': 1, 'stream': False, 'temperature': 0.7}}
DEBUG:openai._base_client:Sending HTTP Request: POST https://api.openai.com/v1/chat/completions
DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>
DEBUG:httpcore.http11:send_request_headers.complete
DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>
DEBUG:httpcore.http11:send_request_body.complete
DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>
DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Mon, 08 Jul 2024 07:24:29 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'openai-organization', b'machinekingsmedia'), (b'openai-processing-ms', b'577'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains'), (b'x-ratelimit-limit-requests', b'10000'), (b'x-ratelimit-limit-tokens', b'1000000'), (b'x-ratelimit-remaining-requests', b'9999'), (b'x-ratelimit-remaining-tokens', b'998794'), (b'x-ratelimit-reset-requests', b'6ms'), (b'x-ratelimit-reset-tokens', b'72ms'), (b'x-request-id', b'req_779ad9ffcc231318f8d0871b7793ffa5'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'89fe55b90a780594-IAD'), (b'Content-Encoding', b'br'), (b'alt-svc', b'h3=":443"; ma=86400')])
INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>
DEBUG:httpcore.http11:receive_response_body.complete
DEBUG:httpcore.http11:response_closed.started
DEBUG:httpcore.http11:response_closed.complete
DEBUG:openai._base_client:HTTP Response: POST https://api.openai.com/v1/chat/completions "200 OK" Headers({'date': 'Mon, 08 Jul 2024 07:24:29 GMT', 'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'openai-organization': 'machinekingsmedia', 'openai-processing-ms': '577', 'openai-version': '2020-10-01', 'strict-transport-security': 'max-age=31536000; includeSubDomains', 'x-ratelimit-limit-requests': '10000', 'x-ratelimit-limit-tokens': '1000000', 'x-ratelimit-remaining-requests': '9999', 'x-ratelimit-remaining-tokens': '998794', 'x-ratelimit-reset-requests': '6ms', 'x-ratelimit-reset-tokens': '72ms', 'x-request-id': 'req_779ad9ffcc231318f8d0871b7793ffa5', 'cf-cache-status': 'DYNAMIC', 'server': 'cloudflare', 'cf-ray': '89fe55b90a780594-IAD', 'content-encoding': 'br', 'alt-svc': 'h3=":443"; ma=86400'})
DEBUG:openai._base_client:request_id: req_779ad9ffcc231318f8d0871b7793ffa5
DEBUG:openai._base_client:Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'json_data': {'messages': [{'content': "Summarize the following chapter in one sentence: were saying and then I started to actually come prepared that are going there because I can't freestyle like literally black Jay Black goes in there he freestyles most of the stuff when we're when we're doing champion I gotta have my notes ready like when he when he sends me the topics I need my notes ready for each topic so I can go in there and make some sense so I started to come prepared and slowly but surely the fans started to come around and started to respect my opinion because I started to back up what I was talking about so if I felt like someone lost the battle I had key points of why I felt as though they lost the battle and that's pretty much how that's pretty much how it happened this she started with the Philly battle rap scene I went", 'role': 'user'}], 'model': 'gpt-3.5-turbo-16k', 'max_tokens': 1000, 'n': 1, 'stream': False, 'temperature': 0.7}}
DEBUG:openai._base_client:Sending HTTP Request: POST https://api.openai.com/v1/chat/completions
DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>
DEBUG:httpcore.http11:send_request_headers.complete
DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>
DEBUG:httpcore.http11:send_request_body.complete
DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>
DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Mon, 08 Jul 2024 07:24:30 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'openai-organization', b'machinekingsmedia'), (b'openai-processing-ms', b'382'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains'), (b'x-ratelimit-limit-requests', b'10000'), (b'x-ratelimit-limit-tokens', b'1000000'), (b'x-ratelimit-remaining-requests', b'9999'), (b'x-ratelimit-remaining-tokens', b'998797'), (b'x-ratelimit-reset-requests', b'6ms'), (b'x-ratelimit-reset-tokens', b'72ms'), (b'x-request-id', b'req_cb48f97b95bd19e976715f2c87548064'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'89fe55bebdda0594-IAD'), (b'Content-Encoding', b'br'), (b'alt-svc', b'h3=":443"; ma=86400')])
INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>
DEBUG:httpcore.http11:receive_response_body.complete
DEBUG:httpcore.http11:response_closed.started
DEBUG:httpcore.http11:response_closed.complete
DEBUG:openai._base_client:HTTP Response: POST https://api.openai.com/v1/chat/completions "200 OK" Headers({'date': 'Mon, 08 Jul 2024 07:24:30 GMT', 'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'openai-organization': 'machinekingsmedia', 'openai-processing-ms': '382', 'openai-version': '2020-10-01', 'strict-transport-security': 'max-age=31536000; includeSubDomains', 'x-ratelimit-limit-requests': '10000', 'x-ratelimit-limit-tokens': '1000000', 'x-ratelimit-remaining-requests': '9999', 'x-ratelimit-remaining-tokens': '998797', 'x-ratelimit-reset-requests': '6ms', 'x-ratelimit-reset-tokens': '72ms', 'x-request-id': 'req_cb48f97b95bd19e976715f2c87548064', 'cf-cache-status': 'DYNAMIC', 'server': 'cloudflare', 'cf-ray': '89fe55bebdda0594-IAD', 'content-encoding': 'br', 'alt-svc': 'h3=":443"; ma=86400'})
DEBUG:openai._base_client:request_id: req_cb48f97b95bd19e976715f2c87548064
DEBUG:openai._base_client:Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'json_data': {'messages': [{'content': "Summarize the following chapter in one sentence: I did one episode one episode turn into two turn into three and now champion is technology black that's you know that's how that started I'm not gonna lie man you guys have definitely turned the culture up with the champion you know I'm saying what you guys have done and everything like that I just want to say um when you guys look at what you've done with the champion of the night just you know your opinions on who wins who loses the power that you guys have to be able to you know have so many people whether they agree or disagree and things like that but they also able to change narratives you know I'm saying through your opinions um how do you feel about just what you guys are building and what do you guys are continuing to build you know I'm saying cuz I see a", 'role': 'user'}], 'model': 'gpt-3.5-turbo-16k', 'max_tokens': 1000, 'n': 1, 'stream': False, 'temperature': 0.7}}
DEBUG:openai._base_client:Sending HTTP Request: POST https://api.openai.com/v1/chat/completions
DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>
DEBUG:httpcore.http11:send_request_headers.complete
DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>
DEBUG:httpcore.http11:send_request_body.complete
DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>
DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Mon, 08 Jul 2024 07:24:32 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'openai-organization', b'machinekingsmedia'), (b'openai-processing-ms', b'1256'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains'), (b'x-ratelimit-limit-requests', b'10000'), (b'x-ratelimit-limit-tokens', b'1000000'), (b'x-ratelimit-remaining-requests', b'9999'), (b'x-ratelimit-remaining-tokens', b'998793'), (b'x-ratelimit-reset-requests', b'6ms'), (b'x-ratelimit-reset-tokens', b'72ms'), (b'x-request-id', b'req_26c5e098887ea956aabf37e4230ac8fb'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'89fe55c349010594-IAD'), (b'Content-Encoding', b'br'), (b'alt-svc', b'h3=":443"; ma=86400')])
INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>
DEBUG:httpcore.http11:receive_response_body.complete
DEBUG:httpcore.http11:response_closed.started
DEBUG:httpcore.http11:response_closed.complete
DEBUG:openai._base_client:HTTP Response: POST https://api.openai.com/v1/chat/completions "200 OK" Headers({'date': 'Mon, 08 Jul 2024 07:24:32 GMT', 'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'openai-organization': 'machinekingsmedia', 'openai-processing-ms': '1256', 'openai-version': '2020-10-01', 'strict-transport-security': 'max-age=31536000; includeSubDomains', 'x-ratelimit-limit-requests': '10000', 'x-ratelimit-limit-tokens': '1000000', 'x-ratelimit-remaining-requests': '9999', 'x-ratelimit-remaining-tokens': '998793', 'x-ratelimit-reset-requests': '6ms', 'x-ratelimit-reset-tokens': '72ms', 'x-request-id': 'req_26c5e098887ea956aabf37e4230ac8fb', 'cf-cache-status': 'DYNAMIC', 'server': 'cloudflare', 'cf-ray': '89fe55c349010594-IAD', 'content-encoding': 'br', 'alt-svc': 'h3=":443"; ma=86400'})
DEBUG:openai._base_client:request_id: req_26c5e098887ea956aabf37e4230ac8fb
DEBUG:openai._base_client:Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'json_data': {'messages': [{'content': 'Summarize the following chapter in one sentence: smack value to one of the biggest cars of the year already champion at night people are calling for that you know what you guys did in Virginia champion of the night what you guys did with old rat champion of the year giving out incentives money to actually help these guys be more motivated to kill and do all this man almost a collective thought process how hard it is', 'role': 'user'}], 'model': 'gpt-3.5-turbo-16k', 'max_tokens': 1000, 'n': 1, 'stream': False, 'temperature': 0.7}}
DEBUG:openai._base_client:Sending HTTP Request: POST https://api.openai.com/v1/chat/completions
DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>
DEBUG:httpcore.http11:send_request_headers.complete
DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>
DEBUG:httpcore.http11:send_request_body.complete
DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>
DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Mon, 08 Jul 2024 07:24:33 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'openai-organization', b'machinekingsmedia'), (b'openai-processing-ms', b'464'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains'), (b'x-ratelimit-limit-requests', b'10000'), (b'x-ratelimit-limit-tokens', b'1000000'), (b'x-ratelimit-remaining-requests', b'9999'), (b'x-ratelimit-remaining-tokens', b'998893'), (b'x-ratelimit-reset-requests', b'6ms'), (b'x-ratelimit-reset-tokens', b'66ms'), (b'x-request-id', b'req_f865769667f095397a1f9e7f7a5d9d95'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'89fe55cc9f220594-IAD'), (b'Content-Encoding', b'br'), (b'alt-svc', b'h3=":443"; ma=86400')])
INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>
DEBUG:httpcore.http11:receive_response_body.complete
DEBUG:httpcore.http11:response_closed.started
DEBUG:httpcore.http11:response_closed.complete
DEBUG:openai._base_client:HTTP Response: POST https://api.openai.com/v1/chat/completions "200 OK" Headers({'date': 'Mon, 08 Jul 2024 07:24:33 GMT', 'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'openai-organization': 'machinekingsmedia', 'openai-processing-ms': '464', 'openai-version': '2020-10-01', 'strict-transport-security': 'max-age=31536000; includeSubDomains', 'x-ratelimit-limit-requests': '10000', 'x-ratelimit-limit-tokens': '1000000', 'x-ratelimit-remaining-requests': '9999', 'x-ratelimit-remaining-tokens': '998893', 'x-ratelimit-reset-requests': '6ms', 'x-ratelimit-reset-tokens': '66ms', 'x-request-id': 'req_f865769667f095397a1f9e7f7a5d9d95', 'cf-cache-status': 'DYNAMIC', 'server': 'cloudflare', 'cf-ray': '89fe55cc9f220594-IAD', 'content-encoding': 'br', 'alt-svc': 'h3=":443"; ma=86400'})
DEBUG:openai._base_client:request_id: req_f865769667f095397a1f9e7f7a5d9d95
DEBUG:openai._base_client:Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'json_data': {'messages': [{'content': "Provide a brief summary of the following transcript: File: 240106_0102.mp3\nRecording Date: 2024-01-06\nDuration: 00:04:47.24\nTranscription Date: 2024-07-07 06:16:45\n\n1\n00:00:00,000 --> 00:00:16,600\nthis is how tech 9 joined champion this is him talking about it well I'm the\n\n2\n00:00:16,600 --> 00:00:22,000\nmajor Jay Black helped me with the trailer for what's that born legacy 3 I\n\n3\n00:00:22,000 --> 00:00:28,000\nbelieve and we had a good conversation we chopped it up yeah hinted towards he\n\n4\n00:00:28,000 --> 00:00:32,160\nwas doing something but at then I was so focused on a Jerry West battle that\n\n5\n00:00:32,160 --> 00:00:42,160\nkind of like stuff just like okay okay yeah yeah yeah okay so tech now was\n\n6\n00:00:42,160 --> 00:00:54,360\nabout to battle Jerry West and then he was shooting a trailer Jay Black helped\n\n7\n00:00:54,360 --> 00:01:00,040\nhim with the trailer that's what he's referring to\n\n8\n00:01:05,199 --> 00:01:12,720\njust being in that position you know I'm saying um what's that born legacy 3 I\n\n9\n00:01:12,720 --> 00:01:18,720\nbelieve and we had a good conversation we chopped it up yeah hinted towards he\n\n10\n00:01:18,720 --> 00:01:22,879\nwas doing something but at then I was so focused on a Jerry West battle that\n\n11\n00:01:22,879 --> 00:01:27,080\nkind of like stuff just like okay okay yeah yeah yeah okay you know and just\n\n12\n00:01:27,080 --> 00:01:32,360\nkeep the conversation going but um I want to say last year sometime early in\n\n13\n00:01:32,360 --> 00:01:37,599\nthe year he hit me up and was like yo I'm doing a show I'm going to show\n\n14\n00:01:37,599 --> 00:01:40,639\nchampion I want you to come up and I want I want you to come up and talk\n\n15\n00:01:40,639 --> 00:01:45,519\nabout the Philly battle rap scene so initially I was supposed to go up there\n\n16\n00:01:45,519 --> 00:01:49,680\ntalk about Philly battle rap me and big hand go up there talk about Philly\n\n17\n00:01:49,680 --> 00:01:54,400\nbattle rap and that was it he just wanted to get my tape on the Philly\n\n18\n00:01:54,400 --> 00:02:01,400\nbattle rap scene okay I got champion and I was like yo yo I'm like I like this\n\n19\n00:02:01,400 --> 00:02:07,360\nyou know I mean I did the Philly I did a Philly battle rap scene we talked about\n\n20\n00:02:07,360 --> 00:02:10,119\nthat the initial reaction from the fans was good\n\n21\n00:02:10,119 --> 00:02:14,919\nOh tech 9 is great you need to have him on the show blah blah blah blah blah so\n\n22\n00:02:14,919 --> 00:02:18,839\nwe counted like black was like already leaning towards oh I want you to come\n\n23\n00:02:18,839 --> 00:02:22,720\nback up and I was like yeah I want to come back up and we both kind of like\n\n24\n00:02:22,720 --> 00:02:29,399\nagreed that it'd be best if I come back up now it wasn't like permanent or\n\n25\n00:02:29,399 --> 00:02:32,800\nnothing it was just like yeah we just need you to come back up so I came back\n\n26\n00:02:32,800 --> 00:02:38,160\nup I started to do a couple more episodes of course the fans hated me you\n\n27\n00:02:38,160 --> 00:02:43,639\nknow but black seen something like now the fans even though they hate you now\n\n28\n00:02:43,639 --> 00:02:50,720\nlike new perfect for this like I know what I'm seeing I know I know what I'm\n\n29\n00:02:50,720 --> 00:02:54,800\nseeing right here so I'm not worried about what they're saying like this is\n\n30\n00:02:54,800 --> 00:02:58,639\nperfect so I started to you know read the comments and listen to what they\n\n31\n00:02:58,639 --> 00:03:02,240\nwere saying and then I started to actually come prepared that are going\n\n32\n00:03:02,240 --> 00:03:05,720\nthere because I can't freestyle like literally black Jay Black goes in there\n\n33\n00:03:05,720 --> 00:03:10,639\nhe freestyles most of the stuff when we're when we're doing champion I gotta\n\n34\n00:03:10,639 --> 00:03:15,199\nhave my notes ready like when he when he sends me the topics I need my notes\n\n35\n00:03:15,199 --> 00:03:20,520\nready for each topic so I can go in there and make some sense so I started\n\n36\n00:03:20,520 --> 00:03:24,199\nto come prepared and slowly but surely the fans st", 'role': 'user'}], 'model': 'gpt-3.5-turbo-16k', 'max_tokens': 1000, 'n': 1, 'stream': False, 'temperature': 0.7}}
DEBUG:openai._base_client:Sending HTTP Request: POST https://api.openai.com/v1/chat/completions
DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>
DEBUG:httpcore.http11:send_request_headers.complete
DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>
DEBUG:httpcore.http11:send_request_body.complete
DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>
DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Mon, 08 Jul 2024 07:24:36 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'openai-organization', b'machinekingsmedia'), (b'openai-processing-ms', b'1825'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains'), (b'x-ratelimit-limit-requests', b'10000'), (b'x-ratelimit-limit-tokens', b'1000000'), (b'x-ratelimit-remaining-requests', b'9999'), (b'x-ratelimit-remaining-tokens', b'997984'), (b'x-ratelimit-reset-requests', b'6ms'), (b'x-ratelimit-reset-tokens', b'120ms'), (b'x-request-id', b'req_e6175edf4a0b17d32eb84d9d858abd7f'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'89fe55d4bc490594-IAD'), (b'Content-Encoding', b'br'), (b'alt-svc', b'h3=":443"; ma=86400')])
INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>
DEBUG:httpcore.http11:receive_response_body.complete
DEBUG:httpcore.http11:response_closed.started
DEBUG:httpcore.http11:response_closed.complete
DEBUG:openai._base_client:HTTP Response: POST https://api.openai.com/v1/chat/completions "200 OK" Headers({'date': 'Mon, 08 Jul 2024 07:24:36 GMT', 'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'openai-organization': 'machinekingsmedia', 'openai-processing-ms': '1825', 'openai-version': '2020-10-01', 'strict-transport-security': 'max-age=31536000; includeSubDomains', 'x-ratelimit-limit-requests': '10000', 'x-ratelimit-limit-tokens': '1000000', 'x-ratelimit-remaining-requests': '9999', 'x-ratelimit-remaining-tokens': '997984', 'x-ratelimit-reset-requests': '6ms', 'x-ratelimit-reset-tokens': '120ms', 'x-request-id': 'req_e6175edf4a0b17d32eb84d9d858abd7f', 'cf-cache-status': 'DYNAMIC', 'server': 'cloudflare', 'cf-ray': '89fe55d4bc490594-IAD', 'content-encoding': 'br', 'alt-svc': 'h3=":443"; ma=86400'})
DEBUG:openai._base_client:request_id: req_e6175edf4a0b17d32eb84d9d858abd7f
DEBUG:openai._base_client:Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'json_data': {'messages': [{'content': "Generate a short, descriptive title for the following chapter content: this is how tech 9 joined champion this is him talking about it well I'm the major Jay Black helped me with the trailer for what's that born legacy 3 I believe and we had a good conversation we chopped it up yeah hinted towards he was doing something but at then I was so focused on a Jerry West battle that kind of like stuff just like okay okay yeah yeah yeah okay so tech now was about to battle Jerry West and then he was shooting a trailer Jay Black helped him with the trailer that's what he's referring to just being in that position you know I'm saying um what's that born legacy 3 I believe and we had a good conversation we chopped it up yeah hinted towards he was doing something but at then I was so focused on a Jerry West battle that", 'role': 'user'}], 'model': 'gpt-3.5-turbo-16k', 'max_tokens': 1000, 'n': 1, 'stream': False, 'temperature': 0.7}}
DEBUG:openai._base_client:Sending HTTP Request: POST https://api.openai.com/v1/chat/completions
DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>
DEBUG:httpcore.http11:send_request_headers.complete
DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>
DEBUG:httpcore.http11:send_request_body.complete
DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>
DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Mon, 08 Jul 2024 07:24:36 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'openai-organization', b'machinekingsmedia'), (b'openai-processing-ms', b'258'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains'), (b'x-ratelimit-limit-requests', b'10000'), (b'x-ratelimit-limit-tokens', b'1000000'), (b'x-ratelimit-remaining-requests', b'9999'), (b'x-ratelimit-remaining-tokens', b'998793'), (b'x-ratelimit-reset-requests', b'6ms'), (b'x-ratelimit-reset-tokens', b'72ms'), (b'x-request-id', b'req_8af0f7a5a3f86ec7488a10a3b5b9cf15'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'89fe55e6ffa10594-IAD'), (b'Content-Encoding', b'br'), (b'alt-svc', b'h3=":443"; ma=86400')])
INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>
DEBUG:httpcore.http11:receive_response_body.complete
DEBUG:httpcore.http11:response_closed.started
DEBUG:httpcore.http11:response_closed.complete
DEBUG:openai._base_client:HTTP Response: POST https://api.openai.com/v1/chat/completions "200 OK" Headers({'date': 'Mon, 08 Jul 2024 07:24:36 GMT', 'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'openai-organization': 'machinekingsmedia', 'openai-processing-ms': '258', 'openai-version': '2020-10-01', 'strict-transport-security': 'max-age=31536000; includeSubDomains', 'x-ratelimit-limit-requests': '10000', 'x-ratelimit-limit-tokens': '1000000', 'x-ratelimit-remaining-requests': '9999', 'x-ratelimit-remaining-tokens': '998793', 'x-ratelimit-reset-requests': '6ms', 'x-ratelimit-reset-tokens': '72ms', 'x-request-id': 'req_8af0f7a5a3f86ec7488a10a3b5b9cf15', 'cf-cache-status': 'DYNAMIC', 'server': 'cloudflare', 'cf-ray': '89fe55e6ffa10594-IAD', 'content-encoding': 'br', 'alt-svc': 'h3=":443"; ma=86400'})
DEBUG:openai._base_client:request_id: req_8af0f7a5a3f86ec7488a10a3b5b9cf15
DEBUG:openai._base_client:Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'json_data': {'messages': [{'content': "Generate a short, descriptive title for the following chapter content: kind of like stuff just like okay okay yeah yeah yeah okay you know and just keep the conversation going but um I want to say last year sometime early in the year he hit me up and was like yo I'm doing a show I'm going to show champion I want you to come up and I want I want you to come up and talk about the Philly battle rap scene so initially I was supposed to go up there talk about Philly battle rap me and big hand go up there talk about Philly battle rap and that was it he just wanted to get my tape on the Philly battle rap scene okay I got champion and I was like yo yo I'm like I like this you know I mean I did the Philly I did a Philly battle rap scene we talked about that the initial reaction from the fans was good", 'role': 'user'}], 'model': 'gpt-3.5-turbo-16k', 'max_tokens': 1000, 'n': 1, 'stream': False, 'temperature': 0.7}}
DEBUG:openai._base_client:Sending HTTP Request: POST https://api.openai.com/v1/chat/completions
DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>
DEBUG:httpcore.http11:send_request_headers.complete
DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>
DEBUG:httpcore.http11:send_request_body.complete
DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>
DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Mon, 08 Jul 2024 07:24:37 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'openai-organization', b'machinekingsmedia'), (b'openai-processing-ms', b'293'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains'), (b'x-ratelimit-limit-requests', b'10000'), (b'x-ratelimit-limit-tokens', b'1000000'), (b'x-ratelimit-remaining-requests', b'9999'), (b'x-ratelimit-remaining-tokens', b'998798'), (b'x-ratelimit-reset-requests', b'6ms'), (b'x-ratelimit-reset-tokens', b'72ms'), (b'x-request-id', b'req_8e58ad5e4f64afda6b0280324adc1dda'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'89fe55ea19860594-IAD'), (b'Content-Encoding', b'br'), (b'alt-svc', b'h3=":443"; ma=86400')])
INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>
DEBUG:httpcore.http11:receive_response_body.complete
DEBUG:httpcore.http11:response_closed.started
DEBUG:httpcore.http11:response_closed.complete
DEBUG:openai._base_client:HTTP Response: POST https://api.openai.com/v1/chat/completions "200 OK" Headers({'date': 'Mon, 08 Jul 2024 07:24:37 GMT', 'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'openai-organization': 'machinekingsmedia', 'openai-processing-ms': '293', 'openai-version': '2020-10-01', 'strict-transport-security': 'max-age=31536000; includeSubDomains', 'x-ratelimit-limit-requests': '10000', 'x-ratelimit-limit-tokens': '1000000', 'x-ratelimit-remaining-requests': '9999', 'x-ratelimit-remaining-tokens': '998798', 'x-ratelimit-reset-requests': '6ms', 'x-ratelimit-reset-tokens': '72ms', 'x-request-id': 'req_8e58ad5e4f64afda6b0280324adc1dda', 'cf-cache-status': 'DYNAMIC', 'server': 'cloudflare', 'cf-ray': '89fe55ea19860594-IAD', 'content-encoding': 'br', 'alt-svc': 'h3=":443"; ma=86400'})
DEBUG:openai._base_client:request_id: req_8e58ad5e4f64afda6b0280324adc1dda
DEBUG:openai._base_client:Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'json_data': {'messages': [{'content': "Generate a short, descriptive title for the following chapter content: Oh tech 9 is great you need to have him on the show blah blah blah blah blah so we counted like black was like already leaning towards oh I want you to come back up and I was like yeah I want to come back up and we both kind of like agreed that it'd be best if I come back up now it wasn't like permanent or nothing it was just like yeah we just need you to come back up so I came back up I started to do a couple more episodes of course the fans hated me you know but black seen something like now the fans even though they hate you now like new perfect for this like I know what I'm seeing I know I know what I'm seeing right here so I'm not worried about what they're saying like this is perfect so I started to you know read the comments and listen to what they", 'role': 'user'}], 'model': 'gpt-3.5-turbo-16k', 'max_tokens': 1000, 'n': 1, 'stream': False, 'temperature': 0.7}}
DEBUG:openai._base_client:Sending HTTP Request: POST https://api.openai.com/v1/chat/completions
DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>
DEBUG:httpcore.http11:send_request_headers.complete
DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>
DEBUG:httpcore.http11:send_request_body.complete
DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>
DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Mon, 08 Jul 2024 07:24:38 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'openai-organization', b'machinekingsmedia'), (b'openai-processing-ms', b'350'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains'), (b'x-ratelimit-limit-requests', b'10000'), (b'x-ratelimit-limit-tokens', b'1000000'), (b'x-ratelimit-remaining-requests', b'9999'), (b'x-ratelimit-remaining-tokens', b'998789'), (b'x-ratelimit-reset-requests', b'6ms'), (b'x-ratelimit-reset-tokens', b'72ms'), (b'x-request-id', b'req_f7a6a8c9f83753d17774f122ed1fca65'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'89fe55edebed0594-IAD'), (b'Content-Encoding', b'br'), (b'alt-svc', b'h3=":443"; ma=86400')])
INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>
DEBUG:httpcore.http11:receive_response_body.complete
DEBUG:httpcore.http11:response_closed.started
DEBUG:httpcore.http11:response_closed.complete
DEBUG:openai._base_client:HTTP Response: POST https://api.openai.com/v1/chat/completions "200 OK" Headers({'date': 'Mon, 08 Jul 2024 07:24:38 GMT', 'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'openai-organization': 'machinekingsmedia', 'openai-processing-ms': '350', 'openai-version': '2020-10-01', 'strict-transport-security': 'max-age=31536000; includeSubDomains', 'x-ratelimit-limit-requests': '10000', 'x-ratelimit-limit-tokens': '1000000', 'x-ratelimit-remaining-requests': '9999', 'x-ratelimit-remaining-tokens': '998789', 'x-ratelimit-reset-requests': '6ms', 'x-ratelimit-reset-tokens': '72ms', 'x-request-id': 'req_f7a6a8c9f83753d17774f122ed1fca65', 'cf-cache-status': 'DYNAMIC', 'server': 'cloudflare', 'cf-ray': '89fe55edebed0594-IAD', 'content-encoding': 'br', 'alt-svc': 'h3=":443"; ma=86400'})
DEBUG:openai._base_client:request_id: req_f7a6a8c9f83753d17774f122ed1fca65
DEBUG:openai._base_client:Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'json_data': {'messages': [{'content': "Generate a short, descriptive title for the following chapter content: were saying and then I started to actually come prepared that are going there because I can't freestyle like literally black Jay Black goes in there he freestyles most of the stuff when we're when we're doing champion I gotta have my notes ready like when he when he sends me the topics I need my notes ready for each topic so I can go in there and make some sense so I started to come prepared and slowly but surely the fans started to come around and started to respect my opinion because I started to back up what I was talking about so if I felt like someone lost the battle I had key points of why I felt as though they lost the battle and that's pretty much how that's pretty much how it happened this she started with the Philly battle rap scene I went", 'role': 'user'}], 'model': 'gpt-3.5-turbo-16k', 'max_tokens': 1000, 'n': 1, 'stream': False, 'temperature': 0.7}}
DEBUG:openai._base_client:Sending HTTP Request: POST https://api.openai.com/v1/chat/completions
DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>
DEBUG:httpcore.http11:send_request_headers.complete
DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>
DEBUG:httpcore.http11:send_request_body.complete
DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>
DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Mon, 08 Jul 2024 07:24:39 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'openai-organization', b'machinekingsmedia'), (b'openai-processing-ms', b'317'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains'), (b'x-ratelimit-limit-requests', b'10000'), (b'x-ratelimit-limit-tokens', b'1000000'), (b'x-ratelimit-remaining-requests', b'9999'), (b'x-ratelimit-remaining-tokens', b'998790'), (b'x-ratelimit-reset-requests', b'6ms'), (b'x-ratelimit-reset-tokens', b'72ms'), (b'x-request-id', b'req_90d5e19cc901d48090c7e11d7aee1612'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'89fe55f578aa0594-IAD'), (b'Content-Encoding', b'br'), (b'alt-svc', b'h3=":443"; ma=86400')])
INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>
DEBUG:httpcore.http11:receive_response_body.complete
DEBUG:httpcore.http11:response_closed.started
DEBUG:httpcore.http11:response_closed.complete
DEBUG:openai._base_client:HTTP Response: POST https://api.openai.com/v1/chat/completions "200 OK" Headers({'date': 'Mon, 08 Jul 2024 07:24:39 GMT', 'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'openai-organization': 'machinekingsmedia', 'openai-processing-ms': '317', 'openai-version': '2020-10-01', 'strict-transport-security': 'max-age=31536000; includeSubDomains', 'x-ratelimit-limit-requests': '10000', 'x-ratelimit-limit-tokens': '1000000', 'x-ratelimit-remaining-requests': '9999', 'x-ratelimit-remaining-tokens': '998790', 'x-ratelimit-reset-requests': '6ms', 'x-ratelimit-reset-tokens': '72ms', 'x-request-id': 'req_90d5e19cc901d48090c7e11d7aee1612', 'cf-cache-status': 'DYNAMIC', 'server': 'cloudflare', 'cf-ray': '89fe55f578aa0594-IAD', 'content-encoding': 'br', 'alt-svc': 'h3=":443"; ma=86400'})
DEBUG:openai._base_client:request_id: req_90d5e19cc901d48090c7e11d7aee1612
DEBUG:openai._base_client:Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'json_data': {'messages': [{'content': "Generate a short, descriptive title for the following chapter content: I did one episode one episode turn into two turn into three and now champion is technology black that's you know that's how that started I'm not gonna lie man you guys have definitely turned the culture up with the champion you know I'm saying what you guys have done and everything like that I just want to say um when you guys look at what you've done with the champion of the night just you know your opinions on who wins who loses the power that you guys have to be able to you know have so many people whether they agree or disagree and things like that but they also able to change narratives you know I'm saying through your opinions um how do you feel about just what you guys are building and what do you guys are continuing to build you know I'm saying cuz I see a", 'role': 'user'}], 'model': 'gpt-3.5-turbo-16k', 'max_tokens': 1000, 'n': 1, 'stream': False, 'temperature': 0.7}}
DEBUG:openai._base_client:Sending HTTP Request: POST https://api.openai.com/v1/chat/completions
DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>
DEBUG:httpcore.http11:send_request_headers.complete
DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>
DEBUG:httpcore.http11:send_request_body.complete
DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>
DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Mon, 08 Jul 2024 07:24:39 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'openai-organization', b'machinekingsmedia'), (b'openai-processing-ms', b'276'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains'), (b'x-ratelimit-limit-requests', b'10000'), (b'x-ratelimit-limit-tokens', b'1000000'), (b'x-ratelimit-remaining-requests', b'9999'), (b'x-ratelimit-remaining-tokens', b'998786'), (b'x-ratelimit-reset-requests', b'6ms'), (b'x-ratelimit-reset-tokens', b'72ms'), (b'x-request-id', b'req_6c789eaae8e8473b8d4bcd054de5f25b'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'89fe55f99b080594-IAD'), (b'Content-Encoding', b'br'), (b'alt-svc', b'h3=":443"; ma=86400')])
INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>
DEBUG:httpcore.http11:receive_response_body.complete
DEBUG:httpcore.http11:response_closed.started
DEBUG:httpcore.http11:response_closed.complete
DEBUG:openai._base_client:HTTP Response: POST https://api.openai.com/v1/chat/completions "200 OK" Headers({'date': 'Mon, 08 Jul 2024 07:24:39 GMT', 'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'openai-organization': 'machinekingsmedia', 'openai-processing-ms': '276', 'openai-version': '2020-10-01', 'strict-transport-security': 'max-age=31536000; includeSubDomains', 'x-ratelimit-limit-requests': '10000', 'x-ratelimit-limit-tokens': '1000000', 'x-ratelimit-remaining-requests': '9999', 'x-ratelimit-remaining-tokens': '998786', 'x-ratelimit-reset-requests': '6ms', 'x-ratelimit-reset-tokens': '72ms', 'x-request-id': 'req_6c789eaae8e8473b8d4bcd054de5f25b', 'cf-cache-status': 'DYNAMIC', 'server': 'cloudflare', 'cf-ray': '89fe55f99b080594-IAD', 'content-encoding': 'br', 'alt-svc': 'h3=":443"; ma=86400'})
DEBUG:openai._base_client:request_id: req_6c789eaae8e8473b8d4bcd054de5f25b
DEBUG:openai._base_client:Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'json_data': {'messages': [{'content': 'Generate a short, descriptive title for the following chapter content: smack value to one of the biggest cars of the year already champion at night people are calling for that you know what you guys did in Virginia champion of the night what you guys did with old rat champion of the year giving out incentives money to actually help these guys be more motivated to kill and do all this man almost a collective thought process how hard it is', 'role': 'user'}], 'model': 'gpt-3.5-turbo-16k', 'max_tokens': 1000, 'n': 1, 'stream': False, 'temperature': 0.7}}
DEBUG:openai._base_client:Sending HTTP Request: POST https://api.openai.com/v1/chat/completions
DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>
DEBUG:httpcore.http11:send_request_headers.complete
DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>
DEBUG:httpcore.http11:send_request_body.complete
DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>
DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Mon, 08 Jul 2024 07:24:41 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'openai-organization', b'machinekingsmedia'), (b'openai-processing-ms', b'369'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains'), (b'x-ratelimit-limit-requests', b'10000'), (b'x-ratelimit-limit-tokens', b'1000000'), (b'x-ratelimit-remaining-requests', b'9999'), (b'x-ratelimit-remaining-tokens', b'998888'), (b'x-ratelimit-reset-requests', b'6ms'), (b'x-ratelimit-reset-tokens', b'66ms'), (b'x-request-id', b'req_8d4cf3a8b2e3afa7e6631b447b06ec5f'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'89fe55fcdcde0594-IAD'), (b'Content-Encoding', b'br'), (b'alt-svc', b'h3=":443"; ma=86400')])
INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>
DEBUG:httpcore.http11:receive_response_body.complete
DEBUG:httpcore.http11:response_closed.started
DEBUG:httpcore.http11:response_closed.complete
DEBUG:openai._base_client:HTTP Response: POST https://api.openai.com/v1/chat/completions "200 OK" Headers({'date': 'Mon, 08 Jul 2024 07:24:41 GMT', 'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'openai-organization': 'machinekingsmedia', 'openai-processing-ms': '369', 'openai-version': '2020-10-01', 'strict-transport-security': 'max-age=31536000; includeSubDomains', 'x-ratelimit-limit-requests': '10000', 'x-ratelimit-limit-tokens': '1000000', 'x-ratelimit-remaining-requests': '9999', 'x-ratelimit-remaining-tokens': '998888', 'x-ratelimit-reset-requests': '6ms', 'x-ratelimit-reset-tokens': '66ms', 'x-request-id': 'req_8d4cf3a8b2e3afa7e6631b447b06ec5f', 'cf-cache-status': 'DYNAMIC', 'server': 'cloudflare', 'cf-ray': '89fe55fcdcde0594-IAD', 'content-encoding': 'br', 'alt-svc': 'h3=":443"; ma=86400'})
DEBUG:openai._base_client:request_id: req_8d4cf3a8b2e3afa7e6631b447b06ec5f
DEBUG:openai._base_client:Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'json_data': {'messages': [{'content': "Summarize the following chapter in one sentence: put it out but so you said you the way you articulated though i really like that you said a cog in the machine they lost a cog in the machine yo did you see my game of thrones breakdown of the of the um the the the leagues in battle right right now how i framed it with you no i gotta seriously because i don't i try not to watch none of this yeah yeah i know you said you was gonna do that too no i was i was that was a little test along with a question because you say you were stepping away from the whole negativity and all of that and yo the fact that is that everybody's saying that is dying right now i think about you so much but anyway uh but what i pointed out was how like it seems like we're starting to feel the after effects of the lack of innovation from that side so you saying that the deal they made with caffeine was like them selling them a machine but a cog was missing in the machine it looked the same from the outside", 'role': 'user'}], 'model': 'gpt-3.5-turbo-16k', 'max_tokens': 1000, 'n': 1, 'stream': False, 'temperature': 0.7}}
DEBUG:openai._base_client:Sending HTTP Request: POST https://api.openai.com/v1/chat/completions
DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>
DEBUG:httpcore.http11:send_request_headers.complete
DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>
DEBUG:httpcore.http11:send_request_body.complete
DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>
DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Mon, 08 Jul 2024 07:24:41 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'openai-organization', b'machinekingsmedia'), (b'openai-processing-ms', b'382'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains'), (b'x-ratelimit-limit-requests', b'10000'), (b'x-ratelimit-limit-tokens', b'1000000'), (b'x-ratelimit-remaining-requests', b'9999'), (b'x-ratelimit-remaining-tokens', b'998751'), (b'x-ratelimit-reset-requests', b'6ms'), (b'x-ratelimit-reset-tokens', b'74ms'), (b'x-request-id', b'req_6fcc995446ae12b2c7baf9f974192c7e'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'89fe560479b30594-IAD'), (b'Content-Encoding', b'br'), (b'alt-svc', b'h3=":443"; ma=86400')])
INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>
DEBUG:httpcore.http11:receive_response_body.complete
DEBUG:httpcore.http11:response_closed.started
DEBUG:httpcore.http11:response_closed.complete
DEBUG:openai._base_client:HTTP Response: POST https://api.openai.com/v1/chat/completions "200 OK" Headers({'date': 'Mon, 08 Jul 2024 07:24:41 GMT', 'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'openai-organization': 'machinekingsmedia', 'openai-processing-ms': '382', 'openai-version': '2020-10-01', 'strict-transport-security': 'max-age=31536000; includeSubDomains', 'x-ratelimit-limit-requests': '10000', 'x-ratelimit-limit-tokens': '1000000', 'x-ratelimit-remaining-requests': '9999', 'x-ratelimit-remaining-tokens': '998751', 'x-ratelimit-reset-requests': '6ms', 'x-ratelimit-reset-tokens': '74ms', 'x-request-id': 'req_6fcc995446ae12b2c7baf9f974192c7e', 'cf-cache-status': 'DYNAMIC', 'server': 'cloudflare', 'cf-ray': '89fe560479b30594-IAD', 'content-encoding': 'br', 'alt-svc': 'h3=":443"; ma=86400'})
DEBUG:openai._base_client:request_id: req_6fcc995446ae12b2c7baf9f974192c7e
DEBUG:openai._base_client:Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'json_data': {'messages': [{'content': "Summarize the following chapter in one sentence: but on the inside an important piece was missing they made it basically look like that i had nothing to do with what went down yeah they framed it like that yeah because you were a talent scout yeah apparently yeah they knew they knew that that's not the case and caffeine knows it's not the case you understand and it's between me and you of course as i was going through this for three years they was sitting in on the meetings with me and my lawyers in this so why would they sit if they think it was important if they they're gonna stand behind these niggas and make it look like a notice just trying to scam these niggas yeah when they realize like hold on my nigga this nigga really did all of this shit and y'all think about it if you could if you acquiring a company and its routes and how it became successful why wouldn't you keep the same team wow yeah yeah i see what you're saying i see what you're saying and i can see that fueling", 'role': 'user'}], 'model': 'gpt-3.5-turbo-16k', 'max_tokens': 1000, 'n': 1, 'stream': False, 'temperature': 0.7}}
DEBUG:openai._base_client:Sending HTTP Request: POST https://api.openai.com/v1/chat/completions
DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>
DEBUG:httpcore.http11:send_request_headers.complete
DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>
DEBUG:httpcore.http11:send_request_body.complete
DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>
DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Mon, 08 Jul 2024 07:24:43 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'openai-organization', b'machinekingsmedia'), (b'openai-processing-ms', b'648'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains'), (b'x-ratelimit-limit-requests', b'10000'), (b'x-ratelimit-limit-tokens', b'1000000'), (b'x-ratelimit-remaining-requests', b'9999'), (b'x-ratelimit-remaining-tokens', b'998750'), (b'x-ratelimit-reset-requests', b'6ms'), (b'x-ratelimit-reset-tokens', b'75ms'), (b'x-request-id', b'req_85d63968f6e438a2af82341d6793b783'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'89fe56085c530594-IAD'), (b'Content-Encoding', b'br'), (b'alt-svc', b'h3=":443"; ma=86400')])
INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>
DEBUG:httpcore.http11:receive_response_body.complete
DEBUG:httpcore.http11:response_closed.started
DEBUG:httpcore.http11:response_closed.complete
DEBUG:openai._base_client:HTTP Response: POST https://api.openai.com/v1/chat/completions "200 OK" Headers({'date': 'Mon, 08 Jul 2024 07:24:43 GMT', 'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'openai-organization': 'machinekingsmedia', 'openai-processing-ms': '648', 'openai-version': '2020-10-01', 'strict-transport-security': 'max-age=31536000; includeSubDomains', 'x-ratelimit-limit-requests': '10000', 'x-ratelimit-limit-tokens': '1000000', 'x-ratelimit-remaining-requests': '9999', 'x-ratelimit-remaining-tokens': '998750', 'x-ratelimit-reset-requests': '6ms', 'x-ratelimit-reset-tokens': '75ms', 'x-request-id': 'req_85d63968f6e438a2af82341d6793b783', 'cf-cache-status': 'DYNAMIC', 'server': 'cloudflare', 'cf-ray': '89fe56085c530594-IAD', 'content-encoding': 'br', 'alt-svc': 'h3=":443"; ma=86400'})
DEBUG:openai._base_client:request_id: req_85d63968f6e438a2af82341d6793b783
DEBUG:openai._base_client:Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'json_data': {'messages': [{'content': "Summarize the following chapter in one sentence: anger amongst their ranks yeah and you gotta think about it sound like it was all a money move with them bro they tried to cut me out they owe me money you know what i'm saying yeah they were wrong and bro when they went to that situation they were like yo you know three ways is better than four yeah yeah the way they looked at it i wanted to give me the credit that i deserved you know i'm saying period yeah yeah my mentality was i'm gonna make this stupid motherfucker look like he's the man sacrifice my own shit you feel me yeah and their mentality was like nah we caught this nigga out we didn't pay and like no like it was just it was just stupid on smack's part bro like you know you know what business you go before i got there and you was falling off the cliff why would you revert to what you was because when i got there this is the highest you've ever been yeah they was with me not with them niggas yeah but but", 'role': 'user'}], 'model': 'gpt-3.5-turbo-16k', 'max_tokens': 1000, 'n': 1, 'stream': False, 'temperature': 0.7}}
DEBUG:openai._base_client:Sending HTTP Request: POST https://api.openai.com/v1/chat/completions
DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>
DEBUG:httpcore.http11:send_request_headers.complete
DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>
DEBUG:httpcore.http11:send_request_body.complete
DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>
DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Mon, 08 Jul 2024 07:24:44 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'openai-organization', b'machinekingsmedia'), (b'openai-processing-ms', b'516'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains'), (b'x-ratelimit-limit-requests', b'10000'), (b'x-ratelimit-limit-tokens', b'1000000'), (b'x-ratelimit-remaining-requests', b'9999'), (b'x-ratelimit-remaining-tokens', b'998755'), (b'x-ratelimit-reset-requests', b'6ms'), (b'x-ratelimit-reset-tokens', b'74ms'), (b'x-request-id', b'req_507a7e74855278888629dc05f1d96191'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'89fe56145c450594-IAD'), (b'Content-Encoding', b'br'), (b'alt-svc', b'h3=":443"; ma=86400')])
INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>
DEBUG:httpcore.http11:receive_response_body.complete
DEBUG:httpcore.http11:response_closed.started
DEBUG:httpcore.http11:response_closed.complete
DEBUG:openai._base_client:HTTP Response: POST https://api.openai.com/v1/chat/completions "200 OK" Headers({'date': 'Mon, 08 Jul 2024 07:24:44 GMT', 'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'openai-organization': 'machinekingsmedia', 'openai-processing-ms': '516', 'openai-version': '2020-10-01', 'strict-transport-security': 'max-age=31536000; includeSubDomains', 'x-ratelimit-limit-requests': '10000', 'x-ratelimit-limit-tokens': '1000000', 'x-ratelimit-remaining-requests': '9999', 'x-ratelimit-remaining-tokens': '998755', 'x-ratelimit-reset-requests': '6ms', 'x-ratelimit-reset-tokens': '74ms', 'x-request-id': 'req_507a7e74855278888629dc05f1d96191', 'cf-cache-status': 'DYNAMIC', 'server': 'cloudflare', 'cf-ray': '89fe56145c450594-IAD', 'content-encoding': 'br', 'alt-svc': 'h3=":443"; ma=86400'})
DEBUG:openai._base_client:request_id: req_507a7e74855278888629dc05f1d96191
DEBUG:openai._base_client:Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'json_data': {'messages': [{'content': "Summarize the following chapter in one sentence: just like just like with uh just like with the dallas cowboys um yeah jerry jerry jerry jones thought that you know he could he could run it was the it was the logo the star whatever he could do it without jimmy johnson and um and they haven't been there since i was so happy they ain't advancing the playoff this year too so we can still run with that it is bro it's like when ufc got bored they didn't give it a thing of white in a way it was the cog he was the yeah but he was the face though he was he was the face i remember too when you were wrong i was one of the faces it was me and smack they asked me to fall back because i was out shining him yeah i remember that but smack wasn't really out there talking to the bloggers you used to go on angry fan and big up smack used to be in here i did so much shit that people don't even know that's what i'm saying like it's just it's just if you see them they don't even know what to do", 'role': 'user'}], 'model': 'gpt-3.5-turbo-16k', 'max_tokens': 1000, 'n': 1, 'stream': False, 'temperature': 0.7}}
DEBUG:openai._base_client:Sending HTTP Request: POST https://api.openai.com/v1/chat/completions
DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>
DEBUG:httpcore.http11:send_request_headers.complete
DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>
DEBUG:httpcore.http11:send_request_body.complete
DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>
DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Mon, 08 Jul 2024 07:24:45 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'openai-organization', b'machinekingsmedia'), (b'openai-processing-ms', b'756'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains'), (b'x-ratelimit-limit-requests', b'10000'), (b'x-ratelimit-limit-tokens', b'1000000'), (b'x-ratelimit-remaining-requests', b'9999'), (b'x-ratelimit-remaining-tokens', b'998752'), (b'x-ratelimit-reset-requests', b'6ms'), (b'x-ratelimit-reset-tokens', b'74ms'), (b'x-request-id', b'req_4933ae2357caf554ecae123e7e6453d1'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'89fe56190fee0594-IAD'), (b'Content-Encoding', b'br'), (b'alt-svc', b'h3=":443"; ma=86400')])
INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>
DEBUG:httpcore.http11:receive_response_body.complete
DEBUG:httpcore.http11:response_closed.started
DEBUG:httpcore.http11:response_closed.complete
DEBUG:openai._base_client:HTTP Response: POST https://api.openai.com/v1/chat/completions "200 OK" Headers({'date': 'Mon, 08 Jul 2024 07:24:45 GMT', 'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'openai-organization': 'machinekingsmedia', 'openai-processing-ms': '756', 'openai-version': '2020-10-01', 'strict-transport-security': 'max-age=31536000; includeSubDomains', 'x-ratelimit-limit-requests': '10000', 'x-ratelimit-limit-tokens': '1000000', 'x-ratelimit-remaining-requests': '9999', 'x-ratelimit-remaining-tokens': '998752', 'x-ratelimit-reset-requests': '6ms', 'x-ratelimit-reset-tokens': '74ms', 'x-request-id': 'req_4933ae2357caf554ecae123e7e6453d1', 'cf-cache-status': 'DYNAMIC', 'server': 'cloudflare', 'cf-ray': '89fe56190fee0594-IAD', 'content-encoding': 'br', 'alt-svc': 'h3=":443"; ma=86400'})
DEBUG:openai._base_client:request_id: req_4933ae2357caf554ecae123e7e6453d1
DEBUG:openai._base_client:Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'json_data': {'messages': [{'content': "Summarize the following chapter in one sentence: they don't know what to say they don't even like yo bro like even in business meetings like bro like i have ideas right now that'll that'll make the shit go crazy but i'd never fuck with you know the amount of money you would have to pay me just to resurrect your company yeah but and i've been hearing the adam rumors getting louder this week too about him retiring like as if he made he's done i told you that shit yeah you did tell me that but i didn't see an official announcement but everybody's talking about it no he was never gonna make an announcement he was just like he's gonna do the last event which was uh that's the full circle oh max out back on his last event okay he's not even there oh and he's just gonna do it he was like he's just tired the same shit i said tired of the culture you're putting up money he said he made enough money in the last event where he's just like yo nigga it's not worth the risk why would i put it", 'role': 'user'}], 'model': 'gpt-3.5-turbo-16k', 'max_tokens': 1000, 'n': 1, 'stream': False, 'temperature': 0.7}}
DEBUG:openai._base_client:Sending HTTP Request: POST https://api.openai.com/v1/chat/completions
DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>
DEBUG:httpcore.http11:send_request_headers.complete
DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>
DEBUG:httpcore.http11:send_request_body.complete
DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>
DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Mon, 08 Jul 2024 07:24:46 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'openai-organization', b'machinekingsmedia'), (b'openai-processing-ms', b'728'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains'), (b'x-ratelimit-limit-requests', b'10000'), (b'x-ratelimit-limit-tokens', b'1000000'), (b'x-ratelimit-remaining-requests', b'9999'), (b'x-ratelimit-remaining-tokens', b'998750'), (b'x-ratelimit-reset-requests', b'6ms'), (b'x-ratelimit-reset-tokens', b'75ms'), (b'x-request-id', b'req_6f70afaccf709113e0380d04d3e6e9ed'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'89fe561f3c360594-IAD'), (b'Content-Encoding', b'br'), (b'alt-svc', b'h3=":443"; ma=86400')])
INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>
DEBUG:httpcore.http11:receive_response_body.complete
DEBUG:httpcore.http11:response_closed.started
DEBUG:httpcore.http11:response_closed.complete
DEBUG:openai._base_client:HTTP Response: POST https://api.openai.com/v1/chat/completions "200 OK" Headers({'date': 'Mon, 08 Jul 2024 07:24:46 GMT', 'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'openai-organization': 'machinekingsmedia', 'openai-processing-ms': '728', 'openai-version': '2020-10-01', 'strict-transport-security': 'max-age=31536000; includeSubDomains', 'x-ratelimit-limit-requests': '10000', 'x-ratelimit-limit-tokens': '1000000', 'x-ratelimit-remaining-requests': '9999', 'x-ratelimit-remaining-tokens': '998750', 'x-ratelimit-reset-requests': '6ms', 'x-ratelimit-reset-tokens': '75ms', 'x-request-id': 'req_6f70afaccf709113e0380d04d3e6e9ed', 'cf-cache-status': 'DYNAMIC', 'server': 'cloudflare', 'cf-ray': '89fe561f3c360594-IAD', 'content-encoding': 'br', 'alt-svc': 'h3=":443"; ma=86400'})
DEBUG:openai._base_client:request_id: req_6f70afaccf709113e0380d04d3e6e9ed
DEBUG:openai._base_client:Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'json_data': {'messages': [{'content': "Summarize the following chapter in one sentence: back in bro these i think the usd shop in sugar selling chicken out of his house wait a minute wait up i thought that let's get one wing straight was a store nigga it's in his crib he's selling chicken plates out of his house my nigga no no i saw somebody say congratulations and then i was okay he got a little store or whatever chicken wings out of his crib no i no all right look for the location yeah i'm going to check the location i'm gonna look that up i'm gonna look that up bro yo dog i'm not even bullshitting him he's selling when he don't have a crib he's selling him out of where he's staying out of the crib just selling wings the nigga got the shotgun sauce like it's crazy let's get one wing straight i i was i was like i was i i imagine like like since nunu got the teeth store i thought he was next up hold on my son is an animal dog it's okay it's okay it's all right", 'role': 'user'}], 'model': 'gpt-3.5-turbo-16k', 'max_tokens': 1000, 'n': 1, 'stream': False, 'temperature': 0.7}}
DEBUG:openai._base_client:Sending HTTP Request: POST https://api.openai.com/v1/chat/completions
DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>
DEBUG:httpcore.http11:send_request_headers.complete
DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>
DEBUG:httpcore.http11:send_request_body.complete
DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>
DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Mon, 08 Jul 2024 07:24:46 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'openai-organization', b'machinekingsmedia'), (b'openai-processing-ms', b'319'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains'), (b'x-ratelimit-limit-requests', b'10000'), (b'x-ratelimit-limit-tokens', b'1000000'), (b'x-ratelimit-remaining-requests', b'9999'), (b'x-ratelimit-remaining-tokens', b'998764'), (b'x-ratelimit-reset-requests', b'6ms'), (b'x-ratelimit-reset-tokens', b'74ms'), (b'x-request-id', b'req_47733d841a4a2c6e6fb44a68a84d5c67'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'89fe5625e8c00594-IAD'), (b'Content-Encoding', b'br'), (b'alt-svc', b'h3=":443"; ma=86400')])
INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>
DEBUG:httpcore.http11:receive_response_body.complete
DEBUG:httpcore.http11:response_closed.started
DEBUG:httpcore.http11:response_closed.complete
DEBUG:openai._base_client:HTTP Response: POST https://api.openai.com/v1/chat/completions "200 OK" Headers({'date': 'Mon, 08 Jul 2024 07:24:46 GMT', 'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'openai-organization': 'machinekingsmedia', 'openai-processing-ms': '319', 'openai-version': '2020-10-01', 'strict-transport-security': 'max-age=31536000; includeSubDomains', 'x-ratelimit-limit-requests': '10000', 'x-ratelimit-limit-tokens': '1000000', 'x-ratelimit-remaining-requests': '9999', 'x-ratelimit-remaining-tokens': '998764', 'x-ratelimit-reset-requests': '6ms', 'x-ratelimit-reset-tokens': '74ms', 'x-request-id': 'req_47733d841a4a2c6e6fb44a68a84d5c67', 'cf-cache-status': 'DYNAMIC', 'server': 'cloudflare', 'cf-ray': '89fe5625e8c00594-IAD', 'content-encoding': 'br', 'alt-svc': 'h3=":443"; ma=86400'})
DEBUG:openai._base_client:request_id: req_47733d841a4a2c6e6fb44a68a84d5c67
DEBUG:openai._base_client:Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'json_data': {'messages': [{'content': "Summarize the following chapter in one sentence: i'm gonna check up on that noise i might have to do a little something on that all you gotta do is be like hey shotgun where's the location at i'm trying to pull up to buy some wings no no i ain't gonna ask him now i'm gonna just do my little research that's very yo bro i know uh i'm a thousand percent sure niggas call me and i started laughing i said wait what are you talking about he's selling wings he's supposed to be delivering them but he he can't so he'll be having he'll have to come to the square to pick it up wow yeah and he turning down uh the eternal he turned down the easy battle because of loyalty to url as well like he he priced himself out rather than saying no to look bad in the culture bro he didn't price myself out he was told not to do it and he was like oh i'm scared of smack and now look at him bro them niggas are broke right now them niggas they're broke i don't know", 'role': 'user'}], 'model': 'gpt-3.5-turbo-16k', 'max_tokens': 1000, 'n': 1, 'stream': False, 'temperature': 0.7}}
DEBUG:openai._base_client:Sending HTTP Request: POST https://api.openai.com/v1/chat/completions
DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>
DEBUG:httpcore.http11:send_request_headers.complete
DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>
DEBUG:httpcore.http11:send_request_body.complete
DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>
DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Mon, 08 Jul 2024 07:24:47 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'openai-organization', b'machinekingsmedia'), (b'openai-processing-ms', b'387'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains'), (b'x-ratelimit-limit-requests', b'10000'), (b'x-ratelimit-limit-tokens', b'1000000'), (b'x-ratelimit-remaining-requests', b'9999'), (b'x-ratelimit-remaining-tokens', b'998760'), (b'x-ratelimit-reset-requests', b'6ms'), (b'x-ratelimit-reset-tokens', b'74ms'), (b'x-request-id', b'req_620fb390f0154731f6b009205482f7c6'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'89fe56298b320594-IAD'), (b'Content-Encoding', b'br'), (b'alt-svc', b'h3=":443"; ma=86400')])
INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>
DEBUG:httpcore.http11:receive_response_body.complete
DEBUG:httpcore.http11:response_closed.started
DEBUG:httpcore.http11:response_closed.complete
DEBUG:openai._base_client:HTTP Response: POST https://api.openai.com/v1/chat/completions "200 OK" Headers({'date': 'Mon, 08 Jul 2024 07:24:47 GMT', 'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'openai-organization': 'machinekingsmedia', 'openai-processing-ms': '387', 'openai-version': '2020-10-01', 'strict-transport-security': 'max-age=31536000; includeSubDomains', 'x-ratelimit-limit-requests': '10000', 'x-ratelimit-limit-tokens': '1000000', 'x-ratelimit-remaining-requests': '9999', 'x-ratelimit-remaining-tokens': '998760', 'x-ratelimit-reset-requests': '6ms', 'x-ratelimit-reset-tokens': '74ms', 'x-request-id': 'req_620fb390f0154731f6b009205482f7c6', 'cf-cache-status': 'DYNAMIC', 'server': 'cloudflare', 'cf-ray': '89fe56298b320594-IAD', 'content-encoding': 'br', 'alt-svc': 'h3=":443"; ma=86400'})
DEBUG:openai._base_client:request_id: req_620fb390f0154731f6b009205482f7c6
DEBUG:openai._base_client:Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'json_data': {'messages': [{'content': "Summarize the following chapter in one sentence: how bad i was gonna eat i heard i heard that nigga tate rockets back at his well he's been back at his mama crib but he's in the basement just chilling like this they got me a cake though yeah and sent sending them to the bahamas or something right come on that's how you satisfy that's how you satisfy a certain section of society yeah you know just a bit of your style you could say that's how you that's how you satisfy them man and and also like like we discussed many many times before um the the blind loyalty to the brand is it's crazy and and i think it has a little bit to do with nostalgia um and the work that you put in and the glory days of old not what's going on right now but the glory days of old but but this app situation just seems like the worst possible thing for the culture as a whole and their mentality it's not even the app", 'role': 'user'}], 'model': 'gpt-3.5-turbo-16k', 'max_tokens': 1000, 'n': 1, 'stream': False, 'temperature': 0.7}}
DEBUG:openai._base_client:Sending HTTP Request: POST https://api.openai.com/v1/chat/completions
DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>
DEBUG:httpcore.http11:send_request_headers.complete
DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>
DEBUG:httpcore.http11:send_request_body.complete
DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>
DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Mon, 08 Jul 2024 07:24:48 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'openai-organization', b'machinekingsmedia'), (b'openai-processing-ms', b'653'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains'), (b'x-ratelimit-limit-requests', b'10000'), (b'x-ratelimit-limit-tokens', b'1000000'), (b'x-ratelimit-remaining-requests', b'9999'), (b'x-ratelimit-remaining-tokens', b'998773'), (b'x-ratelimit-reset-requests', b'6ms'), (b'x-ratelimit-reset-tokens', b'73ms'), (b'x-request-id', b'req_0efb04991d4f87359f7ed407d07796d6'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'89fe562d6dc90594-IAD'), (b'Content-Encoding', b'br'), (b'alt-svc', b'h3=":443"; ma=86400')])
INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>
DEBUG:httpcore.http11:receive_response_body.complete
DEBUG:httpcore.http11:response_closed.started
DEBUG:httpcore.http11:response_closed.complete
DEBUG:openai._base_client:HTTP Response: POST https://api.openai.com/v1/chat/completions "200 OK" Headers({'date': 'Mon, 08 Jul 2024 07:24:48 GMT', 'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'openai-organization': 'machinekingsmedia', 'openai-processing-ms': '653', 'openai-version': '2020-10-01', 'strict-transport-security': 'max-age=31536000; includeSubDomains', 'x-ratelimit-limit-requests': '10000', 'x-ratelimit-limit-tokens': '1000000', 'x-ratelimit-remaining-requests': '9999', 'x-ratelimit-remaining-tokens': '998773', 'x-ratelimit-reset-requests': '6ms', 'x-ratelimit-reset-tokens': '73ms', 'x-request-id': 'req_0efb04991d4f87359f7ed407d07796d6', 'cf-cache-status': 'DYNAMIC', 'server': 'cloudflare', 'cf-ray': '89fe562d6dc90594-IAD', 'content-encoding': 'br', 'alt-svc': 'h3=":443"; ma=86400'})
DEBUG:openai._base_client:request_id: req_0efb04991d4f87359f7ed407d07796d6
DEBUG:openai._base_client:Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'json_data': {'messages': [{'content': "Summarize the following chapter in one sentence: situation no more bro it's the fact that they don't have no backing they're not gonna pay for shit ain't nobody gonna put up no money for the niggas but they they're making their own money but they're spending it on jewelry and cars and things of that nature that um if they don't have an office that might be a reflection of them not reinvested into the business itself um you don't even need all that son you don't even need to over half of this shit these niggas are so fucking dumb like yo bro it's the ad situation they they they probably gonna try to sell off what they can if they can at this point so but what can they sell off the the the the battles somebody used the library worth it you know they already have but i just don't see them cashing out in that situation but what's hold on let me let me get my let me get these niggas no problem all right", 'role': 'user'}], 'model': 'gpt-3.5-turbo-16k', 'max_tokens': 1000, 'n': 1, 'stream': False, 'temperature': 0.7}}
DEBUG:openai._base_client:Sending HTTP Request: POST https://api.openai.com/v1/chat/completions
DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>
DEBUG:httpcore.http11:send_request_headers.complete
DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>
DEBUG:httpcore.http11:send_request_body.complete
DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>
DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Mon, 08 Jul 2024 07:24:50 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'openai-organization', b'machinekingsmedia'), (b'openai-processing-ms', b'634'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains'), (b'x-ratelimit-limit-requests', b'10000'), (b'x-ratelimit-limit-tokens', b'1000000'), (b'x-ratelimit-remaining-requests', b'9999'), (b'x-ratelimit-remaining-tokens', b'998771'), (b'x-ratelimit-reset-requests', b'6ms'), (b'x-ratelimit-reset-tokens', b'73ms'), (b'x-request-id', b'req_4518d6fead7eea4a23eeba7dde82824f'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'89fe563389a30594-IAD'), (b'Content-Encoding', b'br'), (b'alt-svc', b'h3=":443"; ma=86400')])
INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>
DEBUG:httpcore.http11:receive_response_body.complete
DEBUG:httpcore.http11:response_closed.started
DEBUG:httpcore.http11:response_closed.complete
DEBUG:openai._base_client:HTTP Response: POST https://api.openai.com/v1/chat/completions "200 OK" Headers({'date': 'Mon, 08 Jul 2024 07:24:50 GMT', 'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'openai-organization': 'machinekingsmedia', 'openai-processing-ms': '634', 'openai-version': '2020-10-01', 'strict-transport-security': 'max-age=31536000; includeSubDomains', 'x-ratelimit-limit-requests': '10000', 'x-ratelimit-limit-tokens': '1000000', 'x-ratelimit-remaining-requests': '9999', 'x-ratelimit-remaining-tokens': '998771', 'x-ratelimit-reset-requests': '6ms', 'x-ratelimit-reset-tokens': '73ms', 'x-request-id': 'req_4518d6fead7eea4a23eeba7dde82824f', 'cf-cache-status': 'DYNAMIC', 'server': 'cloudflare', 'cf-ray': '89fe563389a30594-IAD', 'content-encoding': 'br', 'alt-svc': 'h3=":443"; ma=86400'})
DEBUG:openai._base_client:request_id: req_4518d6fead7eea4a23eeba7dde82824f
DEBUG:openai._base_client:Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'json_data': {'messages': [{'content': "Summarize the following chapter in one sentence: and like if i'm if i'm a business dude and i purchased a company and you sold me everything but what made the engine run i'm gonna feel a way yeah when you're telling them like yo this shit got no legs he bullshitting you don't think they did their own research they was just like yo we don't want to get sued and miss this shit because then what happens is you look like y'all was in it with them to kick me out yeah so they was just trying to protect their own ass but they did their own research nigga they knew that i wasn't just a scout too many people saying the same shit like come on bro let me just minimize it bro i've seen their faces i i've seen with smack down when we did the depositions i've seen all that shit on those niggas is a dummy but you know you know so you can just tell like like there's one part like i can't really talk about the deposition shit because it's not whatever but i just recall", 'role': 'user'}], 'model': 'gpt-3.5-turbo-16k', 'max_tokens': 1000, 'n': 1, 'stream': False, 'temperature': 0.7}}
DEBUG:openai._base_client:Sending HTTP Request: POST https://api.openai.com/v1/chat/completions
DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>
DEBUG:httpcore.http11:send_request_headers.complete
DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>
DEBUG:httpcore.http11:send_request_body.complete
DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>
DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Mon, 08 Jul 2024 07:24:50 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'openai-organization', b'machinekingsmedia'), (b'openai-processing-ms', b'647'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains'), (b'x-ratelimit-limit-requests', b'10000'), (b'x-ratelimit-limit-tokens', b'1000000'), (b'x-ratelimit-remaining-requests', b'9999'), (b'x-ratelimit-remaining-tokens', b'998756'), (b'x-ratelimit-reset-requests', b'6ms'), (b'x-ratelimit-reset-tokens', b'74ms'), (b'x-request-id', b'req_f6dbaed97b79a2701be875c4aee52a27'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'89fe563cdf710594-IAD'), (b'Content-Encoding', b'br'), (b'alt-svc', b'h3=":443"; ma=86400')])
INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>
DEBUG:httpcore.http11:receive_response_body.complete
DEBUG:httpcore.http11:response_closed.started
DEBUG:httpcore.http11:response_closed.complete
DEBUG:openai._base_client:HTTP Response: POST https://api.openai.com/v1/chat/completions "200 OK" Headers({'date': 'Mon, 08 Jul 2024 07:24:50 GMT', 'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'openai-organization': 'machinekingsmedia', 'openai-processing-ms': '647', 'openai-version': '2020-10-01', 'strict-transport-security': 'max-age=31536000; includeSubDomains', 'x-ratelimit-limit-requests': '10000', 'x-ratelimit-limit-tokens': '1000000', 'x-ratelimit-remaining-requests': '9999', 'x-ratelimit-remaining-tokens': '998756', 'x-ratelimit-reset-requests': '6ms', 'x-ratelimit-reset-tokens': '74ms', 'x-request-id': 'req_f6dbaed97b79a2701be875c4aee52a27', 'cf-cache-status': 'DYNAMIC', 'server': 'cloudflare', 'cf-ray': '89fe563cdf710594-IAD', 'content-encoding': 'br', 'alt-svc': 'h3=":443"; ma=86400'})
DEBUG:openai._base_client:request_id: req_f6dbaed97b79a2701be875c4aee52a27
DEBUG:openai._base_client:Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'json_data': {'messages': [{'content': "Summarize the following chapter in one sentence: yeah don't don't speak on it if you can't speak on it now because i am recording yeah i'm recording now i was gonna say like i could just see it bro i could see it in his face specifically like i know troy and i know what moves troy i know what motivates him and it's it's all stupid shit like you probably say nigga shit you're in your mid-40s my nigga you worried about a chain and a fucking 300 shirt for 500 whatever the fuck you pay for those shirts yeah yeah you can see the way that they dress like the way they dress as of late like they are the accessorized chains yeah you can kind of see that yeah you don't see fucking jay-z doing that like and then there's something about money that's different like when you got money you don't act like you got money you know what i'm saying yeah like it's one of those situations where you kind of like", 'role': 'user'}], 'model': 'gpt-3.5-turbo-16k', 'max_tokens': 1000, 'n': 1, 'stream': False, 'temperature': 0.7}}
DEBUG:openai._base_client:Sending HTTP Request: POST https://api.openai.com/v1/chat/completions
DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>
DEBUG:httpcore.http11:send_request_headers.complete
DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>
DEBUG:httpcore.http11:send_request_body.complete
DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>
DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Mon, 08 Jul 2024 07:24:51 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'openai-organization', b'machinekingsmedia'), (b'openai-processing-ms', b'714'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains'), (b'x-ratelimit-limit-requests', b'10000'), (b'x-ratelimit-limit-tokens', b'1000000'), (b'x-ratelimit-remaining-requests', b'9999'), (b'x-ratelimit-remaining-tokens', b'998772'), (b'x-ratelimit-reset-requests', b'6ms'), (b'x-ratelimit-reset-tokens', b'73ms'), (b'x-request-id', b'req_aa8f61d63f1eff3f47131ec65312abc8'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'89fe56425b130594-IAD'), (b'Content-Encoding', b'br'), (b'alt-svc', b'h3=":443"; ma=86400')])
INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>
DEBUG:httpcore.http11:receive_response_body.complete
DEBUG:httpcore.http11:response_closed.started
DEBUG:httpcore.http11:response_closed.complete
DEBUG:openai._base_client:HTTP Response: POST https://api.openai.com/v1/chat/completions "200 OK" Headers({'date': 'Mon, 08 Jul 2024 07:24:51 GMT', 'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'openai-organization': 'machinekingsmedia', 'openai-processing-ms': '714', 'openai-version': '2020-10-01', 'strict-transport-security': 'max-age=31536000; includeSubDomains', 'x-ratelimit-limit-requests': '10000', 'x-ratelimit-limit-tokens': '1000000', 'x-ratelimit-remaining-requests': '9999', 'x-ratelimit-remaining-tokens': '998772', 'x-ratelimit-reset-requests': '6ms', 'x-ratelimit-reset-tokens': '73ms', 'x-request-id': 'req_aa8f61d63f1eff3f47131ec65312abc8', 'cf-cache-status': 'DYNAMIC', 'server': 'cloudflare', 'cf-ray': '89fe56425b130594-IAD', 'content-encoding': 'br', 'alt-svc': 'h3=":443"; ma=86400'})
DEBUG:openai._base_client:request_id: req_aa8f61d63f1eff3f47131ec65312abc8
DEBUG:openai._base_client:Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'json_data': {'messages': [{'content': "Summarize the following chapter in one sentence: they were niggas with real money and then you see a nigga like smack Logan they laugh at them because they don't even know how to like you got to show off you got but you don't show this shit off you don't how you live but but they they are a hip-hop culture though so there is that but their business as business owners that that work with these independent contractors it just seems as though from my observation of that the subscription money has got they've gotten so much reliable subscription money over the years that they kind of got comfortable and they just haven't they it seemed like they've even given up trying to innovate no i mean bro you understand bro like it's like who said who's not the best when you win a championship you gotta understand and that you gotta continue what you're doing in fact you gotta do more because not as expectation their only expectation was to to fuck bitches and wear", 'role': 'user'}], 'model': 'gpt-3.5-turbo-16k', 'max_tokens': 1000, 'n': 1, 'stream': False, 'temperature': 0.7}}
DEBUG:openai._base_client:Sending HTTP Request: POST https://api.openai.com/v1/chat/completions
DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>
DEBUG:httpcore.http11:send_request_headers.complete
DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>
DEBUG:httpcore.http11:send_request_body.complete
DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>
DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Mon, 08 Jul 2024 07:24:52 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'openai-organization', b'machinekingsmedia'), (b'openai-processing-ms', b'531'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains'), (b'x-ratelimit-limit-requests', b'10000'), (b'x-ratelimit-limit-tokens', b'1000000'), (b'x-ratelimit-remaining-requests', b'9999'), (b'x-ratelimit-remaining-tokens', b'998757'), (b'x-ratelimit-reset-requests', b'6ms'), (b'x-ratelimit-reset-tokens', b'74ms'), (b'x-request-id', b'req_aee927846b1b7fe1ce4241a32f8d9b4b'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'89fe56484e580594-IAD'), (b'Content-Encoding', b'br'), (b'alt-svc', b'h3=":443"; ma=86400')])
INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>
DEBUG:httpcore.http11:receive_response_body.complete
DEBUG:httpcore.http11:response_closed.started
DEBUG:httpcore.http11:response_closed.complete
DEBUG:openai._base_client:HTTP Response: POST https://api.openai.com/v1/chat/completions "200 OK" Headers({'date': 'Mon, 08 Jul 2024 07:24:52 GMT', 'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'openai-organization': 'machinekingsmedia', 'openai-processing-ms': '531', 'openai-version': '2020-10-01', 'strict-transport-security': 'max-age=31536000; includeSubDomains', 'x-ratelimit-limit-requests': '10000', 'x-ratelimit-limit-tokens': '1000000', 'x-ratelimit-remaining-requests': '9999', 'x-ratelimit-remaining-tokens': '998757', 'x-ratelimit-reset-requests': '6ms', 'x-ratelimit-reset-tokens': '74ms', 'x-request-id': 'req_aee927846b1b7fe1ce4241a32f8d9b4b', 'cf-cache-status': 'DYNAMIC', 'server': 'cloudflare', 'cf-ray': '89fe56484e580594-IAD', 'content-encoding': 'br', 'alt-svc': 'h3=":443"; ma=86400'})
DEBUG:openai._base_client:request_id: req_aee927846b1b7fe1ce4241a32f8d9b4b
DEBUG:openai._base_client:Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'json_data': {'messages': [{'content': "Summarize the following chapter in one sentence: jewelry that's they should yo i'm gonna use that that championship shit for that that um that cowboys that cowboys video that i'm gonna make yeah that's a fire analogy i'm gonna use that analogy for them winning a championship with you pretty much and etching themselves in history with you norbs etching themselves in history but then the expectations are there now and now that without you because yo yo i paid i called you tyrian lannister like you you fuck hold on you watch game of thrones i started through i never finished oh okay okay but you still should watch that shit though norbs because that shit that that's that that's a good ass blog that's a good ass blog and i'll watch i'll hit you back i'm gonna get this nigga started i'll watch it right now all right i'm gonna send it uh maybe like nothing i make is over 12 minutes so it's probably like eight or nine minutes maybe i'll send it to you okay i gotta put this nigga he follow me", 'role': 'user'}], 'model': 'gpt-3.5-turbo-16k', 'max_tokens': 1000, 'n': 1, 'stream': False, 'temperature': 0.7}}
DEBUG:openai._base_client:Sending HTTP Request: POST https://api.openai.com/v1/chat/completions
DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>
DEBUG:httpcore.http11:send_request_headers.complete
DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>
DEBUG:httpcore.http11:send_request_body.complete
DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>
DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Mon, 08 Jul 2024 07:24:53 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'openai-organization', b'machinekingsmedia'), (b'openai-processing-ms', b'474'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains'), (b'x-ratelimit-limit-requests', b'10000'), (b'x-ratelimit-limit-tokens', b'1000000'), (b'x-ratelimit-remaining-requests', b'9999'), (b'x-ratelimit-remaining-tokens', b'998748'), (b'x-ratelimit-reset-requests', b'6ms'), (b'x-ratelimit-reset-tokens', b'75ms'), (b'x-request-id', b'req_b5c6a5e6a47e1aa981a2652ec32248b7'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'89fe564d18f00594-IAD'), (b'Content-Encoding', b'br'), (b'alt-svc', b'h3=":443"; ma=86400')])
INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>
DEBUG:httpcore.http11:receive_response_body.complete
DEBUG:httpcore.http11:response_closed.started
DEBUG:httpcore.http11:response_closed.complete
DEBUG:openai._base_client:HTTP Response: POST https://api.openai.com/v1/chat/completions "200 OK" Headers({'date': 'Mon, 08 Jul 2024 07:24:53 GMT', 'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'openai-organization': 'machinekingsmedia', 'openai-processing-ms': '474', 'openai-version': '2020-10-01', 'strict-transport-security': 'max-age=31536000; includeSubDomains', 'x-ratelimit-limit-requests': '10000', 'x-ratelimit-limit-tokens': '1000000', 'x-ratelimit-remaining-requests': '9999', 'x-ratelimit-remaining-tokens': '998748', 'x-ratelimit-reset-requests': '6ms', 'x-ratelimit-reset-tokens': '75ms', 'x-request-id': 'req_b5c6a5e6a47e1aa981a2652ec32248b7', 'cf-cache-status': 'DYNAMIC', 'server': 'cloudflare', 'cf-ray': '89fe564d18f00594-IAD', 'content-encoding': 'br', 'alt-svc': 'h3=":443"; ma=86400'})
DEBUG:openai._base_client:request_id: req_b5c6a5e6a47e1aa981a2652ec32248b7
DEBUG:openai._base_client:Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'json_data': {'messages': [{'content': "Summarize the following chapter in one sentence: around all right my my baby girl i'm gonna be putting her to bed at any moment so um if not then i'll hit you later on tonight all right", 'role': 'user'}], 'model': 'gpt-3.5-turbo-16k', 'max_tokens': 1000, 'n': 1, 'stream': False, 'temperature': 0.7}}
DEBUG:openai._base_client:Sending HTTP Request: POST https://api.openai.com/v1/chat/completions
DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>
DEBUG:httpcore.http11:send_request_headers.complete
DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>
DEBUG:httpcore.http11:send_request_body.complete
DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>
DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Mon, 08 Jul 2024 07:24:53 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'openai-organization', b'machinekingsmedia'), (b'openai-processing-ms', b'356'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains'), (b'x-ratelimit-limit-requests', b'10000'), (b'x-ratelimit-limit-tokens', b'1000000'), (b'x-ratelimit-remaining-requests', b'9999'), (b'x-ratelimit-remaining-tokens', b'998952'), (b'x-ratelimit-reset-requests', b'6ms'), (b'x-ratelimit-reset-tokens', b'62ms'), (b'x-request-id', b'req_11e1d0cf2b338dde736c17808e0bf0c3'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'89fe56518b6a0594-IAD'), (b'Content-Encoding', b'br'), (b'alt-svc', b'h3=":443"; ma=86400')])
INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>
DEBUG:httpcore.http11:receive_response_body.complete
DEBUG:httpcore.http11:response_closed.started
DEBUG:httpcore.http11:response_closed.complete
DEBUG:openai._base_client:HTTP Response: POST https://api.openai.com/v1/chat/completions "200 OK" Headers({'date': 'Mon, 08 Jul 2024 07:24:53 GMT', 'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'openai-organization': 'machinekingsmedia', 'openai-processing-ms': '356', 'openai-version': '2020-10-01', 'strict-transport-security': 'max-age=31536000; includeSubDomains', 'x-ratelimit-limit-requests': '10000', 'x-ratelimit-limit-tokens': '1000000', 'x-ratelimit-remaining-requests': '9999', 'x-ratelimit-remaining-tokens': '998952', 'x-ratelimit-reset-requests': '6ms', 'x-ratelimit-reset-tokens': '62ms', 'x-request-id': 'req_11e1d0cf2b338dde736c17808e0bf0c3', 'cf-cache-status': 'DYNAMIC', 'server': 'cloudflare', 'cf-ray': '89fe56518b6a0594-IAD', 'content-encoding': 'br', 'alt-svc': 'h3=":443"; ma=86400'})
DEBUG:openai._base_client:request_id: req_11e1d0cf2b338dde736c17808e0bf0c3
DEBUG:openai._base_client:Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'json_data': {'messages': [{'content': "Provide a brief summary of the following transcript: File: 240204_1940.mp3\nRecording Date: 2024-02-04\nDuration: 00:13:13.60\nTranscription Date: 2024-07-07 06:17:21\n\n1\n00:00:00,000 --> 00:00:04,800\nput it out but so you said you the way you articulated though i really like that you said\n\n2\n00:00:04,800 --> 00:00:09,120\na cog in the machine they lost a cog in the machine yo did you see my game of thrones\n\n3\n00:00:09,120 --> 00:00:15,600\nbreakdown of the of the um the the the leagues in battle right right now how i framed it with you\n\n4\n00:00:16,719 --> 00:00:20,639\nno i gotta seriously because i don't i try not to watch none of this yeah yeah i know you said\n\n5\n00:00:20,639 --> 00:00:24,879\nyou was gonna do that too no i was i was that was a little test along with a question because\n\n6\n00:00:24,879 --> 00:00:30,959\nyou say you were stepping away from the whole negativity and all of that and yo the fact that\n\n7\n00:00:30,959 --> 00:00:37,840\nis that everybody's saying that is dying right now i think about you so much but anyway uh but\n\n8\n00:00:37,840 --> 00:00:44,880\nwhat i pointed out was how like it seems like we're starting to feel the after effects of the\n\n9\n00:00:44,880 --> 00:00:51,919\nlack of innovation from that side so you saying that the deal they made with caffeine was like\n\n10\n00:00:51,919 --> 00:00:56,959\nthem selling them a machine but a cog was missing in the machine it looked the same from the outside\n\n11\n00:00:56,959 --> 00:01:02,000\nbut on the inside an important piece was missing they made it basically look like that i had\n\n12\n00:01:02,000 --> 00:01:07,599\nnothing to do with what went down yeah they framed it like that yeah because you were a talent scout\n\n13\n00:01:08,320 --> 00:01:16,800\nyeah apparently yeah they knew they knew that that's not the case and caffeine knows it's not\n\n14\n00:01:16,800 --> 00:01:22,959\nthe case you understand and it's between me and you of course as i was going through this for three\n\n15\n00:01:22,959 --> 00:01:28,720\nyears they was sitting in on the meetings with me and my lawyers in this so why would they sit\n\n16\n00:01:28,720 --> 00:01:32,320\nif they think it was important if they they're gonna stand behind these niggas and make it look\n\n17\n00:01:32,320 --> 00:01:37,040\nlike a notice just trying to scam these niggas yeah when they realize like hold on my nigga\n\n18\n00:01:38,320 --> 00:01:43,680\nthis nigga really did all of this shit and y'all think about it if you could if you acquiring a\n\n19\n00:01:43,680 --> 00:01:50,000\ncompany and its routes and how it became successful why wouldn't you keep the same team\n\n20\n00:01:51,919 --> 00:01:57,599\nwow yeah yeah i see what you're saying i see what you're saying and i can see that fueling\n\n21\n00:01:57,599 --> 00:02:03,839\nanger amongst their ranks yeah and you gotta think about it sound like it was all a money\n\n22\n00:02:03,839 --> 00:02:07,680\nmove with them bro they tried to cut me out they owe me money you know what i'm saying yeah\n\n23\n00:02:07,680 --> 00:02:14,559\nthey were wrong and bro when they went to that situation they were like yo you know three ways\n\n24\n00:02:14,559 --> 00:02:20,559\nis better than four yeah yeah the way they looked at it i wanted to give me the credit that i\n\n25\n00:02:20,559 --> 00:02:26,720\ndeserved you know i'm saying period yeah yeah my mentality was i'm gonna make this stupid\n\n26\n00:02:26,720 --> 00:02:33,839\nmotherfucker look like he's the man sacrifice my own shit you feel me yeah and their mentality\n\n27\n00:02:33,839 --> 00:02:38,880\nwas like nah we caught this nigga out we didn't pay and like no like it was just it was just\n\n28\n00:02:38,880 --> 00:02:44,399\nstupid on smack's part bro like you know you know what business you go before i got there\n\n29\n00:02:44,399 --> 00:02:50,639\nand you was falling off the cliff why would you revert to what you was because when i got there\n\n30\n00:02:50,639 --> 00:02:58,559\nthis is the highest you've ever been yeah they was with me not with them niggas yeah but but\n\n31\n00:02:58,559 --> 00:03:07,039\njust like just like with uh ", 'role': 'user'}], 'model': 'gpt-3.5-turbo-16k', 'max_tokens': 1000, 'n': 1, 'stream': False, 'temperature': 0.7}}
DEBUG:openai._base_client:Sending HTTP Request: POST https://api.openai.com/v1/chat/completions
DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>
DEBUG:httpcore.http11:send_request_headers.complete
DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>
DEBUG:httpcore.http11:send_request_body.complete
DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>
DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Mon, 08 Jul 2024 07:24:55 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'openai-organization', b'machinekingsmedia'), (b'openai-processing-ms', b'886'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains'), (b'x-ratelimit-limit-requests', b'10000'), (b'x-ratelimit-limit-tokens', b'1000000'), (b'x-ratelimit-remaining-requests', b'9999'), (b'x-ratelimit-remaining-tokens', b'997985'), (b'x-ratelimit-reset-requests', b'6ms'), (b'x-ratelimit-reset-tokens', b'120ms'), (b'x-request-id', b'req_dcf4e19a6ae3d44c93e3d1ebf5c3eedb'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'89fe56555dc70594-IAD'), (b'Content-Encoding', b'br'), (b'alt-svc', b'h3=":443"; ma=86400')])
INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>
DEBUG:httpcore.http11:receive_response_body.complete
DEBUG:httpcore.http11:response_closed.started
DEBUG:httpcore.http11:response_closed.complete
DEBUG:openai._base_client:HTTP Response: POST https://api.openai.com/v1/chat/completions "200 OK" Headers({'date': 'Mon, 08 Jul 2024 07:24:55 GMT', 'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'openai-organization': 'machinekingsmedia', 'openai-processing-ms': '886', 'openai-version': '2020-10-01', 'strict-transport-security': 'max-age=31536000; includeSubDomains', 'x-ratelimit-limit-requests': '10000', 'x-ratelimit-limit-tokens': '1000000', 'x-ratelimit-remaining-requests': '9999', 'x-ratelimit-remaining-tokens': '997985', 'x-ratelimit-reset-requests': '6ms', 'x-ratelimit-reset-tokens': '120ms', 'x-request-id': 'req_dcf4e19a6ae3d44c93e3d1ebf5c3eedb', 'cf-cache-status': 'DYNAMIC', 'server': 'cloudflare', 'cf-ray': '89fe56555dc70594-IAD', 'content-encoding': 'br', 'alt-svc': 'h3=":443"; ma=86400'})
DEBUG:openai._base_client:request_id: req_dcf4e19a6ae3d44c93e3d1ebf5c3eedb
DEBUG:openai._base_client:Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'json_data': {'messages': [{'content': "Generate a short, descriptive title for the following chapter content: put it out but so you said you the way you articulated though i really like that you said a cog in the machine they lost a cog in the machine yo did you see my game of thrones breakdown of the of the um the the the leagues in battle right right now how i framed it with you no i gotta seriously because i don't i try not to watch none of this yeah yeah i know you said you was gonna do that too no i was i was that was a little test along with a question because you say you were stepping away from the whole negativity and all of that and yo the fact that is that everybody's saying that is dying right now i think about you so much but anyway uh but what i pointed out was how like it seems like we're starting to feel the after effects of the lack of innovation from that side so you saying that the deal they made with caffeine was like them selling them a machine but a cog was missing in the machine it looked the same from the outside", 'role': 'user'}], 'model': 'gpt-3.5-turbo-16k', 'max_tokens': 1000, 'n': 1, 'stream': False, 'temperature': 0.7}}
DEBUG:openai._base_client:Sending HTTP Request: POST https://api.openai.com/v1/chat/completions
DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>
DEBUG:httpcore.http11:send_request_headers.complete
DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>
DEBUG:httpcore.http11:send_request_body.complete
DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>
DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Mon, 08 Jul 2024 07:24:55 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'openai-organization', b'machinekingsmedia'), (b'openai-processing-ms', b'297'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains'), (b'x-ratelimit-limit-requests', b'10000'), (b'x-ratelimit-limit-tokens', b'1000000'), (b'x-ratelimit-remaining-requests', b'9999'), (b'x-ratelimit-remaining-tokens', b'998746'), (b'x-ratelimit-reset-requests', b'6ms'), (b'x-ratelimit-reset-tokens', b'75ms'), (b'x-request-id', b'req_ac7e4064f3fb3374f83ba44a5dc0bab6'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'89fe565c59bc0594-IAD'), (b'Content-Encoding', b'br'), (b'alt-svc', b'h3=":443"; ma=86400')])
INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>
DEBUG:httpcore.http11:receive_response_body.complete
DEBUG:httpcore.http11:response_closed.started
DEBUG:httpcore.http11:response_closed.complete
DEBUG:openai._base_client:HTTP Response: POST https://api.openai.com/v1/chat/completions "200 OK" Headers({'date': 'Mon, 08 Jul 2024 07:24:55 GMT', 'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'openai-organization': 'machinekingsmedia', 'openai-processing-ms': '297', 'openai-version': '2020-10-01', 'strict-transport-security': 'max-age=31536000; includeSubDomains', 'x-ratelimit-limit-requests': '10000', 'x-ratelimit-limit-tokens': '1000000', 'x-ratelimit-remaining-requests': '9999', 'x-ratelimit-remaining-tokens': '998746', 'x-ratelimit-reset-requests': '6ms', 'x-ratelimit-reset-tokens': '75ms', 'x-request-id': 'req_ac7e4064f3fb3374f83ba44a5dc0bab6', 'cf-cache-status': 'DYNAMIC', 'server': 'cloudflare', 'cf-ray': '89fe565c59bc0594-IAD', 'content-encoding': 'br', 'alt-svc': 'h3=":443"; ma=86400'})
DEBUG:openai._base_client:request_id: req_ac7e4064f3fb3374f83ba44a5dc0bab6
DEBUG:openai._base_client:Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'json_data': {'messages': [{'content': "Generate a short, descriptive title for the following chapter content: but on the inside an important piece was missing they made it basically look like that i had nothing to do with what went down yeah they framed it like that yeah because you were a talent scout yeah apparently yeah they knew they knew that that's not the case and caffeine knows it's not the case you understand and it's between me and you of course as i was going through this for three years they was sitting in on the meetings with me and my lawyers in this so why would they sit if they think it was important if they they're gonna stand behind these niggas and make it look like a notice just trying to scam these niggas yeah when they realize like hold on my nigga this nigga really did all of this shit and y'all think about it if you could if you acquiring a company and its routes and how it became successful why wouldn't you keep the same team wow yeah yeah i see what you're saying i see what you're saying and i can see that fueling", 'role': 'user'}], 'model': 'gpt-3.5-turbo-16k', 'max_tokens': 1000, 'n': 1, 'stream': False, 'temperature': 0.7}}
DEBUG:openai._base_client:Sending HTTP Request: POST https://api.openai.com/v1/chat/completions
DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>
DEBUG:httpcore.http11:send_request_headers.complete
DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>
DEBUG:httpcore.http11:send_request_body.complete
DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>
DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Mon, 08 Jul 2024 07:24:56 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'openai-organization', b'machinekingsmedia'), (b'openai-processing-ms', b'264'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains'), (b'x-ratelimit-limit-requests', b'10000'), (b'x-ratelimit-limit-tokens', b'1000000'), (b'x-ratelimit-remaining-requests', b'9999'), (b'x-ratelimit-remaining-tokens', b'998744'), (b'x-ratelimit-reset-requests', b'6ms'), (b'x-ratelimit-reset-tokens', b'75ms'), (b'x-request-id', b'req_bcbc6579ff37238880518afa96236da2'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'89fe56604c1e0594-IAD'), (b'Content-Encoding', b'br'), (b'alt-svc', b'h3=":443"; ma=86400')])
INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>
DEBUG:httpcore.http11:receive_response_body.complete
DEBUG:httpcore.http11:response_closed.started
DEBUG:httpcore.http11:response_closed.complete
DEBUG:openai._base_client:HTTP Response: POST https://api.openai.com/v1/chat/completions "200 OK" Headers({'date': 'Mon, 08 Jul 2024 07:24:56 GMT', 'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'openai-organization': 'machinekingsmedia', 'openai-processing-ms': '264', 'openai-version': '2020-10-01', 'strict-transport-security': 'max-age=31536000; includeSubDomains', 'x-ratelimit-limit-requests': '10000', 'x-ratelimit-limit-tokens': '1000000', 'x-ratelimit-remaining-requests': '9999', 'x-ratelimit-remaining-tokens': '998744', 'x-ratelimit-reset-requests': '6ms', 'x-ratelimit-reset-tokens': '75ms', 'x-request-id': 'req_bcbc6579ff37238880518afa96236da2', 'cf-cache-status': 'DYNAMIC', 'server': 'cloudflare', 'cf-ray': '89fe56604c1e0594-IAD', 'content-encoding': 'br', 'alt-svc': 'h3=":443"; ma=86400'})
DEBUG:openai._base_client:request_id: req_bcbc6579ff37238880518afa96236da2
DEBUG:openai._base_client:Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'json_data': {'messages': [{'content': "Generate a short, descriptive title for the following chapter content: anger amongst their ranks yeah and you gotta think about it sound like it was all a money move with them bro they tried to cut me out they owe me money you know what i'm saying yeah they were wrong and bro when they went to that situation they were like yo you know three ways is better than four yeah yeah the way they looked at it i wanted to give me the credit that i deserved you know i'm saying period yeah yeah my mentality was i'm gonna make this stupid motherfucker look like he's the man sacrifice my own shit you feel me yeah and their mentality was like nah we caught this nigga out we didn't pay and like no like it was just it was just stupid on smack's part bro like you know you know what business you go before i got there and you was falling off the cliff why would you revert to what you was because when i got there this is the highest you've ever been yeah they was with me not with them niggas yeah but but", 'role': 'user'}], 'model': 'gpt-3.5-turbo-16k', 'max_tokens': 1000, 'n': 1, 'stream': False, 'temperature': 0.7}}
DEBUG:openai._base_client:Sending HTTP Request: POST https://api.openai.com/v1/chat/completions
DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>
DEBUG:httpcore.http11:send_request_headers.complete
DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>
DEBUG:httpcore.http11:send_request_body.complete
DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>
DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Mon, 08 Jul 2024 07:24:56 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'openai-organization', b'machinekingsmedia'), (b'openai-processing-ms', b'317'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains'), (b'x-ratelimit-limit-requests', b'10000'), (b'x-ratelimit-limit-tokens', b'1000000'), (b'x-ratelimit-remaining-requests', b'9999'), (b'x-ratelimit-remaining-tokens', b'998748'), (b'x-ratelimit-reset-requests', b'6ms'), (b'x-ratelimit-reset-tokens', b'75ms'), (b'x-request-id', b'req_5bed8b22158741fdec6cc472a5f7bb6b'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'89fe56636df10594-IAD'), (b'Content-Encoding', b'br'), (b'alt-svc', b'h3=":443"; ma=86400')])
INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>
DEBUG:httpcore.http11:receive_response_body.complete
DEBUG:httpcore.http11:response_closed.started
DEBUG:httpcore.http11:response_closed.complete
DEBUG:openai._base_client:HTTP Response: POST https://api.openai.com/v1/chat/completions "200 OK" Headers({'date': 'Mon, 08 Jul 2024 07:24:56 GMT', 'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'openai-organization': 'machinekingsmedia', 'openai-processing-ms': '317', 'openai-version': '2020-10-01', 'strict-transport-security': 'max-age=31536000; includeSubDomains', 'x-ratelimit-limit-requests': '10000', 'x-ratelimit-limit-tokens': '1000000', 'x-ratelimit-remaining-requests': '9999', 'x-ratelimit-remaining-tokens': '998748', 'x-ratelimit-reset-requests': '6ms', 'x-ratelimit-reset-tokens': '75ms', 'x-request-id': 'req_5bed8b22158741fdec6cc472a5f7bb6b', 'cf-cache-status': 'DYNAMIC', 'server': 'cloudflare', 'cf-ray': '89fe56636df10594-IAD', 'content-encoding': 'br', 'alt-svc': 'h3=":443"; ma=86400'})
DEBUG:openai._base_client:request_id: req_5bed8b22158741fdec6cc472a5f7bb6b
DEBUG:openai._base_client:Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'json_data': {'messages': [{'content': "Generate a short, descriptive title for the following chapter content: just like just like with uh just like with the dallas cowboys um yeah jerry jerry jerry jones thought that you know he could he could run it was the it was the logo the star whatever he could do it without jimmy johnson and um and they haven't been there since i was so happy they ain't advancing the playoff this year too so we can still run with that it is bro it's like when ufc got bored they didn't give it a thing of white in a way it was the cog he was the yeah but he was the face though he was he was the face i remember too when you were wrong i was one of the faces it was me and smack they asked me to fall back because i was out shining him yeah i remember that but smack wasn't really out there talking to the bloggers you used to go on angry fan and big up smack used to be in here i did so much shit that people don't even know that's what i'm saying like it's just it's just if you see them they don't even know what to do", 'role': 'user'}], 'model': 'gpt-3.5-turbo-16k', 'max_tokens': 1000, 'n': 1, 'stream': False, 'temperature': 0.7}}
DEBUG:openai._base_client:Sending HTTP Request: POST https://api.openai.com/v1/chat/completions
DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>
DEBUG:httpcore.http11:send_request_headers.complete
DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>
DEBUG:httpcore.http11:send_request_body.complete
DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>
DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Mon, 08 Jul 2024 07:24:57 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'openai-organization', b'machinekingsmedia'), (b'openai-processing-ms', b'315'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains'), (b'x-ratelimit-limit-requests', b'10000'), (b'x-ratelimit-limit-tokens', b'1000000'), (b'x-ratelimit-remaining-requests', b'9999'), (b'x-ratelimit-remaining-tokens', b'998746'), (b'x-ratelimit-reset-requests', b'6ms'), (b'x-ratelimit-reset-tokens', b'75ms'), (b'x-request-id', b'req_9bb775ee016ddd4d34d2ced7c2db2ca8'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'89fe5666ffed0594-IAD'), (b'Content-Encoding', b'br'), (b'alt-svc', b'h3=":443"; ma=86400')])
INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>
DEBUG:httpcore.http11:receive_response_body.complete
DEBUG:httpcore.http11:response_closed.started
DEBUG:httpcore.http11:response_closed.complete
DEBUG:openai._base_client:HTTP Response: POST https://api.openai.com/v1/chat/completions "200 OK" Headers({'date': 'Mon, 08 Jul 2024 07:24:57 GMT', 'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'openai-organization': 'machinekingsmedia', 'openai-processing-ms': '315', 'openai-version': '2020-10-01', 'strict-transport-security': 'max-age=31536000; includeSubDomains', 'x-ratelimit-limit-requests': '10000', 'x-ratelimit-limit-tokens': '1000000', 'x-ratelimit-remaining-requests': '9999', 'x-ratelimit-remaining-tokens': '998746', 'x-ratelimit-reset-requests': '6ms', 'x-ratelimit-reset-tokens': '75ms', 'x-request-id': 'req_9bb775ee016ddd4d34d2ced7c2db2ca8', 'cf-cache-status': 'DYNAMIC', 'server': 'cloudflare', 'cf-ray': '89fe5666ffed0594-IAD', 'content-encoding': 'br', 'alt-svc': 'h3=":443"; ma=86400'})
DEBUG:openai._base_client:request_id: req_9bb775ee016ddd4d34d2ced7c2db2ca8
DEBUG:openai._base_client:Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'json_data': {'messages': [{'content': "Generate a short, descriptive title for the following chapter content: they don't know what to say they don't even like yo bro like even in business meetings like bro like i have ideas right now that'll that'll make the shit go crazy but i'd never fuck with you know the amount of money you would have to pay me just to resurrect your company yeah but and i've been hearing the adam rumors getting louder this week too about him retiring like as if he made he's done i told you that shit yeah you did tell me that but i didn't see an official announcement but everybody's talking about it no he was never gonna make an announcement he was just like he's gonna do the last event which was uh that's the full circle oh max out back on his last event okay he's not even there oh and he's just gonna do it he was like he's just tired the same shit i said tired of the culture you're putting up money he said he made enough money in the last event where he's just like yo nigga it's not worth the risk why would i put it", 'role': 'user'}], 'model': 'gpt-3.5-turbo-16k', 'max_tokens': 1000, 'n': 1, 'stream': False, 'temperature': 0.7}}
DEBUG:openai._base_client:Sending HTTP Request: POST https://api.openai.com/v1/chat/completions
DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>
DEBUG:httpcore.http11:send_request_headers.complete
DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>
DEBUG:httpcore.http11:send_request_body.complete
DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>
DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Mon, 08 Jul 2024 07:24:57 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'openai-organization', b'machinekingsmedia'), (b'openai-processing-ms', b'238'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains'), (b'x-ratelimit-limit-requests', b'10000'), (b'x-ratelimit-limit-tokens', b'1000000'), (b'x-ratelimit-remaining-requests', b'9999'), (b'x-ratelimit-remaining-tokens', b'998744'), (b'x-ratelimit-reset-requests', b'6ms'), (b'x-ratelimit-reset-tokens', b'75ms'), (b'x-request-id', b'req_98915e971a48b177e36bdb5d973a4c18'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'89fe566a69ec0594-IAD'), (b'Content-Encoding', b'br'), (b'alt-svc', b'h3=":443"; ma=86400')])
INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>
DEBUG:httpcore.http11:receive_response_body.complete
DEBUG:httpcore.http11:response_closed.started
DEBUG:httpcore.http11:response_closed.complete
DEBUG:openai._base_client:HTTP Response: POST https://api.openai.com/v1/chat/completions "200 OK" Headers({'date': 'Mon, 08 Jul 2024 07:24:57 GMT', 'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'openai-organization': 'machinekingsmedia', 'openai-processing-ms': '238', 'openai-version': '2020-10-01', 'strict-transport-security': 'max-age=31536000; includeSubDomains', 'x-ratelimit-limit-requests': '10000', 'x-ratelimit-limit-tokens': '1000000', 'x-ratelimit-remaining-requests': '9999', 'x-ratelimit-remaining-tokens': '998744', 'x-ratelimit-reset-requests': '6ms', 'x-ratelimit-reset-tokens': '75ms', 'x-request-id': 'req_98915e971a48b177e36bdb5d973a4c18', 'cf-cache-status': 'DYNAMIC', 'server': 'cloudflare', 'cf-ray': '89fe566a69ec0594-IAD', 'content-encoding': 'br', 'alt-svc': 'h3=":443"; ma=86400'})
DEBUG:openai._base_client:request_id: req_98915e971a48b177e36bdb5d973a4c18
DEBUG:openai._base_client:Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'json_data': {'messages': [{'content': "Generate a short, descriptive title for the following chapter content: back in bro these i think the usd shop in sugar selling chicken out of his house wait a minute wait up i thought that let's get one wing straight was a store nigga it's in his crib he's selling chicken plates out of his house my nigga no no i saw somebody say congratulations and then i was okay he got a little store or whatever chicken wings out of his crib no i no all right look for the location yeah i'm going to check the location i'm gonna look that up i'm gonna look that up bro yo dog i'm not even bullshitting him he's selling when he don't have a crib he's selling him out of where he's staying out of the crib just selling wings the nigga got the shotgun sauce like it's crazy let's get one wing straight i i was i was like i was i i imagine like like since nunu got the teeth store i thought he was next up hold on my son is an animal dog it's okay it's okay it's all right", 'role': 'user'}], 'model': 'gpt-3.5-turbo-16k', 'max_tokens': 1000, 'n': 1, 'stream': False, 'temperature': 0.7}}
DEBUG:openai._base_client:Sending HTTP Request: POST https://api.openai.com/v1/chat/completions
DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>
DEBUG:httpcore.http11:send_request_headers.complete
DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>
DEBUG:httpcore.http11:send_request_body.complete
DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>
DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Mon, 08 Jul 2024 07:24:58 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'openai-organization', b'machinekingsmedia'), (b'openai-processing-ms', b'242'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains'), (b'x-ratelimit-limit-requests', b'10000'), (b'x-ratelimit-limit-tokens', b'1000000'), (b'x-ratelimit-remaining-requests', b'9999'), (b'x-ratelimit-remaining-tokens', b'998759'), (b'x-ratelimit-reset-requests', b'6ms'), (b'x-ratelimit-reset-tokens', b'74ms'), (b'x-request-id', b'req_8572bdd163d2d58609ed608bf6b81870'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'89fe566debe90594-IAD'), (b'Content-Encoding', b'br'), (b'alt-svc', b'h3=":443"; ma=86400')])
INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>
DEBUG:httpcore.http11:receive_response_body.complete
DEBUG:httpcore.http11:response_closed.started
DEBUG:httpcore.http11:response_closed.complete
DEBUG:openai._base_client:HTTP Response: POST https://api.openai.com/v1/chat/completions "200 OK" Headers({'date': 'Mon, 08 Jul 2024 07:24:58 GMT', 'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'openai-organization': 'machinekingsmedia', 'openai-processing-ms': '242', 'openai-version': '2020-10-01', 'strict-transport-security': 'max-age=31536000; includeSubDomains', 'x-ratelimit-limit-requests': '10000', 'x-ratelimit-limit-tokens': '1000000', 'x-ratelimit-remaining-requests': '9999', 'x-ratelimit-remaining-tokens': '998759', 'x-ratelimit-reset-requests': '6ms', 'x-ratelimit-reset-tokens': '74ms', 'x-request-id': 'req_8572bdd163d2d58609ed608bf6b81870', 'cf-cache-status': 'DYNAMIC', 'server': 'cloudflare', 'cf-ray': '89fe566debe90594-IAD', 'content-encoding': 'br', 'alt-svc': 'h3=":443"; ma=86400'})
DEBUG:openai._base_client:request_id: req_8572bdd163d2d58609ed608bf6b81870
DEBUG:openai._base_client:Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'json_data': {'messages': [{'content': "Generate a short, descriptive title for the following chapter content: i'm gonna check up on that noise i might have to do a little something on that all you gotta do is be like hey shotgun where's the location at i'm trying to pull up to buy some wings no no i ain't gonna ask him now i'm gonna just do my little research that's very yo bro i know uh i'm a thousand percent sure niggas call me and i started laughing i said wait what are you talking about he's selling wings he's supposed to be delivering them but he he can't so he'll be having he'll have to come to the square to pick it up wow yeah and he turning down uh the eternal he turned down the easy battle because of loyalty to url as well like he he priced himself out rather than saying no to look bad in the culture bro he didn't price myself out he was told not to do it and he was like oh i'm scared of smack and now look at him bro them niggas are broke right now them niggas they're broke i don't know", 'role': 'user'}], 'model': 'gpt-3.5-turbo-16k', 'max_tokens': 1000, 'n': 1, 'stream': False, 'temperature': 0.7}}
DEBUG:openai._base_client:Sending HTTP Request: POST https://api.openai.com/v1/chat/completions
DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>
DEBUG:httpcore.http11:send_request_headers.complete
DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>
DEBUG:httpcore.http11:send_request_body.complete
DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>
DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Mon, 08 Jul 2024 07:24:58 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'openai-organization', b'machinekingsmedia'), (b'openai-processing-ms', b'302'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains'), (b'x-ratelimit-limit-requests', b'10000'), (b'x-ratelimit-limit-tokens', b'1000000'), (b'x-ratelimit-remaining-requests', b'9999'), (b'x-ratelimit-remaining-tokens', b'998756'), (b'x-ratelimit-reset-requests', b'6ms'), (b'x-ratelimit-reset-tokens', b'74ms'), (b'x-request-id', b'req_104900de72e3b7313584bb8218400eab'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'89fe5670ed870594-IAD'), (b'Content-Encoding', b'br'), (b'alt-svc', b'h3=":443"; ma=86400')])
INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>
DEBUG:httpcore.http11:receive_response_body.complete
DEBUG:httpcore.http11:response_closed.started
DEBUG:httpcore.http11:response_closed.complete
DEBUG:openai._base_client:HTTP Response: POST https://api.openai.com/v1/chat/completions "200 OK" Headers({'date': 'Mon, 08 Jul 2024 07:24:58 GMT', 'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'openai-organization': 'machinekingsmedia', 'openai-processing-ms': '302', 'openai-version': '2020-10-01', 'strict-transport-security': 'max-age=31536000; includeSubDomains', 'x-ratelimit-limit-requests': '10000', 'x-ratelimit-limit-tokens': '1000000', 'x-ratelimit-remaining-requests': '9999', 'x-ratelimit-remaining-tokens': '998756', 'x-ratelimit-reset-requests': '6ms', 'x-ratelimit-reset-tokens': '74ms', 'x-request-id': 'req_104900de72e3b7313584bb8218400eab', 'cf-cache-status': 'DYNAMIC', 'server': 'cloudflare', 'cf-ray': '89fe5670ed870594-IAD', 'content-encoding': 'br', 'alt-svc': 'h3=":443"; ma=86400'})
DEBUG:openai._base_client:request_id: req_104900de72e3b7313584bb8218400eab
DEBUG:openai._base_client:Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'json_data': {'messages': [{'content': "Generate a short, descriptive title for the following chapter content: how bad i was gonna eat i heard i heard that nigga tate rockets back at his well he's been back at his mama crib but he's in the basement just chilling like this they got me a cake though yeah and sent sending them to the bahamas or something right come on that's how you satisfy that's how you satisfy a certain section of society yeah you know just a bit of your style you could say that's how you that's how you satisfy them man and and also like like we discussed many many times before um the the blind loyalty to the brand is it's crazy and and i think it has a little bit to do with nostalgia um and the work that you put in and the glory days of old not what's going on right now but the glory days of old but but this app situation just seems like the worst possible thing for the culture as a whole and their mentality it's not even the app", 'role': 'user'}], 'model': 'gpt-3.5-turbo-16k', 'max_tokens': 1000, 'n': 1, 'stream': False, 'temperature': 0.7}}
DEBUG:openai._base_client:Sending HTTP Request: POST https://api.openai.com/v1/chat/completions
DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>
DEBUG:httpcore.http11:send_request_headers.complete
DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>
DEBUG:httpcore.http11:send_request_body.complete
DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>
DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Mon, 08 Jul 2024 07:24:59 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'openai-organization', b'machinekingsmedia'), (b'openai-processing-ms', b'284'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains'), (b'x-ratelimit-limit-requests', b'10000'), (b'x-ratelimit-limit-tokens', b'1000000'), (b'x-ratelimit-remaining-requests', b'9999'), (b'x-ratelimit-remaining-tokens', b'998768'), (b'x-ratelimit-reset-requests', b'6ms'), (b'x-ratelimit-reset-tokens', b'73ms'), (b'x-request-id', b'req_6a07a3af974184976e7775f415a02608'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'89fe56744fa00594-IAD'), (b'Content-Encoding', b'br'), (b'alt-svc', b'h3=":443"; ma=86400')])
INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>
DEBUG:httpcore.http11:receive_response_body.complete
DEBUG:httpcore.http11:response_closed.started
DEBUG:httpcore.http11:response_closed.complete
DEBUG:openai._base_client:HTTP Response: POST https://api.openai.com/v1/chat/completions "200 OK" Headers({'date': 'Mon, 08 Jul 2024 07:24:59 GMT', 'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'openai-organization': 'machinekingsmedia', 'openai-processing-ms': '284', 'openai-version': '2020-10-01', 'strict-transport-security': 'max-age=31536000; includeSubDomains', 'x-ratelimit-limit-requests': '10000', 'x-ratelimit-limit-tokens': '1000000', 'x-ratelimit-remaining-requests': '9999', 'x-ratelimit-remaining-tokens': '998768', 'x-ratelimit-reset-requests': '6ms', 'x-ratelimit-reset-tokens': '73ms', 'x-request-id': 'req_6a07a3af974184976e7775f415a02608', 'cf-cache-status': 'DYNAMIC', 'server': 'cloudflare', 'cf-ray': '89fe56744fa00594-IAD', 'content-encoding': 'br', 'alt-svc': 'h3=":443"; ma=86400'})
DEBUG:openai._base_client:request_id: req_6a07a3af974184976e7775f415a02608
DEBUG:openai._base_client:Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'json_data': {'messages': [{'content': "Generate a short, descriptive title for the following chapter content: situation no more bro it's the fact that they don't have no backing they're not gonna pay for shit ain't nobody gonna put up no money for the niggas but they they're making their own money but they're spending it on jewelry and cars and things of that nature that um if they don't have an office that might be a reflection of them not reinvested into the business itself um you don't even need all that son you don't even need to over half of this shit these niggas are so fucking dumb like yo bro it's the ad situation they they they probably gonna try to sell off what they can if they can at this point so but what can they sell off the the the the battles somebody used the library worth it you know they already have but i just don't see them cashing out in that situation but what's hold on let me let me get my let me get these niggas no problem all right", 'role': 'user'}], 'model': 'gpt-3.5-turbo-16k', 'max_tokens': 1000, 'n': 1, 'stream': False, 'temperature': 0.7}}
DEBUG:openai._base_client:Sending HTTP Request: POST https://api.openai.com/v1/chat/completions
DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>
DEBUG:httpcore.http11:send_request_headers.complete
DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>
DEBUG:httpcore.http11:send_request_body.complete
DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>
DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Mon, 08 Jul 2024 07:24:59 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'openai-organization', b'machinekingsmedia'), (b'openai-processing-ms', b'311'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains'), (b'x-ratelimit-limit-requests', b'10000'), (b'x-ratelimit-limit-tokens', b'1000000'), (b'x-ratelimit-remaining-requests', b'9999'), (b'x-ratelimit-remaining-tokens', b'998764'), (b'x-ratelimit-reset-requests', b'6ms'), (b'x-ratelimit-reset-tokens', b'74ms'), (b'x-request-id', b'req_3346333a9db1e72969835a316227843d'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'89fe5677794a0594-IAD'), (b'Content-Encoding', b'br'), (b'alt-svc', b'h3=":443"; ma=86400')])
INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>
DEBUG:httpcore.http11:receive_response_body.complete
DEBUG:httpcore.http11:response_closed.started
DEBUG:httpcore.http11:response_closed.complete
DEBUG:openai._base_client:HTTP Response: POST https://api.openai.com/v1/chat/completions "200 OK" Headers({'date': 'Mon, 08 Jul 2024 07:24:59 GMT', 'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'openai-organization': 'machinekingsmedia', 'openai-processing-ms': '311', 'openai-version': '2020-10-01', 'strict-transport-security': 'max-age=31536000; includeSubDomains', 'x-ratelimit-limit-requests': '10000', 'x-ratelimit-limit-tokens': '1000000', 'x-ratelimit-remaining-requests': '9999', 'x-ratelimit-remaining-tokens': '998764', 'x-ratelimit-reset-requests': '6ms', 'x-ratelimit-reset-tokens': '74ms', 'x-request-id': 'req_3346333a9db1e72969835a316227843d', 'cf-cache-status': 'DYNAMIC', 'server': 'cloudflare', 'cf-ray': '89fe5677794a0594-IAD', 'content-encoding': 'br', 'alt-svc': 'h3=":443"; ma=86400'})
DEBUG:openai._base_client:request_id: req_3346333a9db1e72969835a316227843d
DEBUG:openai._base_client:Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'json_data': {'messages': [{'content': "Generate a short, descriptive title for the following chapter content: and like if i'm if i'm a business dude and i purchased a company and you sold me everything but what made the engine run i'm gonna feel a way yeah when you're telling them like yo this shit got no legs he bullshitting you don't think they did their own research they was just like yo we don't want to get sued and miss this shit because then what happens is you look like y'all was in it with them to kick me out yeah so they was just trying to protect their own ass but they did their own research nigga they knew that i wasn't just a scout too many people saying the same shit like come on bro let me just minimize it bro i've seen their faces i i've seen with smack down when we did the depositions i've seen all that shit on those niggas is a dummy but you know you know so you can just tell like like there's one part like i can't really talk about the deposition shit because it's not whatever but i just recall", 'role': 'user'}], 'model': 'gpt-3.5-turbo-16k', 'max_tokens': 1000, 'n': 1, 'stream': False, 'temperature': 0.7}}
DEBUG:openai._base_client:Sending HTTP Request: POST https://api.openai.com/v1/chat/completions
DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>
DEBUG:httpcore.http11:send_request_headers.complete
DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>
DEBUG:httpcore.http11:send_request_body.complete
DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>
DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Mon, 08 Jul 2024 07:25:00 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'openai-organization', b'machinekingsmedia'), (b'openai-processing-ms', b'258'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains'), (b'x-ratelimit-limit-requests', b'10000'), (b'x-ratelimit-limit-tokens', b'1000000'), (b'x-ratelimit-remaining-requests', b'9999'), (b'x-ratelimit-remaining-tokens', b'998751'), (b'x-ratelimit-reset-requests', b'6ms'), (b'x-ratelimit-reset-tokens', b'74ms'), (b'x-request-id', b'req_3395e916f0e3fb3778359dffc39ab985'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'89fe567aeb1c0594-IAD'), (b'Content-Encoding', b'br'), (b'alt-svc', b'h3=":443"; ma=86400')])
INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>
DEBUG:httpcore.http11:receive_response_body.complete
DEBUG:httpcore.http11:response_closed.started
DEBUG:httpcore.http11:response_closed.complete
DEBUG:openai._base_client:HTTP Response: POST https://api.openai.com/v1/chat/completions "200 OK" Headers({'date': 'Mon, 08 Jul 2024 07:25:00 GMT', 'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'openai-organization': 'machinekingsmedia', 'openai-processing-ms': '258', 'openai-version': '2020-10-01', 'strict-transport-security': 'max-age=31536000; includeSubDomains', 'x-ratelimit-limit-requests': '10000', 'x-ratelimit-limit-tokens': '1000000', 'x-ratelimit-remaining-requests': '9999', 'x-ratelimit-remaining-tokens': '998751', 'x-ratelimit-reset-requests': '6ms', 'x-ratelimit-reset-tokens': '74ms', 'x-request-id': 'req_3395e916f0e3fb3778359dffc39ab985', 'cf-cache-status': 'DYNAMIC', 'server': 'cloudflare', 'cf-ray': '89fe567aeb1c0594-IAD', 'content-encoding': 'br', 'alt-svc': 'h3=":443"; ma=86400'})
DEBUG:openai._base_client:request_id: req_3395e916f0e3fb3778359dffc39ab985
DEBUG:openai._base_client:Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'json_data': {'messages': [{'content': "Generate a short, descriptive title for the following chapter content: yeah don't don't speak on it if you can't speak on it now because i am recording yeah i'm recording now i was gonna say like i could just see it bro i could see it in his face specifically like i know troy and i know what moves troy i know what motivates him and it's it's all stupid shit like you probably say nigga shit you're in your mid-40s my nigga you worried about a chain and a fucking 300 shirt for 500 whatever the fuck you pay for those shirts yeah yeah you can see the way that they dress like the way they dress as of late like they are the accessorized chains yeah you can kind of see that yeah you don't see fucking jay-z doing that like and then there's something about money that's different like when you got money you don't act like you got money you know what i'm saying yeah like it's one of those situations where you kind of like", 'role': 'user'}], 'model': 'gpt-3.5-turbo-16k', 'max_tokens': 1000, 'n': 1, 'stream': False, 'temperature': 0.7}}
DEBUG:openai._base_client:Sending HTTP Request: POST https://api.openai.com/v1/chat/completions
DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>
DEBUG:httpcore.http11:send_request_headers.complete
DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>
DEBUG:httpcore.http11:send_request_body.complete
DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>
DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Mon, 08 Jul 2024 07:25:00 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'openai-organization', b'machinekingsmedia'), (b'openai-processing-ms', b'294'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains'), (b'x-ratelimit-limit-requests', b'10000'), (b'x-ratelimit-limit-tokens', b'1000000'), (b'x-ratelimit-remaining-requests', b'9999'), (b'x-ratelimit-remaining-tokens', b'998768'), (b'x-ratelimit-reset-requests', b'6ms'), (b'x-ratelimit-reset-tokens', b'73ms'), (b'x-request-id', b'req_e75bc387b4f626af5186347cb67fb51f'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'89fe567e0d210594-IAD'), (b'Content-Encoding', b'br'), (b'alt-svc', b'h3=":443"; ma=86400')])
INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>
DEBUG:httpcore.http11:receive_response_body.complete
DEBUG:httpcore.http11:response_closed.started
DEBUG:httpcore.http11:response_closed.complete
DEBUG:openai._base_client:HTTP Response: POST https://api.openai.com/v1/chat/completions "200 OK" Headers({'date': 'Mon, 08 Jul 2024 07:25:00 GMT', 'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'openai-organization': 'machinekingsmedia', 'openai-processing-ms': '294', 'openai-version': '2020-10-01', 'strict-transport-security': 'max-age=31536000; includeSubDomains', 'x-ratelimit-limit-requests': '10000', 'x-ratelimit-limit-tokens': '1000000', 'x-ratelimit-remaining-requests': '9999', 'x-ratelimit-remaining-tokens': '998768', 'x-ratelimit-reset-requests': '6ms', 'x-ratelimit-reset-tokens': '73ms', 'x-request-id': 'req_e75bc387b4f626af5186347cb67fb51f', 'cf-cache-status': 'DYNAMIC', 'server': 'cloudflare', 'cf-ray': '89fe567e0d210594-IAD', 'content-encoding': 'br', 'alt-svc': 'h3=":443"; ma=86400'})
DEBUG:openai._base_client:request_id: req_e75bc387b4f626af5186347cb67fb51f
DEBUG:openai._base_client:Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'json_data': {'messages': [{'content': "Generate a short, descriptive title for the following chapter content: they were niggas with real money and then you see a nigga like smack Logan they laugh at them because they don't even know how to like you got to show off you got but you don't show this shit off you don't how you live but but they they are a hip-hop culture though so there is that but their business as business owners that that work with these independent contractors it just seems as though from my observation of that the subscription money has got they've gotten so much reliable subscription money over the years that they kind of got comfortable and they just haven't they it seemed like they've even given up trying to innovate no i mean bro you understand bro like it's like who said who's not the best when you win a championship you gotta understand and that you gotta continue what you're doing in fact you gotta do more because not as expectation their only expectation was to to fuck bitches and wear", 'role': 'user'}], 'model': 'gpt-3.5-turbo-16k', 'max_tokens': 1000, 'n': 1, 'stream': False, 'temperature': 0.7}}
DEBUG:openai._base_client:Sending HTTP Request: POST https://api.openai.com/v1/chat/completions
DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>
DEBUG:httpcore.http11:send_request_headers.complete
DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>
DEBUG:httpcore.http11:send_request_body.complete
DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>
DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Mon, 08 Jul 2024 07:25:01 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'openai-organization', b'machinekingsmedia'), (b'openai-processing-ms', b'469'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains'), (b'x-ratelimit-limit-requests', b'10000'), (b'x-ratelimit-limit-tokens', b'1000000'), (b'x-ratelimit-remaining-requests', b'9999'), (b'x-ratelimit-remaining-tokens', b'998751'), (b'x-ratelimit-reset-requests', b'6ms'), (b'x-ratelimit-reset-tokens', b'74ms'), (b'x-request-id', b'req_efc895b877db3461036c2cfcb457dcb7'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'89fe56809f050594-IAD'), (b'Content-Encoding', b'br'), (b'alt-svc', b'h3=":443"; ma=86400')])
INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>
DEBUG:httpcore.http11:receive_response_body.complete
DEBUG:httpcore.http11:response_closed.started
DEBUG:httpcore.http11:response_closed.complete
DEBUG:openai._base_client:HTTP Response: POST https://api.openai.com/v1/chat/completions "200 OK" Headers({'date': 'Mon, 08 Jul 2024 07:25:01 GMT', 'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'openai-organization': 'machinekingsmedia', 'openai-processing-ms': '469', 'openai-version': '2020-10-01', 'strict-transport-security': 'max-age=31536000; includeSubDomains', 'x-ratelimit-limit-requests': '10000', 'x-ratelimit-limit-tokens': '1000000', 'x-ratelimit-remaining-requests': '9999', 'x-ratelimit-remaining-tokens': '998751', 'x-ratelimit-reset-requests': '6ms', 'x-ratelimit-reset-tokens': '74ms', 'x-request-id': 'req_efc895b877db3461036c2cfcb457dcb7', 'cf-cache-status': 'DYNAMIC', 'server': 'cloudflare', 'cf-ray': '89fe56809f050594-IAD', 'content-encoding': 'br', 'alt-svc': 'h3=":443"; ma=86400'})
DEBUG:openai._base_client:request_id: req_efc895b877db3461036c2cfcb457dcb7
DEBUG:openai._base_client:Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'json_data': {'messages': [{'content': "Generate a short, descriptive title for the following chapter content: jewelry that's they should yo i'm gonna use that that championship shit for that that um that cowboys that cowboys video that i'm gonna make yeah that's a fire analogy i'm gonna use that analogy for them winning a championship with you pretty much and etching themselves in history with you norbs etching themselves in history but then the expectations are there now and now that without you because yo yo i paid i called you tyrian lannister like you you fuck hold on you watch game of thrones i started through i never finished oh okay okay but you still should watch that shit though norbs because that shit that that's that that's a good ass blog that's a good ass blog and i'll watch i'll hit you back i'm gonna get this nigga started i'll watch it right now all right i'm gonna send it uh maybe like nothing i make is over 12 minutes so it's probably like eight or nine minutes maybe i'll send it to you okay i gotta put this nigga he follow me", 'role': 'user'}], 'model': 'gpt-3.5-turbo-16k', 'max_tokens': 1000, 'n': 1, 'stream': False, 'temperature': 0.7}}
DEBUG:openai._base_client:Sending HTTP Request: POST https://api.openai.com/v1/chat/completions
DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>
DEBUG:httpcore.http11:send_request_headers.complete
DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>
DEBUG:httpcore.http11:send_request_body.complete
DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>
DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Mon, 08 Jul 2024 07:25:01 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'openai-organization', b'machinekingsmedia'), (b'openai-processing-ms', b'268'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains'), (b'x-ratelimit-limit-requests', b'10000'), (b'x-ratelimit-limit-tokens', b'1000000'), (b'x-ratelimit-remaining-requests', b'9999'), (b'x-ratelimit-remaining-tokens', b'998743'), (b'x-ratelimit-reset-requests', b'6ms'), (b'x-ratelimit-reset-tokens', b'75ms'), (b'x-request-id', b'req_4f7e691779fb8509c2b558f70d91da22'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'89fe5684399f0594-IAD'), (b'Content-Encoding', b'br'), (b'alt-svc', b'h3=":443"; ma=86400')])
INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>
DEBUG:httpcore.http11:receive_response_body.complete
DEBUG:httpcore.http11:response_closed.started
DEBUG:httpcore.http11:response_closed.complete
DEBUG:openai._base_client:HTTP Response: POST https://api.openai.com/v1/chat/completions "200 OK" Headers({'date': 'Mon, 08 Jul 2024 07:25:01 GMT', 'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'openai-organization': 'machinekingsmedia', 'openai-processing-ms': '268', 'openai-version': '2020-10-01', 'strict-transport-security': 'max-age=31536000; includeSubDomains', 'x-ratelimit-limit-requests': '10000', 'x-ratelimit-limit-tokens': '1000000', 'x-ratelimit-remaining-requests': '9999', 'x-ratelimit-remaining-tokens': '998743', 'x-ratelimit-reset-requests': '6ms', 'x-ratelimit-reset-tokens': '75ms', 'x-request-id': 'req_4f7e691779fb8509c2b558f70d91da22', 'cf-cache-status': 'DYNAMIC', 'server': 'cloudflare', 'cf-ray': '89fe5684399f0594-IAD', 'content-encoding': 'br', 'alt-svc': 'h3=":443"; ma=86400'})
DEBUG:openai._base_client:request_id: req_4f7e691779fb8509c2b558f70d91da22
DEBUG:openai._base_client:Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'json_data': {'messages': [{'content': "Generate a short, descriptive title for the following chapter content: around all right my my baby girl i'm gonna be putting her to bed at any moment so um if not then i'll hit you later on tonight all right", 'role': 'user'}], 'model': 'gpt-3.5-turbo-16k', 'max_tokens': 1000, 'n': 1, 'stream': False, 'temperature': 0.7}}
DEBUG:openai._base_client:Sending HTTP Request: POST https://api.openai.com/v1/chat/completions
DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>
DEBUG:httpcore.http11:send_request_headers.complete
DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>
DEBUG:httpcore.http11:send_request_body.complete
DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>
DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Mon, 08 Jul 2024 07:25:02 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'openai-organization', b'machinekingsmedia'), (b'openai-processing-ms', b'233'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains'), (b'x-ratelimit-limit-requests', b'10000'), (b'x-ratelimit-limit-tokens', b'1000000'), (b'x-ratelimit-remaining-requests', b'9999'), (b'x-ratelimit-remaining-tokens', b'998947'), (b'x-ratelimit-reset-requests', b'6ms'), (b'x-ratelimit-reset-tokens', b'63ms'), (b'x-request-id', b'req_918259fad89f692a07cacec60396be01'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'89fe56869b200594-IAD'), (b'Content-Encoding', b'br'), (b'alt-svc', b'h3=":443"; ma=86400')])
INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>
DEBUG:httpcore.http11:receive_response_body.complete
DEBUG:httpcore.http11:response_closed.started
DEBUG:httpcore.http11:response_closed.complete
DEBUG:openai._base_client:HTTP Response: POST https://api.openai.com/v1/chat/completions "200 OK" Headers({'date': 'Mon, 08 Jul 2024 07:25:02 GMT', 'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'openai-organization': 'machinekingsmedia', 'openai-processing-ms': '233', 'openai-version': '2020-10-01', 'strict-transport-security': 'max-age=31536000; includeSubDomains', 'x-ratelimit-limit-requests': '10000', 'x-ratelimit-limit-tokens': '1000000', 'x-ratelimit-remaining-requests': '9999', 'x-ratelimit-remaining-tokens': '998947', 'x-ratelimit-reset-requests': '6ms', 'x-ratelimit-reset-tokens': '63ms', 'x-request-id': 'req_918259fad89f692a07cacec60396be01', 'cf-cache-status': 'DYNAMIC', 'server': 'cloudflare', 'cf-ray': '89fe56869b200594-IAD', 'content-encoding': 'br', 'alt-svc': 'h3=":443"; ma=86400'})
DEBUG:openai._base_client:request_id: req_918259fad89f692a07cacec60396be01
DEBUG:openai._base_client:Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'json_data': {'messages': [{'content': "Summarize the following chapter in one sentence: I would think though that if if they wanted to if they wanted to they could easily decide you know what we done with battle rap we gonna just collect the revenue from the from the app until it until whenever but the app always it's gonna always slowly decline but they gonna continue to get something from it and I would just I would just guess that it was slowly decline because of I forget what that word is called but people gonna slowly unsubscribe and they don't really have a lot of pressure to put new content on there like it seems because people want to look at it's so many battles up there they don't want to pay for shit bro yeah so what I was what I was writing is exactly right they've had they've had that app revenue they just don't want to", 'role': 'user'}], 'model': 'gpt-3.5-turbo-16k', 'max_tokens': 1000, 'n': 1, 'stream': False, 'temperature': 0.7}}
DEBUG:openai._base_client:Sending HTTP Request: POST https://api.openai.com/v1/chat/completions
DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>
DEBUG:httpcore.http11:send_request_headers.complete
DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>
DEBUG:httpcore.http11:send_request_body.complete
DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>
DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Mon, 08 Jul 2024 07:25:04 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'openai-organization', b'machinekingsmedia'), (b'openai-processing-ms', b'1075'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains'), (b'x-ratelimit-limit-requests', b'10000'), (b'x-ratelimit-limit-tokens', b'1000000'), (b'x-ratelimit-remaining-requests', b'9999'), (b'x-ratelimit-remaining-tokens', b'998797'), (b'x-ratelimit-reset-requests', b'6ms'), (b'x-ratelimit-reset-tokens', b'72ms'), (b'x-request-id', b'req_286abf512d6c9b3ee3685d9ce0ad8785'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'89fe5688cc700594-IAD'), (b'Content-Encoding', b'br'), (b'alt-svc', b'h3=":443"; ma=86400')])
INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>
DEBUG:httpcore.http11:receive_response_body.complete
DEBUG:httpcore.http11:response_closed.started
DEBUG:httpcore.http11:response_closed.complete
DEBUG:openai._base_client:HTTP Response: POST https://api.openai.com/v1/chat/completions "200 OK" Headers({'date': 'Mon, 08 Jul 2024 07:25:04 GMT', 'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'openai-organization': 'machinekingsmedia', 'openai-processing-ms': '1075', 'openai-version': '2020-10-01', 'strict-transport-security': 'max-age=31536000; includeSubDomains', 'x-ratelimit-limit-requests': '10000', 'x-ratelimit-limit-tokens': '1000000', 'x-ratelimit-remaining-requests': '9999', 'x-ratelimit-remaining-tokens': '998797', 'x-ratelimit-reset-requests': '6ms', 'x-ratelimit-reset-tokens': '72ms', 'x-request-id': 'req_286abf512d6c9b3ee3685d9ce0ad8785', 'cf-cache-status': 'DYNAMIC', 'server': 'cloudflare', 'cf-ray': '89fe5688cc700594-IAD', 'content-encoding': 'br', 'alt-svc': 'h3=":443"; ma=86400'})
DEBUG:openai._base_client:request_id: req_286abf512d6c9b3ee3685d9ce0ad8785
DEBUG:openai._base_client:Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'json_data': {'messages': [{'content': "Summarize the following chapter in one sentence: use the app revenue you think they want to start now yeah Harry and then the transition over to caffeine yeah yeah oh these niggas are greedy but they want to pay me what you think they want to pay these bum ass niggas these cats and it's like battle rap if you had like tears and rap and hip-hop right battle rap would be the welfare division these niggas are happy to get $3,000 to rap but that's the only thing they're gonna get they're gonna pay their phone bill by weed by liquor and maybe buy some milk for their mom cuz that's what he most like it's just one of them things bro like it's just the lowest class of individuals you'll ever find and I'm not even also hey I'm keeping it real bro yeah yeah yeah but it was like they gave him a cake he was happy yeah yeah that's", 'role': 'user'}], 'model': 'gpt-3.5-turbo-16k', 'max_tokens': 1000, 'n': 1, 'stream': False, 'temperature': 0.7}}
DEBUG:openai._base_client:Sending HTTP Request: POST https://api.openai.com/v1/chat/completions
DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>
DEBUG:httpcore.http11:send_request_headers.complete
DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>
DEBUG:httpcore.http11:send_request_body.complete
DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>
DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Mon, 08 Jul 2024 07:25:04 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'openai-organization', b'machinekingsmedia'), (b'openai-processing-ms', b'375'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains'), (b'x-ratelimit-limit-requests', b'10000'), (b'x-ratelimit-limit-tokens', b'1000000'), (b'x-ratelimit-remaining-requests', b'9999'), (b'x-ratelimit-remaining-tokens', b'998790'), (b'x-ratelimit-reset-requests', b'6ms'), (b'x-ratelimit-reset-tokens', b'72ms'), (b'x-request-id', b'req_e522ac1e0c86df87b322452108303a60'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'89fe56951b4b0594-IAD'), (b'Content-Encoding', b'br'), (b'alt-svc', b'h3=":443"; ma=86400')])
INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>
DEBUG:httpcore.http11:receive_response_body.complete
DEBUG:httpcore.http11:response_closed.started
DEBUG:httpcore.http11:response_closed.complete
DEBUG:openai._base_client:HTTP Response: POST https://api.openai.com/v1/chat/completions "200 OK" Headers({'date': 'Mon, 08 Jul 2024 07:25:04 GMT', 'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'openai-organization': 'machinekingsmedia', 'openai-processing-ms': '375', 'openai-version': '2020-10-01', 'strict-transport-security': 'max-age=31536000; includeSubDomains', 'x-ratelimit-limit-requests': '10000', 'x-ratelimit-limit-tokens': '1000000', 'x-ratelimit-remaining-requests': '9999', 'x-ratelimit-remaining-tokens': '998790', 'x-ratelimit-reset-requests': '6ms', 'x-ratelimit-reset-tokens': '72ms', 'x-request-id': 'req_e522ac1e0c86df87b322452108303a60', 'cf-cache-status': 'DYNAMIC', 'server': 'cloudflare', 'cf-ray': '89fe56951b4b0594-IAD', 'content-encoding': 'br', 'alt-svc': 'h3=":443"; ma=86400'})
DEBUG:openai._base_client:request_id: req_e522ac1e0c86df87b322452108303a60
DEBUG:openai._base_client:Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'json_data': {'messages': [{'content': "Summarize the following chapter in one sentence: pretty sad though but we kind of knew I mean this sound crazy but we kind of knew what we kind of knew T-Rox level when we looked at his girl we kind of he ain't got no location he had a store that he said I'm gonna pull up you was gonna pull up to a project are you hungry order now let's get one wing Street one size I mean what restaurant you know has one side it's a new business my nigga what business you know sells one type of window one type of chicken one type of anything my nigga oh but that's the name let's get one wing straight there we have one one type of wing we deliver nigga you know where's the location we deliver motherfucker this would put the address of the location he's selling it from his house", 'role': 'user'}], 'model': 'gpt-3.5-turbo-16k', 'max_tokens': 1000, 'n': 1, 'stream': False, 'temperature': 0.7}}
DEBUG:openai._base_client:Sending HTTP Request: POST https://api.openai.com/v1/chat/completions
DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>
DEBUG:httpcore.http11:send_request_headers.complete
DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>
DEBUG:httpcore.http11:send_request_body.complete
DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>
DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Mon, 08 Jul 2024 07:25:06 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'openai-organization', b'machinekingsmedia'), (b'openai-processing-ms', b'641'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains'), (b'x-ratelimit-limit-requests', b'10000'), (b'x-ratelimit-limit-tokens', b'1000000'), (b'x-ratelimit-remaining-requests', b'9999'), (b'x-ratelimit-remaining-tokens', b'998806'), (b'x-ratelimit-reset-requests', b'6ms'), (b'x-ratelimit-reset-tokens', b'71ms'), (b'x-request-id', b'req_be042e0d16e8ee6d7556e8172e984340'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'89fe5698edbc0594-IAD'), (b'Content-Encoding', b'br'), (b'alt-svc', b'h3=":443"; ma=86400')])
INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>
DEBUG:httpcore.http11:receive_response_body.complete
DEBUG:httpcore.http11:response_closed.started
DEBUG:httpcore.http11:response_closed.complete
DEBUG:openai._base_client:HTTP Response: POST https://api.openai.com/v1/chat/completions "200 OK" Headers({'date': 'Mon, 08 Jul 2024 07:25:06 GMT', 'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'openai-organization': 'machinekingsmedia', 'openai-processing-ms': '641', 'openai-version': '2020-10-01', 'strict-transport-security': 'max-age=31536000; includeSubDomains', 'x-ratelimit-limit-requests': '10000', 'x-ratelimit-limit-tokens': '1000000', 'x-ratelimit-remaining-requests': '9999', 'x-ratelimit-remaining-tokens': '998806', 'x-ratelimit-reset-requests': '6ms', 'x-ratelimit-reset-tokens': '71ms', 'x-request-id': 'req_be042e0d16e8ee6d7556e8172e984340', 'cf-cache-status': 'DYNAMIC', 'server': 'cloudflare', 'cf-ray': '89fe5698edbc0594-IAD', 'content-encoding': 'br', 'alt-svc': 'h3=":443"; ma=86400'})
DEBUG:openai._base_client:request_id: req_be042e0d16e8ee6d7556e8172e984340
DEBUG:openai._base_client:Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'json_data': {'messages': [{'content': "Summarize the following chapter in one sentence: no yeah I'm looking at it yeah I don't see I don't see address on somebody in the council you gotta put more rice on that joint oh somebody else posted some wings or did this look pretty good and I'm looking at but oh yeah that is yeah you're right this is yeah so this is sugar in the window he got the money in his hand the wings right this on the spot the video is called on the spot review and then you see a house right there and then when he flip it around it's another camera he might want to be careful you probably need a lot of business license to do that out of the out of your own house yeah that's that's kind of crazy that is kind of yeah but that's different man that's different", 'role': 'user'}], 'model': 'gpt-3.5-turbo-16k', 'max_tokens': 1000, 'n': 1, 'stream': False, 'temperature': 0.7}}
DEBUG:openai._base_client:Sending HTTP Request: POST https://api.openai.com/v1/chat/completions
DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>
DEBUG:httpcore.http11:send_request_headers.complete
DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>
DEBUG:httpcore.http11:send_request_body.complete
DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>
DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Mon, 08 Jul 2024 07:25:07 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'openai-organization', b'machinekingsmedia'), (b'openai-processing-ms', b'553'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains'), (b'x-ratelimit-limit-requests', b'10000'), (b'x-ratelimit-limit-tokens', b'1000000'), (b'x-ratelimit-remaining-requests', b'9999'), (b'x-ratelimit-remaining-tokens', b'998813'), (b'x-ratelimit-reset-requests', b'6ms'), (b'x-ratelimit-reset-tokens', b'71ms'), (b'x-request-id', b'req_22902812367e73edcc618e38930c69ff'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'89fe56a27b8c0594-IAD'), (b'Content-Encoding', b'br'), (b'alt-svc', b'h3=":443"; ma=86400')])
INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>
DEBUG:httpcore.http11:receive_response_body.complete
DEBUG:httpcore.http11:response_closed.started
DEBUG:httpcore.http11:response_closed.complete
DEBUG:openai._base_client:HTTP Response: POST https://api.openai.com/v1/chat/completions "200 OK" Headers({'date': 'Mon, 08 Jul 2024 07:25:07 GMT', 'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'openai-organization': 'machinekingsmedia', 'openai-processing-ms': '553', 'openai-version': '2020-10-01', 'strict-transport-security': 'max-age=31536000; includeSubDomains', 'x-ratelimit-limit-requests': '10000', 'x-ratelimit-limit-tokens': '1000000', 'x-ratelimit-remaining-requests': '9999', 'x-ratelimit-remaining-tokens': '998813', 'x-ratelimit-reset-requests': '6ms', 'x-ratelimit-reset-tokens': '71ms', 'x-request-id': 'req_22902812367e73edcc618e38930c69ff', 'cf-cache-status': 'DYNAMIC', 'server': 'cloudflare', 'cf-ray': '89fe56a27b8c0594-IAD', 'content-encoding': 'br', 'alt-svc': 'h3=":443"; ma=86400'})
DEBUG:openai._base_client:request_id: req_22902812367e73edcc618e38930c69ff
DEBUG:openai._base_client:Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'json_data': {'messages': [{'content': "Summarize the following chapter in one sentence: location what do fuck niggas that we deliver right but you should notice go to his crib yeah yes that's crazy man", 'role': 'user'}], 'model': 'gpt-3.5-turbo-16k', 'max_tokens': 1000, 'n': 1, 'stream': False, 'temperature': 0.7}}
DEBUG:openai._base_client:Sending HTTP Request: POST https://api.openai.com/v1/chat/completions
DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>
DEBUG:httpcore.http11:send_request_headers.complete
DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>
DEBUG:httpcore.http11:send_request_body.complete
DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>
DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Mon, 08 Jul 2024 07:25:07 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'openai-organization', b'machinekingsmedia'), (b'openai-processing-ms', b'475'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains'), (b'x-ratelimit-limit-requests', b'10000'), (b'x-ratelimit-limit-tokens', b'1000000'), (b'x-ratelimit-remaining-requests', b'9999'), (b'x-ratelimit-remaining-tokens', b'998957'), (b'x-ratelimit-reset-requests', b'6ms'), (b'x-ratelimit-reset-tokens', b'62ms'), (b'x-request-id', b'req_1a65a0f7c8829b61020f059353c5df55'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'89fe56a7fe800594-IAD'), (b'Content-Encoding', b'br'), (b'alt-svc', b'h3=":443"; ma=86400')])
INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>
DEBUG:httpcore.http11:receive_response_body.complete
DEBUG:httpcore.http11:response_closed.started
DEBUG:httpcore.http11:response_closed.complete
DEBUG:openai._base_client:HTTP Response: POST https://api.openai.com/v1/chat/completions "200 OK" Headers({'date': 'Mon, 08 Jul 2024 07:25:07 GMT', 'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'openai-organization': 'machinekingsmedia', 'openai-processing-ms': '475', 'openai-version': '2020-10-01', 'strict-transport-security': 'max-age=31536000; includeSubDomains', 'x-ratelimit-limit-requests': '10000', 'x-ratelimit-limit-tokens': '1000000', 'x-ratelimit-remaining-requests': '9999', 'x-ratelimit-remaining-tokens': '998957', 'x-ratelimit-reset-requests': '6ms', 'x-ratelimit-reset-tokens': '62ms', 'x-request-id': 'req_1a65a0f7c8829b61020f059353c5df55', 'cf-cache-status': 'DYNAMIC', 'server': 'cloudflare', 'cf-ray': '89fe56a7fe800594-IAD', 'content-encoding': 'br', 'alt-svc': 'h3=":443"; ma=86400'})
DEBUG:openai._base_client:request_id: req_1a65a0f7c8829b61020f059353c5df55
DEBUG:openai._base_client:Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'json_data': {'messages': [{'content': "Provide a brief summary of the following transcript: File: 240204_2138.mp3\nRecording Date: 2024-02-04\nDuration: 00:07:26.36\nTranscription Date: 2024-07-07 06:17:37\n\n1\n00:00:00,000 --> 00:00:12,600\nI would think though that if if they wanted to if they wanted to they could\n\n2\n00:00:12,600 --> 00:00:19,799\neasily decide you know what we done with battle rap we gonna just collect the\n\n3\n00:00:19,799 --> 00:00:26,200\nrevenue from the from the app until it until whenever but the app always it's\n\n4\n00:00:26,200 --> 00:00:31,200\ngonna always slowly decline but they gonna continue to get something from it\n\n5\n00:00:31,200 --> 00:00:35,799\nand I would just I would just guess that it was slowly decline because of I\n\n6\n00:00:35,799 --> 00:00:41,639\nforget what that word is called but people gonna slowly unsubscribe and they\n\n7\n00:00:41,639 --> 00:00:47,200\ndon't really have a lot of pressure to put new content on there like it seems\n\n8\n00:00:47,200 --> 00:00:53,680\nbecause people want to look at it's so many battles up there\n\n9\n00:00:53,680 --> 00:01:14,400\nthey don't want to pay for shit bro yeah so what I was what I was writing is\n\n10\n00:01:14,400 --> 00:01:19,519\nexactly right they've had they've had that app revenue they just don't want to\n\n11\n00:01:19,519 --> 00:01:32,680\nuse the app revenue you think they want to start now yeah Harry and then the\n\n12\n00:01:32,680 --> 00:01:38,639\ntransition over to caffeine yeah yeah oh these niggas are greedy but they want to\n\n13\n00:01:38,639 --> 00:01:43,680\npay me what you think they want to pay these bum ass niggas these cats and it's\n\n14\n00:01:43,680 --> 00:01:50,839\nlike battle rap if you had like tears and rap and hip-hop right battle rap\n\n15\n00:01:50,839 --> 00:01:57,559\nwould be the welfare division these niggas are happy to get $3,000 to rap\n\n16\n00:01:57,559 --> 00:02:01,080\nbut that's the only thing they're gonna get they're gonna pay their phone bill\n\n17\n00:02:01,080 --> 00:02:06,440\nby weed by liquor and maybe buy some milk for their mom cuz that's what he\n\n18\n00:02:07,000 --> 00:02:12,440\nmost like it's just one of them things bro like it's just the lowest class of\n\n19\n00:02:12,440 --> 00:02:19,720\nindividuals you'll ever find and I'm not even also hey I'm keeping it real bro\n\n20\n00:02:20,839 --> 00:02:28,639\nyeah yeah yeah but it was like they gave him a cake he was happy yeah yeah that's\n\n21\n00:02:28,639 --> 00:02:36,960\npretty sad though but we kind of knew I mean this sound crazy but we kind of\n\n22\n00:02:36,960 --> 00:02:41,679\nknew what we kind of knew T-Rox level when we looked at his girl we kind of\n\n23\n00:03:11,679 --> 00:03:16,600\nhe ain't got no location he had a store that he said I'm gonna pull up you was\n\n24\n00:03:16,600 --> 00:03:33,720\ngonna pull up to a project are you hungry order now let's get one wing\n\n25\n00:03:33,720 --> 00:03:42,320\nStreet one size I mean what restaurant you know has one side it's a new\n\n26\n00:03:42,320 --> 00:03:49,800\nbusiness my nigga what business you know sells one type of window one type of\n\n27\n00:03:49,800 --> 00:03:57,279\nchicken one type of anything my nigga oh but that's the name let's get one wing\n\n28\n00:03:57,339 --> 00:04:03,020\nstraight there we have one one type of wing\n\n29\n00:04:07,500 --> 00:04:14,500\nwe deliver nigga you know where's the location we deliver motherfucker\n\n30\n00:04:17,500 --> 00:04:22,519\nthis would put the address of the location he's selling it from his house\n\n31\n00:04:22,519 --> 00:04:28,600\nno yeah I'm looking at it yeah I don't see I don't see address on somebody in\n\n32\n00:04:28,600 --> 00:04:33,519\nthe council you gotta put more rice on that joint\n\n33\n00:04:52,519 --> 00:04:58,519\noh somebody else posted some wings or did this look pretty good and I'm\n\n34\n00:04:58,519 --> 00:05:21,480\nlooking at but oh yeah that is yeah you're right this is yeah so this is\n\n35\n00:05:21,480 --> 00:05:25,600\nsugar in the window he got the money in his hand the wings right this on the\n\n36\n00:05:25,600 --> 00:05:29,880\nspot the video is called on the spot review and then you see a h", 'role': 'user'}], 'model': 'gpt-3.5-turbo-16k', 'max_tokens': 1000, 'n': 1, 'stream': False, 'temperature': 0.7}}
DEBUG:openai._base_client:Sending HTTP Request: POST https://api.openai.com/v1/chat/completions
DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>
DEBUG:httpcore.http11:send_request_headers.complete
DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>
DEBUG:httpcore.http11:send_request_body.complete
DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>
DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Mon, 08 Jul 2024 07:25:10 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'openai-organization', b'machinekingsmedia'), (b'openai-processing-ms', b'1651'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains'), (b'x-ratelimit-limit-requests', b'10000'), (b'x-ratelimit-limit-tokens', b'1000000'), (b'x-ratelimit-remaining-requests', b'9999'), (b'x-ratelimit-remaining-tokens', b'997984'), (b'x-ratelimit-reset-requests', b'6ms'), (b'x-ratelimit-reset-tokens', b'120ms'), (b'x-request-id', b'req_ed6cda737f1593b2f3cd272ad529af00'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'89fe56aba87c0594-IAD'), (b'Content-Encoding', b'br'), (b'alt-svc', b'h3=":443"; ma=86400')])
INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>
DEBUG:httpcore.http11:receive_response_body.complete
DEBUG:httpcore.http11:response_closed.started
DEBUG:httpcore.http11:response_closed.complete
DEBUG:openai._base_client:HTTP Response: POST https://api.openai.com/v1/chat/completions "200 OK" Headers({'date': 'Mon, 08 Jul 2024 07:25:10 GMT', 'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'openai-organization': 'machinekingsmedia', 'openai-processing-ms': '1651', 'openai-version': '2020-10-01', 'strict-transport-security': 'max-age=31536000; includeSubDomains', 'x-ratelimit-limit-requests': '10000', 'x-ratelimit-limit-tokens': '1000000', 'x-ratelimit-remaining-requests': '9999', 'x-ratelimit-remaining-tokens': '997984', 'x-ratelimit-reset-requests': '6ms', 'x-ratelimit-reset-tokens': '120ms', 'x-request-id': 'req_ed6cda737f1593b2f3cd272ad529af00', 'cf-cache-status': 'DYNAMIC', 'server': 'cloudflare', 'cf-ray': '89fe56aba87c0594-IAD', 'content-encoding': 'br', 'alt-svc': 'h3=":443"; ma=86400'})
DEBUG:openai._base_client:request_id: req_ed6cda737f1593b2f3cd272ad529af00
DEBUG:openai._base_client:Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'json_data': {'messages': [{'content': "Generate a short, descriptive title for the following chapter content: I would think though that if if they wanted to if they wanted to they could easily decide you know what we done with battle rap we gonna just collect the revenue from the from the app until it until whenever but the app always it's gonna always slowly decline but they gonna continue to get something from it and I would just I would just guess that it was slowly decline because of I forget what that word is called but people gonna slowly unsubscribe and they don't really have a lot of pressure to put new content on there like it seems because people want to look at it's so many battles up there they don't want to pay for shit bro yeah so what I was what I was writing is exactly right they've had they've had that app revenue they just don't want to", 'role': 'user'}], 'model': 'gpt-3.5-turbo-16k', 'max_tokens': 1000, 'n': 1, 'stream': False, 'temperature': 0.7}}
DEBUG:openai._base_client:Sending HTTP Request: POST https://api.openai.com/v1/chat/completions
DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>
DEBUG:httpcore.http11:send_request_headers.complete
DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>
DEBUG:httpcore.http11:send_request_body.complete
DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>
DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Mon, 08 Jul 2024 07:25:11 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'openai-organization', b'machinekingsmedia'), (b'openai-processing-ms', b'210'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains'), (b'x-ratelimit-limit-requests', b'10000'), (b'x-ratelimit-limit-tokens', b'1000000'), (b'x-ratelimit-remaining-requests', b'9999'), (b'x-ratelimit-remaining-tokens', b'998792'), (b'x-ratelimit-reset-requests', b'6ms'), (b'x-ratelimit-reset-tokens', b'72ms'), (b'x-request-id', b'req_ac0c37998592302999493e0d47b428e2'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'89fe56bcda6c0594-IAD'), (b'Content-Encoding', b'br'), (b'alt-svc', b'h3=":443"; ma=86400')])
INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>
DEBUG:httpcore.http11:receive_response_body.complete
DEBUG:httpcore.http11:response_closed.started
DEBUG:httpcore.http11:response_closed.complete
DEBUG:openai._base_client:HTTP Response: POST https://api.openai.com/v1/chat/completions "200 OK" Headers({'date': 'Mon, 08 Jul 2024 07:25:11 GMT', 'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'openai-organization': 'machinekingsmedia', 'openai-processing-ms': '210', 'openai-version': '2020-10-01', 'strict-transport-security': 'max-age=31536000; includeSubDomains', 'x-ratelimit-limit-requests': '10000', 'x-ratelimit-limit-tokens': '1000000', 'x-ratelimit-remaining-requests': '9999', 'x-ratelimit-remaining-tokens': '998792', 'x-ratelimit-reset-requests': '6ms', 'x-ratelimit-reset-tokens': '72ms', 'x-request-id': 'req_ac0c37998592302999493e0d47b428e2', 'cf-cache-status': 'DYNAMIC', 'server': 'cloudflare', 'cf-ray': '89fe56bcda6c0594-IAD', 'content-encoding': 'br', 'alt-svc': 'h3=":443"; ma=86400'})
DEBUG:openai._base_client:request_id: req_ac0c37998592302999493e0d47b428e2
DEBUG:openai._base_client:Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'json_data': {'messages': [{'content': "Generate a short, descriptive title for the following chapter content: use the app revenue you think they want to start now yeah Harry and then the transition over to caffeine yeah yeah oh these niggas are greedy but they want to pay me what you think they want to pay these bum ass niggas these cats and it's like battle rap if you had like tears and rap and hip-hop right battle rap would be the welfare division these niggas are happy to get $3,000 to rap but that's the only thing they're gonna get they're gonna pay their phone bill by weed by liquor and maybe buy some milk for their mom cuz that's what he most like it's just one of them things bro like it's just the lowest class of individuals you'll ever find and I'm not even also hey I'm keeping it real bro yeah yeah yeah but it was like they gave him a cake he was happy yeah yeah that's", 'role': 'user'}], 'model': 'gpt-3.5-turbo-16k', 'max_tokens': 1000, 'n': 1, 'stream': False, 'temperature': 0.7}}
DEBUG:openai._base_client:Sending HTTP Request: POST https://api.openai.com/v1/chat/completions
DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>
DEBUG:httpcore.http11:send_request_headers.complete
DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>
DEBUG:httpcore.http11:send_request_body.complete
DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>
DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Mon, 08 Jul 2024 07:25:11 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'openai-organization', b'machinekingsmedia'), (b'openai-processing-ms', b'239'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains'), (b'x-ratelimit-limit-requests', b'10000'), (b'x-ratelimit-limit-tokens', b'1000000'), (b'x-ratelimit-remaining-requests', b'9999'), (b'x-ratelimit-remaining-tokens', b'998785'), (b'x-ratelimit-reset-requests', b'6ms'), (b'x-ratelimit-reset-tokens', b'72ms'), (b'x-request-id', b'req_9dffde87c855ad3b11d83e597689b83c'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'89fe56c38ecc0594-IAD'), (b'Content-Encoding', b'br'), (b'alt-svc', b'h3=":443"; ma=86400')])
INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>
DEBUG:httpcore.http11:receive_response_body.complete
DEBUG:httpcore.http11:response_closed.started
DEBUG:httpcore.http11:response_closed.complete
DEBUG:openai._base_client:HTTP Response: POST https://api.openai.com/v1/chat/completions "200 OK" Headers({'date': 'Mon, 08 Jul 2024 07:25:11 GMT', 'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'openai-organization': 'machinekingsmedia', 'openai-processing-ms': '239', 'openai-version': '2020-10-01', 'strict-transport-security': 'max-age=31536000; includeSubDomains', 'x-ratelimit-limit-requests': '10000', 'x-ratelimit-limit-tokens': '1000000', 'x-ratelimit-remaining-requests': '9999', 'x-ratelimit-remaining-tokens': '998785', 'x-ratelimit-reset-requests': '6ms', 'x-ratelimit-reset-tokens': '72ms', 'x-request-id': 'req_9dffde87c855ad3b11d83e597689b83c', 'cf-cache-status': 'DYNAMIC', 'server': 'cloudflare', 'cf-ray': '89fe56c38ecc0594-IAD', 'content-encoding': 'br', 'alt-svc': 'h3=":443"; ma=86400'})
DEBUG:openai._base_client:request_id: req_9dffde87c855ad3b11d83e597689b83c
DEBUG:openai._base_client:Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'json_data': {'messages': [{'content': "Generate a short, descriptive title for the following chapter content: pretty sad though but we kind of knew I mean this sound crazy but we kind of knew what we kind of knew T-Rox level when we looked at his girl we kind of he ain't got no location he had a store that he said I'm gonna pull up you was gonna pull up to a project are you hungry order now let's get one wing Street one size I mean what restaurant you know has one side it's a new business my nigga what business you know sells one type of window one type of chicken one type of anything my nigga oh but that's the name let's get one wing straight there we have one one type of wing we deliver nigga you know where's the location we deliver motherfucker this would put the address of the location he's selling it from his house", 'role': 'user'}], 'model': 'gpt-3.5-turbo-16k', 'max_tokens': 1000, 'n': 1, 'stream': False, 'temperature': 0.7}}
DEBUG:openai._base_client:Sending HTTP Request: POST https://api.openai.com/v1/chat/completions
DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>
DEBUG:httpcore.http11:send_request_headers.complete
DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>
DEBUG:httpcore.http11:send_request_body.complete
DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>
DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Mon, 08 Jul 2024 07:25:12 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'openai-organization', b'machinekingsmedia'), (b'openai-processing-ms', b'351'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains'), (b'x-ratelimit-limit-requests', b'10000'), (b'x-ratelimit-limit-tokens', b'1000000'), (b'x-ratelimit-remaining-requests', b'9999'), (b'x-ratelimit-remaining-tokens', b'998801'), (b'x-ratelimit-reset-requests', b'6ms'), (b'x-ratelimit-reset-tokens', b'71ms'), (b'x-request-id', b'req_2261159df538e68a249c5bc05e193322'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'89fe56c5c84b0594-IAD'), (b'Content-Encoding', b'br'), (b'alt-svc', b'h3=":443"; ma=86400')])
INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>
DEBUG:httpcore.http11:receive_response_body.complete
DEBUG:httpcore.http11:response_closed.started
DEBUG:httpcore.http11:response_closed.complete
DEBUG:openai._base_client:HTTP Response: POST https://api.openai.com/v1/chat/completions "200 OK" Headers({'date': 'Mon, 08 Jul 2024 07:25:12 GMT', 'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'openai-organization': 'machinekingsmedia', 'openai-processing-ms': '351', 'openai-version': '2020-10-01', 'strict-transport-security': 'max-age=31536000; includeSubDomains', 'x-ratelimit-limit-requests': '10000', 'x-ratelimit-limit-tokens': '1000000', 'x-ratelimit-remaining-requests': '9999', 'x-ratelimit-remaining-tokens': '998801', 'x-ratelimit-reset-requests': '6ms', 'x-ratelimit-reset-tokens': '71ms', 'x-request-id': 'req_2261159df538e68a249c5bc05e193322', 'cf-cache-status': 'DYNAMIC', 'server': 'cloudflare', 'cf-ray': '89fe56c5c84b0594-IAD', 'content-encoding': 'br', 'alt-svc': 'h3=":443"; ma=86400'})
DEBUG:openai._base_client:request_id: req_2261159df538e68a249c5bc05e193322
DEBUG:openai._base_client:Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'json_data': {'messages': [{'content': "Generate a short, descriptive title for the following chapter content: no yeah I'm looking at it yeah I don't see I don't see address on somebody in the council you gotta put more rice on that joint oh somebody else posted some wings or did this look pretty good and I'm looking at but oh yeah that is yeah you're right this is yeah so this is sugar in the window he got the money in his hand the wings right this on the spot the video is called on the spot review and then you see a house right there and then when he flip it around it's another camera he might want to be careful you probably need a lot of business license to do that out of the out of your own house yeah that's that's kind of crazy that is kind of yeah but that's different man that's different", 'role': 'user'}], 'model': 'gpt-3.5-turbo-16k', 'max_tokens': 1000, 'n': 1, 'stream': False, 'temperature': 0.7}}
DEBUG:openai._base_client:Sending HTTP Request: POST https://api.openai.com/v1/chat/completions
DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>
DEBUG:httpcore.http11:send_request_headers.complete
DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>
DEBUG:httpcore.http11:send_request_body.complete
DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>
DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Mon, 08 Jul 2024 07:25:12 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'openai-organization', b'machinekingsmedia'), (b'openai-processing-ms', b'275'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains'), (b'x-ratelimit-limit-requests', b'10000'), (b'x-ratelimit-limit-tokens', b'1000000'), (b'x-ratelimit-remaining-requests', b'9999'), (b'x-ratelimit-remaining-tokens', b'998806'), (b'x-ratelimit-reset-requests', b'6ms'), (b'x-ratelimit-reset-tokens', b'71ms'), (b'x-request-id', b'req_78792fe497a667ad10a6ac06df7cb062'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'89fe56c8a9e10594-IAD'), (b'Content-Encoding', b'br'), (b'alt-svc', b'h3=":443"; ma=86400')])
INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>
DEBUG:httpcore.http11:receive_response_body.complete
DEBUG:httpcore.http11:response_closed.started
DEBUG:httpcore.http11:response_closed.complete
DEBUG:openai._base_client:HTTP Response: POST https://api.openai.com/v1/chat/completions "200 OK" Headers({'date': 'Mon, 08 Jul 2024 07:25:12 GMT', 'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'openai-organization': 'machinekingsmedia', 'openai-processing-ms': '275', 'openai-version': '2020-10-01', 'strict-transport-security': 'max-age=31536000; includeSubDomains', 'x-ratelimit-limit-requests': '10000', 'x-ratelimit-limit-tokens': '1000000', 'x-ratelimit-remaining-requests': '9999', 'x-ratelimit-remaining-tokens': '998806', 'x-ratelimit-reset-requests': '6ms', 'x-ratelimit-reset-tokens': '71ms', 'x-request-id': 'req_78792fe497a667ad10a6ac06df7cb062', 'cf-cache-status': 'DYNAMIC', 'server': 'cloudflare', 'cf-ray': '89fe56c8a9e10594-IAD', 'content-encoding': 'br', 'alt-svc': 'h3=":443"; ma=86400'})
DEBUG:openai._base_client:request_id: req_78792fe497a667ad10a6ac06df7cb062
DEBUG:openai._base_client:Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'json_data': {'messages': [{'content': "Generate a short, descriptive title for the following chapter content: location what do fuck niggas that we deliver right but you should notice go to his crib yeah yes that's crazy man", 'role': 'user'}], 'model': 'gpt-3.5-turbo-16k', 'max_tokens': 1000, 'n': 1, 'stream': False, 'temperature': 0.7}}
DEBUG:openai._base_client:Sending HTTP Request: POST https://api.openai.com/v1/chat/completions
DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>
DEBUG:httpcore.http11:send_request_headers.complete
DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>
DEBUG:httpcore.http11:send_request_body.complete
DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>
DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Mon, 08 Jul 2024 07:25:14 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'openai-organization', b'machinekingsmedia'), (b'openai-processing-ms', b'588'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains'), (b'x-ratelimit-limit-requests', b'10000'), (b'x-ratelimit-limit-tokens', b'1000000'), (b'x-ratelimit-remaining-requests', b'9999'), (b'x-ratelimit-remaining-tokens', b'998952'), (b'x-ratelimit-reset-requests', b'6ms'), (b'x-ratelimit-reset-tokens', b'62ms'), (b'x-request-id', b'req_302815db9b6935dece34c68414f6ab9e'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'89fe56cbdbc60594-IAD'), (b'Content-Encoding', b'br'), (b'alt-svc', b'h3=":443"; ma=86400')])
INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>
DEBUG:httpcore.http11:receive_response_body.complete
DEBUG:httpcore.http11:response_closed.started
DEBUG:httpcore.http11:response_closed.complete
DEBUG:openai._base_client:HTTP Response: POST https://api.openai.com/v1/chat/completions "200 OK" Headers({'date': 'Mon, 08 Jul 2024 07:25:14 GMT', 'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'openai-organization': 'machinekingsmedia', 'openai-processing-ms': '588', 'openai-version': '2020-10-01', 'strict-transport-security': 'max-age=31536000; includeSubDomains', 'x-ratelimit-limit-requests': '10000', 'x-ratelimit-limit-tokens': '1000000', 'x-ratelimit-remaining-requests': '9999', 'x-ratelimit-remaining-tokens': '998952', 'x-ratelimit-reset-requests': '6ms', 'x-ratelimit-reset-tokens': '62ms', 'x-request-id': 'req_302815db9b6935dece34c68414f6ab9e', 'cf-cache-status': 'DYNAMIC', 'server': 'cloudflare', 'cf-ray': '89fe56cbdbc60594-IAD', 'content-encoding': 'br', 'alt-svc': 'h3=":443"; ma=86400'})
DEBUG:openai._base_client:request_id: req_302815db9b6935dece34c68414f6ab9e
DEBUG:openai._base_client:Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'json_data': {'messages': [{'content': 'Summarize the following chapter in one sentence: regarding the app You saying that? That having a subscription model is where you saw what what things kind of went wrong Once I caught the deal with caffeine, I would have to add free get as many subscribers as possible and then have the stream pay-per-view on the app So bring it Once you have Snapchat and these these Instagrams they get paid off for promotion They get paid off of being brought out by the Googles and stuff like that where they can put their ads So the more subscriptions you get the more', 'role': 'user'}], 'model': 'gpt-3.5-turbo-16k', 'max_tokens': 1000, 'n': 1, 'stream': False, 'temperature': 0.7}}
DEBUG:openai._base_client:Sending HTTP Request: POST https://api.openai.com/v1/chat/completions
DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>
DEBUG:httpcore.http11:send_request_headers.complete
DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>
DEBUG:httpcore.http11:send_request_body.complete
DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>
DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Mon, 08 Jul 2024 07:25:14 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'openai-organization', b'machinekingsmedia'), (b'openai-processing-ms', b'279'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains'), (b'x-ratelimit-limit-requests', b'10000'), (b'x-ratelimit-limit-tokens', b'1000000'), (b'x-ratelimit-remaining-requests', b'9999'), (b'x-ratelimit-remaining-tokens', b'998859'), (b'x-ratelimit-reset-requests', b'6ms'), (b'x-ratelimit-reset-tokens', b'68ms'), (b'x-request-id', b'req_12497e964c0e9d285ae51c5edf96c723'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'89fe56d4c8e40594-IAD'), (b'Content-Encoding', b'br'), (b'alt-svc', b'h3=":443"; ma=86400')])
INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>
DEBUG:httpcore.http11:receive_response_body.complete
DEBUG:httpcore.http11:response_closed.started
DEBUG:httpcore.http11:response_closed.complete
DEBUG:openai._base_client:HTTP Response: POST https://api.openai.com/v1/chat/completions "200 OK" Headers({'date': 'Mon, 08 Jul 2024 07:25:14 GMT', 'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'openai-organization': 'machinekingsmedia', 'openai-processing-ms': '279', 'openai-version': '2020-10-01', 'strict-transport-security': 'max-age=31536000; includeSubDomains', 'x-ratelimit-limit-requests': '10000', 'x-ratelimit-limit-tokens': '1000000', 'x-ratelimit-remaining-requests': '9999', 'x-ratelimit-remaining-tokens': '998859', 'x-ratelimit-reset-requests': '6ms', 'x-ratelimit-reset-tokens': '68ms', 'x-request-id': 'req_12497e964c0e9d285ae51c5edf96c723', 'cf-cache-status': 'DYNAMIC', 'server': 'cloudflare', 'cf-ray': '89fe56d4c8e40594-IAD', 'content-encoding': 'br', 'alt-svc': 'h3=":443"; ma=86400'})
DEBUG:openai._base_client:request_id: req_12497e964c0e9d285ae51c5edf96c723
DEBUG:openai._base_client:Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'json_data': {'messages': [{'content': "Summarize the following chapter in one sentence: Volume you get the model now you go to these networks, but I know I got 2 million subscribers on this app But they went nah We get $7.99 for these dudes, we good. Plus we getting paid from caffeine They didn't look at the bigger picture. If they would have got more subscribers, more subscribers, they can say yo We're gonna do a pay-per-view. You got 2-3 million people. All you need is 100,000 people to pay for that pay-per-view. At that point, you're good. Yeah The zone did. The zone did. They charge, right? And they could have got it could have put ads on there as well They could have got a Google to purchase them. Some of that. Because it's all about the traffic on there and what you do is you create the content on there as well", 'role': 'user'}], 'model': 'gpt-3.5-turbo-16k', 'max_tokens': 1000, 'n': 1, 'stream': False, 'temperature': 0.7}}
DEBUG:openai._base_client:Sending HTTP Request: POST https://api.openai.com/v1/chat/completions
DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>
DEBUG:httpcore.http11:send_request_headers.complete
DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>
DEBUG:httpcore.http11:send_request_body.complete
DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>
DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Mon, 08 Jul 2024 07:25:15 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'openai-organization', b'machinekingsmedia'), (b'openai-processing-ms', b'531'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains'), (b'x-ratelimit-limit-requests', b'10000'), (b'x-ratelimit-limit-tokens', b'1000000'), (b'x-ratelimit-remaining-requests', b'9999'), (b'x-ratelimit-remaining-tokens', b'998801'), (b'x-ratelimit-reset-requests', b'6ms'), (b'x-ratelimit-reset-tokens', b'71ms'), (b'x-request-id', b'req_ff4b52dd17a7886d9d2040a58d610ac8'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'89fe56d81a9b0594-IAD'), (b'Content-Encoding', b'br'), (b'alt-svc', b'h3=":443"; ma=86400')])
INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>
DEBUG:httpcore.http11:receive_response_body.complete
DEBUG:httpcore.http11:response_closed.started
DEBUG:httpcore.http11:response_closed.complete
DEBUG:openai._base_client:HTTP Response: POST https://api.openai.com/v1/chat/completions "200 OK" Headers({'date': 'Mon, 08 Jul 2024 07:25:15 GMT', 'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'openai-organization': 'machinekingsmedia', 'openai-processing-ms': '531', 'openai-version': '2020-10-01', 'strict-transport-security': 'max-age=31536000; includeSubDomains', 'x-ratelimit-limit-requests': '10000', 'x-ratelimit-limit-tokens': '1000000', 'x-ratelimit-remaining-requests': '9999', 'x-ratelimit-remaining-tokens': '998801', 'x-ratelimit-reset-requests': '6ms', 'x-ratelimit-reset-tokens': '71ms', 'x-request-id': 'req_ff4b52dd17a7886d9d2040a58d610ac8', 'cf-cache-status': 'DYNAMIC', 'server': 'cloudflare', 'cf-ray': '89fe56d81a9b0594-IAD', 'content-encoding': 'br', 'alt-svc': 'h3=":443"; ma=86400'})
DEBUG:openai._base_client:request_id: req_ff4b52dd17a7886d9d2040a58d610ac8
DEBUG:openai._base_client:Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'json_data': {'messages': [{'content': "Summarize the following chapter in one sentence: You know, it was just they just did not structure it smart And it was it was it wasn't bright bro, like So so the app is One thing no knobs, but the exit from YouTube is a completely different thing Let's start there, it's a free platform you open a free platform for that material Why would you didn't go charge when you know, I'm saying when and you also realize to you're the biggest but you don't everybody All these guys are spread out Fuck you, too. If you do tell me right now niggas a yo", 'role': 'user'}], 'model': 'gpt-3.5-turbo-16k', 'max_tokens': 1000, 'n': 1, 'stream': False, 'temperature': 0.7}}
DEBUG:openai._base_client:Sending HTTP Request: POST https://api.openai.com/v1/chat/completions
DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>
DEBUG:httpcore.http11:send_request_headers.complete
DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>
DEBUG:httpcore.http11:send_request_body.complete
DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>
DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Mon, 08 Jul 2024 07:25:16 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'openai-organization', b'machinekingsmedia'), (b'openai-processing-ms', b'766'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains'), (b'x-ratelimit-limit-requests', b'10000'), (b'x-ratelimit-limit-tokens', b'1000000'), (b'x-ratelimit-remaining-requests', b'9999'), (b'x-ratelimit-remaining-tokens', b'998863'), (b'x-ratelimit-reset-requests', b'6ms'), (b'x-ratelimit-reset-tokens', b'68ms'), (b'x-request-id', b'req_653c92644515299afc30be5b9b1ad90f'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'89fe56dcedf90594-IAD'), (b'Content-Encoding', b'br'), (b'alt-svc', b'h3=":443"; ma=86400')])
INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>
DEBUG:httpcore.http11:receive_response_body.complete
DEBUG:httpcore.http11:response_closed.started
DEBUG:httpcore.http11:response_closed.complete
DEBUG:openai._base_client:HTTP Response: POST https://api.openai.com/v1/chat/completions "200 OK" Headers({'date': 'Mon, 08 Jul 2024 07:25:16 GMT', 'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'openai-organization': 'machinekingsmedia', 'openai-processing-ms': '766', 'openai-version': '2020-10-01', 'strict-transport-security': 'max-age=31536000; includeSubDomains', 'x-ratelimit-limit-requests': '10000', 'x-ratelimit-limit-tokens': '1000000', 'x-ratelimit-remaining-requests': '9999', 'x-ratelimit-remaining-tokens': '998863', 'x-ratelimit-reset-requests': '6ms', 'x-ratelimit-reset-tokens': '68ms', 'x-request-id': 'req_653c92644515299afc30be5b9b1ad90f', 'cf-cache-status': 'DYNAMIC', 'server': 'cloudflare', 'cf-ray': '89fe56dcedf90594-IAD', 'content-encoding': 'br', 'alt-svc': 'h3=":443"; ma=86400'})
DEBUG:openai._base_client:request_id: req_653c92644515299afc30be5b9b1ad90f
DEBUG:openai._base_client:Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'json_data': {'messages': [{'content': "Summarize the following chapter in one sentence: We don't have we got you got a watch on that. It's free. You don't think They wanted just to watch it on YouTube You would have had a couple me you would have taken same subscribers from YouTube and probably got more But they got greedy they didn't market it well, then they also gonna have no material bro, you can't like the zone They don't have boxing every week, but they have shows on there They have things that you can watch outside to keep you engaged So the app doesn't have like behind the scene like Literally had DNA's wrestling show, which had nothing to do He had a podcast about wrestling on there", 'role': 'user'}], 'model': 'gpt-3.5-turbo-16k', 'max_tokens': 1000, 'n': 1, 'stream': False, 'temperature': 0.7}}
DEBUG:openai._base_client:Sending HTTP Request: POST https://api.openai.com/v1/chat/completions
DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>
DEBUG:httpcore.http11:send_request_headers.complete
DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>
DEBUG:httpcore.http11:send_request_body.complete
DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>
DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Mon, 08 Jul 2024 07:25:18 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'openai-organization', b'machinekingsmedia'), (b'openai-processing-ms', b'411'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains'), (b'x-ratelimit-limit-requests', b'10000'), (b'x-ratelimit-limit-tokens', b'1000000'), (b'x-ratelimit-remaining-requests', b'9999'), (b'x-ratelimit-remaining-tokens', b'998832'), (b'x-ratelimit-reset-requests', b'6ms'), (b'x-ratelimit-reset-tokens', b'70ms'), (b'x-request-id', b'req_9192e60f368b05b04ed043cb72703189'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'89fe56e3d9eb0594-IAD'), (b'Content-Encoding', b'br'), (b'alt-svc', b'h3=":443"; ma=86400')])
INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>
DEBUG:httpcore.http11:receive_response_body.complete
DEBUG:httpcore.http11:response_closed.started
DEBUG:httpcore.http11:response_closed.complete
DEBUG:openai._base_client:HTTP Response: POST https://api.openai.com/v1/chat/completions "200 OK" Headers({'date': 'Mon, 08 Jul 2024 07:25:18 GMT', 'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'openai-organization': 'machinekingsmedia', 'openai-processing-ms': '411', 'openai-version': '2020-10-01', 'strict-transport-security': 'max-age=31536000; includeSubDomains', 'x-ratelimit-limit-requests': '10000', 'x-ratelimit-limit-tokens': '1000000', 'x-ratelimit-remaining-requests': '9999', 'x-ratelimit-remaining-tokens': '998832', 'x-ratelimit-reset-requests': '6ms', 'x-ratelimit-reset-tokens': '70ms', 'x-request-id': 'req_9192e60f368b05b04ed043cb72703189', 'cf-cache-status': 'DYNAMIC', 'server': 'cloudflare', 'cf-ray': '89fe56e3d9eb0594-IAD', 'content-encoding': 'br', 'alt-svc': 'h3=":443"; ma=86400'})
DEBUG:openai._base_client:request_id: req_9192e60f368b05b04ed043cb72703189
DEBUG:openai._base_client:Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'json_data': {'messages': [{'content': "Summarize the following chapter in one sentence: by who Was DNA sure he was just or he would he would get wrestlers on the fucking app Oh, you told my the caffeine out me to my URL I Say hey, let me pay angry fan. Let me pay All these bloggers to debut their content on this fucking thing in order to get people to Tune in and then they can really sell YouTube afterwards What you do is you're drawing more people and if it's free and caffeine's pulling a bunch of together You're not paying for shit. You just join subscribers and subscribers and subscribers there What are you losing?", 'role': 'user'}], 'model': 'gpt-3.5-turbo-16k', 'max_tokens': 1000, 'n': 1, 'stream': False, 'temperature': 0.7}}
DEBUG:openai._base_client:Sending HTTP Request: POST https://api.openai.com/v1/chat/completions
DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>
DEBUG:httpcore.http11:send_request_headers.complete
DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>
DEBUG:httpcore.http11:send_request_body.complete
DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>
DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Mon, 08 Jul 2024 07:25:18 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'openai-organization', b'machinekingsmedia'), (b'openai-processing-ms', b'464'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains'), (b'x-ratelimit-limit-requests', b'10000'), (b'x-ratelimit-limit-tokens', b'1000000'), (b'x-ratelimit-remaining-requests', b'9999'), (b'x-ratelimit-remaining-tokens', b'998852'), (b'x-ratelimit-reset-requests', b'6ms'), (b'x-ratelimit-reset-tokens', b'68ms'), (b'x-request-id', b'req_9c570fd3e44cbb63ed534324e1debf22'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'89fe56ec8fc40594-IAD'), (b'Content-Encoding', b'br'), (b'alt-svc', b'h3=":443"; ma=86400')])
INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>
DEBUG:httpcore.http11:receive_response_body.complete
DEBUG:httpcore.http11:response_closed.started
DEBUG:httpcore.http11:response_closed.complete
DEBUG:openai._base_client:HTTP Response: POST https://api.openai.com/v1/chat/completions "200 OK" Headers({'date': 'Mon, 08 Jul 2024 07:25:18 GMT', 'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'openai-organization': 'machinekingsmedia', 'openai-processing-ms': '464', 'openai-version': '2020-10-01', 'strict-transport-security': 'max-age=31536000; includeSubDomains', 'x-ratelimit-limit-requests': '10000', 'x-ratelimit-limit-tokens': '1000000', 'x-ratelimit-remaining-requests': '9999', 'x-ratelimit-remaining-tokens': '998852', 'x-ratelimit-reset-requests': '6ms', 'x-ratelimit-reset-tokens': '68ms', 'x-request-id': 'req_9c570fd3e44cbb63ed534324e1debf22', 'cf-cache-status': 'DYNAMIC', 'server': 'cloudflare', 'cf-ray': '89fe56ec8fc40594-IAD', 'content-encoding': 'br', 'alt-svc': 'h3=":443"; ma=86400'})
DEBUG:openai._base_client:request_id: req_9c570fd3e44cbb63ed534324e1debf22
DEBUG:openai._base_client:Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'json_data': {'messages': [{'content': "Summarize the following chapter in one sentence: I really Stupid They should have did that and when they were away father. They should have took that to Netflix Netflix just signed multi-billion dollar deal with the WWE I Really hope I can find that twist tweet where he said smack just sold the culture when he made the the I don't know if it was the caffeine deal or the app or The the app deal or may have YouTube exclusivity But I I do feel as though right now. I mean that decision to go off YouTube and to take that money from the subscriptions and To stay off of YouTube at this point is basically killing", 'role': 'user'}], 'model': 'gpt-3.5-turbo-16k', 'max_tokens': 1000, 'n': 1, 'stream': False, 'temperature': 0.7}}
DEBUG:openai._base_client:Sending HTTP Request: POST https://api.openai.com/v1/chat/completions
DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>
DEBUG:httpcore.http11:send_request_headers.complete
DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>
DEBUG:httpcore.http11:send_request_body.complete
DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>
DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Mon, 08 Jul 2024 07:25:20 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'openai-organization', b'machinekingsmedia'), (b'openai-processing-ms', b'581'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains'), (b'x-ratelimit-limit-requests', b'10000'), (b'x-ratelimit-limit-tokens', b'1000000'), (b'x-ratelimit-remaining-requests', b'9999'), (b'x-ratelimit-remaining-tokens', b'998846'), (b'x-ratelimit-reset-requests', b'6ms'), (b'x-ratelimit-reset-tokens', b'69ms'), (b'x-request-id', b'req_307219879e2ab65a0cdd2f80fcf9951e'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'89fe56f0fa860594-IAD'), (b'Content-Encoding', b'br'), (b'alt-svc', b'h3=":443"; ma=86400')])
INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>
DEBUG:httpcore.http11:receive_response_body.complete
DEBUG:httpcore.http11:response_closed.started
DEBUG:httpcore.http11:response_closed.complete
DEBUG:openai._base_client:HTTP Response: POST https://api.openai.com/v1/chat/completions "200 OK" Headers({'date': 'Mon, 08 Jul 2024 07:25:20 GMT', 'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'openai-organization': 'machinekingsmedia', 'openai-processing-ms': '581', 'openai-version': '2020-10-01', 'strict-transport-security': 'max-age=31536000; includeSubDomains', 'x-ratelimit-limit-requests': '10000', 'x-ratelimit-limit-tokens': '1000000', 'x-ratelimit-remaining-requests': '9999', 'x-ratelimit-remaining-tokens': '998846', 'x-ratelimit-reset-requests': '6ms', 'x-ratelimit-reset-tokens': '69ms', 'x-request-id': 'req_307219879e2ab65a0cdd2f80fcf9951e', 'cf-cache-status': 'DYNAMIC', 'server': 'cloudflare', 'cf-ray': '89fe56f0fa860594-IAD', 'content-encoding': 'br', 'alt-svc': 'h3=":443"; ma=86400'})
DEBUG:openai._base_client:request_id: req_307219879e2ab65a0cdd2f80fcf9951e
DEBUG:openai._base_client:Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'json_data': {'messages': [{'content': 'Summarize the following chapter in one sentence: Any Killing that killing that company and like putting all of that history behind a paywall Why Would you then turn around and charge for the app when you could have became a tech company because once you open up you Actually become a tech company Become you become a tech company and tech companies are looked at differently But these niggas got me you could have went got ten, but they could have got up to like five six million subscribers Wait a minute, wait a minute. Wait a minute. They never they never share their subscriber numbers, right? We We good', 'role': 'user'}], 'model': 'gpt-3.5-turbo-16k', 'max_tokens': 1000, 'n': 1, 'stream': False, 'temperature': 0.7}}
DEBUG:openai._base_client:Sending HTTP Request: POST https://api.openai.com/v1/chat/completions
DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>
DEBUG:httpcore.http11:send_request_headers.complete
DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>
DEBUG:httpcore.http11:send_request_body.complete
DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>
DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Mon, 08 Jul 2024 07:25:20 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'openai-organization', b'machinekingsmedia'), (b'openai-processing-ms', b'446'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains'), (b'x-ratelimit-limit-requests', b'10000'), (b'x-ratelimit-limit-tokens', b'1000000'), (b'x-ratelimit-remaining-requests', b'9999'), (b'x-ratelimit-remaining-tokens', b'998847'), (b'x-ratelimit-reset-requests', b'6ms'), (b'x-ratelimit-reset-tokens', b'69ms'), (b'x-request-id', b'req_ec7b37e4949962b5365359174f658817'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'89fe56f83eef0594-IAD'), (b'Content-Encoding', b'br'), (b'alt-svc', b'h3=":443"; ma=86400')])
INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>
DEBUG:httpcore.http11:receive_response_body.complete
DEBUG:httpcore.http11:response_closed.started
DEBUG:httpcore.http11:response_closed.complete
DEBUG:openai._base_client:HTTP Response: POST https://api.openai.com/v1/chat/completions "200 OK" Headers({'date': 'Mon, 08 Jul 2024 07:25:20 GMT', 'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'openai-organization': 'machinekingsmedia', 'openai-processing-ms': '446', 'openai-version': '2020-10-01', 'strict-transport-security': 'max-age=31536000; includeSubDomains', 'x-ratelimit-limit-requests': '10000', 'x-ratelimit-limit-tokens': '1000000', 'x-ratelimit-remaining-requests': '9999', 'x-ratelimit-remaining-tokens': '998847', 'x-ratelimit-reset-requests': '6ms', 'x-ratelimit-reset-tokens': '69ms', 'x-request-id': 'req_ec7b37e4949962b5365359174f658817', 'cf-cache-status': 'DYNAMIC', 'server': 'cloudflare', 'cf-ray': '89fe56f83eef0594-IAD', 'content-encoding': 'br', 'alt-svc': 'h3=":443"; ma=86400'})
DEBUG:openai._base_client:request_id: req_ec7b37e4949962b5365359174f658817
DEBUG:openai._base_client:Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'json_data': {'messages': [{'content': "Summarize the following chapter in one sentence: But they are they also have to know that the subscriber numbers on this app are Declining over time. They aren't going up anymore because there's no way No way that they haven't already reached the peak numbers on that app because Huh? Numbers to get out because", 'role': 'user'}], 'model': 'gpt-3.5-turbo-16k', 'max_tokens': 1000, 'n': 1, 'stream': False, 'temperature': 0.7}}
DEBUG:openai._base_client:Sending HTTP Request: POST https://api.openai.com/v1/chat/completions
DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>
DEBUG:httpcore.http11:send_request_headers.complete
DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>
DEBUG:httpcore.http11:send_request_body.complete
DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>
DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Mon, 08 Jul 2024 07:25:21 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'openai-organization', b'machinekingsmedia'), (b'openai-processing-ms', b'388'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains'), (b'x-ratelimit-limit-requests', b'10000'), (b'x-ratelimit-limit-tokens', b'1000000'), (b'x-ratelimit-remaining-requests', b'9999'), (b'x-ratelimit-remaining-tokens', b'998921'), (b'x-ratelimit-reset-requests', b'6ms'), (b'x-ratelimit-reset-tokens', b'64ms'), (b'x-request-id', b'req_d0438ccb4a9cfbbd806f8a6a05140713'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'89fe56fd1a1c0594-IAD'), (b'Content-Encoding', b'br'), (b'alt-svc', b'h3=":443"; ma=86400')])
INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>
DEBUG:httpcore.http11:receive_response_body.complete
DEBUG:httpcore.http11:response_closed.started
DEBUG:httpcore.http11:response_closed.complete
DEBUG:openai._base_client:HTTP Response: POST https://api.openai.com/v1/chat/completions "200 OK" Headers({'date': 'Mon, 08 Jul 2024 07:25:21 GMT', 'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'openai-organization': 'machinekingsmedia', 'openai-processing-ms': '388', 'openai-version': '2020-10-01', 'strict-transport-security': 'max-age=31536000; includeSubDomains', 'x-ratelimit-limit-requests': '10000', 'x-ratelimit-limit-tokens': '1000000', 'x-ratelimit-remaining-requests': '9999', 'x-ratelimit-remaining-tokens': '998921', 'x-ratelimit-reset-requests': '6ms', 'x-ratelimit-reset-tokens': '64ms', 'x-request-id': 'req_d0438ccb4a9cfbbd806f8a6a05140713', 'cf-cache-status': 'DYNAMIC', 'server': 'cloudflare', 'cf-ray': '89fe56fd1a1c0594-IAD', 'content-encoding': 'br', 'alt-svc': 'h3=":443"; ma=86400'})
DEBUG:openai._base_client:request_id: req_d0438ccb4a9cfbbd806f8a6a05140713
DEBUG:openai._base_client:Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'json_data': {'messages': [{'content': "Provide a brief summary of the following transcript: File: 240204_2206.mp3\nRecording Date: 2024-02-04\nDuration: 00:06:14.28\nTranscription Date: 2024-07-07 06:19:09\n\n1\n00:00:00,000 --> 00:00:02,000\nregarding the app\n\n2\n00:00:02,160 --> 00:00:04,160\nYou saying that?\n\n3\n00:00:04,520 --> 00:00:10,600\nThat having a subscription model is where you saw what what things kind of went wrong\n\n4\n00:00:13,520 --> 00:00:21,160\nOnce I caught the deal with caffeine, I would have to add free get as many subscribers as possible and then have the stream\n\n5\n00:00:21,799 --> 00:00:23,580\npay-per-view on the app\n\n6\n00:00:23,580 --> 00:00:25,580\nSo bring it\n\n7\n00:00:27,400 --> 00:00:29,400\nOnce you have\n\n8\n00:00:30,760 --> 00:00:34,040\nSnapchat and these these Instagrams they get paid off for promotion\n\n9\n00:00:34,040 --> 00:00:38,240\nThey get paid off of being brought out by the Googles and stuff like that where they can put their ads\n\n10\n00:00:38,880 --> 00:00:41,680\nSo the more subscriptions you get the more\n\n11\n00:00:42,360 --> 00:00:45,560\nVolume you get the model now you go to these networks, but I know I got\n\n12\n00:00:46,320 --> 00:00:48,320\n2 million subscribers on this app\n\n13\n00:00:48,560 --> 00:00:50,360\nBut they went nah\n\n14\n00:00:50,360 --> 00:00:54,720\nWe get $7.99 for these dudes, we good. Plus we getting paid from caffeine\n\n15\n00:00:54,720 --> 00:00:59,520\nThey didn't look at the bigger picture. If they would have got more subscribers, more subscribers, they can say yo\n\n16\n00:01:00,000 --> 00:01:05,279\nWe're gonna do a pay-per-view. You got 2-3 million people. All you need is 100,000 people to pay for that pay-per-view. At that point, you're good.\n\n17\n00:01:05,720 --> 00:01:06,839\nYeah\n\n18\n00:01:06,839 --> 00:01:13,040\nThe zone did. The zone did. They charge, right? And they could have got it could have put ads on there as well\n\n19\n00:01:13,040 --> 00:01:15,040\nThey could have got a Google to purchase them. Some of that.\n\n20\n00:01:15,040 --> 00:01:21,839\nBecause it's all about the traffic on there and what you do is you create the content on there as well\n\n21\n00:01:22,080 --> 00:01:25,599\nYou know, it was just they just did not structure it smart\n\n22\n00:01:26,599 --> 00:01:29,400\nAnd it was it was it wasn't bright bro, like\n\n23\n00:01:31,360 --> 00:01:32,480\nSo\n\n24\n00:01:32,480 --> 00:01:34,480\nso the app is\n\n25\n00:01:34,800 --> 00:01:40,519\nOne thing no knobs, but the exit from YouTube is a completely different thing\n\n26\n00:01:46,160 --> 00:01:50,339\nLet's start there, it's a free platform you open a free platform\n\n27\n00:01:51,199 --> 00:01:53,120\nfor that material\n\n28\n00:01:53,120 --> 00:02:00,040\nWhy would you didn't go charge when you know, I'm saying when and you also realize to you're the biggest but you don't everybody\n\n29\n00:02:00,040 --> 00:02:01,800\nAll these guys are spread out\n\n30\n00:02:01,800 --> 00:02:04,440\nFuck you, too. If you do tell me right now niggas a yo\n\n31\n00:02:05,000 --> 00:02:08,639\nWe don't have we got you got a watch on that. It's free. You don't think\n\n32\n00:02:13,720 --> 00:02:15,720\nThey wanted just to watch it on YouTube\n\n33\n00:02:17,399 --> 00:02:21,240\nYou would have had a couple me you would have taken same subscribers from YouTube and probably got more\n\n34\n00:02:22,240 --> 00:02:29,000\nBut they got greedy they didn't market it well, then they also gonna have no material bro, you can't like the zone\n\n35\n00:02:30,119 --> 00:02:33,720\nThey don't have boxing every week, but they have shows on there\n\n36\n00:02:33,720 --> 00:02:36,600\nThey have things that you can watch outside to keep you engaged\n\n37\n00:02:37,279 --> 00:02:41,000\nSo the app doesn't have like behind the scene\n\n38\n00:02:41,679 --> 00:02:43,679\nlike\n\n39\n00:02:43,679 --> 00:02:47,320\nLiterally had DNA's wrestling show, which had nothing to do\n\n40\n00:02:49,600 --> 00:02:51,720\nHe had a podcast about wrestling on there\n\n41\n00:02:53,039 --> 00:02:55,039\nby who\n\n42\n00:02:55,119 --> 00:02:58,800\nWas DNA sure he was just or he would he would get wrestlers on the fucking app\n\n43\n00:02:58,800 --> 00:0", 'role': 'user'}], 'model': 'gpt-3.5-turbo-16k', 'max_tokens': 1000, 'n': 1, 'stream': False, 'temperature': 0.7}}
DEBUG:openai._base_client:Sending HTTP Request: POST https://api.openai.com/v1/chat/completions
DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>
DEBUG:httpcore.http11:send_request_headers.complete
DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>
DEBUG:httpcore.http11:send_request_body.complete
DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>
DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Mon, 08 Jul 2024 07:25:23 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'openai-organization', b'machinekingsmedia'), (b'openai-processing-ms', b'1572'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains'), (b'x-ratelimit-limit-requests', b'10000'), (b'x-ratelimit-limit-tokens', b'1000000'), (b'x-ratelimit-remaining-requests', b'9999'), (b'x-ratelimit-remaining-tokens', b'997985'), (b'x-ratelimit-reset-requests', b'6ms'), (b'x-ratelimit-reset-tokens', b'120ms'), (b'x-request-id', b'req_f46a6b1e1835cf37cb630d2a31f60ffd'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'89fe57019d3b0594-IAD'), (b'Content-Encoding', b'br'), (b'alt-svc', b'h3=":443"; ma=86400')])
INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>
DEBUG:httpcore.http11:receive_response_body.complete
DEBUG:httpcore.http11:response_closed.started
DEBUG:httpcore.http11:response_closed.complete
DEBUG:openai._base_client:HTTP Response: POST https://api.openai.com/v1/chat/completions "200 OK" Headers({'date': 'Mon, 08 Jul 2024 07:25:23 GMT', 'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'openai-organization': 'machinekingsmedia', 'openai-processing-ms': '1572', 'openai-version': '2020-10-01', 'strict-transport-security': 'max-age=31536000; includeSubDomains', 'x-ratelimit-limit-requests': '10000', 'x-ratelimit-limit-tokens': '1000000', 'x-ratelimit-remaining-requests': '9999', 'x-ratelimit-remaining-tokens': '997985', 'x-ratelimit-reset-requests': '6ms', 'x-ratelimit-reset-tokens': '120ms', 'x-request-id': 'req_f46a6b1e1835cf37cb630d2a31f60ffd', 'cf-cache-status': 'DYNAMIC', 'server': 'cloudflare', 'cf-ray': '89fe57019d3b0594-IAD', 'content-encoding': 'br', 'alt-svc': 'h3=":443"; ma=86400'})
DEBUG:openai._base_client:request_id: req_f46a6b1e1835cf37cb630d2a31f60ffd
DEBUG:openai._base_client:Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'json_data': {'messages': [{'content': 'Generate a short, descriptive title for the following chapter content: regarding the app You saying that? That having a subscription model is where you saw what what things kind of went wrong Once I caught the deal with caffeine, I would have to add free get as many subscribers as possible and then have the stream pay-per-view on the app So bring it Once you have Snapchat and these these Instagrams they get paid off for promotion They get paid off of being brought out by the Googles and stuff like that where they can put their ads So the more subscriptions you get the more', 'role': 'user'}], 'model': 'gpt-3.5-turbo-16k', 'max_tokens': 1000, 'n': 1, 'stream': False, 'temperature': 0.7}}
DEBUG:openai._base_client:Sending HTTP Request: POST https://api.openai.com/v1/chat/completions
DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>
DEBUG:httpcore.http11:send_request_headers.complete
DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>
DEBUG:httpcore.http11:send_request_body.complete
DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>
DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Mon, 08 Jul 2024 07:25:24 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'openai-organization', b'machinekingsmedia'), (b'openai-processing-ms', b'269'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains'), (b'x-ratelimit-limit-requests', b'10000'), (b'x-ratelimit-limit-tokens', b'1000000'), (b'x-ratelimit-remaining-requests', b'9999'), (b'x-ratelimit-remaining-tokens', b'998853'), (b'x-ratelimit-reset-requests', b'6ms'), (b'x-ratelimit-reset-tokens', b'68ms'), (b'x-request-id', b'req_5d130ff91025b28cc33c6e4da5473778'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'89fe570cfcb60594-IAD'), (b'Content-Encoding', b'br'), (b'alt-svc', b'h3=":443"; ma=86400')])
INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>
DEBUG:httpcore.http11:receive_response_body.complete
DEBUG:httpcore.http11:response_closed.started
DEBUG:httpcore.http11:response_closed.complete
DEBUG:openai._base_client:HTTP Response: POST https://api.openai.com/v1/chat/completions "200 OK" Headers({'date': 'Mon, 08 Jul 2024 07:25:24 GMT', 'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'openai-organization': 'machinekingsmedia', 'openai-processing-ms': '269', 'openai-version': '2020-10-01', 'strict-transport-security': 'max-age=31536000; includeSubDomains', 'x-ratelimit-limit-requests': '10000', 'x-ratelimit-limit-tokens': '1000000', 'x-ratelimit-remaining-requests': '9999', 'x-ratelimit-remaining-tokens': '998853', 'x-ratelimit-reset-requests': '6ms', 'x-ratelimit-reset-tokens': '68ms', 'x-request-id': 'req_5d130ff91025b28cc33c6e4da5473778', 'cf-cache-status': 'DYNAMIC', 'server': 'cloudflare', 'cf-ray': '89fe570cfcb60594-IAD', 'content-encoding': 'br', 'alt-svc': 'h3=":443"; ma=86400'})
DEBUG:openai._base_client:request_id: req_5d130ff91025b28cc33c6e4da5473778
DEBUG:openai._base_client:Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'json_data': {'messages': [{'content': "Generate a short, descriptive title for the following chapter content: Volume you get the model now you go to these networks, but I know I got 2 million subscribers on this app But they went nah We get $7.99 for these dudes, we good. Plus we getting paid from caffeine They didn't look at the bigger picture. If they would have got more subscribers, more subscribers, they can say yo We're gonna do a pay-per-view. You got 2-3 million people. All you need is 100,000 people to pay for that pay-per-view. At that point, you're good. Yeah The zone did. The zone did. They charge, right? And they could have got it could have put ads on there as well They could have got a Google to purchase them. Some of that. Because it's all about the traffic on there and what you do is you create the content on there as well", 'role': 'user'}], 'model': 'gpt-3.5-turbo-16k', 'max_tokens': 1000, 'n': 1, 'stream': False, 'temperature': 0.7}}
DEBUG:openai._base_client:Sending HTTP Request: POST https://api.openai.com/v1/chat/completions
DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>
DEBUG:httpcore.http11:send_request_headers.complete
DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>
DEBUG:httpcore.http11:send_request_body.complete
DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>
DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Mon, 08 Jul 2024 07:25:25 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'openai-organization', b'machinekingsmedia'), (b'openai-processing-ms', b'873'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains'), (b'x-ratelimit-limit-requests', b'10000'), (b'x-ratelimit-limit-tokens', b'1000000'), (b'x-ratelimit-remaining-requests', b'9999'), (b'x-ratelimit-remaining-tokens', b'998796'), (b'x-ratelimit-reset-requests', b'6ms'), (b'x-ratelimit-reset-tokens', b'72ms'), (b'x-request-id', b'req_25d89127b5aaf6a6be48cf0d5e1d5609'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'89fe5713f9a60594-IAD'), (b'Content-Encoding', b'br'), (b'alt-svc', b'h3=":443"; ma=86400')])
INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>
DEBUG:httpcore.http11:receive_response_body.complete
DEBUG:httpcore.http11:response_closed.started
DEBUG:httpcore.http11:response_closed.complete
DEBUG:openai._base_client:HTTP Response: POST https://api.openai.com/v1/chat/completions "200 OK" Headers({'date': 'Mon, 08 Jul 2024 07:25:25 GMT', 'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'openai-organization': 'machinekingsmedia', 'openai-processing-ms': '873', 'openai-version': '2020-10-01', 'strict-transport-security': 'max-age=31536000; includeSubDomains', 'x-ratelimit-limit-requests': '10000', 'x-ratelimit-limit-tokens': '1000000', 'x-ratelimit-remaining-requests': '9999', 'x-ratelimit-remaining-tokens': '998796', 'x-ratelimit-reset-requests': '6ms', 'x-ratelimit-reset-tokens': '72ms', 'x-request-id': 'req_25d89127b5aaf6a6be48cf0d5e1d5609', 'cf-cache-status': 'DYNAMIC', 'server': 'cloudflare', 'cf-ray': '89fe5713f9a60594-IAD', 'content-encoding': 'br', 'alt-svc': 'h3=":443"; ma=86400'})
DEBUG:openai._base_client:request_id: req_25d89127b5aaf6a6be48cf0d5e1d5609
DEBUG:openai._base_client:Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'json_data': {'messages': [{'content': "Generate a short, descriptive title for the following chapter content: You know, it was just they just did not structure it smart And it was it was it wasn't bright bro, like So so the app is One thing no knobs, but the exit from YouTube is a completely different thing Let's start there, it's a free platform you open a free platform for that material Why would you didn't go charge when you know, I'm saying when and you also realize to you're the biggest but you don't everybody All these guys are spread out Fuck you, too. If you do tell me right now niggas a yo", 'role': 'user'}], 'model': 'gpt-3.5-turbo-16k', 'max_tokens': 1000, 'n': 1, 'stream': False, 'temperature': 0.7}}
DEBUG:openai._base_client:Sending HTTP Request: POST https://api.openai.com/v1/chat/completions
DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>
DEBUG:httpcore.http11:send_request_headers.complete
DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>
DEBUG:httpcore.http11:send_request_body.complete
DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>
DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Mon, 08 Jul 2024 07:25:25 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'openai-organization', b'machinekingsmedia'), (b'openai-processing-ms', b'294'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains'), (b'x-ratelimit-limit-requests', b'10000'), (b'x-ratelimit-limit-tokens', b'1000000'), (b'x-ratelimit-remaining-requests', b'9999'), (b'x-ratelimit-remaining-tokens', b'998856'), (b'x-ratelimit-reset-requests', b'6ms'), (b'x-ratelimit-reset-tokens', b'68ms'), (b'x-request-id', b'req_5fc8848645eebb8877b2f8e88c05c5bb'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'89fe571a2d4d0594-IAD'), (b'Content-Encoding', b'br'), (b'alt-svc', b'h3=":443"; ma=86400')])
INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>
DEBUG:httpcore.http11:receive_response_body.complete
DEBUG:httpcore.http11:response_closed.started
DEBUG:httpcore.http11:response_closed.complete
DEBUG:openai._base_client:HTTP Response: POST https://api.openai.com/v1/chat/completions "200 OK" Headers({'date': 'Mon, 08 Jul 2024 07:25:25 GMT', 'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'openai-organization': 'machinekingsmedia', 'openai-processing-ms': '294', 'openai-version': '2020-10-01', 'strict-transport-security': 'max-age=31536000; includeSubDomains', 'x-ratelimit-limit-requests': '10000', 'x-ratelimit-limit-tokens': '1000000', 'x-ratelimit-remaining-requests': '9999', 'x-ratelimit-remaining-tokens': '998856', 'x-ratelimit-reset-requests': '6ms', 'x-ratelimit-reset-tokens': '68ms', 'x-request-id': 'req_5fc8848645eebb8877b2f8e88c05c5bb', 'cf-cache-status': 'DYNAMIC', 'server': 'cloudflare', 'cf-ray': '89fe571a2d4d0594-IAD', 'content-encoding': 'br', 'alt-svc': 'h3=":443"; ma=86400'})
DEBUG:openai._base_client:request_id: req_5fc8848645eebb8877b2f8e88c05c5bb
DEBUG:openai._base_client:Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'json_data': {'messages': [{'content': "Generate a short, descriptive title for the following chapter content: We don't have we got you got a watch on that. It's free. You don't think They wanted just to watch it on YouTube You would have had a couple me you would have taken same subscribers from YouTube and probably got more But they got greedy they didn't market it well, then they also gonna have no material bro, you can't like the zone They don't have boxing every week, but they have shows on there They have things that you can watch outside to keep you engaged So the app doesn't have like behind the scene like Literally had DNA's wrestling show, which had nothing to do He had a podcast about wrestling on there", 'role': 'user'}], 'model': 'gpt-3.5-turbo-16k', 'max_tokens': 1000, 'n': 1, 'stream': False, 'temperature': 0.7}}
DEBUG:openai._base_client:Sending HTTP Request: POST https://api.openai.com/v1/chat/completions
DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>
DEBUG:httpcore.http11:send_request_headers.complete
DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>
DEBUG:httpcore.http11:send_request_body.complete
DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>
DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Mon, 08 Jul 2024 07:25:27 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'openai-organization', b'machinekingsmedia'), (b'openai-processing-ms', b'343'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains'), (b'x-ratelimit-limit-requests', b'10000'), (b'x-ratelimit-limit-tokens', b'1000000'), (b'x-ratelimit-remaining-requests', b'9999'), (b'x-ratelimit-remaining-tokens', b'998827'), (b'x-ratelimit-reset-requests', b'6ms'), (b'x-ratelimit-reset-tokens', b'70ms'), (b'x-request-id', b'req_c951543c6bb1c4f6ae1a6bfc4de168dc'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'89fe571ccecf0594-IAD'), (b'Content-Encoding', b'br'), (b'alt-svc', b'h3=":443"; ma=86400')])
INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>
DEBUG:httpcore.http11:receive_response_body.complete
DEBUG:httpcore.http11:response_closed.started
DEBUG:httpcore.http11:response_closed.complete
DEBUG:openai._base_client:HTTP Response: POST https://api.openai.com/v1/chat/completions "200 OK" Headers({'date': 'Mon, 08 Jul 2024 07:25:27 GMT', 'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'openai-organization': 'machinekingsmedia', 'openai-processing-ms': '343', 'openai-version': '2020-10-01', 'strict-transport-security': 'max-age=31536000; includeSubDomains', 'x-ratelimit-limit-requests': '10000', 'x-ratelimit-limit-tokens': '1000000', 'x-ratelimit-remaining-requests': '9999', 'x-ratelimit-remaining-tokens': '998827', 'x-ratelimit-reset-requests': '6ms', 'x-ratelimit-reset-tokens': '70ms', 'x-request-id': 'req_c951543c6bb1c4f6ae1a6bfc4de168dc', 'cf-cache-status': 'DYNAMIC', 'server': 'cloudflare', 'cf-ray': '89fe571ccecf0594-IAD', 'content-encoding': 'br', 'alt-svc': 'h3=":443"; ma=86400'})
DEBUG:openai._base_client:request_id: req_c951543c6bb1c4f6ae1a6bfc4de168dc
DEBUG:openai._base_client:Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'json_data': {'messages': [{'content': "Generate a short, descriptive title for the following chapter content: by who Was DNA sure he was just or he would he would get wrestlers on the fucking app Oh, you told my the caffeine out me to my URL I Say hey, let me pay angry fan. Let me pay All these bloggers to debut their content on this fucking thing in order to get people to Tune in and then they can really sell YouTube afterwards What you do is you're drawing more people and if it's free and caffeine's pulling a bunch of together You're not paying for shit. You just join subscribers and subscribers and subscribers there What are you losing?", 'role': 'user'}], 'model': 'gpt-3.5-turbo-16k', 'max_tokens': 1000, 'n': 1, 'stream': False, 'temperature': 0.7}}
DEBUG:openai._base_client:Sending HTTP Request: POST https://api.openai.com/v1/chat/completions
DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>
DEBUG:httpcore.http11:send_request_headers.complete
DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>
DEBUG:httpcore.http11:send_request_body.complete
DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>
DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Mon, 08 Jul 2024 07:25:28 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'openai-organization', b'machinekingsmedia'), (b'openai-processing-ms', b'347'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains'), (b'x-ratelimit-limit-requests', b'10000'), (b'x-ratelimit-limit-tokens', b'1000000'), (b'x-ratelimit-remaining-requests', b'9999'), (b'x-ratelimit-remaining-tokens', b'998847'), (b'x-ratelimit-reset-requests', b'6ms'), (b'x-ratelimit-reset-tokens', b'69ms'), (b'x-request-id', b'req_d38f21c58150c0e772dec619eacbc84b'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'89fe57247b670594-IAD'), (b'Content-Encoding', b'br'), (b'alt-svc', b'h3=":443"; ma=86400')])
INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>
DEBUG:httpcore.http11:receive_response_body.complete
DEBUG:httpcore.http11:response_closed.started
DEBUG:httpcore.http11:response_closed.complete
DEBUG:openai._base_client:HTTP Response: POST https://api.openai.com/v1/chat/completions "200 OK" Headers({'date': 'Mon, 08 Jul 2024 07:25:28 GMT', 'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'openai-organization': 'machinekingsmedia', 'openai-processing-ms': '347', 'openai-version': '2020-10-01', 'strict-transport-security': 'max-age=31536000; includeSubDomains', 'x-ratelimit-limit-requests': '10000', 'x-ratelimit-limit-tokens': '1000000', 'x-ratelimit-remaining-requests': '9999', 'x-ratelimit-remaining-tokens': '998847', 'x-ratelimit-reset-requests': '6ms', 'x-ratelimit-reset-tokens': '69ms', 'x-request-id': 'req_d38f21c58150c0e772dec619eacbc84b', 'cf-cache-status': 'DYNAMIC', 'server': 'cloudflare', 'cf-ray': '89fe57247b670594-IAD', 'content-encoding': 'br', 'alt-svc': 'h3=":443"; ma=86400'})
DEBUG:openai._base_client:request_id: req_d38f21c58150c0e772dec619eacbc84b
DEBUG:openai._base_client:Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'json_data': {'messages': [{'content': "Generate a short, descriptive title for the following chapter content: I really Stupid They should have did that and when they were away father. They should have took that to Netflix Netflix just signed multi-billion dollar deal with the WWE I Really hope I can find that twist tweet where he said smack just sold the culture when he made the the I don't know if it was the caffeine deal or the app or The the app deal or may have YouTube exclusivity But I I do feel as though right now. I mean that decision to go off YouTube and to take that money from the subscriptions and To stay off of YouTube at this point is basically killing", 'role': 'user'}], 'model': 'gpt-3.5-turbo-16k', 'max_tokens': 1000, 'n': 1, 'stream': False, 'temperature': 0.7}}
DEBUG:openai._base_client:Sending HTTP Request: POST https://api.openai.com/v1/chat/completions
DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>
DEBUG:httpcore.http11:send_request_headers.complete
DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>
DEBUG:httpcore.http11:send_request_body.complete
DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>
DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Mon, 08 Jul 2024 07:25:28 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'openai-organization', b'machinekingsmedia'), (b'openai-processing-ms', b'261'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains'), (b'x-ratelimit-limit-requests', b'10000'), (b'x-ratelimit-limit-tokens', b'1000000'), (b'x-ratelimit-remaining-requests', b'9999'), (b'x-ratelimit-remaining-tokens', b'998840'), (b'x-ratelimit-reset-requests', b'6ms'), (b'x-ratelimit-reset-tokens', b'69ms'), (b'x-request-id', b'req_489c032c7fae8de7c315335a65b72975'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'89fe572bffe70594-IAD'), (b'Content-Encoding', b'br'), (b'alt-svc', b'h3=":443"; ma=86400')])
INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>
DEBUG:httpcore.http11:receive_response_body.complete
DEBUG:httpcore.http11:response_closed.started
DEBUG:httpcore.http11:response_closed.complete
DEBUG:openai._base_client:HTTP Response: POST https://api.openai.com/v1/chat/completions "200 OK" Headers({'date': 'Mon, 08 Jul 2024 07:25:28 GMT', 'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'openai-organization': 'machinekingsmedia', 'openai-processing-ms': '261', 'openai-version': '2020-10-01', 'strict-transport-security': 'max-age=31536000; includeSubDomains', 'x-ratelimit-limit-requests': '10000', 'x-ratelimit-limit-tokens': '1000000', 'x-ratelimit-remaining-requests': '9999', 'x-ratelimit-remaining-tokens': '998840', 'x-ratelimit-reset-requests': '6ms', 'x-ratelimit-reset-tokens': '69ms', 'x-request-id': 'req_489c032c7fae8de7c315335a65b72975', 'cf-cache-status': 'DYNAMIC', 'server': 'cloudflare', 'cf-ray': '89fe572bffe70594-IAD', 'content-encoding': 'br', 'alt-svc': 'h3=":443"; ma=86400'})
DEBUG:openai._base_client:request_id: req_489c032c7fae8de7c315335a65b72975
DEBUG:openai._base_client:Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'json_data': {'messages': [{'content': 'Generate a short, descriptive title for the following chapter content: Any Killing that killing that company and like putting all of that history behind a paywall Why Would you then turn around and charge for the app when you could have became a tech company because once you open up you Actually become a tech company Become you become a tech company and tech companies are looked at differently But these niggas got me you could have went got ten, but they could have got up to like five six million subscribers Wait a minute, wait a minute. Wait a minute. They never they never share their subscriber numbers, right? We We good', 'role': 'user'}], 'model': 'gpt-3.5-turbo-16k', 'max_tokens': 1000, 'n': 1, 'stream': False, 'temperature': 0.7}}
DEBUG:openai._base_client:Sending HTTP Request: POST https://api.openai.com/v1/chat/completions
DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>
DEBUG:httpcore.http11:send_request_headers.complete
DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>
DEBUG:httpcore.http11:send_request_body.complete
DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>
DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Mon, 08 Jul 2024 07:25:29 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'openai-organization', b'machinekingsmedia'), (b'openai-processing-ms', b'250'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains'), (b'x-ratelimit-limit-requests', b'10000'), (b'x-ratelimit-limit-tokens', b'1000000'), (b'x-ratelimit-remaining-requests', b'9999'), (b'x-ratelimit-remaining-tokens', b'998840'), (b'x-ratelimit-reset-requests', b'6ms'), (b'x-ratelimit-reset-tokens', b'69ms'), (b'x-request-id', b'req_679023fc53775ed0190ed1a63b11b164'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'89fe572fb9fd0594-IAD'), (b'Content-Encoding', b'br'), (b'alt-svc', b'h3=":443"; ma=86400')])
INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>
DEBUG:httpcore.http11:receive_response_body.complete
DEBUG:httpcore.http11:response_closed.started
DEBUG:httpcore.http11:response_closed.complete
DEBUG:openai._base_client:HTTP Response: POST https://api.openai.com/v1/chat/completions "200 OK" Headers({'date': 'Mon, 08 Jul 2024 07:25:29 GMT', 'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'openai-organization': 'machinekingsmedia', 'openai-processing-ms': '250', 'openai-version': '2020-10-01', 'strict-transport-security': 'max-age=31536000; includeSubDomains', 'x-ratelimit-limit-requests': '10000', 'x-ratelimit-limit-tokens': '1000000', 'x-ratelimit-remaining-requests': '9999', 'x-ratelimit-remaining-tokens': '998840', 'x-ratelimit-reset-requests': '6ms', 'x-ratelimit-reset-tokens': '69ms', 'x-request-id': 'req_679023fc53775ed0190ed1a63b11b164', 'cf-cache-status': 'DYNAMIC', 'server': 'cloudflare', 'cf-ray': '89fe572fb9fd0594-IAD', 'content-encoding': 'br', 'alt-svc': 'h3=":443"; ma=86400'})
DEBUG:openai._base_client:request_id: req_679023fc53775ed0190ed1a63b11b164
DEBUG:openai._base_client:Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'json_data': {'messages': [{'content': "Generate a short, descriptive title for the following chapter content: But they are they also have to know that the subscriber numbers on this app are Declining over time. They aren't going up anymore because there's no way No way that they haven't already reached the peak numbers on that app because Huh? Numbers to get out because", 'role': 'user'}], 'model': 'gpt-3.5-turbo-16k', 'max_tokens': 1000, 'n': 1, 'stream': False, 'temperature': 0.7}}
DEBUG:openai._base_client:Sending HTTP Request: POST https://api.openai.com/v1/chat/completions
DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>
DEBUG:httpcore.http11:send_request_headers.complete
DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>
DEBUG:httpcore.http11:send_request_body.complete
DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>
DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Mon, 08 Jul 2024 07:25:29 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'openai-organization', b'machinekingsmedia'), (b'openai-processing-ms', b'264'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains'), (b'x-ratelimit-limit-requests', b'10000'), (b'x-ratelimit-limit-tokens', b'1000000'), (b'x-ratelimit-remaining-requests', b'9999'), (b'x-ratelimit-remaining-tokens', b'998914'), (b'x-ratelimit-reset-requests', b'6ms'), (b'x-ratelimit-reset-tokens', b'65ms'), (b'x-request-id', b'req_e408d0987fd7d7ae90c3f0aad8c58970'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'89fe57336c150594-IAD'), (b'Content-Encoding', b'br'), (b'alt-svc', b'h3=":443"; ma=86400')])
INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>
DEBUG:httpcore.http11:receive_response_body.complete
DEBUG:httpcore.http11:response_closed.started
DEBUG:httpcore.http11:response_closed.complete
DEBUG:openai._base_client:HTTP Response: POST https://api.openai.com/v1/chat/completions "200 OK" Headers({'date': 'Mon, 08 Jul 2024 07:25:29 GMT', 'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'openai-organization': 'machinekingsmedia', 'openai-processing-ms': '264', 'openai-version': '2020-10-01', 'strict-transport-security': 'max-age=31536000; includeSubDomains', 'x-ratelimit-limit-requests': '10000', 'x-ratelimit-limit-tokens': '1000000', 'x-ratelimit-remaining-requests': '9999', 'x-ratelimit-remaining-tokens': '998914', 'x-ratelimit-reset-requests': '6ms', 'x-ratelimit-reset-tokens': '65ms', 'x-request-id': 'req_e408d0987fd7d7ae90c3f0aad8c58970', 'cf-cache-status': 'DYNAMIC', 'server': 'cloudflare', 'cf-ray': '89fe57336c150594-IAD', 'content-encoding': 'br', 'alt-svc': 'h3=":443"; ma=86400'})
DEBUG:openai._base_client:request_id: req_e408d0987fd7d7ae90c3f0aad8c58970
DEBUG:openai._base_client:Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'json_data': {'messages': [{'content': 'Summarize the following chapter in one sentence: MBC \ub274\uc2a4 \uc774\ub355\uc601\uc785\ub2c8\ub2e4.', 'role': 'user'}], 'model': 'gpt-3.5-turbo-16k', 'max_tokens': 1000, 'n': 1, 'stream': False, 'temperature': 0.7}}
DEBUG:openai._base_client:Sending HTTP Request: POST https://api.openai.com/v1/chat/completions
DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>
DEBUG:httpcore.http11:send_request_headers.complete
DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>
DEBUG:httpcore.http11:send_request_body.complete
DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>
DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Mon, 08 Jul 2024 07:25:30 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'openai-organization', b'machinekingsmedia'), (b'openai-processing-ms', b'253'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains'), (b'x-ratelimit-limit-requests', b'10000'), (b'x-ratelimit-limit-tokens', b'1000000'), (b'x-ratelimit-remaining-requests', b'9999'), (b'x-ratelimit-remaining-tokens', b'998978'), (b'x-ratelimit-reset-requests', b'6ms'), (b'x-ratelimit-reset-tokens', b'61ms'), (b'x-request-id', b'req_c8f182c2ced9de534469c3a0d80acfa6'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'89fe57369e470594-IAD'), (b'Content-Encoding', b'br'), (b'alt-svc', b'h3=":443"; ma=86400')])
INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>
DEBUG:httpcore.http11:receive_response_body.complete
DEBUG:httpcore.http11:response_closed.started
DEBUG:httpcore.http11:response_closed.complete
DEBUG:openai._base_client:HTTP Response: POST https://api.openai.com/v1/chat/completions "200 OK" Headers({'date': 'Mon, 08 Jul 2024 07:25:30 GMT', 'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'openai-organization': 'machinekingsmedia', 'openai-processing-ms': '253', 'openai-version': '2020-10-01', 'strict-transport-security': 'max-age=31536000; includeSubDomains', 'x-ratelimit-limit-requests': '10000', 'x-ratelimit-limit-tokens': '1000000', 'x-ratelimit-remaining-requests': '9999', 'x-ratelimit-remaining-tokens': '998978', 'x-ratelimit-reset-requests': '6ms', 'x-ratelimit-reset-tokens': '61ms', 'x-request-id': 'req_c8f182c2ced9de534469c3a0d80acfa6', 'cf-cache-status': 'DYNAMIC', 'server': 'cloudflare', 'cf-ray': '89fe57369e470594-IAD', 'content-encoding': 'br', 'alt-svc': 'h3=":443"; ma=86400'})
DEBUG:openai._base_client:request_id: req_c8f182c2ced9de534469c3a0d80acfa6
DEBUG:openai._base_client:Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'json_data': {'messages': [{'content': 'Provide a brief summary of the following transcript: File: 240213_1234.mp3\nRecording Date: 2024-02-13\nDuration: 00:00:04.00\nTranscription Date: 2024-07-07 06:19:10\n\n1\n00:00:00,000 --> 00:00:04,000\nMBC \ub274\uc2a4 \uc774\ub355\uc601\uc785\ub2c8\ub2e4.\n\n\n', 'role': 'user'}], 'model': 'gpt-3.5-turbo-16k', 'max_tokens': 1000, 'n': 1, 'stream': False, 'temperature': 0.7}}
DEBUG:openai._base_client:Sending HTTP Request: POST https://api.openai.com/v1/chat/completions
DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>
DEBUG:httpcore.http11:send_request_headers.complete
DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>
DEBUG:httpcore.http11:send_request_body.complete
DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>
DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Mon, 08 Jul 2024 07:25:31 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'openai-organization', b'machinekingsmedia'), (b'openai-processing-ms', b'599'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains'), (b'x-ratelimit-limit-requests', b'10000'), (b'x-ratelimit-limit-tokens', b'1000000'), (b'x-ratelimit-remaining-requests', b'9999'), (b'x-ratelimit-remaining-tokens', b'998940'), (b'x-ratelimit-reset-requests', b'6ms'), (b'x-ratelimit-reset-tokens', b'63ms'), (b'x-request-id', b'req_0fb2cec501fd87be12f3d3facd477371'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'89fe5739a8090594-IAD'), (b'Content-Encoding', b'br'), (b'alt-svc', b'h3=":443"; ma=86400')])
INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>
DEBUG:httpcore.http11:receive_response_body.complete
DEBUG:httpcore.http11:response_closed.started
DEBUG:httpcore.http11:response_closed.complete
DEBUG:openai._base_client:HTTP Response: POST https://api.openai.com/v1/chat/completions "200 OK" Headers({'date': 'Mon, 08 Jul 2024 07:25:31 GMT', 'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'openai-organization': 'machinekingsmedia', 'openai-processing-ms': '599', 'openai-version': '2020-10-01', 'strict-transport-security': 'max-age=31536000; includeSubDomains', 'x-ratelimit-limit-requests': '10000', 'x-ratelimit-limit-tokens': '1000000', 'x-ratelimit-remaining-requests': '9999', 'x-ratelimit-remaining-tokens': '998940', 'x-ratelimit-reset-requests': '6ms', 'x-ratelimit-reset-tokens': '63ms', 'x-request-id': 'req_0fb2cec501fd87be12f3d3facd477371', 'cf-cache-status': 'DYNAMIC', 'server': 'cloudflare', 'cf-ray': '89fe5739a8090594-IAD', 'content-encoding': 'br', 'alt-svc': 'h3=":443"; ma=86400'})
DEBUG:openai._base_client:request_id: req_0fb2cec501fd87be12f3d3facd477371
DEBUG:openai._base_client:Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'json_data': {'messages': [{'content': 'Generate a short, descriptive title for the following chapter content: MBC \ub274\uc2a4 \uc774\ub355\uc601\uc785\ub2c8\ub2e4.', 'role': 'user'}], 'model': 'gpt-3.5-turbo-16k', 'max_tokens': 1000, 'n': 1, 'stream': False, 'temperature': 0.7}}
DEBUG:openai._base_client:Sending HTTP Request: POST https://api.openai.com/v1/chat/completions
DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>
DEBUG:httpcore.http11:send_request_headers.complete
DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>
DEBUG:httpcore.http11:send_request_body.complete
DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>
DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Mon, 08 Jul 2024 07:25:31 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'openai-organization', b'machinekingsmedia'), (b'openai-processing-ms', b'193'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains'), (b'x-ratelimit-limit-requests', b'10000'), (b'x-ratelimit-limit-tokens', b'1000000'), (b'x-ratelimit-remaining-requests', b'9999'), (b'x-ratelimit-remaining-tokens', b'998973'), (b'x-ratelimit-reset-requests', b'6ms'), (b'x-ratelimit-reset-tokens', b'61ms'), (b'x-request-id', b'req_d7989e82419386b169be26cef1a42c1c'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'89fe573efb210594-IAD'), (b'Content-Encoding', b'br'), (b'alt-svc', b'h3=":443"; ma=86400')])
INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>
DEBUG:httpcore.http11:receive_response_body.complete
DEBUG:httpcore.http11:response_closed.started
DEBUG:httpcore.http11:response_closed.complete
DEBUG:openai._base_client:HTTP Response: POST https://api.openai.com/v1/chat/completions "200 OK" Headers({'date': 'Mon, 08 Jul 2024 07:25:31 GMT', 'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'openai-organization': 'machinekingsmedia', 'openai-processing-ms': '193', 'openai-version': '2020-10-01', 'strict-transport-security': 'max-age=31536000; includeSubDomains', 'x-ratelimit-limit-requests': '10000', 'x-ratelimit-limit-tokens': '1000000', 'x-ratelimit-remaining-requests': '9999', 'x-ratelimit-remaining-tokens': '998973', 'x-ratelimit-reset-requests': '6ms', 'x-ratelimit-reset-tokens': '61ms', 'x-request-id': 'req_d7989e82419386b169be26cef1a42c1c', 'cf-cache-status': 'DYNAMIC', 'server': 'cloudflare', 'cf-ray': '89fe573efb210594-IAD', 'content-encoding': 'br', 'alt-svc': 'h3=":443"; ma=86400'})
DEBUG:openai._base_client:request_id: req_d7989e82419386b169be26cef1a42c1c
DEBUG:openai._base_client:Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'json_data': {'messages': [{'content': "Summarize the following chapter in one sentence: If you if if I can conceptualize an idea of how an app could work like you could really like tell it to like give you a methodology that software developers use in order to come up with with a process for actually getting this getting the app getting the first iteration of the app and the idea of like if you build it and give it away like if you just build it make it open source it's an open source app that you can use on Windows that you give the code away bro if you put that on your resume that carry way more weight bro then then you working for General Electric I'm gonna give it open AI document I'm recording this part now just so I can map it out of my brain I'm gonna give it yeah yeah I will I will I will", 'role': 'user'}], 'model': 'gpt-3.5-turbo-16k', 'max_tokens': 1000, 'n': 1, 'stream': False, 'temperature': 0.7}}
DEBUG:openai._base_client:Sending HTTP Request: POST https://api.openai.com/v1/chat/completions
DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>
DEBUG:httpcore.http11:send_request_headers.complete
DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>
DEBUG:httpcore.http11:send_request_body.complete
DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>
DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Mon, 08 Jul 2024 07:25:32 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'openai-organization', b'machinekingsmedia'), (b'openai-processing-ms', b'365'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains'), (b'x-ratelimit-limit-requests', b'10000'), (b'x-ratelimit-limit-tokens', b'1000000'), (b'x-ratelimit-remaining-requests', b'9999'), (b'x-ratelimit-remaining-tokens', b'998806'), (b'x-ratelimit-reset-requests', b'6ms'), (b'x-ratelimit-reset-tokens', b'71ms'), (b'x-request-id', b'req_102f26b14a2b4b7dad4d1d1d0b72188a'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'89fe57426d240594-IAD'), (b'Content-Encoding', b'br'), (b'alt-svc', b'h3=":443"; ma=86400')])
INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>
DEBUG:httpcore.http11:receive_response_body.complete
DEBUG:httpcore.http11:response_closed.started
DEBUG:httpcore.http11:response_closed.complete
DEBUG:openai._base_client:HTTP Response: POST https://api.openai.com/v1/chat/completions "200 OK" Headers({'date': 'Mon, 08 Jul 2024 07:25:32 GMT', 'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'openai-organization': 'machinekingsmedia', 'openai-processing-ms': '365', 'openai-version': '2020-10-01', 'strict-transport-security': 'max-age=31536000; includeSubDomains', 'x-ratelimit-limit-requests': '10000', 'x-ratelimit-limit-tokens': '1000000', 'x-ratelimit-remaining-requests': '9999', 'x-ratelimit-remaining-tokens': '998806', 'x-ratelimit-reset-requests': '6ms', 'x-ratelimit-reset-tokens': '71ms', 'x-request-id': 'req_102f26b14a2b4b7dad4d1d1d0b72188a', 'cf-cache-status': 'DYNAMIC', 'server': 'cloudflare', 'cf-ray': '89fe57426d240594-IAD', 'content-encoding': 'br', 'alt-svc': 'h3=":443"; ma=86400'})
DEBUG:openai._base_client:request_id: req_102f26b14a2b4b7dad4d1d1d0b72188a
DEBUG:openai._base_client:Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'json_data': {'messages': [{'content': "Provide a brief summary of the following transcript: File: 240319_1332.mp3\nRecording Date: 2024-03-19\nDuration: 00:01:01.24\nTranscription Date: 2024-07-07 06:19:13\n\n1\n00:00:00,000 --> 00:00:06,720\nIf you if if I can conceptualize an idea of how an app could work like you could\n\n2\n00:00:06,720 --> 00:00:13,200\nreally like tell it to like give you a methodology that software developers use\n\n3\n00:00:13,200 --> 00:00:19,000\nin order to come up with with a process for actually getting this getting the\n\n4\n00:00:19,000 --> 00:00:26,559\napp getting the first iteration of the app and the idea of like if you build it\n\n5\n00:00:26,559 --> 00:00:30,320\nand give it away like if you just build it make it open source it's an open\n\n6\n00:00:30,320 --> 00:00:36,080\nsource app that you can use on Windows that you give the code away bro if you\n\n7\n00:00:36,080 --> 00:00:42,680\nput that on your resume that carry way more weight bro then then you working\n\n8\n00:00:42,680 --> 00:00:52,759\nfor General Electric I'm gonna give it open AI document I'm recording this part\n\n9\n00:00:52,759 --> 00:00:59,000\nnow just so I can map it out of my brain I'm gonna give it yeah yeah I will I will\n\n10\n00:00:59,000 --> 00:01:01,240\nI will\n\n\n", 'role': 'user'}], 'model': 'gpt-3.5-turbo-16k', 'max_tokens': 1000, 'n': 1, 'stream': False, 'temperature': 0.7}}
DEBUG:openai._base_client:Sending HTTP Request: POST https://api.openai.com/v1/chat/completions
DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>
DEBUG:httpcore.http11:send_request_headers.complete
DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>
DEBUG:httpcore.http11:send_request_body.complete
DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>
DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Mon, 08 Jul 2024 07:25:33 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'openai-organization', b'machinekingsmedia'), (b'openai-processing-ms', b'1119'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains'), (b'x-ratelimit-limit-requests', b'10000'), (b'x-ratelimit-limit-tokens', b'1000000'), (b'x-ratelimit-remaining-requests', b'9999'), (b'x-ratelimit-remaining-tokens', b'998694'), (b'x-ratelimit-reset-requests', b'6ms'), (b'x-ratelimit-reset-tokens', b'78ms'), (b'x-request-id', b'req_c3f0748827b25ea44298bec0630760ca'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'89fe57462f8e0594-IAD'), (b'Content-Encoding', b'br'), (b'alt-svc', b'h3=":443"; ma=86400')])
INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>
DEBUG:httpcore.http11:receive_response_body.complete
DEBUG:httpcore.http11:response_closed.started
DEBUG:httpcore.http11:response_closed.complete
DEBUG:openai._base_client:HTTP Response: POST https://api.openai.com/v1/chat/completions "200 OK" Headers({'date': 'Mon, 08 Jul 2024 07:25:33 GMT', 'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'openai-organization': 'machinekingsmedia', 'openai-processing-ms': '1119', 'openai-version': '2020-10-01', 'strict-transport-security': 'max-age=31536000; includeSubDomains', 'x-ratelimit-limit-requests': '10000', 'x-ratelimit-limit-tokens': '1000000', 'x-ratelimit-remaining-requests': '9999', 'x-ratelimit-remaining-tokens': '998694', 'x-ratelimit-reset-requests': '6ms', 'x-ratelimit-reset-tokens': '78ms', 'x-request-id': 'req_c3f0748827b25ea44298bec0630760ca', 'cf-cache-status': 'DYNAMIC', 'server': 'cloudflare', 'cf-ray': '89fe57462f8e0594-IAD', 'content-encoding': 'br', 'alt-svc': 'h3=":443"; ma=86400'})
DEBUG:openai._base_client:request_id: req_c3f0748827b25ea44298bec0630760ca
DEBUG:openai._base_client:Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'json_data': {'messages': [{'content': "Generate a short, descriptive title for the following chapter content: If you if if I can conceptualize an idea of how an app could work like you could really like tell it to like give you a methodology that software developers use in order to come up with with a process for actually getting this getting the app getting the first iteration of the app and the idea of like if you build it and give it away like if you just build it make it open source it's an open source app that you can use on Windows that you give the code away bro if you put that on your resume that carry way more weight bro then then you working for General Electric I'm gonna give it open AI document I'm recording this part now just so I can map it out of my brain I'm gonna give it yeah yeah I will I will I will", 'role': 'user'}], 'model': 'gpt-3.5-turbo-16k', 'max_tokens': 1000, 'n': 1, 'stream': False, 'temperature': 0.7}}
DEBUG:openai._base_client:Sending HTTP Request: POST https://api.openai.com/v1/chat/completions
DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>
DEBUG:httpcore.http11:send_request_headers.complete
DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>
DEBUG:httpcore.http11:send_request_body.complete
DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>
DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Mon, 08 Jul 2024 07:25:34 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'openai-organization', b'machinekingsmedia'), (b'openai-processing-ms', b'269'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains'), (b'x-ratelimit-limit-requests', b'10000'), (b'x-ratelimit-limit-tokens', b'1000000'), (b'x-ratelimit-remaining-requests', b'9999'), (b'x-ratelimit-remaining-tokens', b'998801'), (b'x-ratelimit-reset-requests', b'6ms'), (b'x-ratelimit-reset-tokens', b'71ms'), (b'x-request-id', b'req_2b1a93d7a2e19288d8b3a52000a69f1f'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'89fe574f4c930594-IAD'), (b'Content-Encoding', b'br'), (b'alt-svc', b'h3=":443"; ma=86400')])
INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>
DEBUG:httpcore.http11:receive_response_body.complete
DEBUG:httpcore.http11:response_closed.started
DEBUG:httpcore.http11:response_closed.complete
DEBUG:openai._base_client:HTTP Response: POST https://api.openai.com/v1/chat/completions "200 OK" Headers({'date': 'Mon, 08 Jul 2024 07:25:34 GMT', 'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'openai-organization': 'machinekingsmedia', 'openai-processing-ms': '269', 'openai-version': '2020-10-01', 'strict-transport-security': 'max-age=31536000; includeSubDomains', 'x-ratelimit-limit-requests': '10000', 'x-ratelimit-limit-tokens': '1000000', 'x-ratelimit-remaining-requests': '9999', 'x-ratelimit-remaining-tokens': '998801', 'x-ratelimit-reset-requests': '6ms', 'x-ratelimit-reset-tokens': '71ms', 'x-request-id': 'req_2b1a93d7a2e19288d8b3a52000a69f1f', 'cf-cache-status': 'DYNAMIC', 'server': 'cloudflare', 'cf-ray': '89fe574f4c930594-IAD', 'content-encoding': 'br', 'alt-svc': 'h3=":443"; ma=86400'})
DEBUG:openai._base_client:request_id: req_2b1a93d7a2e19288d8b3a52000a69f1f
DEBUG:openai._base_client:Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'json_data': {'messages': [{'content': 'Summarize the following chapter in one sentence: \u8fd9\u662f\u6811\u679c\u67d1\u6746,\u672c\u4e16\u7d00rent\u7f51\u53cb\u9001\u7ed9\u6211\u7684 tremendous artwork \u6811\u679c\u67d1\u6746 \u6ca1\u6709\u756a\ufffd Prince', 'role': 'user'}], 'model': 'gpt-3.5-turbo-16k', 'max_tokens': 1000, 'n': 1, 'stream': False, 'temperature': 0.7}}
DEBUG:openai._base_client:Sending HTTP Request: POST https://api.openai.com/v1/chat/completions
DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>
DEBUG:httpcore.http11:send_request_headers.complete
DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>
DEBUG:httpcore.http11:send_request_body.complete
DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>
DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Mon, 08 Jul 2024 07:25:35 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'openai-organization', b'machinekingsmedia'), (b'openai-processing-ms', b'367'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains'), (b'x-ratelimit-limit-requests', b'10000'), (b'x-ratelimit-limit-tokens', b'1000000'), (b'x-ratelimit-remaining-requests', b'9999'), (b'x-ratelimit-remaining-tokens', b'998960'), (b'x-ratelimit-reset-requests', b'6ms'), (b'x-ratelimit-reset-tokens', b'62ms'), (b'x-request-id', b'req_f6ccdecb7c6569b2e8871dc5d0ac4f31'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'89fe5751adf10594-IAD'), (b'Content-Encoding', b'br'), (b'alt-svc', b'h3=":443"; ma=86400')])
INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>
DEBUG:httpcore.http11:receive_response_body.complete
DEBUG:httpcore.http11:response_closed.started
DEBUG:httpcore.http11:response_closed.complete
DEBUG:openai._base_client:HTTP Response: POST https://api.openai.com/v1/chat/completions "200 OK" Headers({'date': 'Mon, 08 Jul 2024 07:25:35 GMT', 'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'openai-organization': 'machinekingsmedia', 'openai-processing-ms': '367', 'openai-version': '2020-10-01', 'strict-transport-security': 'max-age=31536000; includeSubDomains', 'x-ratelimit-limit-requests': '10000', 'x-ratelimit-limit-tokens': '1000000', 'x-ratelimit-remaining-requests': '9999', 'x-ratelimit-remaining-tokens': '998960', 'x-ratelimit-reset-requests': '6ms', 'x-ratelimit-reset-tokens': '62ms', 'x-request-id': 'req_f6ccdecb7c6569b2e8871dc5d0ac4f31', 'cf-cache-status': 'DYNAMIC', 'server': 'cloudflare', 'cf-ray': '89fe5751adf10594-IAD', 'content-encoding': 'br', 'alt-svc': 'h3=":443"; ma=86400'})
DEBUG:openai._base_client:request_id: req_f6ccdecb7c6569b2e8871dc5d0ac4f31
DEBUG:openai._base_client:Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'json_data': {'messages': [{'content': 'Provide a brief summary of the following transcript: File: 240319_1829.mp3\nRecording Date: 2024-03-19\nDuration: 00:00:16.90\nTranscription Date: 2024-07-07 06:19:21\n\n1\n00:00:00,000 --> 00:00:10,100\n\u8fd9\u662f\u6811\u679c\u67d1\u6746,\u672c\u4e16\u7d00rent\u7f51\u53cb\u9001\u7ed9\u6211\u7684 tremendous artwork\n\n2\n00:00:10,100 --> 00:00:14,900\n\u6811\u679c\u67d1\u6746\n\n3\n00:00:14,900 --> 00:00:16,900\n\u6ca1\u6709\u756a\ufffd Prince\n\n\n', 'role': 'user'}], 'model': 'gpt-3.5-turbo-16k', 'max_tokens': 1000, 'n': 1, 'stream': False, 'temperature': 0.7}}
DEBUG:openai._base_client:Sending HTTP Request: POST https://api.openai.com/v1/chat/completions
DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>
DEBUG:httpcore.http11:send_request_headers.complete
DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>
DEBUG:httpcore.http11:send_request_body.complete
DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>
DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Mon, 08 Jul 2024 07:25:36 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'openai-organization', b'machinekingsmedia'), (b'openai-processing-ms', b'859'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains'), (b'x-ratelimit-limit-requests', b'10000'), (b'x-ratelimit-limit-tokens', b'1000000'), (b'x-ratelimit-remaining-requests', b'9999'), (b'x-ratelimit-remaining-tokens', b'998906'), (b'x-ratelimit-reset-requests', b'6ms'), (b'x-ratelimit-reset-tokens', b'65ms'), (b'x-request-id', b'req_08677d38870907c783f0b1cb9a4aac13'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'89fe57560fea0594-IAD'), (b'Content-Encoding', b'br'), (b'alt-svc', b'h3=":443"; ma=86400')])
INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>
DEBUG:httpcore.http11:receive_response_body.complete
DEBUG:httpcore.http11:response_closed.started
DEBUG:httpcore.http11:response_closed.complete
DEBUG:openai._base_client:HTTP Response: POST https://api.openai.com/v1/chat/completions "200 OK" Headers({'date': 'Mon, 08 Jul 2024 07:25:36 GMT', 'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'openai-organization': 'machinekingsmedia', 'openai-processing-ms': '859', 'openai-version': '2020-10-01', 'strict-transport-security': 'max-age=31536000; includeSubDomains', 'x-ratelimit-limit-requests': '10000', 'x-ratelimit-limit-tokens': '1000000', 'x-ratelimit-remaining-requests': '9999', 'x-ratelimit-remaining-tokens': '998906', 'x-ratelimit-reset-requests': '6ms', 'x-ratelimit-reset-tokens': '65ms', 'x-request-id': 'req_08677d38870907c783f0b1cb9a4aac13', 'cf-cache-status': 'DYNAMIC', 'server': 'cloudflare', 'cf-ray': '89fe57560fea0594-IAD', 'content-encoding': 'br', 'alt-svc': 'h3=":443"; ma=86400'})
DEBUG:openai._base_client:request_id: req_08677d38870907c783f0b1cb9a4aac13
DEBUG:openai._base_client:Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'json_data': {'messages': [{'content': 'Generate a short, descriptive title for the following chapter content: \u8fd9\u662f\u6811\u679c\u67d1\u6746,\u672c\u4e16\u7d00rent\u7f51\u53cb\u9001\u7ed9\u6211\u7684 tremendous artwork \u6811\u679c\u67d1\u6746 \u6ca1\u6709\u756a\ufffd Prince', 'role': 'user'}], 'model': 'gpt-3.5-turbo-16k', 'max_tokens': 1000, 'n': 1, 'stream': False, 'temperature': 0.7}}
DEBUG:openai._base_client:Sending HTTP Request: POST https://api.openai.com/v1/chat/completions
DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>
DEBUG:httpcore.http11:send_request_headers.complete
DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>
DEBUG:httpcore.http11:send_request_body.complete
DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>
DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Mon, 08 Jul 2024 07:25:36 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'openai-organization', b'machinekingsmedia'), (b'openai-processing-ms', b'538'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains'), (b'x-ratelimit-limit-requests', b'10000'), (b'x-ratelimit-limit-tokens', b'1000000'), (b'x-ratelimit-remaining-requests', b'9999'), (b'x-ratelimit-remaining-tokens', b'998955'), (b'x-ratelimit-reset-requests', b'6ms'), (b'x-ratelimit-reset-tokens', b'62ms'), (b'x-request-id', b'req_5d1c45182686ce49e311dd6408a51f89'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'89fe575ccbcc0594-IAD'), (b'Content-Encoding', b'br'), (b'alt-svc', b'h3=":443"; ma=86400')])
INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>
DEBUG:httpcore.http11:receive_response_body.complete
DEBUG:httpcore.http11:response_closed.started
DEBUG:httpcore.http11:response_closed.complete
DEBUG:openai._base_client:HTTP Response: POST https://api.openai.com/v1/chat/completions "200 OK" Headers({'date': 'Mon, 08 Jul 2024 07:25:36 GMT', 'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'openai-organization': 'machinekingsmedia', 'openai-processing-ms': '538', 'openai-version': '2020-10-01', 'strict-transport-security': 'max-age=31536000; includeSubDomains', 'x-ratelimit-limit-requests': '10000', 'x-ratelimit-limit-tokens': '1000000', 'x-ratelimit-remaining-requests': '9999', 'x-ratelimit-remaining-tokens': '998955', 'x-ratelimit-reset-requests': '6ms', 'x-ratelimit-reset-tokens': '62ms', 'x-request-id': 'req_5d1c45182686ce49e311dd6408a51f89', 'cf-cache-status': 'DYNAMIC', 'server': 'cloudflare', 'cf-ray': '89fe575ccbcc0594-IAD', 'content-encoding': 'br', 'alt-svc': 'h3=":443"; ma=86400'})
DEBUG:openai._base_client:request_id: req_5d1c45182686ce49e311dd6408a51f89
ERROR:root:Error processing directory: 'text'
Traceback (most recent call last):
  File "D:\github\chappymedium\chappie.py", line 333, in process_directory
    results = self.chappie_processor.process_directory(directory)
  File "D:\github\chappymedium\chappie_processor.py", line 108, in process_directory
    results[filename] = self.process_srt(srt_content)
  File "D:\github\chappymedium\chappie_processor.py", line 21, in process_srt
    chapters = self._generate_chapters(entries)
  File "D:\github\chappymedium\chappie_processor.py", line 51, in _generate_chapters
    current_chapter_text += entry['text'] + " "
KeyError: 'text'
